{
  "best_metric": 0.24910381436347961,
  "best_model_checkpoint": "/scratch/lmbanr001/SSM_100M/checkpoint-250000",
  "epoch": 2.4313320269002037,
  "eval_steps": 10000,
  "global_step": 250000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 9.725308656983501e-06,
      "grad_norm": 68.03314971923828,
      "learning_rate": 1e-08,
      "loss": 7.8364,
      "step": 1
    },
    {
      "epoch": 0.0009725308656983501,
      "grad_norm": 11.58285903930664,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 6.0477,
      "step": 100
    },
    {
      "epoch": 0.0019450617313967002,
      "grad_norm": 2.1636996269226074,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.275,
      "step": 200
    },
    {
      "epoch": 0.00291759259709505,
      "grad_norm": 1.8717432022094727,
      "learning_rate": 3e-06,
      "loss": 1.1547,
      "step": 300
    },
    {
      "epoch": 0.0038901234627934005,
      "grad_norm": 1.837195634841919,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.9176,
      "step": 400
    },
    {
      "epoch": 0.00486265432849175,
      "grad_norm": 2.087167739868164,
      "learning_rate": 5e-06,
      "loss": 0.8235,
      "step": 500
    },
    {
      "epoch": 0.0058351851941901,
      "grad_norm": 2.287243604660034,
      "learning_rate": 6e-06,
      "loss": 0.762,
      "step": 600
    },
    {
      "epoch": 0.006807716059888451,
      "grad_norm": 3.3494553565979004,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.7233,
      "step": 700
    },
    {
      "epoch": 0.007780246925586801,
      "grad_norm": 2.798947811126709,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.684,
      "step": 800
    },
    {
      "epoch": 0.008752777791285152,
      "grad_norm": 6.114620208740234,
      "learning_rate": 9e-06,
      "loss": 0.6493,
      "step": 900
    },
    {
      "epoch": 0.0097253086569835,
      "grad_norm": 5.02738618850708,
      "learning_rate": 1e-05,
      "loss": 0.63,
      "step": 1000
    },
    {
      "epoch": 0.010697839522681852,
      "grad_norm": 4.239689826965332,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.6066,
      "step": 1100
    },
    {
      "epoch": 0.0116703703883802,
      "grad_norm": 5.8938398361206055,
      "learning_rate": 1.2e-05,
      "loss": 0.5864,
      "step": 1200
    },
    {
      "epoch": 0.012642901254078551,
      "grad_norm": 5.147154331207275,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.5689,
      "step": 1300
    },
    {
      "epoch": 0.013615432119776902,
      "grad_norm": 2.864254951477051,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.5575,
      "step": 1400
    },
    {
      "epoch": 0.014587962985475251,
      "grad_norm": 6.69548225402832,
      "learning_rate": 1.5e-05,
      "loss": 0.5353,
      "step": 1500
    },
    {
      "epoch": 0.015560493851173602,
      "grad_norm": 5.015992164611816,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.525,
      "step": 1600
    },
    {
      "epoch": 0.01653302471687195,
      "grad_norm": 3.4774255752563477,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.511,
      "step": 1700
    },
    {
      "epoch": 0.017505555582570304,
      "grad_norm": 4.320530414581299,
      "learning_rate": 1.8e-05,
      "loss": 0.4992,
      "step": 1800
    },
    {
      "epoch": 0.018478086448268653,
      "grad_norm": 3.4558730125427246,
      "learning_rate": 1.9e-05,
      "loss": 0.4887,
      "step": 1900
    },
    {
      "epoch": 0.019450617313967,
      "grad_norm": 3.7415425777435303,
      "learning_rate": 2e-05,
      "loss": 0.4788,
      "step": 2000
    },
    {
      "epoch": 0.02042314817966535,
      "grad_norm": 5.2520623207092285,
      "learning_rate": 2.1e-05,
      "loss": 0.4718,
      "step": 2100
    },
    {
      "epoch": 0.021395679045363703,
      "grad_norm": 3.187537431716919,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.4651,
      "step": 2200
    },
    {
      "epoch": 0.022368209911062052,
      "grad_norm": 5.1164469718933105,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.4565,
      "step": 2300
    },
    {
      "epoch": 0.0233407407767604,
      "grad_norm": 3.5351574420928955,
      "learning_rate": 2.4e-05,
      "loss": 0.4463,
      "step": 2400
    },
    {
      "epoch": 0.024313271642458754,
      "grad_norm": 2.80237078666687,
      "learning_rate": 2.5e-05,
      "loss": 0.4444,
      "step": 2500
    },
    {
      "epoch": 0.025285802508157103,
      "grad_norm": 2.4107108116149902,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4387,
      "step": 2600
    },
    {
      "epoch": 0.026258333373855452,
      "grad_norm": 3.5691397190093994,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.4358,
      "step": 2700
    },
    {
      "epoch": 0.027230864239553804,
      "grad_norm": 2.8349497318267822,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.4291,
      "step": 2800
    },
    {
      "epoch": 0.028203395105252153,
      "grad_norm": 2.540428638458252,
      "learning_rate": 2.9e-05,
      "loss": 0.4229,
      "step": 2900
    },
    {
      "epoch": 0.029175925970950502,
      "grad_norm": 1.8779996633529663,
      "learning_rate": 3e-05,
      "loss": 0.4214,
      "step": 3000
    },
    {
      "epoch": 0.03014845683664885,
      "grad_norm": 2.3209946155548096,
      "learning_rate": 3.1e-05,
      "loss": 0.4158,
      "step": 3100
    },
    {
      "epoch": 0.031120987702347204,
      "grad_norm": 1.9220993518829346,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.4131,
      "step": 3200
    },
    {
      "epoch": 0.032093518568045556,
      "grad_norm": 1.8268182277679443,
      "learning_rate": 3.3e-05,
      "loss": 0.409,
      "step": 3300
    },
    {
      "epoch": 0.0330660494337439,
      "grad_norm": 1.9847965240478516,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.4055,
      "step": 3400
    },
    {
      "epoch": 0.034038580299442255,
      "grad_norm": 1.7421098947525024,
      "learning_rate": 3.5e-05,
      "loss": 0.4076,
      "step": 3500
    },
    {
      "epoch": 0.03501111116514061,
      "grad_norm": 1.6128088235855103,
      "learning_rate": 3.6e-05,
      "loss": 0.4005,
      "step": 3600
    },
    {
      "epoch": 0.03598364203083895,
      "grad_norm": 1.7787764072418213,
      "learning_rate": 3.7e-05,
      "loss": 0.4012,
      "step": 3700
    },
    {
      "epoch": 0.036956172896537305,
      "grad_norm": 1.5475764274597168,
      "learning_rate": 3.8e-05,
      "loss": 0.3971,
      "step": 3800
    },
    {
      "epoch": 0.03792870376223565,
      "grad_norm": 1.6840656995773315,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.3936,
      "step": 3900
    },
    {
      "epoch": 0.038901234627934,
      "grad_norm": 2.155431032180786,
      "learning_rate": 4e-05,
      "loss": 0.39,
      "step": 4000
    },
    {
      "epoch": 0.039873765493632356,
      "grad_norm": 1.6827366352081299,
      "learning_rate": 4.1e-05,
      "loss": 0.3912,
      "step": 4100
    },
    {
      "epoch": 0.0408462963593307,
      "grad_norm": 1.2431269884109497,
      "learning_rate": 4.2e-05,
      "loss": 0.3896,
      "step": 4200
    },
    {
      "epoch": 0.041818827225029054,
      "grad_norm": 1.409041404724121,
      "learning_rate": 4.3e-05,
      "loss": 0.3861,
      "step": 4300
    },
    {
      "epoch": 0.042791358090727406,
      "grad_norm": 1.2467234134674072,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.3857,
      "step": 4400
    },
    {
      "epoch": 0.04376388895642575,
      "grad_norm": 1.6756150722503662,
      "learning_rate": 4.5e-05,
      "loss": 0.3849,
      "step": 4500
    },
    {
      "epoch": 0.044736419822124104,
      "grad_norm": 1.1824986934661865,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.3823,
      "step": 4600
    },
    {
      "epoch": 0.04570895068782246,
      "grad_norm": 1.306132197380066,
      "learning_rate": 4.7e-05,
      "loss": 0.3784,
      "step": 4700
    },
    {
      "epoch": 0.0466814815535208,
      "grad_norm": 1.1188077926635742,
      "learning_rate": 4.8e-05,
      "loss": 0.3749,
      "step": 4800
    },
    {
      "epoch": 0.047654012419219155,
      "grad_norm": 1.0603104829788208,
      "learning_rate": 4.9e-05,
      "loss": 0.3756,
      "step": 4900
    },
    {
      "epoch": 0.04862654328491751,
      "grad_norm": 1.2965503931045532,
      "learning_rate": 5e-05,
      "loss": 0.375,
      "step": 5000
    },
    {
      "epoch": 0.04959907415061585,
      "grad_norm": 1.1068416833877563,
      "learning_rate": 5.1000000000000006e-05,
      "loss": 0.3743,
      "step": 5100
    },
    {
      "epoch": 0.050571605016314206,
      "grad_norm": 1.1797109842300415,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.3747,
      "step": 5200
    },
    {
      "epoch": 0.05154413588201256,
      "grad_norm": 1.264952540397644,
      "learning_rate": 5.300000000000001e-05,
      "loss": 0.3719,
      "step": 5300
    },
    {
      "epoch": 0.052516666747710904,
      "grad_norm": 0.9221693277359009,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.3707,
      "step": 5400
    },
    {
      "epoch": 0.053489197613409256,
      "grad_norm": 0.9495670795440674,
      "learning_rate": 5.500000000000001e-05,
      "loss": 0.3689,
      "step": 5500
    },
    {
      "epoch": 0.05446172847910761,
      "grad_norm": 1.1507681608200073,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.3652,
      "step": 5600
    },
    {
      "epoch": 0.055434259344805954,
      "grad_norm": 0.9130218625068665,
      "learning_rate": 5.6999999999999996e-05,
      "loss": 0.3661,
      "step": 5700
    },
    {
      "epoch": 0.05640679021050431,
      "grad_norm": 0.9684855937957764,
      "learning_rate": 5.8e-05,
      "loss": 0.3659,
      "step": 5800
    },
    {
      "epoch": 0.05737932107620266,
      "grad_norm": 0.8776360154151917,
      "learning_rate": 5.9e-05,
      "loss": 0.3647,
      "step": 5900
    },
    {
      "epoch": 0.058351851941901005,
      "grad_norm": 0.8977740406990051,
      "learning_rate": 6e-05,
      "loss": 0.3628,
      "step": 6000
    },
    {
      "epoch": 0.05932438280759936,
      "grad_norm": 0.8111858367919922,
      "learning_rate": 6.1e-05,
      "loss": 0.3609,
      "step": 6100
    },
    {
      "epoch": 0.0602969136732977,
      "grad_norm": 0.8679625391960144,
      "learning_rate": 6.2e-05,
      "loss": 0.3634,
      "step": 6200
    },
    {
      "epoch": 0.061269444538996055,
      "grad_norm": 0.8741799592971802,
      "learning_rate": 6.3e-05,
      "loss": 0.3595,
      "step": 6300
    },
    {
      "epoch": 0.06224197540469441,
      "grad_norm": 0.7691898941993713,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.3579,
      "step": 6400
    },
    {
      "epoch": 0.06321450627039275,
      "grad_norm": 0.7829748392105103,
      "learning_rate": 6.500000000000001e-05,
      "loss": 0.3576,
      "step": 6500
    },
    {
      "epoch": 0.06418703713609111,
      "grad_norm": 0.8200333118438721,
      "learning_rate": 6.6e-05,
      "loss": 0.3557,
      "step": 6600
    },
    {
      "epoch": 0.06515956800178946,
      "grad_norm": 0.7424418926239014,
      "learning_rate": 6.7e-05,
      "loss": 0.3538,
      "step": 6700
    },
    {
      "epoch": 0.0661320988674878,
      "grad_norm": 0.7637820243835449,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.3523,
      "step": 6800
    },
    {
      "epoch": 0.06710462973318616,
      "grad_norm": 0.7950167059898376,
      "learning_rate": 6.9e-05,
      "loss": 0.354,
      "step": 6900
    },
    {
      "epoch": 0.06807716059888451,
      "grad_norm": 0.7919567227363586,
      "learning_rate": 7e-05,
      "loss": 0.3537,
      "step": 7000
    },
    {
      "epoch": 0.06904969146458285,
      "grad_norm": 0.7492082118988037,
      "learning_rate": 7.1e-05,
      "loss": 0.3528,
      "step": 7100
    },
    {
      "epoch": 0.07002222233028121,
      "grad_norm": 0.8911268711090088,
      "learning_rate": 7.2e-05,
      "loss": 0.3507,
      "step": 7200
    },
    {
      "epoch": 0.07099475319597956,
      "grad_norm": 0.8183059096336365,
      "learning_rate": 7.3e-05,
      "loss": 0.3502,
      "step": 7300
    },
    {
      "epoch": 0.0719672840616779,
      "grad_norm": 0.7316902875900269,
      "learning_rate": 7.4e-05,
      "loss": 0.348,
      "step": 7400
    },
    {
      "epoch": 0.07293981492737625,
      "grad_norm": 0.9733338952064514,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.3499,
      "step": 7500
    },
    {
      "epoch": 0.07391234579307461,
      "grad_norm": 0.741125762462616,
      "learning_rate": 7.6e-05,
      "loss": 0.3486,
      "step": 7600
    },
    {
      "epoch": 0.07488487665877296,
      "grad_norm": 0.7178155183792114,
      "learning_rate": 7.7e-05,
      "loss": 0.3449,
      "step": 7700
    },
    {
      "epoch": 0.0758574075244713,
      "grad_norm": 0.8097409605979919,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.3454,
      "step": 7800
    },
    {
      "epoch": 0.07682993839016966,
      "grad_norm": 0.6652030348777771,
      "learning_rate": 7.900000000000001e-05,
      "loss": 0.3478,
      "step": 7900
    },
    {
      "epoch": 0.077802469255868,
      "grad_norm": 0.6682866215705872,
      "learning_rate": 8e-05,
      "loss": 0.3498,
      "step": 8000
    },
    {
      "epoch": 0.07877500012156635,
      "grad_norm": 0.7992380261421204,
      "learning_rate": 8.1e-05,
      "loss": 0.3484,
      "step": 8100
    },
    {
      "epoch": 0.07974753098726471,
      "grad_norm": 0.5744084119796753,
      "learning_rate": 8.2e-05,
      "loss": 0.3462,
      "step": 8200
    },
    {
      "epoch": 0.08072006185296306,
      "grad_norm": 0.6582862734794617,
      "learning_rate": 8.3e-05,
      "loss": 0.3475,
      "step": 8300
    },
    {
      "epoch": 0.0816925927186614,
      "grad_norm": 0.7523730993270874,
      "learning_rate": 8.4e-05,
      "loss": 0.3464,
      "step": 8400
    },
    {
      "epoch": 0.08266512358435976,
      "grad_norm": 0.6626291871070862,
      "learning_rate": 8.5e-05,
      "loss": 0.3446,
      "step": 8500
    },
    {
      "epoch": 0.08363765445005811,
      "grad_norm": 0.6636067032814026,
      "learning_rate": 8.6e-05,
      "loss": 0.3426,
      "step": 8600
    },
    {
      "epoch": 0.08461018531575645,
      "grad_norm": 0.688332200050354,
      "learning_rate": 8.7e-05,
      "loss": 0.343,
      "step": 8700
    },
    {
      "epoch": 0.08558271618145481,
      "grad_norm": 0.6067557334899902,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.3424,
      "step": 8800
    },
    {
      "epoch": 0.08655524704715316,
      "grad_norm": 0.5574405193328857,
      "learning_rate": 8.900000000000001e-05,
      "loss": 0.343,
      "step": 8900
    },
    {
      "epoch": 0.0875277779128515,
      "grad_norm": 0.7428500652313232,
      "learning_rate": 9e-05,
      "loss": 0.3405,
      "step": 9000
    },
    {
      "epoch": 0.08850030877854986,
      "grad_norm": 0.6118855476379395,
      "learning_rate": 9.1e-05,
      "loss": 0.3422,
      "step": 9100
    },
    {
      "epoch": 0.08947283964424821,
      "grad_norm": 0.5294031500816345,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.3402,
      "step": 9200
    },
    {
      "epoch": 0.09044537050994655,
      "grad_norm": 0.6604881286621094,
      "learning_rate": 9.300000000000001e-05,
      "loss": 0.34,
      "step": 9300
    },
    {
      "epoch": 0.09141790137564491,
      "grad_norm": 0.5738776922225952,
      "learning_rate": 9.4e-05,
      "loss": 0.3416,
      "step": 9400
    },
    {
      "epoch": 0.09239043224134326,
      "grad_norm": 0.5082493424415588,
      "learning_rate": 9.5e-05,
      "loss": 0.3394,
      "step": 9500
    },
    {
      "epoch": 0.0933629631070416,
      "grad_norm": 0.5631380081176758,
      "learning_rate": 9.6e-05,
      "loss": 0.3387,
      "step": 9600
    },
    {
      "epoch": 0.09433549397273996,
      "grad_norm": 0.533414363861084,
      "learning_rate": 9.7e-05,
      "loss": 0.3387,
      "step": 9700
    },
    {
      "epoch": 0.09530802483843831,
      "grad_norm": 0.49132299423217773,
      "learning_rate": 9.8e-05,
      "loss": 0.338,
      "step": 9800
    },
    {
      "epoch": 0.09628055570413666,
      "grad_norm": 0.5433980226516724,
      "learning_rate": 9.900000000000001e-05,
      "loss": 0.339,
      "step": 9900
    },
    {
      "epoch": 0.09725308656983501,
      "grad_norm": 0.5214909315109253,
      "learning_rate": 0.0001,
      "loss": 0.3378,
      "step": 10000
    },
    {
      "epoch": 0.09725308656983501,
      "eval_loss": 0.3373788297176361,
      "eval_runtime": 3227.4296,
      "eval_samples_per_second": 707.916,
      "eval_steps_per_second": 7.079,
      "step": 10000
    },
    {
      "epoch": 0.09822561743553336,
      "grad_norm": 0.4768601059913635,
      "learning_rate": 9.995833333333334e-05,
      "loss": 0.3391,
      "step": 10100
    },
    {
      "epoch": 0.0991981483012317,
      "grad_norm": 0.48882022500038147,
      "learning_rate": 9.991666666666666e-05,
      "loss": 0.3355,
      "step": 10200
    },
    {
      "epoch": 0.10017067916693007,
      "grad_norm": 0.499231219291687,
      "learning_rate": 9.9875e-05,
      "loss": 0.336,
      "step": 10300
    },
    {
      "epoch": 0.10114321003262841,
      "grad_norm": 0.5069817900657654,
      "learning_rate": 9.983333333333334e-05,
      "loss": 0.3346,
      "step": 10400
    },
    {
      "epoch": 0.10211574089832676,
      "grad_norm": 0.4925673007965088,
      "learning_rate": 9.979166666666668e-05,
      "loss": 0.3347,
      "step": 10500
    },
    {
      "epoch": 0.10308827176402512,
      "grad_norm": 0.5062903761863708,
      "learning_rate": 9.975000000000001e-05,
      "loss": 0.3345,
      "step": 10600
    },
    {
      "epoch": 0.10406080262972346,
      "grad_norm": 0.5066821575164795,
      "learning_rate": 9.970833333333334e-05,
      "loss": 0.3339,
      "step": 10700
    },
    {
      "epoch": 0.10503333349542181,
      "grad_norm": 0.49103498458862305,
      "learning_rate": 9.966666666666667e-05,
      "loss": 0.3345,
      "step": 10800
    },
    {
      "epoch": 0.10600586436112017,
      "grad_norm": 0.46138593554496765,
      "learning_rate": 9.9625e-05,
      "loss": 0.3335,
      "step": 10900
    },
    {
      "epoch": 0.10697839522681851,
      "grad_norm": 0.560661256313324,
      "learning_rate": 9.958333333333335e-05,
      "loss": 0.333,
      "step": 11000
    },
    {
      "epoch": 0.10795092609251686,
      "grad_norm": 0.474653035402298,
      "learning_rate": 9.954166666666667e-05,
      "loss": 0.3321,
      "step": 11100
    },
    {
      "epoch": 0.10892345695821522,
      "grad_norm": 0.44538071751594543,
      "learning_rate": 9.95e-05,
      "loss": 0.3302,
      "step": 11200
    },
    {
      "epoch": 0.10989598782391356,
      "grad_norm": 0.5044377446174622,
      "learning_rate": 9.945833333333334e-05,
      "loss": 0.3322,
      "step": 11300
    },
    {
      "epoch": 0.11086851868961191,
      "grad_norm": 0.5168125033378601,
      "learning_rate": 9.941666666666667e-05,
      "loss": 0.3311,
      "step": 11400
    },
    {
      "epoch": 0.11184104955531027,
      "grad_norm": 0.41176551580429077,
      "learning_rate": 9.9375e-05,
      "loss": 0.3302,
      "step": 11500
    },
    {
      "epoch": 0.11281358042100861,
      "grad_norm": 0.4288245737552643,
      "learning_rate": 9.933333333333334e-05,
      "loss": 0.33,
      "step": 11600
    },
    {
      "epoch": 0.11378611128670696,
      "grad_norm": 0.4339621961116791,
      "learning_rate": 9.929166666666668e-05,
      "loss": 0.3284,
      "step": 11700
    },
    {
      "epoch": 0.11475864215240532,
      "grad_norm": 0.44435247778892517,
      "learning_rate": 9.925000000000001e-05,
      "loss": 0.3293,
      "step": 11800
    },
    {
      "epoch": 0.11573117301810366,
      "grad_norm": 0.4760226011276245,
      "learning_rate": 9.920833333333334e-05,
      "loss": 0.328,
      "step": 11900
    },
    {
      "epoch": 0.11670370388380201,
      "grad_norm": 0.494277685880661,
      "learning_rate": 9.916666666666667e-05,
      "loss": 0.3266,
      "step": 12000
    },
    {
      "epoch": 0.11767623474950037,
      "grad_norm": 0.49016234278678894,
      "learning_rate": 9.9125e-05,
      "loss": 0.3304,
      "step": 12100
    },
    {
      "epoch": 0.11864876561519871,
      "grad_norm": 0.4195379912853241,
      "learning_rate": 9.908333333333333e-05,
      "loss": 0.327,
      "step": 12200
    },
    {
      "epoch": 0.11962129648089706,
      "grad_norm": 0.4375922381877899,
      "learning_rate": 9.904166666666667e-05,
      "loss": 0.3265,
      "step": 12300
    },
    {
      "epoch": 0.1205938273465954,
      "grad_norm": 0.42676109075546265,
      "learning_rate": 9.900000000000001e-05,
      "loss": 0.3251,
      "step": 12400
    },
    {
      "epoch": 0.12156635821229377,
      "grad_norm": 0.4223244786262512,
      "learning_rate": 9.895833333333334e-05,
      "loss": 0.326,
      "step": 12500
    },
    {
      "epoch": 0.12253888907799211,
      "grad_norm": 0.4099119007587433,
      "learning_rate": 9.891666666666667e-05,
      "loss": 0.3247,
      "step": 12600
    },
    {
      "epoch": 0.12351141994369046,
      "grad_norm": 0.38311707973480225,
      "learning_rate": 9.8875e-05,
      "loss": 0.3266,
      "step": 12700
    },
    {
      "epoch": 0.12448395080938882,
      "grad_norm": 0.43764209747314453,
      "learning_rate": 9.883333333333333e-05,
      "loss": 0.326,
      "step": 12800
    },
    {
      "epoch": 0.12545648167508716,
      "grad_norm": 0.4255622923374176,
      "learning_rate": 9.879166666666666e-05,
      "loss": 0.3241,
      "step": 12900
    },
    {
      "epoch": 0.1264290125407855,
      "grad_norm": 0.3633895516395569,
      "learning_rate": 9.875000000000002e-05,
      "loss": 0.322,
      "step": 13000
    },
    {
      "epoch": 0.12740154340648385,
      "grad_norm": 0.38803213834762573,
      "learning_rate": 9.870833333333334e-05,
      "loss": 0.3237,
      "step": 13100
    },
    {
      "epoch": 0.12837407427218223,
      "grad_norm": 0.3667122423648834,
      "learning_rate": 9.866666666666668e-05,
      "loss": 0.3232,
      "step": 13200
    },
    {
      "epoch": 0.12934660513788057,
      "grad_norm": 0.37382665276527405,
      "learning_rate": 9.8625e-05,
      "loss": 0.3233,
      "step": 13300
    },
    {
      "epoch": 0.13031913600357892,
      "grad_norm": 0.388312429189682,
      "learning_rate": 9.858333333333334e-05,
      "loss": 0.3238,
      "step": 13400
    },
    {
      "epoch": 0.13129166686927726,
      "grad_norm": 0.4598686397075653,
      "learning_rate": 9.854166666666667e-05,
      "loss": 0.3237,
      "step": 13500
    },
    {
      "epoch": 0.1322641977349756,
      "grad_norm": 0.3959265649318695,
      "learning_rate": 9.850000000000001e-05,
      "loss": 0.3227,
      "step": 13600
    },
    {
      "epoch": 0.13323672860067395,
      "grad_norm": 0.34968647360801697,
      "learning_rate": 9.845833333333335e-05,
      "loss": 0.3222,
      "step": 13700
    },
    {
      "epoch": 0.13420925946637233,
      "grad_norm": 0.37274983525276184,
      "learning_rate": 9.841666666666667e-05,
      "loss": 0.3214,
      "step": 13800
    },
    {
      "epoch": 0.13518179033207067,
      "grad_norm": 0.39001497626304626,
      "learning_rate": 9.8375e-05,
      "loss": 0.3201,
      "step": 13900
    },
    {
      "epoch": 0.13615432119776902,
      "grad_norm": 0.3556927442550659,
      "learning_rate": 9.833333333333333e-05,
      "loss": 0.3228,
      "step": 14000
    },
    {
      "epoch": 0.13712685206346736,
      "grad_norm": 0.4535696506500244,
      "learning_rate": 9.829166666666667e-05,
      "loss": 0.3213,
      "step": 14100
    },
    {
      "epoch": 0.1380993829291657,
      "grad_norm": 0.3719218969345093,
      "learning_rate": 9.825e-05,
      "loss": 0.3184,
      "step": 14200
    },
    {
      "epoch": 0.13907191379486405,
      "grad_norm": 0.35622939467430115,
      "learning_rate": 9.820833333333334e-05,
      "loss": 0.3224,
      "step": 14300
    },
    {
      "epoch": 0.14004444466056243,
      "grad_norm": 0.3671138882637024,
      "learning_rate": 9.816666666666668e-05,
      "loss": 0.3194,
      "step": 14400
    },
    {
      "epoch": 0.14101697552626077,
      "grad_norm": 0.3842686116695404,
      "learning_rate": 9.8125e-05,
      "loss": 0.3194,
      "step": 14500
    },
    {
      "epoch": 0.14198950639195912,
      "grad_norm": 0.39358049631118774,
      "learning_rate": 9.808333333333334e-05,
      "loss": 0.3188,
      "step": 14600
    },
    {
      "epoch": 0.14296203725765746,
      "grad_norm": 0.33885499835014343,
      "learning_rate": 9.804166666666667e-05,
      "loss": 0.3184,
      "step": 14700
    },
    {
      "epoch": 0.1439345681233558,
      "grad_norm": 0.3706458806991577,
      "learning_rate": 9.8e-05,
      "loss": 0.3167,
      "step": 14800
    },
    {
      "epoch": 0.14490709898905416,
      "grad_norm": 0.3891547918319702,
      "learning_rate": 9.795833333333335e-05,
      "loss": 0.3195,
      "step": 14900
    },
    {
      "epoch": 0.1458796298547525,
      "grad_norm": 0.3761049211025238,
      "learning_rate": 9.791666666666667e-05,
      "loss": 0.3195,
      "step": 15000
    },
    {
      "epoch": 0.14685216072045087,
      "grad_norm": 0.33674943447113037,
      "learning_rate": 9.787500000000001e-05,
      "loss": 0.318,
      "step": 15100
    },
    {
      "epoch": 0.14782469158614922,
      "grad_norm": 0.3450821042060852,
      "learning_rate": 9.783333333333334e-05,
      "loss": 0.3159,
      "step": 15200
    },
    {
      "epoch": 0.14879722245184757,
      "grad_norm": 0.36482110619544983,
      "learning_rate": 9.779166666666667e-05,
      "loss": 0.318,
      "step": 15300
    },
    {
      "epoch": 0.1497697533175459,
      "grad_norm": 0.35922110080718994,
      "learning_rate": 9.775e-05,
      "loss": 0.3159,
      "step": 15400
    },
    {
      "epoch": 0.15074228418324426,
      "grad_norm": 0.33498749136924744,
      "learning_rate": 9.770833333333334e-05,
      "loss": 0.3188,
      "step": 15500
    },
    {
      "epoch": 0.1517148150489426,
      "grad_norm": 0.3651781678199768,
      "learning_rate": 9.766666666666668e-05,
      "loss": 0.3169,
      "step": 15600
    },
    {
      "epoch": 0.15268734591464098,
      "grad_norm": 0.336200475692749,
      "learning_rate": 9.7625e-05,
      "loss": 0.3147,
      "step": 15700
    },
    {
      "epoch": 0.15365987678033932,
      "grad_norm": 0.3387849032878876,
      "learning_rate": 9.758333333333334e-05,
      "loss": 0.3154,
      "step": 15800
    },
    {
      "epoch": 0.15463240764603767,
      "grad_norm": 0.36113041639328003,
      "learning_rate": 9.754166666666667e-05,
      "loss": 0.3165,
      "step": 15900
    },
    {
      "epoch": 0.155604938511736,
      "grad_norm": 0.31112200021743774,
      "learning_rate": 9.75e-05,
      "loss": 0.3151,
      "step": 16000
    },
    {
      "epoch": 0.15657746937743436,
      "grad_norm": 0.3472239077091217,
      "learning_rate": 9.745833333333334e-05,
      "loss": 0.3145,
      "step": 16100
    },
    {
      "epoch": 0.1575500002431327,
      "grad_norm": 0.3393404185771942,
      "learning_rate": 9.741666666666667e-05,
      "loss": 0.3162,
      "step": 16200
    },
    {
      "epoch": 0.15852253110883108,
      "grad_norm": 0.32386893033981323,
      "learning_rate": 9.737500000000001e-05,
      "loss": 0.3154,
      "step": 16300
    },
    {
      "epoch": 0.15949506197452942,
      "grad_norm": 0.35164910554885864,
      "learning_rate": 9.733333333333335e-05,
      "loss": 0.3171,
      "step": 16400
    },
    {
      "epoch": 0.16046759284022777,
      "grad_norm": 0.3386598825454712,
      "learning_rate": 9.729166666666667e-05,
      "loss": 0.3117,
      "step": 16500
    },
    {
      "epoch": 0.16144012370592611,
      "grad_norm": 0.3541555404663086,
      "learning_rate": 9.725e-05,
      "loss": 0.314,
      "step": 16600
    },
    {
      "epoch": 0.16241265457162446,
      "grad_norm": 0.32366347312927246,
      "learning_rate": 9.720833333333333e-05,
      "loss": 0.3134,
      "step": 16700
    },
    {
      "epoch": 0.1633851854373228,
      "grad_norm": 0.335360050201416,
      "learning_rate": 9.716666666666667e-05,
      "loss": 0.3125,
      "step": 16800
    },
    {
      "epoch": 0.16435771630302118,
      "grad_norm": 0.3255518972873688,
      "learning_rate": 9.7125e-05,
      "loss": 0.3119,
      "step": 16900
    },
    {
      "epoch": 0.16533024716871952,
      "grad_norm": 0.3295859694480896,
      "learning_rate": 9.708333333333334e-05,
      "loss": 0.3128,
      "step": 17000
    },
    {
      "epoch": 0.16630277803441787,
      "grad_norm": 0.32104429602622986,
      "learning_rate": 9.704166666666668e-05,
      "loss": 0.3122,
      "step": 17100
    },
    {
      "epoch": 0.16727530890011622,
      "grad_norm": 0.31615835428237915,
      "learning_rate": 9.7e-05,
      "loss": 0.3125,
      "step": 17200
    },
    {
      "epoch": 0.16824783976581456,
      "grad_norm": 0.3089175820350647,
      "learning_rate": 9.695833333333334e-05,
      "loss": 0.3128,
      "step": 17300
    },
    {
      "epoch": 0.1692203706315129,
      "grad_norm": 0.3418733775615692,
      "learning_rate": 9.691666666666667e-05,
      "loss": 0.3118,
      "step": 17400
    },
    {
      "epoch": 0.17019290149721128,
      "grad_norm": 0.31500327587127686,
      "learning_rate": 9.687500000000001e-05,
      "loss": 0.3122,
      "step": 17500
    },
    {
      "epoch": 0.17116543236290963,
      "grad_norm": 0.32200902700424194,
      "learning_rate": 9.683333333333335e-05,
      "loss": 0.3126,
      "step": 17600
    },
    {
      "epoch": 0.17213796322860797,
      "grad_norm": 0.344367116689682,
      "learning_rate": 9.679166666666667e-05,
      "loss": 0.3127,
      "step": 17700
    },
    {
      "epoch": 0.17311049409430632,
      "grad_norm": 0.3235596716403961,
      "learning_rate": 9.675000000000001e-05,
      "loss": 0.3103,
      "step": 17800
    },
    {
      "epoch": 0.17408302496000466,
      "grad_norm": 0.3381333351135254,
      "learning_rate": 9.670833333333333e-05,
      "loss": 0.3116,
      "step": 17900
    },
    {
      "epoch": 0.175055555825703,
      "grad_norm": 0.31100645661354065,
      "learning_rate": 9.666666666666667e-05,
      "loss": 0.3106,
      "step": 18000
    },
    {
      "epoch": 0.17602808669140138,
      "grad_norm": 0.31827396154403687,
      "learning_rate": 9.6625e-05,
      "loss": 0.3127,
      "step": 18100
    },
    {
      "epoch": 0.17700061755709973,
      "grad_norm": 0.32963821291923523,
      "learning_rate": 9.658333333333334e-05,
      "loss": 0.3103,
      "step": 18200
    },
    {
      "epoch": 0.17797314842279807,
      "grad_norm": 0.3341643214225769,
      "learning_rate": 9.654166666666668e-05,
      "loss": 0.3096,
      "step": 18300
    },
    {
      "epoch": 0.17894567928849642,
      "grad_norm": 0.30816808342933655,
      "learning_rate": 9.65e-05,
      "loss": 0.3092,
      "step": 18400
    },
    {
      "epoch": 0.17991821015419476,
      "grad_norm": 0.30688026547431946,
      "learning_rate": 9.645833333333334e-05,
      "loss": 0.309,
      "step": 18500
    },
    {
      "epoch": 0.1808907410198931,
      "grad_norm": 0.28870946168899536,
      "learning_rate": 9.641666666666666e-05,
      "loss": 0.3089,
      "step": 18600
    },
    {
      "epoch": 0.18186327188559148,
      "grad_norm": 0.327122300863266,
      "learning_rate": 9.6375e-05,
      "loss": 0.3083,
      "step": 18700
    },
    {
      "epoch": 0.18283580275128983,
      "grad_norm": 0.3104759156703949,
      "learning_rate": 9.633333333333335e-05,
      "loss": 0.3084,
      "step": 18800
    },
    {
      "epoch": 0.18380833361698817,
      "grad_norm": 0.28594970703125,
      "learning_rate": 9.629166666666667e-05,
      "loss": 0.3111,
      "step": 18900
    },
    {
      "epoch": 0.18478086448268652,
      "grad_norm": 0.3448643982410431,
      "learning_rate": 9.625000000000001e-05,
      "loss": 0.3094,
      "step": 19000
    },
    {
      "epoch": 0.18575339534838486,
      "grad_norm": 0.32005852460861206,
      "learning_rate": 9.620833333333333e-05,
      "loss": 0.3102,
      "step": 19100
    },
    {
      "epoch": 0.1867259262140832,
      "grad_norm": 0.27301645278930664,
      "learning_rate": 9.616666666666667e-05,
      "loss": 0.3076,
      "step": 19200
    },
    {
      "epoch": 0.18769845707978158,
      "grad_norm": 0.2956153452396393,
      "learning_rate": 9.6125e-05,
      "loss": 0.3081,
      "step": 19300
    },
    {
      "epoch": 0.18867098794547993,
      "grad_norm": 0.3186676800251007,
      "learning_rate": 9.608333333333334e-05,
      "loss": 0.3072,
      "step": 19400
    },
    {
      "epoch": 0.18964351881117827,
      "grad_norm": 0.285645991563797,
      "learning_rate": 9.604166666666668e-05,
      "loss": 0.3098,
      "step": 19500
    },
    {
      "epoch": 0.19061604967687662,
      "grad_norm": 0.2974565029144287,
      "learning_rate": 9.6e-05,
      "loss": 0.3075,
      "step": 19600
    },
    {
      "epoch": 0.19158858054257497,
      "grad_norm": 0.29535743594169617,
      "learning_rate": 9.595833333333334e-05,
      "loss": 0.3061,
      "step": 19700
    },
    {
      "epoch": 0.1925611114082733,
      "grad_norm": 0.3008219599723816,
      "learning_rate": 9.591666666666666e-05,
      "loss": 0.3079,
      "step": 19800
    },
    {
      "epoch": 0.19353364227397166,
      "grad_norm": 0.293601930141449,
      "learning_rate": 9.5875e-05,
      "loss": 0.3056,
      "step": 19900
    },
    {
      "epoch": 0.19450617313967003,
      "grad_norm": 0.30891555547714233,
      "learning_rate": 9.583333333333334e-05,
      "loss": 0.307,
      "step": 20000
    },
    {
      "epoch": 0.19450617313967003,
      "eval_loss": 0.30657270550727844,
      "eval_runtime": 3092.6372,
      "eval_samples_per_second": 738.771,
      "eval_steps_per_second": 7.388,
      "step": 20000
    },
    {
      "epoch": 0.19547870400536838,
      "grad_norm": 0.2663520276546478,
      "learning_rate": 9.579166666666667e-05,
      "loss": 0.3071,
      "step": 20100
    },
    {
      "epoch": 0.19645123487106672,
      "grad_norm": 0.2592669129371643,
      "learning_rate": 9.575000000000001e-05,
      "loss": 0.3061,
      "step": 20200
    },
    {
      "epoch": 0.19742376573676507,
      "grad_norm": 0.30807793140411377,
      "learning_rate": 9.570833333333333e-05,
      "loss": 0.3074,
      "step": 20300
    },
    {
      "epoch": 0.1983962966024634,
      "grad_norm": 0.3048373758792877,
      "learning_rate": 9.566666666666667e-05,
      "loss": 0.3046,
      "step": 20400
    },
    {
      "epoch": 0.19936882746816176,
      "grad_norm": 0.3302573561668396,
      "learning_rate": 9.562500000000001e-05,
      "loss": 0.3075,
      "step": 20500
    },
    {
      "epoch": 0.20034135833386013,
      "grad_norm": 0.2922709882259369,
      "learning_rate": 9.558333333333333e-05,
      "loss": 0.3069,
      "step": 20600
    },
    {
      "epoch": 0.20131388919955848,
      "grad_norm": 0.33838585019111633,
      "learning_rate": 9.554166666666667e-05,
      "loss": 0.3058,
      "step": 20700
    },
    {
      "epoch": 0.20228642006525682,
      "grad_norm": 0.3231334686279297,
      "learning_rate": 9.55e-05,
      "loss": 0.3068,
      "step": 20800
    },
    {
      "epoch": 0.20325895093095517,
      "grad_norm": 0.30601373314857483,
      "learning_rate": 9.545833333333334e-05,
      "loss": 0.3053,
      "step": 20900
    },
    {
      "epoch": 0.2042314817966535,
      "grad_norm": 0.29868796467781067,
      "learning_rate": 9.541666666666668e-05,
      "loss": 0.3061,
      "step": 21000
    },
    {
      "epoch": 0.20520401266235186,
      "grad_norm": 0.30377939343452454,
      "learning_rate": 9.5375e-05,
      "loss": 0.3046,
      "step": 21100
    },
    {
      "epoch": 0.20617654352805023,
      "grad_norm": 0.2640807032585144,
      "learning_rate": 9.533333333333334e-05,
      "loss": 0.3043,
      "step": 21200
    },
    {
      "epoch": 0.20714907439374858,
      "grad_norm": 0.295136034488678,
      "learning_rate": 9.529166666666667e-05,
      "loss": 0.3056,
      "step": 21300
    },
    {
      "epoch": 0.20812160525944692,
      "grad_norm": 0.2827106714248657,
      "learning_rate": 9.525000000000001e-05,
      "loss": 0.3022,
      "step": 21400
    },
    {
      "epoch": 0.20909413612514527,
      "grad_norm": 0.27424606680870056,
      "learning_rate": 9.520833333333333e-05,
      "loss": 0.3027,
      "step": 21500
    },
    {
      "epoch": 0.21006666699084361,
      "grad_norm": 0.2691164016723633,
      "learning_rate": 9.516666666666667e-05,
      "loss": 0.3055,
      "step": 21600
    },
    {
      "epoch": 0.21103919785654196,
      "grad_norm": 0.3001590073108673,
      "learning_rate": 9.512500000000001e-05,
      "loss": 0.3038,
      "step": 21700
    },
    {
      "epoch": 0.21201172872224033,
      "grad_norm": 0.279934287071228,
      "learning_rate": 9.508333333333333e-05,
      "loss": 0.3035,
      "step": 21800
    },
    {
      "epoch": 0.21298425958793868,
      "grad_norm": 0.2950795888900757,
      "learning_rate": 9.504166666666667e-05,
      "loss": 0.3035,
      "step": 21900
    },
    {
      "epoch": 0.21395679045363702,
      "grad_norm": 0.2872313857078552,
      "learning_rate": 9.5e-05,
      "loss": 0.3032,
      "step": 22000
    },
    {
      "epoch": 0.21492932131933537,
      "grad_norm": 0.28139805793762207,
      "learning_rate": 9.495833333333334e-05,
      "loss": 0.3039,
      "step": 22100
    },
    {
      "epoch": 0.21590185218503372,
      "grad_norm": 0.28631237149238586,
      "learning_rate": 9.491666666666668e-05,
      "loss": 0.3022,
      "step": 22200
    },
    {
      "epoch": 0.21687438305073206,
      "grad_norm": 0.29385530948638916,
      "learning_rate": 9.4875e-05,
      "loss": 0.3031,
      "step": 22300
    },
    {
      "epoch": 0.21784691391643043,
      "grad_norm": 0.2824866771697998,
      "learning_rate": 9.483333333333334e-05,
      "loss": 0.3022,
      "step": 22400
    },
    {
      "epoch": 0.21881944478212878,
      "grad_norm": 0.30556756258010864,
      "learning_rate": 9.479166666666666e-05,
      "loss": 0.3055,
      "step": 22500
    },
    {
      "epoch": 0.21979197564782713,
      "grad_norm": 0.29588714241981506,
      "learning_rate": 9.475e-05,
      "loss": 0.303,
      "step": 22600
    },
    {
      "epoch": 0.22076450651352547,
      "grad_norm": 0.2775481641292572,
      "learning_rate": 9.470833333333335e-05,
      "loss": 0.3013,
      "step": 22700
    },
    {
      "epoch": 0.22173703737922382,
      "grad_norm": 0.3164386749267578,
      "learning_rate": 9.466666666666667e-05,
      "loss": 0.3044,
      "step": 22800
    },
    {
      "epoch": 0.22270956824492216,
      "grad_norm": 0.26901867985725403,
      "learning_rate": 9.462500000000001e-05,
      "loss": 0.2994,
      "step": 22900
    },
    {
      "epoch": 0.22368209911062054,
      "grad_norm": 0.30491572618484497,
      "learning_rate": 9.458333333333333e-05,
      "loss": 0.3031,
      "step": 23000
    },
    {
      "epoch": 0.22465462997631888,
      "grad_norm": 0.284429132938385,
      "learning_rate": 9.454166666666667e-05,
      "loss": 0.3016,
      "step": 23100
    },
    {
      "epoch": 0.22562716084201723,
      "grad_norm": 0.26541051268577576,
      "learning_rate": 9.449999999999999e-05,
      "loss": 0.301,
      "step": 23200
    },
    {
      "epoch": 0.22659969170771557,
      "grad_norm": 0.26261696219444275,
      "learning_rate": 9.445833333333334e-05,
      "loss": 0.3014,
      "step": 23300
    },
    {
      "epoch": 0.22757222257341392,
      "grad_norm": 0.2929598093032837,
      "learning_rate": 9.441666666666668e-05,
      "loss": 0.3015,
      "step": 23400
    },
    {
      "epoch": 0.22854475343911226,
      "grad_norm": 0.2636703848838806,
      "learning_rate": 9.4375e-05,
      "loss": 0.3006,
      "step": 23500
    },
    {
      "epoch": 0.22951728430481064,
      "grad_norm": 0.268208771944046,
      "learning_rate": 9.433333333333334e-05,
      "loss": 0.3019,
      "step": 23600
    },
    {
      "epoch": 0.23048981517050898,
      "grad_norm": 0.2738412022590637,
      "learning_rate": 9.429166666666666e-05,
      "loss": 0.3019,
      "step": 23700
    },
    {
      "epoch": 0.23146234603620733,
      "grad_norm": 0.2656060755252838,
      "learning_rate": 9.425e-05,
      "loss": 0.3005,
      "step": 23800
    },
    {
      "epoch": 0.23243487690190567,
      "grad_norm": 0.300933837890625,
      "learning_rate": 9.420833333333334e-05,
      "loss": 0.3004,
      "step": 23900
    },
    {
      "epoch": 0.23340740776760402,
      "grad_norm": 0.2616328299045563,
      "learning_rate": 9.416666666666667e-05,
      "loss": 0.2999,
      "step": 24000
    },
    {
      "epoch": 0.23437993863330236,
      "grad_norm": 0.2800791561603546,
      "learning_rate": 9.412500000000001e-05,
      "loss": 0.299,
      "step": 24100
    },
    {
      "epoch": 0.23535246949900074,
      "grad_norm": 0.2536715865135193,
      "learning_rate": 9.408333333333333e-05,
      "loss": 0.2989,
      "step": 24200
    },
    {
      "epoch": 0.23632500036469908,
      "grad_norm": 0.2726067006587982,
      "learning_rate": 9.404166666666667e-05,
      "loss": 0.3015,
      "step": 24300
    },
    {
      "epoch": 0.23729753123039743,
      "grad_norm": 0.28242817521095276,
      "learning_rate": 9.4e-05,
      "loss": 0.3008,
      "step": 24400
    },
    {
      "epoch": 0.23827006209609577,
      "grad_norm": 0.27083465456962585,
      "learning_rate": 9.395833333333333e-05,
      "loss": 0.2998,
      "step": 24500
    },
    {
      "epoch": 0.23924259296179412,
      "grad_norm": 0.25746363401412964,
      "learning_rate": 9.391666666666668e-05,
      "loss": 0.3014,
      "step": 24600
    },
    {
      "epoch": 0.24021512382749247,
      "grad_norm": 0.26089948415756226,
      "learning_rate": 9.3875e-05,
      "loss": 0.3008,
      "step": 24700
    },
    {
      "epoch": 0.2411876546931908,
      "grad_norm": 0.2491711527109146,
      "learning_rate": 9.383333333333334e-05,
      "loss": 0.2994,
      "step": 24800
    },
    {
      "epoch": 0.24216018555888918,
      "grad_norm": 0.2692364454269409,
      "learning_rate": 9.379166666666667e-05,
      "loss": 0.2997,
      "step": 24900
    },
    {
      "epoch": 0.24313271642458753,
      "grad_norm": 0.2713834047317505,
      "learning_rate": 9.375e-05,
      "loss": 0.2986,
      "step": 25000
    },
    {
      "epoch": 0.24410524729028588,
      "grad_norm": 0.25930288434028625,
      "learning_rate": 9.370833333333334e-05,
      "loss": 0.3012,
      "step": 25100
    },
    {
      "epoch": 0.24507777815598422,
      "grad_norm": 0.27840882539749146,
      "learning_rate": 9.366666666666668e-05,
      "loss": 0.2985,
      "step": 25200
    },
    {
      "epoch": 0.24605030902168257,
      "grad_norm": 0.2481825202703476,
      "learning_rate": 9.362500000000001e-05,
      "loss": 0.2988,
      "step": 25300
    },
    {
      "epoch": 0.2470228398873809,
      "grad_norm": 0.28635174036026,
      "learning_rate": 9.358333333333334e-05,
      "loss": 0.2996,
      "step": 25400
    },
    {
      "epoch": 0.2479953707530793,
      "grad_norm": 0.26634883880615234,
      "learning_rate": 9.354166666666667e-05,
      "loss": 0.2986,
      "step": 25500
    },
    {
      "epoch": 0.24896790161877763,
      "grad_norm": 0.2660938799381256,
      "learning_rate": 9.350000000000001e-05,
      "loss": 0.2993,
      "step": 25600
    },
    {
      "epoch": 0.24994043248447598,
      "grad_norm": 0.2514820098876953,
      "learning_rate": 9.345833333333333e-05,
      "loss": 0.298,
      "step": 25700
    },
    {
      "epoch": 0.2509129633501743,
      "grad_norm": 0.2851623594760895,
      "learning_rate": 9.341666666666667e-05,
      "loss": 0.2996,
      "step": 25800
    },
    {
      "epoch": 0.2518854942158727,
      "grad_norm": 0.26138824224472046,
      "learning_rate": 9.3375e-05,
      "loss": 0.2991,
      "step": 25900
    },
    {
      "epoch": 0.252858025081571,
      "grad_norm": 0.26871880888938904,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.2996,
      "step": 26000
    },
    {
      "epoch": 0.2538305559472694,
      "grad_norm": 0.24440231919288635,
      "learning_rate": 9.329166666666667e-05,
      "loss": 0.2986,
      "step": 26100
    },
    {
      "epoch": 0.2548030868129677,
      "grad_norm": 0.2511691451072693,
      "learning_rate": 9.325e-05,
      "loss": 0.2978,
      "step": 26200
    },
    {
      "epoch": 0.2557756176786661,
      "grad_norm": 0.26232871413230896,
      "learning_rate": 9.320833333333334e-05,
      "loss": 0.2989,
      "step": 26300
    },
    {
      "epoch": 0.25674814854436445,
      "grad_norm": 0.27634692192077637,
      "learning_rate": 9.316666666666666e-05,
      "loss": 0.2974,
      "step": 26400
    },
    {
      "epoch": 0.25772067941006277,
      "grad_norm": 0.27570393681526184,
      "learning_rate": 9.3125e-05,
      "loss": 0.2997,
      "step": 26500
    },
    {
      "epoch": 0.25869321027576114,
      "grad_norm": 0.2706893980503082,
      "learning_rate": 9.308333333333334e-05,
      "loss": 0.2995,
      "step": 26600
    },
    {
      "epoch": 0.25966574114145946,
      "grad_norm": 0.2777135968208313,
      "learning_rate": 9.304166666666667e-05,
      "loss": 0.2958,
      "step": 26700
    },
    {
      "epoch": 0.26063827200715783,
      "grad_norm": 0.2939475476741791,
      "learning_rate": 9.300000000000001e-05,
      "loss": 0.2978,
      "step": 26800
    },
    {
      "epoch": 0.26161080287285615,
      "grad_norm": 0.2724330723285675,
      "learning_rate": 9.295833333333333e-05,
      "loss": 0.2961,
      "step": 26900
    },
    {
      "epoch": 0.2625833337385545,
      "grad_norm": 0.2710447311401367,
      "learning_rate": 9.291666666666667e-05,
      "loss": 0.2964,
      "step": 27000
    },
    {
      "epoch": 0.2635558646042529,
      "grad_norm": 0.278411328792572,
      "learning_rate": 9.2875e-05,
      "loss": 0.298,
      "step": 27100
    },
    {
      "epoch": 0.2645283954699512,
      "grad_norm": 0.2576330304145813,
      "learning_rate": 9.283333333333334e-05,
      "loss": 0.2966,
      "step": 27200
    },
    {
      "epoch": 0.2655009263356496,
      "grad_norm": 0.2794477641582489,
      "learning_rate": 9.279166666666667e-05,
      "loss": 0.2971,
      "step": 27300
    },
    {
      "epoch": 0.2664734572013479,
      "grad_norm": 0.26318278908729553,
      "learning_rate": 9.275e-05,
      "loss": 0.2976,
      "step": 27400
    },
    {
      "epoch": 0.2674459880670463,
      "grad_norm": 0.2590227425098419,
      "learning_rate": 9.270833333333334e-05,
      "loss": 0.2963,
      "step": 27500
    },
    {
      "epoch": 0.26841851893274465,
      "grad_norm": 0.2557487487792969,
      "learning_rate": 9.266666666666666e-05,
      "loss": 0.2976,
      "step": 27600
    },
    {
      "epoch": 0.26939104979844297,
      "grad_norm": 0.24719713628292084,
      "learning_rate": 9.2625e-05,
      "loss": 0.2955,
      "step": 27700
    },
    {
      "epoch": 0.27036358066414135,
      "grad_norm": 0.28546464443206787,
      "learning_rate": 9.258333333333334e-05,
      "loss": 0.2985,
      "step": 27800
    },
    {
      "epoch": 0.27133611152983966,
      "grad_norm": 0.2735351622104645,
      "learning_rate": 9.254166666666668e-05,
      "loss": 0.2957,
      "step": 27900
    },
    {
      "epoch": 0.27230864239553804,
      "grad_norm": 0.2712380290031433,
      "learning_rate": 9.250000000000001e-05,
      "loss": 0.2951,
      "step": 28000
    },
    {
      "epoch": 0.27328117326123635,
      "grad_norm": 0.26533541083335876,
      "learning_rate": 9.245833333333334e-05,
      "loss": 0.2952,
      "step": 28100
    },
    {
      "epoch": 0.2742537041269347,
      "grad_norm": 0.24769780039787292,
      "learning_rate": 9.241666666666667e-05,
      "loss": 0.2956,
      "step": 28200
    },
    {
      "epoch": 0.2752262349926331,
      "grad_norm": 0.25848057866096497,
      "learning_rate": 9.2375e-05,
      "loss": 0.294,
      "step": 28300
    },
    {
      "epoch": 0.2761987658583314,
      "grad_norm": 0.22893022000789642,
      "learning_rate": 9.233333333333333e-05,
      "loss": 0.2954,
      "step": 28400
    },
    {
      "epoch": 0.2771712967240298,
      "grad_norm": 0.26093530654907227,
      "learning_rate": 9.229166666666668e-05,
      "loss": 0.2954,
      "step": 28500
    },
    {
      "epoch": 0.2781438275897281,
      "grad_norm": 0.24068091809749603,
      "learning_rate": 9.225e-05,
      "loss": 0.2962,
      "step": 28600
    },
    {
      "epoch": 0.2791163584554265,
      "grad_norm": 0.2602939009666443,
      "learning_rate": 9.220833333333334e-05,
      "loss": 0.295,
      "step": 28700
    },
    {
      "epoch": 0.28008888932112486,
      "grad_norm": 0.256083220243454,
      "learning_rate": 9.216666666666667e-05,
      "loss": 0.2946,
      "step": 28800
    },
    {
      "epoch": 0.2810614201868232,
      "grad_norm": 0.2686990201473236,
      "learning_rate": 9.2125e-05,
      "loss": 0.2953,
      "step": 28900
    },
    {
      "epoch": 0.28203395105252155,
      "grad_norm": 0.2572813332080841,
      "learning_rate": 9.208333333333333e-05,
      "loss": 0.2965,
      "step": 29000
    },
    {
      "epoch": 0.28300648191821987,
      "grad_norm": 0.24532102048397064,
      "learning_rate": 9.204166666666668e-05,
      "loss": 0.2959,
      "step": 29100
    },
    {
      "epoch": 0.28397901278391824,
      "grad_norm": 0.25591257214546204,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.2956,
      "step": 29200
    },
    {
      "epoch": 0.28495154364961656,
      "grad_norm": 0.25297853350639343,
      "learning_rate": 9.195833333333334e-05,
      "loss": 0.2965,
      "step": 29300
    },
    {
      "epoch": 0.28592407451531493,
      "grad_norm": 0.23952274024486542,
      "learning_rate": 9.191666666666667e-05,
      "loss": 0.2953,
      "step": 29400
    },
    {
      "epoch": 0.2868966053810133,
      "grad_norm": 0.250018835067749,
      "learning_rate": 9.1875e-05,
      "loss": 0.294,
      "step": 29500
    },
    {
      "epoch": 0.2878691362467116,
      "grad_norm": 0.26095157861709595,
      "learning_rate": 9.183333333333333e-05,
      "loss": 0.2944,
      "step": 29600
    },
    {
      "epoch": 0.28884166711241,
      "grad_norm": 0.25782227516174316,
      "learning_rate": 9.179166666666667e-05,
      "loss": 0.2939,
      "step": 29700
    },
    {
      "epoch": 0.2898141979781083,
      "grad_norm": 0.2380521148443222,
      "learning_rate": 9.175000000000001e-05,
      "loss": 0.2945,
      "step": 29800
    },
    {
      "epoch": 0.2907867288438067,
      "grad_norm": 0.2770138680934906,
      "learning_rate": 9.170833333333334e-05,
      "loss": 0.2936,
      "step": 29900
    },
    {
      "epoch": 0.291759259709505,
      "grad_norm": 0.25458279252052307,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.2948,
      "step": 30000
    },
    {
      "epoch": 0.291759259709505,
      "eval_loss": 0.2945513129234314,
      "eval_runtime": 3209.4879,
      "eval_samples_per_second": 711.874,
      "eval_steps_per_second": 7.119,
      "step": 30000
    },
    {
      "epoch": 0.2927317905752034,
      "grad_norm": 0.2680201232433319,
      "learning_rate": 9.1625e-05,
      "loss": 0.2947,
      "step": 30100
    },
    {
      "epoch": 0.29370432144090175,
      "grad_norm": 0.24552887678146362,
      "learning_rate": 9.158333333333334e-05,
      "loss": 0.2946,
      "step": 30200
    },
    {
      "epoch": 0.29467685230660007,
      "grad_norm": 0.2563554346561432,
      "learning_rate": 9.154166666666666e-05,
      "loss": 0.2945,
      "step": 30300
    },
    {
      "epoch": 0.29564938317229844,
      "grad_norm": 0.2727108597755432,
      "learning_rate": 9.15e-05,
      "loss": 0.2966,
      "step": 30400
    },
    {
      "epoch": 0.29662191403799676,
      "grad_norm": 0.2543666958808899,
      "learning_rate": 9.145833333333334e-05,
      "loss": 0.2938,
      "step": 30500
    },
    {
      "epoch": 0.29759444490369513,
      "grad_norm": 0.24210196733474731,
      "learning_rate": 9.141666666666668e-05,
      "loss": 0.2949,
      "step": 30600
    },
    {
      "epoch": 0.2985669757693935,
      "grad_norm": 0.23182745277881622,
      "learning_rate": 9.1375e-05,
      "loss": 0.2945,
      "step": 30700
    },
    {
      "epoch": 0.2995395066350918,
      "grad_norm": 0.2550813555717468,
      "learning_rate": 9.133333333333334e-05,
      "loss": 0.2929,
      "step": 30800
    },
    {
      "epoch": 0.3005120375007902,
      "grad_norm": 0.23296059668064117,
      "learning_rate": 9.129166666666667e-05,
      "loss": 0.2932,
      "step": 30900
    },
    {
      "epoch": 0.3014845683664885,
      "grad_norm": 0.23223282396793365,
      "learning_rate": 9.125e-05,
      "loss": 0.2926,
      "step": 31000
    },
    {
      "epoch": 0.3024570992321869,
      "grad_norm": 0.2607433795928955,
      "learning_rate": 9.120833333333335e-05,
      "loss": 0.2939,
      "step": 31100
    },
    {
      "epoch": 0.3034296300978852,
      "grad_norm": 0.27229568362236023,
      "learning_rate": 9.116666666666667e-05,
      "loss": 0.2922,
      "step": 31200
    },
    {
      "epoch": 0.3044021609635836,
      "grad_norm": 0.25349339842796326,
      "learning_rate": 9.1125e-05,
      "loss": 0.2933,
      "step": 31300
    },
    {
      "epoch": 0.30537469182928195,
      "grad_norm": 0.26859259605407715,
      "learning_rate": 9.108333333333334e-05,
      "loss": 0.2929,
      "step": 31400
    },
    {
      "epoch": 0.30634722269498027,
      "grad_norm": 0.25140634179115295,
      "learning_rate": 9.104166666666667e-05,
      "loss": 0.2941,
      "step": 31500
    },
    {
      "epoch": 0.30731975356067864,
      "grad_norm": 0.2491023689508438,
      "learning_rate": 9.1e-05,
      "loss": 0.2939,
      "step": 31600
    },
    {
      "epoch": 0.30829228442637696,
      "grad_norm": 0.2573309540748596,
      "learning_rate": 9.095833333333334e-05,
      "loss": 0.2915,
      "step": 31700
    },
    {
      "epoch": 0.30926481529207533,
      "grad_norm": 0.2691701054573059,
      "learning_rate": 9.091666666666668e-05,
      "loss": 0.294,
      "step": 31800
    },
    {
      "epoch": 0.3102373461577737,
      "grad_norm": 0.2629852890968323,
      "learning_rate": 9.0875e-05,
      "loss": 0.2923,
      "step": 31900
    },
    {
      "epoch": 0.311209877023472,
      "grad_norm": 0.24604031443595886,
      "learning_rate": 9.083333333333334e-05,
      "loss": 0.2932,
      "step": 32000
    },
    {
      "epoch": 0.3121824078891704,
      "grad_norm": 0.2368324100971222,
      "learning_rate": 9.079166666666667e-05,
      "loss": 0.2928,
      "step": 32100
    },
    {
      "epoch": 0.3131549387548687,
      "grad_norm": 0.20243839919567108,
      "learning_rate": 9.075e-05,
      "loss": 0.2915,
      "step": 32200
    },
    {
      "epoch": 0.3141274696205671,
      "grad_norm": 0.2670537829399109,
      "learning_rate": 9.070833333333333e-05,
      "loss": 0.2937,
      "step": 32300
    },
    {
      "epoch": 0.3151000004862654,
      "grad_norm": 0.26121726632118225,
      "learning_rate": 9.066666666666667e-05,
      "loss": 0.2942,
      "step": 32400
    },
    {
      "epoch": 0.3160725313519638,
      "grad_norm": 0.25509166717529297,
      "learning_rate": 9.062500000000001e-05,
      "loss": 0.2917,
      "step": 32500
    },
    {
      "epoch": 0.31704506221766215,
      "grad_norm": 0.2549728751182556,
      "learning_rate": 9.058333333333334e-05,
      "loss": 0.2924,
      "step": 32600
    },
    {
      "epoch": 0.3180175930833605,
      "grad_norm": 0.2707057595252991,
      "learning_rate": 9.054166666666667e-05,
      "loss": 0.2935,
      "step": 32700
    },
    {
      "epoch": 0.31899012394905885,
      "grad_norm": 0.2379462718963623,
      "learning_rate": 9.05e-05,
      "loss": 0.291,
      "step": 32800
    },
    {
      "epoch": 0.31996265481475716,
      "grad_norm": 0.25235724449157715,
      "learning_rate": 9.045833333333333e-05,
      "loss": 0.2925,
      "step": 32900
    },
    {
      "epoch": 0.32093518568045554,
      "grad_norm": 0.2464640885591507,
      "learning_rate": 9.041666666666668e-05,
      "loss": 0.2907,
      "step": 33000
    },
    {
      "epoch": 0.3219077165461539,
      "grad_norm": 0.2464553564786911,
      "learning_rate": 9.037500000000001e-05,
      "loss": 0.2902,
      "step": 33100
    },
    {
      "epoch": 0.32288024741185223,
      "grad_norm": 0.24293436110019684,
      "learning_rate": 9.033333333333334e-05,
      "loss": 0.2921,
      "step": 33200
    },
    {
      "epoch": 0.3238527782775506,
      "grad_norm": 0.2472950667142868,
      "learning_rate": 9.029166666666667e-05,
      "loss": 0.2922,
      "step": 33300
    },
    {
      "epoch": 0.3248253091432489,
      "grad_norm": 0.23093585669994354,
      "learning_rate": 9.025e-05,
      "loss": 0.2893,
      "step": 33400
    },
    {
      "epoch": 0.3257978400089473,
      "grad_norm": 0.24209007620811462,
      "learning_rate": 9.020833333333334e-05,
      "loss": 0.2887,
      "step": 33500
    },
    {
      "epoch": 0.3267703708746456,
      "grad_norm": 0.2488320916891098,
      "learning_rate": 9.016666666666667e-05,
      "loss": 0.2927,
      "step": 33600
    },
    {
      "epoch": 0.327742901740344,
      "grad_norm": 0.21909861266613007,
      "learning_rate": 9.012500000000001e-05,
      "loss": 0.2916,
      "step": 33700
    },
    {
      "epoch": 0.32871543260604236,
      "grad_norm": 0.2567775249481201,
      "learning_rate": 9.008333333333335e-05,
      "loss": 0.2925,
      "step": 33800
    },
    {
      "epoch": 0.3296879634717407,
      "grad_norm": 0.22824148833751678,
      "learning_rate": 9.004166666666667e-05,
      "loss": 0.2897,
      "step": 33900
    },
    {
      "epoch": 0.33066049433743905,
      "grad_norm": 0.24759039282798767,
      "learning_rate": 9e-05,
      "loss": 0.2913,
      "step": 34000
    },
    {
      "epoch": 0.33163302520313737,
      "grad_norm": 0.2380143105983734,
      "learning_rate": 8.995833333333333e-05,
      "loss": 0.2912,
      "step": 34100
    },
    {
      "epoch": 0.33260555606883574,
      "grad_norm": 0.24341124296188354,
      "learning_rate": 8.991666666666667e-05,
      "loss": 0.2891,
      "step": 34200
    },
    {
      "epoch": 0.33357808693453406,
      "grad_norm": 0.2532952129840851,
      "learning_rate": 8.9875e-05,
      "loss": 0.2905,
      "step": 34300
    },
    {
      "epoch": 0.33455061780023243,
      "grad_norm": 0.2312133014202118,
      "learning_rate": 8.983333333333334e-05,
      "loss": 0.2913,
      "step": 34400
    },
    {
      "epoch": 0.3355231486659308,
      "grad_norm": 0.26245754957199097,
      "learning_rate": 8.979166666666668e-05,
      "loss": 0.2921,
      "step": 34500
    },
    {
      "epoch": 0.3364956795316291,
      "grad_norm": 0.2335652858018875,
      "learning_rate": 8.975e-05,
      "loss": 0.2901,
      "step": 34600
    },
    {
      "epoch": 0.3374682103973275,
      "grad_norm": 0.25509563088417053,
      "learning_rate": 8.970833333333334e-05,
      "loss": 0.2911,
      "step": 34700
    },
    {
      "epoch": 0.3384407412630258,
      "grad_norm": 0.2317580133676529,
      "learning_rate": 8.966666666666666e-05,
      "loss": 0.2913,
      "step": 34800
    },
    {
      "epoch": 0.3394132721287242,
      "grad_norm": 0.2312333881855011,
      "learning_rate": 8.962500000000001e-05,
      "loss": 0.2917,
      "step": 34900
    },
    {
      "epoch": 0.34038580299442256,
      "grad_norm": 0.24448128044605255,
      "learning_rate": 8.958333333333335e-05,
      "loss": 0.2919,
      "step": 35000
    },
    {
      "epoch": 0.3413583338601209,
      "grad_norm": 0.2198508381843567,
      "learning_rate": 8.954166666666667e-05,
      "loss": 0.2915,
      "step": 35100
    },
    {
      "epoch": 0.34233086472581925,
      "grad_norm": 0.24950014054775238,
      "learning_rate": 8.950000000000001e-05,
      "loss": 0.2907,
      "step": 35200
    },
    {
      "epoch": 0.34330339559151757,
      "grad_norm": 0.2448570728302002,
      "learning_rate": 8.945833333333333e-05,
      "loss": 0.29,
      "step": 35300
    },
    {
      "epoch": 0.34427592645721594,
      "grad_norm": 0.24573840200901031,
      "learning_rate": 8.941666666666667e-05,
      "loss": 0.2917,
      "step": 35400
    },
    {
      "epoch": 0.34524845732291426,
      "grad_norm": 0.22597819566726685,
      "learning_rate": 8.9375e-05,
      "loss": 0.2882,
      "step": 35500
    },
    {
      "epoch": 0.34622098818861263,
      "grad_norm": 0.24227407574653625,
      "learning_rate": 8.933333333333334e-05,
      "loss": 0.2908,
      "step": 35600
    },
    {
      "epoch": 0.347193519054311,
      "grad_norm": 0.23543964326381683,
      "learning_rate": 8.929166666666668e-05,
      "loss": 0.2891,
      "step": 35700
    },
    {
      "epoch": 0.3481660499200093,
      "grad_norm": 0.2449793517589569,
      "learning_rate": 8.925e-05,
      "loss": 0.2888,
      "step": 35800
    },
    {
      "epoch": 0.3491385807857077,
      "grad_norm": 0.2301981896162033,
      "learning_rate": 8.920833333333334e-05,
      "loss": 0.2897,
      "step": 35900
    },
    {
      "epoch": 0.350111111651406,
      "grad_norm": 0.22120144963264465,
      "learning_rate": 8.916666666666667e-05,
      "loss": 0.29,
      "step": 36000
    },
    {
      "epoch": 0.3510836425171044,
      "grad_norm": 0.24864748120307922,
      "learning_rate": 8.9125e-05,
      "loss": 0.2881,
      "step": 36100
    },
    {
      "epoch": 0.35205617338280276,
      "grad_norm": 0.2303978055715561,
      "learning_rate": 8.908333333333333e-05,
      "loss": 0.2895,
      "step": 36200
    },
    {
      "epoch": 0.3530287042485011,
      "grad_norm": 0.23751448094844818,
      "learning_rate": 8.904166666666667e-05,
      "loss": 0.2898,
      "step": 36300
    },
    {
      "epoch": 0.35400123511419945,
      "grad_norm": 0.24029278755187988,
      "learning_rate": 8.900000000000001e-05,
      "loss": 0.2917,
      "step": 36400
    },
    {
      "epoch": 0.35497376597989777,
      "grad_norm": 0.24289587140083313,
      "learning_rate": 8.895833333333333e-05,
      "loss": 0.2914,
      "step": 36500
    },
    {
      "epoch": 0.35594629684559614,
      "grad_norm": 0.24324652552604675,
      "learning_rate": 8.891666666666667e-05,
      "loss": 0.2889,
      "step": 36600
    },
    {
      "epoch": 0.35691882771129446,
      "grad_norm": 0.23396459221839905,
      "learning_rate": 8.8875e-05,
      "loss": 0.288,
      "step": 36700
    },
    {
      "epoch": 0.35789135857699284,
      "grad_norm": 0.23148095607757568,
      "learning_rate": 8.883333333333333e-05,
      "loss": 0.2894,
      "step": 36800
    },
    {
      "epoch": 0.3588638894426912,
      "grad_norm": 0.23736366629600525,
      "learning_rate": 8.879166666666668e-05,
      "loss": 0.2868,
      "step": 36900
    },
    {
      "epoch": 0.3598364203083895,
      "grad_norm": 0.24479630589485168,
      "learning_rate": 8.875e-05,
      "loss": 0.2906,
      "step": 37000
    },
    {
      "epoch": 0.3608089511740879,
      "grad_norm": 0.21158164739608765,
      "learning_rate": 8.870833333333334e-05,
      "loss": 0.2899,
      "step": 37100
    },
    {
      "epoch": 0.3617814820397862,
      "grad_norm": 0.247767835855484,
      "learning_rate": 8.866666666666668e-05,
      "loss": 0.2878,
      "step": 37200
    },
    {
      "epoch": 0.3627540129054846,
      "grad_norm": 0.24697238206863403,
      "learning_rate": 8.8625e-05,
      "loss": 0.2876,
      "step": 37300
    },
    {
      "epoch": 0.36372654377118296,
      "grad_norm": 0.24256454408168793,
      "learning_rate": 8.858333333333334e-05,
      "loss": 0.2874,
      "step": 37400
    },
    {
      "epoch": 0.3646990746368813,
      "grad_norm": 0.230172261595726,
      "learning_rate": 8.854166666666667e-05,
      "loss": 0.2886,
      "step": 37500
    },
    {
      "epoch": 0.36567160550257966,
      "grad_norm": 0.22899693250656128,
      "learning_rate": 8.850000000000001e-05,
      "loss": 0.288,
      "step": 37600
    },
    {
      "epoch": 0.366644136368278,
      "grad_norm": 0.22410500049591064,
      "learning_rate": 8.845833333333335e-05,
      "loss": 0.29,
      "step": 37700
    },
    {
      "epoch": 0.36761666723397635,
      "grad_norm": 0.22012878954410553,
      "learning_rate": 8.841666666666667e-05,
      "loss": 0.2881,
      "step": 37800
    },
    {
      "epoch": 0.36858919809967466,
      "grad_norm": 0.22442784905433655,
      "learning_rate": 8.837500000000001e-05,
      "loss": 0.2903,
      "step": 37900
    },
    {
      "epoch": 0.36956172896537304,
      "grad_norm": 0.22256219387054443,
      "learning_rate": 8.833333333333333e-05,
      "loss": 0.2876,
      "step": 38000
    },
    {
      "epoch": 0.3705342598310714,
      "grad_norm": 0.23211902379989624,
      "learning_rate": 8.829166666666667e-05,
      "loss": 0.2875,
      "step": 38100
    },
    {
      "epoch": 0.37150679069676973,
      "grad_norm": 0.2401670217514038,
      "learning_rate": 8.825e-05,
      "loss": 0.2904,
      "step": 38200
    },
    {
      "epoch": 0.3724793215624681,
      "grad_norm": 0.23528023064136505,
      "learning_rate": 8.820833333333334e-05,
      "loss": 0.287,
      "step": 38300
    },
    {
      "epoch": 0.3734518524281664,
      "grad_norm": 0.24156738817691803,
      "learning_rate": 8.816666666666668e-05,
      "loss": 0.2877,
      "step": 38400
    },
    {
      "epoch": 0.3744243832938648,
      "grad_norm": 0.23606964945793152,
      "learning_rate": 8.8125e-05,
      "loss": 0.2874,
      "step": 38500
    },
    {
      "epoch": 0.37539691415956317,
      "grad_norm": 0.27374470233917236,
      "learning_rate": 8.808333333333334e-05,
      "loss": 0.2889,
      "step": 38600
    },
    {
      "epoch": 0.3763694450252615,
      "grad_norm": 0.2300308793783188,
      "learning_rate": 8.804166666666666e-05,
      "loss": 0.2869,
      "step": 38700
    },
    {
      "epoch": 0.37734197589095986,
      "grad_norm": 0.2345016896724701,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.2894,
      "step": 38800
    },
    {
      "epoch": 0.3783145067566582,
      "grad_norm": 0.217355877161026,
      "learning_rate": 8.795833333333335e-05,
      "loss": 0.2869,
      "step": 38900
    },
    {
      "epoch": 0.37928703762235655,
      "grad_norm": 0.23566943407058716,
      "learning_rate": 8.791666666666667e-05,
      "loss": 0.2881,
      "step": 39000
    },
    {
      "epoch": 0.38025956848805487,
      "grad_norm": 0.22702179849147797,
      "learning_rate": 8.787500000000001e-05,
      "loss": 0.2876,
      "step": 39100
    },
    {
      "epoch": 0.38123209935375324,
      "grad_norm": 0.24505837261676788,
      "learning_rate": 8.783333333333333e-05,
      "loss": 0.2886,
      "step": 39200
    },
    {
      "epoch": 0.3822046302194516,
      "grad_norm": 0.23099638521671295,
      "learning_rate": 8.779166666666667e-05,
      "loss": 0.2877,
      "step": 39300
    },
    {
      "epoch": 0.38317716108514993,
      "grad_norm": 0.22783039510250092,
      "learning_rate": 8.775e-05,
      "loss": 0.2883,
      "step": 39400
    },
    {
      "epoch": 0.3841496919508483,
      "grad_norm": 0.24774181842803955,
      "learning_rate": 8.770833333333334e-05,
      "loss": 0.287,
      "step": 39500
    },
    {
      "epoch": 0.3851222228165466,
      "grad_norm": 0.22621451318264008,
      "learning_rate": 8.766666666666668e-05,
      "loss": 0.2864,
      "step": 39600
    },
    {
      "epoch": 0.386094753682245,
      "grad_norm": 0.22486229240894318,
      "learning_rate": 8.7625e-05,
      "loss": 0.2878,
      "step": 39700
    },
    {
      "epoch": 0.3870672845479433,
      "grad_norm": 0.23414640128612518,
      "learning_rate": 8.758333333333334e-05,
      "loss": 0.288,
      "step": 39800
    },
    {
      "epoch": 0.3880398154136417,
      "grad_norm": 0.21405328810214996,
      "learning_rate": 8.754166666666666e-05,
      "loss": 0.2876,
      "step": 39900
    },
    {
      "epoch": 0.38901234627934006,
      "grad_norm": 0.2386101931333542,
      "learning_rate": 8.75e-05,
      "loss": 0.2864,
      "step": 40000
    },
    {
      "epoch": 0.38901234627934006,
      "eval_loss": 0.28716006875038147,
      "eval_runtime": 3219.8325,
      "eval_samples_per_second": 709.587,
      "eval_steps_per_second": 7.096,
      "step": 40000
    },
    {
      "epoch": 0.3899848771450384,
      "grad_norm": 0.26306939125061035,
      "learning_rate": 8.745833333333334e-05,
      "loss": 0.2858,
      "step": 40100
    },
    {
      "epoch": 0.39095740801073675,
      "grad_norm": 0.2339586466550827,
      "learning_rate": 8.741666666666667e-05,
      "loss": 0.2873,
      "step": 40200
    },
    {
      "epoch": 0.39192993887643507,
      "grad_norm": 0.23300065100193024,
      "learning_rate": 8.737500000000001e-05,
      "loss": 0.2858,
      "step": 40300
    },
    {
      "epoch": 0.39290246974213344,
      "grad_norm": 0.2433430552482605,
      "learning_rate": 8.733333333333333e-05,
      "loss": 0.2885,
      "step": 40400
    },
    {
      "epoch": 0.3938750006078318,
      "grad_norm": 0.24756556749343872,
      "learning_rate": 8.729166666666667e-05,
      "loss": 0.2887,
      "step": 40500
    },
    {
      "epoch": 0.39484753147353013,
      "grad_norm": 0.21755623817443848,
      "learning_rate": 8.725e-05,
      "loss": 0.2854,
      "step": 40600
    },
    {
      "epoch": 0.3958200623392285,
      "grad_norm": 0.21065185964107513,
      "learning_rate": 8.720833333333333e-05,
      "loss": 0.287,
      "step": 40700
    },
    {
      "epoch": 0.3967925932049268,
      "grad_norm": 0.22883208096027374,
      "learning_rate": 8.716666666666668e-05,
      "loss": 0.2878,
      "step": 40800
    },
    {
      "epoch": 0.3977651240706252,
      "grad_norm": 0.2345852106809616,
      "learning_rate": 8.7125e-05,
      "loss": 0.2868,
      "step": 40900
    },
    {
      "epoch": 0.3987376549363235,
      "grad_norm": 0.2182864099740982,
      "learning_rate": 8.708333333333334e-05,
      "loss": 0.2869,
      "step": 41000
    },
    {
      "epoch": 0.3997101858020219,
      "grad_norm": 0.2294345200061798,
      "learning_rate": 8.704166666666666e-05,
      "loss": 0.2862,
      "step": 41100
    },
    {
      "epoch": 0.40068271666772026,
      "grad_norm": 0.22198089957237244,
      "learning_rate": 8.7e-05,
      "loss": 0.2863,
      "step": 41200
    },
    {
      "epoch": 0.4016552475334186,
      "grad_norm": 0.2201153188943863,
      "learning_rate": 8.695833333333334e-05,
      "loss": 0.2858,
      "step": 41300
    },
    {
      "epoch": 0.40262777839911695,
      "grad_norm": 0.224227637052536,
      "learning_rate": 8.691666666666667e-05,
      "loss": 0.2864,
      "step": 41400
    },
    {
      "epoch": 0.40360030926481527,
      "grad_norm": 0.2373197376728058,
      "learning_rate": 8.687500000000001e-05,
      "loss": 0.2857,
      "step": 41500
    },
    {
      "epoch": 0.40457284013051364,
      "grad_norm": 0.2587372362613678,
      "learning_rate": 8.683333333333333e-05,
      "loss": 0.2875,
      "step": 41600
    },
    {
      "epoch": 0.405545370996212,
      "grad_norm": 0.22813494503498077,
      "learning_rate": 8.679166666666667e-05,
      "loss": 0.2866,
      "step": 41700
    },
    {
      "epoch": 0.40651790186191034,
      "grad_norm": 0.22430923581123352,
      "learning_rate": 8.675000000000001e-05,
      "loss": 0.2854,
      "step": 41800
    },
    {
      "epoch": 0.4074904327276087,
      "grad_norm": 0.23479129374027252,
      "learning_rate": 8.670833333333333e-05,
      "loss": 0.2857,
      "step": 41900
    },
    {
      "epoch": 0.408462963593307,
      "grad_norm": 0.21653422713279724,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.2855,
      "step": 42000
    },
    {
      "epoch": 0.4094354944590054,
      "grad_norm": 0.22388310730457306,
      "learning_rate": 8.6625e-05,
      "loss": 0.2872,
      "step": 42100
    },
    {
      "epoch": 0.4104080253247037,
      "grad_norm": 0.22617733478546143,
      "learning_rate": 8.658333333333334e-05,
      "loss": 0.2858,
      "step": 42200
    },
    {
      "epoch": 0.4113805561904021,
      "grad_norm": 0.21286672353744507,
      "learning_rate": 8.654166666666667e-05,
      "loss": 0.2856,
      "step": 42300
    },
    {
      "epoch": 0.41235308705610046,
      "grad_norm": 0.22880975902080536,
      "learning_rate": 8.65e-05,
      "loss": 0.2842,
      "step": 42400
    },
    {
      "epoch": 0.4133256179217988,
      "grad_norm": 0.23808443546295166,
      "learning_rate": 8.645833333333334e-05,
      "loss": 0.2872,
      "step": 42500
    },
    {
      "epoch": 0.41429814878749716,
      "grad_norm": 0.2445829063653946,
      "learning_rate": 8.641666666666666e-05,
      "loss": 0.2853,
      "step": 42600
    },
    {
      "epoch": 0.4152706796531955,
      "grad_norm": 0.23677681386470795,
      "learning_rate": 8.637500000000001e-05,
      "loss": 0.2844,
      "step": 42700
    },
    {
      "epoch": 0.41624321051889385,
      "grad_norm": 0.2285664975643158,
      "learning_rate": 8.633333333333334e-05,
      "loss": 0.2864,
      "step": 42800
    },
    {
      "epoch": 0.4172157413845922,
      "grad_norm": 0.2091132402420044,
      "learning_rate": 8.629166666666667e-05,
      "loss": 0.2841,
      "step": 42900
    },
    {
      "epoch": 0.41818827225029054,
      "grad_norm": 0.20357847213745117,
      "learning_rate": 8.625000000000001e-05,
      "loss": 0.2868,
      "step": 43000
    },
    {
      "epoch": 0.4191608031159889,
      "grad_norm": 0.22360746562480927,
      "learning_rate": 8.620833333333333e-05,
      "loss": 0.2846,
      "step": 43100
    },
    {
      "epoch": 0.42013333398168723,
      "grad_norm": 0.22769369184970856,
      "learning_rate": 8.616666666666667e-05,
      "loss": 0.2848,
      "step": 43200
    },
    {
      "epoch": 0.4211058648473856,
      "grad_norm": 0.221785306930542,
      "learning_rate": 8.6125e-05,
      "loss": 0.2854,
      "step": 43300
    },
    {
      "epoch": 0.4220783957130839,
      "grad_norm": 0.25617510080337524,
      "learning_rate": 8.608333333333334e-05,
      "loss": 0.284,
      "step": 43400
    },
    {
      "epoch": 0.4230509265787823,
      "grad_norm": 0.22574204206466675,
      "learning_rate": 8.604166666666668e-05,
      "loss": 0.2852,
      "step": 43500
    },
    {
      "epoch": 0.42402345744448067,
      "grad_norm": 0.22190921008586884,
      "learning_rate": 8.6e-05,
      "loss": 0.2844,
      "step": 43600
    },
    {
      "epoch": 0.424995988310179,
      "grad_norm": 0.22023499011993408,
      "learning_rate": 8.595833333333334e-05,
      "loss": 0.2848,
      "step": 43700
    },
    {
      "epoch": 0.42596851917587736,
      "grad_norm": 0.22469405829906464,
      "learning_rate": 8.591666666666666e-05,
      "loss": 0.2859,
      "step": 43800
    },
    {
      "epoch": 0.4269410500415757,
      "grad_norm": 0.215491384267807,
      "learning_rate": 8.5875e-05,
      "loss": 0.2844,
      "step": 43900
    },
    {
      "epoch": 0.42791358090727405,
      "grad_norm": 0.21941758692264557,
      "learning_rate": 8.583333333333334e-05,
      "loss": 0.2837,
      "step": 44000
    },
    {
      "epoch": 0.42888611177297237,
      "grad_norm": 0.22222831845283508,
      "learning_rate": 8.579166666666667e-05,
      "loss": 0.2852,
      "step": 44100
    },
    {
      "epoch": 0.42985864263867074,
      "grad_norm": 0.22483354806900024,
      "learning_rate": 8.575000000000001e-05,
      "loss": 0.2866,
      "step": 44200
    },
    {
      "epoch": 0.4308311735043691,
      "grad_norm": 0.24669137597084045,
      "learning_rate": 8.570833333333333e-05,
      "loss": 0.2851,
      "step": 44300
    },
    {
      "epoch": 0.43180370437006743,
      "grad_norm": 0.24056407809257507,
      "learning_rate": 8.566666666666667e-05,
      "loss": 0.2849,
      "step": 44400
    },
    {
      "epoch": 0.4327762352357658,
      "grad_norm": 0.22128067910671234,
      "learning_rate": 8.5625e-05,
      "loss": 0.2839,
      "step": 44500
    },
    {
      "epoch": 0.4337487661014641,
      "grad_norm": 0.21552687883377075,
      "learning_rate": 8.558333333333333e-05,
      "loss": 0.287,
      "step": 44600
    },
    {
      "epoch": 0.4347212969671625,
      "grad_norm": 0.2244190275669098,
      "learning_rate": 8.554166666666668e-05,
      "loss": 0.2837,
      "step": 44700
    },
    {
      "epoch": 0.43569382783286087,
      "grad_norm": 0.23316869139671326,
      "learning_rate": 8.55e-05,
      "loss": 0.284,
      "step": 44800
    },
    {
      "epoch": 0.4366663586985592,
      "grad_norm": 0.22696056962013245,
      "learning_rate": 8.545833333333334e-05,
      "loss": 0.2845,
      "step": 44900
    },
    {
      "epoch": 0.43763888956425756,
      "grad_norm": 0.2570878267288208,
      "learning_rate": 8.541666666666666e-05,
      "loss": 0.2834,
      "step": 45000
    },
    {
      "epoch": 0.4386114204299559,
      "grad_norm": 0.21637457609176636,
      "learning_rate": 8.5375e-05,
      "loss": 0.2858,
      "step": 45100
    },
    {
      "epoch": 0.43958395129565425,
      "grad_norm": 0.23630887269973755,
      "learning_rate": 8.533333333333334e-05,
      "loss": 0.2842,
      "step": 45200
    },
    {
      "epoch": 0.44055648216135257,
      "grad_norm": 0.228251114487648,
      "learning_rate": 8.529166666666668e-05,
      "loss": 0.2854,
      "step": 45300
    },
    {
      "epoch": 0.44152901302705094,
      "grad_norm": 0.2189309149980545,
      "learning_rate": 8.525000000000001e-05,
      "loss": 0.2838,
      "step": 45400
    },
    {
      "epoch": 0.4425015438927493,
      "grad_norm": 0.211065873503685,
      "learning_rate": 8.520833333333334e-05,
      "loss": 0.2848,
      "step": 45500
    },
    {
      "epoch": 0.44347407475844763,
      "grad_norm": 0.2433454692363739,
      "learning_rate": 8.516666666666667e-05,
      "loss": 0.2858,
      "step": 45600
    },
    {
      "epoch": 0.444446605624146,
      "grad_norm": 0.2260812222957611,
      "learning_rate": 8.5125e-05,
      "loss": 0.284,
      "step": 45700
    },
    {
      "epoch": 0.4454191364898443,
      "grad_norm": 0.215134397149086,
      "learning_rate": 8.508333333333333e-05,
      "loss": 0.2845,
      "step": 45800
    },
    {
      "epoch": 0.4463916673555427,
      "grad_norm": 0.20912860333919525,
      "learning_rate": 8.504166666666667e-05,
      "loss": 0.2828,
      "step": 45900
    },
    {
      "epoch": 0.44736419822124107,
      "grad_norm": 0.20574447512626648,
      "learning_rate": 8.5e-05,
      "loss": 0.2839,
      "step": 46000
    },
    {
      "epoch": 0.4483367290869394,
      "grad_norm": 0.2131207138299942,
      "learning_rate": 8.495833333333334e-05,
      "loss": 0.2838,
      "step": 46100
    },
    {
      "epoch": 0.44930925995263776,
      "grad_norm": 0.24141600728034973,
      "learning_rate": 8.491666666666667e-05,
      "loss": 0.2841,
      "step": 46200
    },
    {
      "epoch": 0.4502817908183361,
      "grad_norm": 0.22500662505626678,
      "learning_rate": 8.4875e-05,
      "loss": 0.2837,
      "step": 46300
    },
    {
      "epoch": 0.45125432168403445,
      "grad_norm": 0.2305113822221756,
      "learning_rate": 8.483333333333334e-05,
      "loss": 0.2839,
      "step": 46400
    },
    {
      "epoch": 0.45222685254973277,
      "grad_norm": 0.2179139405488968,
      "learning_rate": 8.479166666666666e-05,
      "loss": 0.2837,
      "step": 46500
    },
    {
      "epoch": 0.45319938341543115,
      "grad_norm": 0.2199602574110031,
      "learning_rate": 8.475000000000001e-05,
      "loss": 0.2828,
      "step": 46600
    },
    {
      "epoch": 0.4541719142811295,
      "grad_norm": 0.20976881682872772,
      "learning_rate": 8.470833333333334e-05,
      "loss": 0.2843,
      "step": 46700
    },
    {
      "epoch": 0.45514444514682784,
      "grad_norm": 0.22357934713363647,
      "learning_rate": 8.466666666666667e-05,
      "loss": 0.2834,
      "step": 46800
    },
    {
      "epoch": 0.4561169760125262,
      "grad_norm": 0.2342623472213745,
      "learning_rate": 8.4625e-05,
      "loss": 0.2834,
      "step": 46900
    },
    {
      "epoch": 0.4570895068782245,
      "grad_norm": 0.21757104992866516,
      "learning_rate": 8.458333333333333e-05,
      "loss": 0.283,
      "step": 47000
    },
    {
      "epoch": 0.4580620377439229,
      "grad_norm": 0.20770405232906342,
      "learning_rate": 8.454166666666667e-05,
      "loss": 0.2838,
      "step": 47100
    },
    {
      "epoch": 0.4590345686096213,
      "grad_norm": 0.22410747408866882,
      "learning_rate": 8.450000000000001e-05,
      "loss": 0.2839,
      "step": 47200
    },
    {
      "epoch": 0.4600070994753196,
      "grad_norm": 0.22451195120811462,
      "learning_rate": 8.445833333333334e-05,
      "loss": 0.2833,
      "step": 47300
    },
    {
      "epoch": 0.46097963034101797,
      "grad_norm": 0.2169613540172577,
      "learning_rate": 8.441666666666667e-05,
      "loss": 0.2827,
      "step": 47400
    },
    {
      "epoch": 0.4619521612067163,
      "grad_norm": 0.20910429954528809,
      "learning_rate": 8.4375e-05,
      "loss": 0.284,
      "step": 47500
    },
    {
      "epoch": 0.46292469207241466,
      "grad_norm": 0.20809797942638397,
      "learning_rate": 8.433333333333334e-05,
      "loss": 0.2814,
      "step": 47600
    },
    {
      "epoch": 0.463897222938113,
      "grad_norm": 0.2489084154367447,
      "learning_rate": 8.429166666666666e-05,
      "loss": 0.2859,
      "step": 47700
    },
    {
      "epoch": 0.46486975380381135,
      "grad_norm": 0.21316811442375183,
      "learning_rate": 8.425e-05,
      "loss": 0.2848,
      "step": 47800
    },
    {
      "epoch": 0.4658422846695097,
      "grad_norm": 0.22219440340995789,
      "learning_rate": 8.420833333333334e-05,
      "loss": 0.2839,
      "step": 47900
    },
    {
      "epoch": 0.46681481553520804,
      "grad_norm": 0.2344864010810852,
      "learning_rate": 8.416666666666668e-05,
      "loss": 0.2838,
      "step": 48000
    },
    {
      "epoch": 0.4677873464009064,
      "grad_norm": 0.23795531690120697,
      "learning_rate": 8.412500000000001e-05,
      "loss": 0.2842,
      "step": 48100
    },
    {
      "epoch": 0.46875987726660473,
      "grad_norm": 0.20211488008499146,
      "learning_rate": 8.408333333333334e-05,
      "loss": 0.2839,
      "step": 48200
    },
    {
      "epoch": 0.4697324081323031,
      "grad_norm": 0.22309939563274384,
      "learning_rate": 8.404166666666667e-05,
      "loss": 0.284,
      "step": 48300
    },
    {
      "epoch": 0.4707049389980015,
      "grad_norm": 0.2087896466255188,
      "learning_rate": 8.4e-05,
      "loss": 0.2831,
      "step": 48400
    },
    {
      "epoch": 0.4716774698636998,
      "grad_norm": 0.22950294613838196,
      "learning_rate": 8.395833333333333e-05,
      "loss": 0.2839,
      "step": 48500
    },
    {
      "epoch": 0.47265000072939817,
      "grad_norm": 0.2277429699897766,
      "learning_rate": 8.391666666666667e-05,
      "loss": 0.2808,
      "step": 48600
    },
    {
      "epoch": 0.4736225315950965,
      "grad_norm": 0.2154340296983719,
      "learning_rate": 8.3875e-05,
      "loss": 0.2847,
      "step": 48700
    },
    {
      "epoch": 0.47459506246079486,
      "grad_norm": 0.2349119335412979,
      "learning_rate": 8.383333333333334e-05,
      "loss": 0.2828,
      "step": 48800
    },
    {
      "epoch": 0.4755675933264932,
      "grad_norm": 0.21176700294017792,
      "learning_rate": 8.379166666666667e-05,
      "loss": 0.2841,
      "step": 48900
    },
    {
      "epoch": 0.47654012419219155,
      "grad_norm": 0.1967632919549942,
      "learning_rate": 8.375e-05,
      "loss": 0.2816,
      "step": 49000
    },
    {
      "epoch": 0.4775126550578899,
      "grad_norm": 0.21489425003528595,
      "learning_rate": 8.370833333333334e-05,
      "loss": 0.2824,
      "step": 49100
    },
    {
      "epoch": 0.47848518592358824,
      "grad_norm": 0.22650983929634094,
      "learning_rate": 8.366666666666668e-05,
      "loss": 0.2805,
      "step": 49200
    },
    {
      "epoch": 0.4794577167892866,
      "grad_norm": 0.2580493986606598,
      "learning_rate": 8.362500000000001e-05,
      "loss": 0.2836,
      "step": 49300
    },
    {
      "epoch": 0.48043024765498493,
      "grad_norm": 0.21940484642982483,
      "learning_rate": 8.358333333333334e-05,
      "loss": 0.281,
      "step": 49400
    },
    {
      "epoch": 0.4814027785206833,
      "grad_norm": 0.22072041034698486,
      "learning_rate": 8.354166666666667e-05,
      "loss": 0.2834,
      "step": 49500
    },
    {
      "epoch": 0.4823753093863816,
      "grad_norm": 0.218195378780365,
      "learning_rate": 8.35e-05,
      "loss": 0.2804,
      "step": 49600
    },
    {
      "epoch": 0.48334784025208,
      "grad_norm": 0.20461131632328033,
      "learning_rate": 8.345833333333333e-05,
      "loss": 0.282,
      "step": 49700
    },
    {
      "epoch": 0.48432037111777837,
      "grad_norm": 0.22906650602817535,
      "learning_rate": 8.341666666666667e-05,
      "loss": 0.2796,
      "step": 49800
    },
    {
      "epoch": 0.4852929019834767,
      "grad_norm": 0.22134211659431458,
      "learning_rate": 8.337500000000001e-05,
      "loss": 0.2807,
      "step": 49900
    },
    {
      "epoch": 0.48626543284917506,
      "grad_norm": 0.20638519525527954,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.2808,
      "step": 50000
    },
    {
      "epoch": 0.48626543284917506,
      "eval_loss": 0.28212738037109375,
      "eval_runtime": 3133.1343,
      "eval_samples_per_second": 729.222,
      "eval_steps_per_second": 7.292,
      "step": 50000
    },
    {
      "epoch": 0.4872379637148734,
      "grad_norm": 0.22744187712669373,
      "learning_rate": 8.329166666666667e-05,
      "loss": 0.2809,
      "step": 50100
    },
    {
      "epoch": 0.48821049458057175,
      "grad_norm": 0.22519299387931824,
      "learning_rate": 8.325e-05,
      "loss": 0.2816,
      "step": 50200
    },
    {
      "epoch": 0.4891830254462701,
      "grad_norm": 0.220259428024292,
      "learning_rate": 8.320833333333333e-05,
      "loss": 0.2817,
      "step": 50300
    },
    {
      "epoch": 0.49015555631196844,
      "grad_norm": 0.22512568533420563,
      "learning_rate": 8.316666666666666e-05,
      "loss": 0.2807,
      "step": 50400
    },
    {
      "epoch": 0.4911280871776668,
      "grad_norm": 0.20364037156105042,
      "learning_rate": 8.312500000000001e-05,
      "loss": 0.2812,
      "step": 50500
    },
    {
      "epoch": 0.49210061804336513,
      "grad_norm": 0.21056635677814484,
      "learning_rate": 8.308333333333334e-05,
      "loss": 0.2819,
      "step": 50600
    },
    {
      "epoch": 0.4930731489090635,
      "grad_norm": 0.2323596179485321,
      "learning_rate": 8.304166666666667e-05,
      "loss": 0.2828,
      "step": 50700
    },
    {
      "epoch": 0.4940456797747618,
      "grad_norm": 0.20901747047901154,
      "learning_rate": 8.3e-05,
      "loss": 0.2813,
      "step": 50800
    },
    {
      "epoch": 0.4950182106404602,
      "grad_norm": 0.2210780680179596,
      "learning_rate": 8.295833333333333e-05,
      "loss": 0.2816,
      "step": 50900
    },
    {
      "epoch": 0.4959907415061586,
      "grad_norm": 0.2139698565006256,
      "learning_rate": 8.291666666666667e-05,
      "loss": 0.2813,
      "step": 51000
    },
    {
      "epoch": 0.4969632723718569,
      "grad_norm": 0.22663290798664093,
      "learning_rate": 8.287500000000001e-05,
      "loss": 0.2824,
      "step": 51100
    },
    {
      "epoch": 0.49793580323755526,
      "grad_norm": 0.23406846821308136,
      "learning_rate": 8.283333333333335e-05,
      "loss": 0.2814,
      "step": 51200
    },
    {
      "epoch": 0.4989083341032536,
      "grad_norm": 0.2461007833480835,
      "learning_rate": 8.279166666666667e-05,
      "loss": 0.2822,
      "step": 51300
    },
    {
      "epoch": 0.49988086496895195,
      "grad_norm": 0.21302881836891174,
      "learning_rate": 8.275e-05,
      "loss": 0.2806,
      "step": 51400
    },
    {
      "epoch": 0.5008533958346503,
      "grad_norm": 0.23353607952594757,
      "learning_rate": 8.270833333333333e-05,
      "loss": 0.2822,
      "step": 51500
    },
    {
      "epoch": 0.5018259267003486,
      "grad_norm": 0.23186154663562775,
      "learning_rate": 8.266666666666667e-05,
      "loss": 0.2812,
      "step": 51600
    },
    {
      "epoch": 0.502798457566047,
      "grad_norm": 0.21299658715724945,
      "learning_rate": 8.2625e-05,
      "loss": 0.2821,
      "step": 51700
    },
    {
      "epoch": 0.5037709884317454,
      "grad_norm": 0.21252137422561646,
      "learning_rate": 8.258333333333334e-05,
      "loss": 0.2815,
      "step": 51800
    },
    {
      "epoch": 0.5047435192974437,
      "grad_norm": 0.22656264901161194,
      "learning_rate": 8.254166666666668e-05,
      "loss": 0.2823,
      "step": 51900
    },
    {
      "epoch": 0.505716050163142,
      "grad_norm": 0.23454535007476807,
      "learning_rate": 8.25e-05,
      "loss": 0.2805,
      "step": 52000
    },
    {
      "epoch": 0.5066885810288404,
      "grad_norm": 0.2213151901960373,
      "learning_rate": 8.245833333333334e-05,
      "loss": 0.2804,
      "step": 52100
    },
    {
      "epoch": 0.5076611118945388,
      "grad_norm": 0.22490741312503815,
      "learning_rate": 8.241666666666667e-05,
      "loss": 0.2794,
      "step": 52200
    },
    {
      "epoch": 0.5086336427602371,
      "grad_norm": 0.2081965208053589,
      "learning_rate": 8.2375e-05,
      "loss": 0.2821,
      "step": 52300
    },
    {
      "epoch": 0.5096061736259354,
      "grad_norm": 0.18744675815105438,
      "learning_rate": 8.233333333333333e-05,
      "loss": 0.2818,
      "step": 52400
    },
    {
      "epoch": 0.5105787044916338,
      "grad_norm": 0.21308951079845428,
      "learning_rate": 8.229166666666667e-05,
      "loss": 0.2815,
      "step": 52500
    },
    {
      "epoch": 0.5115512353573322,
      "grad_norm": 0.21926720440387726,
      "learning_rate": 8.225000000000001e-05,
      "loss": 0.2825,
      "step": 52600
    },
    {
      "epoch": 0.5125237662230305,
      "grad_norm": 0.2223844975233078,
      "learning_rate": 8.220833333333334e-05,
      "loss": 0.2812,
      "step": 52700
    },
    {
      "epoch": 0.5134962970887289,
      "grad_norm": 0.2221844643354416,
      "learning_rate": 8.216666666666667e-05,
      "loss": 0.2793,
      "step": 52800
    },
    {
      "epoch": 0.5144688279544272,
      "grad_norm": 0.21201279759407043,
      "learning_rate": 8.2125e-05,
      "loss": 0.2825,
      "step": 52900
    },
    {
      "epoch": 0.5154413588201255,
      "grad_norm": 0.2244330793619156,
      "learning_rate": 8.208333333333334e-05,
      "loss": 0.283,
      "step": 53000
    },
    {
      "epoch": 0.5164138896858239,
      "grad_norm": 0.22637596726417542,
      "learning_rate": 8.204166666666668e-05,
      "loss": 0.2811,
      "step": 53100
    },
    {
      "epoch": 0.5173864205515223,
      "grad_norm": 0.22265732288360596,
      "learning_rate": 8.2e-05,
      "loss": 0.2803,
      "step": 53200
    },
    {
      "epoch": 0.5183589514172207,
      "grad_norm": 0.24311281740665436,
      "learning_rate": 8.195833333333334e-05,
      "loss": 0.2814,
      "step": 53300
    },
    {
      "epoch": 0.5193314822829189,
      "grad_norm": 0.20241095125675201,
      "learning_rate": 8.191666666666667e-05,
      "loss": 0.282,
      "step": 53400
    },
    {
      "epoch": 0.5203040131486173,
      "grad_norm": 0.23274974524974823,
      "learning_rate": 8.1875e-05,
      "loss": 0.2803,
      "step": 53500
    },
    {
      "epoch": 0.5212765440143157,
      "grad_norm": 0.22515998780727386,
      "learning_rate": 8.183333333333333e-05,
      "loss": 0.2813,
      "step": 53600
    },
    {
      "epoch": 0.522249074880014,
      "grad_norm": 0.23559625446796417,
      "learning_rate": 8.179166666666667e-05,
      "loss": 0.2791,
      "step": 53700
    },
    {
      "epoch": 0.5232216057457123,
      "grad_norm": 0.21829408407211304,
      "learning_rate": 8.175000000000001e-05,
      "loss": 0.2826,
      "step": 53800
    },
    {
      "epoch": 0.5241941366114107,
      "grad_norm": 0.20003020763397217,
      "learning_rate": 8.170833333333335e-05,
      "loss": 0.2828,
      "step": 53900
    },
    {
      "epoch": 0.525166667477109,
      "grad_norm": 0.22315917909145355,
      "learning_rate": 8.166666666666667e-05,
      "loss": 0.2808,
      "step": 54000
    },
    {
      "epoch": 0.5261391983428074,
      "grad_norm": 0.22702158987522125,
      "learning_rate": 8.1625e-05,
      "loss": 0.2795,
      "step": 54100
    },
    {
      "epoch": 0.5271117292085058,
      "grad_norm": 0.19760891795158386,
      "learning_rate": 8.158333333333333e-05,
      "loss": 0.2808,
      "step": 54200
    },
    {
      "epoch": 0.5280842600742041,
      "grad_norm": 0.20714426040649414,
      "learning_rate": 8.154166666666667e-05,
      "loss": 0.2817,
      "step": 54300
    },
    {
      "epoch": 0.5290567909399024,
      "grad_norm": 0.21994976699352264,
      "learning_rate": 8.15e-05,
      "loss": 0.2795,
      "step": 54400
    },
    {
      "epoch": 0.5300293218056008,
      "grad_norm": 0.20640702545642853,
      "learning_rate": 8.145833333333334e-05,
      "loss": 0.2803,
      "step": 54500
    },
    {
      "epoch": 0.5310018526712992,
      "grad_norm": 0.19372232258319855,
      "learning_rate": 8.141666666666668e-05,
      "loss": 0.2805,
      "step": 54600
    },
    {
      "epoch": 0.5319743835369976,
      "grad_norm": 0.21093536913394928,
      "learning_rate": 8.1375e-05,
      "loss": 0.2785,
      "step": 54700
    },
    {
      "epoch": 0.5329469144026958,
      "grad_norm": 0.22199329733848572,
      "learning_rate": 8.133333333333334e-05,
      "loss": 0.2802,
      "step": 54800
    },
    {
      "epoch": 0.5339194452683942,
      "grad_norm": 0.22758963704109192,
      "learning_rate": 8.129166666666666e-05,
      "loss": 0.2805,
      "step": 54900
    },
    {
      "epoch": 0.5348919761340926,
      "grad_norm": 0.22035165131092072,
      "learning_rate": 8.125000000000001e-05,
      "loss": 0.2808,
      "step": 55000
    },
    {
      "epoch": 0.5358645069997909,
      "grad_norm": 0.2382200062274933,
      "learning_rate": 8.120833333333335e-05,
      "loss": 0.2789,
      "step": 55100
    },
    {
      "epoch": 0.5368370378654893,
      "grad_norm": 0.2327306717634201,
      "learning_rate": 8.116666666666667e-05,
      "loss": 0.2798,
      "step": 55200
    },
    {
      "epoch": 0.5378095687311876,
      "grad_norm": 0.23207610845565796,
      "learning_rate": 8.112500000000001e-05,
      "loss": 0.2792,
      "step": 55300
    },
    {
      "epoch": 0.5387820995968859,
      "grad_norm": 0.20501285791397095,
      "learning_rate": 8.108333333333333e-05,
      "loss": 0.2801,
      "step": 55400
    },
    {
      "epoch": 0.5397546304625843,
      "grad_norm": 0.22380192577838898,
      "learning_rate": 8.104166666666667e-05,
      "loss": 0.2809,
      "step": 55500
    },
    {
      "epoch": 0.5407271613282827,
      "grad_norm": 0.20583952963352203,
      "learning_rate": 8.1e-05,
      "loss": 0.2816,
      "step": 55600
    },
    {
      "epoch": 0.541699692193981,
      "grad_norm": 0.22530880570411682,
      "learning_rate": 8.095833333333334e-05,
      "loss": 0.2797,
      "step": 55700
    },
    {
      "epoch": 0.5426722230596793,
      "grad_norm": 0.21367307007312775,
      "learning_rate": 8.091666666666668e-05,
      "loss": 0.2822,
      "step": 55800
    },
    {
      "epoch": 0.5436447539253777,
      "grad_norm": 0.20810258388519287,
      "learning_rate": 8.0875e-05,
      "loss": 0.2791,
      "step": 55900
    },
    {
      "epoch": 0.5446172847910761,
      "grad_norm": 0.21525560319423676,
      "learning_rate": 8.083333333333334e-05,
      "loss": 0.278,
      "step": 56000
    },
    {
      "epoch": 0.5455898156567744,
      "grad_norm": 0.2218308001756668,
      "learning_rate": 8.079166666666666e-05,
      "loss": 0.2807,
      "step": 56100
    },
    {
      "epoch": 0.5465623465224727,
      "grad_norm": 0.21163582801818848,
      "learning_rate": 8.075e-05,
      "loss": 0.2807,
      "step": 56200
    },
    {
      "epoch": 0.5475348773881711,
      "grad_norm": 0.19277098774909973,
      "learning_rate": 8.070833333333335e-05,
      "loss": 0.278,
      "step": 56300
    },
    {
      "epoch": 0.5485074082538695,
      "grad_norm": 0.20550067722797394,
      "learning_rate": 8.066666666666667e-05,
      "loss": 0.2791,
      "step": 56400
    },
    {
      "epoch": 0.5494799391195678,
      "grad_norm": 0.206670880317688,
      "learning_rate": 8.062500000000001e-05,
      "loss": 0.279,
      "step": 56500
    },
    {
      "epoch": 0.5504524699852662,
      "grad_norm": 0.22239089012145996,
      "learning_rate": 8.058333333333333e-05,
      "loss": 0.2788,
      "step": 56600
    },
    {
      "epoch": 0.5514250008509645,
      "grad_norm": 0.20519809424877167,
      "learning_rate": 8.054166666666667e-05,
      "loss": 0.2796,
      "step": 56700
    },
    {
      "epoch": 0.5523975317166628,
      "grad_norm": 0.2089201956987381,
      "learning_rate": 8.05e-05,
      "loss": 0.2789,
      "step": 56800
    },
    {
      "epoch": 0.5533700625823612,
      "grad_norm": 0.20794880390167236,
      "learning_rate": 8.045833333333334e-05,
      "loss": 0.2791,
      "step": 56900
    },
    {
      "epoch": 0.5543425934480596,
      "grad_norm": 0.20731692016124725,
      "learning_rate": 8.041666666666668e-05,
      "loss": 0.279,
      "step": 57000
    },
    {
      "epoch": 0.555315124313758,
      "grad_norm": 0.2177899330854416,
      "learning_rate": 8.0375e-05,
      "loss": 0.2776,
      "step": 57100
    },
    {
      "epoch": 0.5562876551794562,
      "grad_norm": 0.21141879260540009,
      "learning_rate": 8.033333333333334e-05,
      "loss": 0.2782,
      "step": 57200
    },
    {
      "epoch": 0.5572601860451546,
      "grad_norm": 0.2573419213294983,
      "learning_rate": 8.029166666666666e-05,
      "loss": 0.2783,
      "step": 57300
    },
    {
      "epoch": 0.558232716910853,
      "grad_norm": 0.2194548398256302,
      "learning_rate": 8.025e-05,
      "loss": 0.2774,
      "step": 57400
    },
    {
      "epoch": 0.5592052477765513,
      "grad_norm": 0.23357753455638885,
      "learning_rate": 8.020833333333334e-05,
      "loss": 0.2788,
      "step": 57500
    },
    {
      "epoch": 0.5601777786422497,
      "grad_norm": 0.20239321887493134,
      "learning_rate": 8.016666666666667e-05,
      "loss": 0.2803,
      "step": 57600
    },
    {
      "epoch": 0.561150309507948,
      "grad_norm": 0.2139279991388321,
      "learning_rate": 8.012500000000001e-05,
      "loss": 0.2782,
      "step": 57700
    },
    {
      "epoch": 0.5621228403736463,
      "grad_norm": 0.22376514971256256,
      "learning_rate": 8.008333333333333e-05,
      "loss": 0.2794,
      "step": 57800
    },
    {
      "epoch": 0.5630953712393447,
      "grad_norm": 0.2202487289905548,
      "learning_rate": 8.004166666666667e-05,
      "loss": 0.2804,
      "step": 57900
    },
    {
      "epoch": 0.5640679021050431,
      "grad_norm": 0.2110002040863037,
      "learning_rate": 8e-05,
      "loss": 0.2786,
      "step": 58000
    },
    {
      "epoch": 0.5650404329707414,
      "grad_norm": 0.21412590146064758,
      "learning_rate": 7.995833333333333e-05,
      "loss": 0.2797,
      "step": 58100
    },
    {
      "epoch": 0.5660129638364397,
      "grad_norm": 0.20784977078437805,
      "learning_rate": 7.991666666666667e-05,
      "loss": 0.2789,
      "step": 58200
    },
    {
      "epoch": 0.5669854947021381,
      "grad_norm": 0.21609677374362946,
      "learning_rate": 7.9875e-05,
      "loss": 0.2805,
      "step": 58300
    },
    {
      "epoch": 0.5679580255678365,
      "grad_norm": 0.22699305415153503,
      "learning_rate": 7.983333333333334e-05,
      "loss": 0.2782,
      "step": 58400
    },
    {
      "epoch": 0.5689305564335349,
      "grad_norm": 0.19172540307044983,
      "learning_rate": 7.979166666666668e-05,
      "loss": 0.279,
      "step": 58500
    },
    {
      "epoch": 0.5699030872992331,
      "grad_norm": 0.210921972990036,
      "learning_rate": 7.975e-05,
      "loss": 0.2792,
      "step": 58600
    },
    {
      "epoch": 0.5708756181649315,
      "grad_norm": 0.20235320925712585,
      "learning_rate": 7.970833333333334e-05,
      "loss": 0.2793,
      "step": 58700
    },
    {
      "epoch": 0.5718481490306299,
      "grad_norm": 0.21075908839702606,
      "learning_rate": 7.966666666666666e-05,
      "loss": 0.2788,
      "step": 58800
    },
    {
      "epoch": 0.5728206798963282,
      "grad_norm": 0.2301768660545349,
      "learning_rate": 7.962500000000001e-05,
      "loss": 0.2791,
      "step": 58900
    },
    {
      "epoch": 0.5737932107620266,
      "grad_norm": 0.1919289082288742,
      "learning_rate": 7.958333333333333e-05,
      "loss": 0.2786,
      "step": 59000
    },
    {
      "epoch": 0.5747657416277249,
      "grad_norm": 0.20522449910640717,
      "learning_rate": 7.954166666666667e-05,
      "loss": 0.2798,
      "step": 59100
    },
    {
      "epoch": 0.5757382724934232,
      "grad_norm": 0.21856504678726196,
      "learning_rate": 7.950000000000001e-05,
      "loss": 0.2785,
      "step": 59200
    },
    {
      "epoch": 0.5767108033591216,
      "grad_norm": 0.21249370276927948,
      "learning_rate": 7.945833333333333e-05,
      "loss": 0.2782,
      "step": 59300
    },
    {
      "epoch": 0.57768333422482,
      "grad_norm": 0.2294391393661499,
      "learning_rate": 7.941666666666667e-05,
      "loss": 0.2807,
      "step": 59400
    },
    {
      "epoch": 0.5786558650905184,
      "grad_norm": 0.20459343492984772,
      "learning_rate": 7.9375e-05,
      "loss": 0.2786,
      "step": 59500
    },
    {
      "epoch": 0.5796283959562166,
      "grad_norm": 0.21943248808383942,
      "learning_rate": 7.933333333333334e-05,
      "loss": 0.2783,
      "step": 59600
    },
    {
      "epoch": 0.580600926821915,
      "grad_norm": 0.2187725454568863,
      "learning_rate": 7.929166666666668e-05,
      "loss": 0.2775,
      "step": 59700
    },
    {
      "epoch": 0.5815734576876134,
      "grad_norm": 0.20831115543842316,
      "learning_rate": 7.925e-05,
      "loss": 0.2768,
      "step": 59800
    },
    {
      "epoch": 0.5825459885533117,
      "grad_norm": 0.2292390912771225,
      "learning_rate": 7.920833333333334e-05,
      "loss": 0.2784,
      "step": 59900
    },
    {
      "epoch": 0.58351851941901,
      "grad_norm": 0.21877196431159973,
      "learning_rate": 7.916666666666666e-05,
      "loss": 0.2781,
      "step": 60000
    },
    {
      "epoch": 0.58351851941901,
      "eval_loss": 0.27845078706741333,
      "eval_runtime": 3187.8515,
      "eval_samples_per_second": 716.705,
      "eval_steps_per_second": 7.167,
      "step": 60000
    },
    {
      "epoch": 0.5844910502847084,
      "grad_norm": 0.21604232490062714,
      "learning_rate": 7.9125e-05,
      "loss": 0.279,
      "step": 60100
    },
    {
      "epoch": 0.5854635811504068,
      "grad_norm": 0.20910006761550903,
      "learning_rate": 7.908333333333335e-05,
      "loss": 0.2796,
      "step": 60200
    },
    {
      "epoch": 0.5864361120161051,
      "grad_norm": 0.21536727249622345,
      "learning_rate": 7.904166666666667e-05,
      "loss": 0.2787,
      "step": 60300
    },
    {
      "epoch": 0.5874086428818035,
      "grad_norm": 0.21708761155605316,
      "learning_rate": 7.900000000000001e-05,
      "loss": 0.2785,
      "step": 60400
    },
    {
      "epoch": 0.5883811737475018,
      "grad_norm": 0.20533901453018188,
      "learning_rate": 7.895833333333333e-05,
      "loss": 0.28,
      "step": 60500
    },
    {
      "epoch": 0.5893537046132001,
      "grad_norm": 0.19677278399467468,
      "learning_rate": 7.891666666666667e-05,
      "loss": 0.2799,
      "step": 60600
    },
    {
      "epoch": 0.5903262354788985,
      "grad_norm": 0.21951694786548615,
      "learning_rate": 7.887499999999999e-05,
      "loss": 0.2787,
      "step": 60700
    },
    {
      "epoch": 0.5912987663445969,
      "grad_norm": 0.21376292407512665,
      "learning_rate": 7.883333333333334e-05,
      "loss": 0.2788,
      "step": 60800
    },
    {
      "epoch": 0.5922712972102953,
      "grad_norm": 0.22222831845283508,
      "learning_rate": 7.879166666666668e-05,
      "loss": 0.2773,
      "step": 60900
    },
    {
      "epoch": 0.5932438280759935,
      "grad_norm": 0.21173995733261108,
      "learning_rate": 7.875e-05,
      "loss": 0.2775,
      "step": 61000
    },
    {
      "epoch": 0.5942163589416919,
      "grad_norm": 0.20626892149448395,
      "learning_rate": 7.870833333333334e-05,
      "loss": 0.2773,
      "step": 61100
    },
    {
      "epoch": 0.5951888898073903,
      "grad_norm": 0.2053682804107666,
      "learning_rate": 7.866666666666666e-05,
      "loss": 0.2797,
      "step": 61200
    },
    {
      "epoch": 0.5961614206730886,
      "grad_norm": 0.19695016741752625,
      "learning_rate": 7.8625e-05,
      "loss": 0.276,
      "step": 61300
    },
    {
      "epoch": 0.597133951538787,
      "grad_norm": 0.210657000541687,
      "learning_rate": 7.858333333333334e-05,
      "loss": 0.2783,
      "step": 61400
    },
    {
      "epoch": 0.5981064824044853,
      "grad_norm": 0.2140790969133377,
      "learning_rate": 7.854166666666667e-05,
      "loss": 0.2771,
      "step": 61500
    },
    {
      "epoch": 0.5990790132701836,
      "grad_norm": 0.1814352571964264,
      "learning_rate": 7.850000000000001e-05,
      "loss": 0.2775,
      "step": 61600
    },
    {
      "epoch": 0.600051544135882,
      "grad_norm": 0.183314248919487,
      "learning_rate": 7.845833333333333e-05,
      "loss": 0.2777,
      "step": 61700
    },
    {
      "epoch": 0.6010240750015804,
      "grad_norm": 0.20968088507652283,
      "learning_rate": 7.841666666666667e-05,
      "loss": 0.2763,
      "step": 61800
    },
    {
      "epoch": 0.6019966058672788,
      "grad_norm": 0.22200822830200195,
      "learning_rate": 7.8375e-05,
      "loss": 0.2764,
      "step": 61900
    },
    {
      "epoch": 0.602969136732977,
      "grad_norm": 0.19491001963615417,
      "learning_rate": 7.833333333333333e-05,
      "loss": 0.2764,
      "step": 62000
    },
    {
      "epoch": 0.6039416675986754,
      "grad_norm": 0.2087949514389038,
      "learning_rate": 7.829166666666667e-05,
      "loss": 0.2777,
      "step": 62100
    },
    {
      "epoch": 0.6049141984643738,
      "grad_norm": 0.2071024626493454,
      "learning_rate": 7.825e-05,
      "loss": 0.2788,
      "step": 62200
    },
    {
      "epoch": 0.6058867293300721,
      "grad_norm": 0.2029825747013092,
      "learning_rate": 7.820833333333334e-05,
      "loss": 0.2774,
      "step": 62300
    },
    {
      "epoch": 0.6068592601957704,
      "grad_norm": 0.2262592315673828,
      "learning_rate": 7.816666666666666e-05,
      "loss": 0.2779,
      "step": 62400
    },
    {
      "epoch": 0.6078317910614688,
      "grad_norm": 0.20250137150287628,
      "learning_rate": 7.8125e-05,
      "loss": 0.278,
      "step": 62500
    },
    {
      "epoch": 0.6088043219271672,
      "grad_norm": 0.21563051640987396,
      "learning_rate": 7.808333333333334e-05,
      "loss": 0.2765,
      "step": 62600
    },
    {
      "epoch": 0.6097768527928655,
      "grad_norm": 0.22470813989639282,
      "learning_rate": 7.804166666666666e-05,
      "loss": 0.2775,
      "step": 62700
    },
    {
      "epoch": 0.6107493836585639,
      "grad_norm": 0.23657700419425964,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.2786,
      "step": 62800
    },
    {
      "epoch": 0.6117219145242622,
      "grad_norm": 0.21368642151355743,
      "learning_rate": 7.795833333333334e-05,
      "loss": 0.2782,
      "step": 62900
    },
    {
      "epoch": 0.6126944453899605,
      "grad_norm": 0.23055987060070038,
      "learning_rate": 7.791666666666667e-05,
      "loss": 0.2777,
      "step": 63000
    },
    {
      "epoch": 0.6136669762556589,
      "grad_norm": 0.2092658430337906,
      "learning_rate": 7.787500000000001e-05,
      "loss": 0.2775,
      "step": 63100
    },
    {
      "epoch": 0.6146395071213573,
      "grad_norm": 0.20356041193008423,
      "learning_rate": 7.783333333333333e-05,
      "loss": 0.2806,
      "step": 63200
    },
    {
      "epoch": 0.6156120379870557,
      "grad_norm": 0.2073959857225418,
      "learning_rate": 7.779166666666667e-05,
      "loss": 0.2771,
      "step": 63300
    },
    {
      "epoch": 0.6165845688527539,
      "grad_norm": 0.20169280469417572,
      "learning_rate": 7.775e-05,
      "loss": 0.2791,
      "step": 63400
    },
    {
      "epoch": 0.6175570997184523,
      "grad_norm": 0.20668599009513855,
      "learning_rate": 7.770833333333334e-05,
      "loss": 0.2775,
      "step": 63500
    },
    {
      "epoch": 0.6185296305841507,
      "grad_norm": 0.21659916639328003,
      "learning_rate": 7.766666666666667e-05,
      "loss": 0.2775,
      "step": 63600
    },
    {
      "epoch": 0.619502161449849,
      "grad_norm": 0.2110241800546646,
      "learning_rate": 7.7625e-05,
      "loss": 0.2765,
      "step": 63700
    },
    {
      "epoch": 0.6204746923155474,
      "grad_norm": 0.23211480677127838,
      "learning_rate": 7.758333333333334e-05,
      "loss": 0.2774,
      "step": 63800
    },
    {
      "epoch": 0.6214472231812457,
      "grad_norm": 0.2429051548242569,
      "learning_rate": 7.754166666666666e-05,
      "loss": 0.2761,
      "step": 63900
    },
    {
      "epoch": 0.622419754046944,
      "grad_norm": 0.2217550277709961,
      "learning_rate": 7.75e-05,
      "loss": 0.2771,
      "step": 64000
    },
    {
      "epoch": 0.6233922849126424,
      "grad_norm": 0.23588886857032776,
      "learning_rate": 7.745833333333334e-05,
      "loss": 0.2804,
      "step": 64100
    },
    {
      "epoch": 0.6243648157783408,
      "grad_norm": 0.22043974697589874,
      "learning_rate": 7.741666666666667e-05,
      "loss": 0.2762,
      "step": 64200
    },
    {
      "epoch": 0.6253373466440391,
      "grad_norm": 0.22469045221805573,
      "learning_rate": 7.737500000000001e-05,
      "loss": 0.2773,
      "step": 64300
    },
    {
      "epoch": 0.6263098775097374,
      "grad_norm": 0.21922391653060913,
      "learning_rate": 7.733333333333333e-05,
      "loss": 0.2762,
      "step": 64400
    },
    {
      "epoch": 0.6272824083754358,
      "grad_norm": 0.2039521485567093,
      "learning_rate": 7.729166666666667e-05,
      "loss": 0.2784,
      "step": 64500
    },
    {
      "epoch": 0.6282549392411342,
      "grad_norm": 0.21670381724834442,
      "learning_rate": 7.725e-05,
      "loss": 0.2784,
      "step": 64600
    },
    {
      "epoch": 0.6292274701068326,
      "grad_norm": 0.1990271806716919,
      "learning_rate": 7.720833333333334e-05,
      "loss": 0.2778,
      "step": 64700
    },
    {
      "epoch": 0.6302000009725308,
      "grad_norm": 0.2069244086742401,
      "learning_rate": 7.716666666666667e-05,
      "loss": 0.2768,
      "step": 64800
    },
    {
      "epoch": 0.6311725318382292,
      "grad_norm": 0.20790375769138336,
      "learning_rate": 7.7125e-05,
      "loss": 0.2754,
      "step": 64900
    },
    {
      "epoch": 0.6321450627039276,
      "grad_norm": 0.228584423661232,
      "learning_rate": 7.708333333333334e-05,
      "loss": 0.2783,
      "step": 65000
    },
    {
      "epoch": 0.6331175935696259,
      "grad_norm": 0.20281289517879486,
      "learning_rate": 7.704166666666666e-05,
      "loss": 0.2771,
      "step": 65100
    },
    {
      "epoch": 0.6340901244353243,
      "grad_norm": 0.2254587709903717,
      "learning_rate": 7.7e-05,
      "loss": 0.2769,
      "step": 65200
    },
    {
      "epoch": 0.6350626553010226,
      "grad_norm": 0.1958795189857483,
      "learning_rate": 7.695833333333334e-05,
      "loss": 0.2774,
      "step": 65300
    },
    {
      "epoch": 0.636035186166721,
      "grad_norm": 0.21124927699565887,
      "learning_rate": 7.691666666666668e-05,
      "loss": 0.2767,
      "step": 65400
    },
    {
      "epoch": 0.6370077170324193,
      "grad_norm": 0.20412537455558777,
      "learning_rate": 7.687500000000001e-05,
      "loss": 0.2772,
      "step": 65500
    },
    {
      "epoch": 0.6379802478981177,
      "grad_norm": 0.19546884298324585,
      "learning_rate": 7.683333333333334e-05,
      "loss": 0.2749,
      "step": 65600
    },
    {
      "epoch": 0.6389527787638161,
      "grad_norm": 0.19703112542629242,
      "learning_rate": 7.679166666666667e-05,
      "loss": 0.2764,
      "step": 65700
    },
    {
      "epoch": 0.6399253096295143,
      "grad_norm": 0.2050933539867401,
      "learning_rate": 7.675e-05,
      "loss": 0.2748,
      "step": 65800
    },
    {
      "epoch": 0.6408978404952127,
      "grad_norm": 0.213220477104187,
      "learning_rate": 7.670833333333333e-05,
      "loss": 0.2771,
      "step": 65900
    },
    {
      "epoch": 0.6418703713609111,
      "grad_norm": 0.21314525604248047,
      "learning_rate": 7.666666666666667e-05,
      "loss": 0.2758,
      "step": 66000
    },
    {
      "epoch": 0.6428429022266094,
      "grad_norm": 0.20935069024562836,
      "learning_rate": 7.6625e-05,
      "loss": 0.2785,
      "step": 66100
    },
    {
      "epoch": 0.6438154330923078,
      "grad_norm": 0.20613819360733032,
      "learning_rate": 7.658333333333334e-05,
      "loss": 0.2757,
      "step": 66200
    },
    {
      "epoch": 0.6447879639580061,
      "grad_norm": 0.20552603900432587,
      "learning_rate": 7.654166666666667e-05,
      "loss": 0.2768,
      "step": 66300
    },
    {
      "epoch": 0.6457604948237045,
      "grad_norm": 0.196486696600914,
      "learning_rate": 7.65e-05,
      "loss": 0.2755,
      "step": 66400
    },
    {
      "epoch": 0.6467330256894028,
      "grad_norm": 0.2055903673171997,
      "learning_rate": 7.645833333333333e-05,
      "loss": 0.2776,
      "step": 66500
    },
    {
      "epoch": 0.6477055565551012,
      "grad_norm": 0.1904560625553131,
      "learning_rate": 7.641666666666668e-05,
      "loss": 0.2784,
      "step": 66600
    },
    {
      "epoch": 0.6486780874207995,
      "grad_norm": 0.20363196730613708,
      "learning_rate": 7.637500000000001e-05,
      "loss": 0.2777,
      "step": 66700
    },
    {
      "epoch": 0.6496506182864978,
      "grad_norm": 0.20578545331954956,
      "learning_rate": 7.633333333333334e-05,
      "loss": 0.2783,
      "step": 66800
    },
    {
      "epoch": 0.6506231491521962,
      "grad_norm": 0.234724760055542,
      "learning_rate": 7.629166666666667e-05,
      "loss": 0.2783,
      "step": 66900
    },
    {
      "epoch": 0.6515956800178946,
      "grad_norm": 0.23168496787548065,
      "learning_rate": 7.625e-05,
      "loss": 0.2757,
      "step": 67000
    },
    {
      "epoch": 0.652568210883593,
      "grad_norm": 0.20032145082950592,
      "learning_rate": 7.620833333333333e-05,
      "loss": 0.2759,
      "step": 67100
    },
    {
      "epoch": 0.6535407417492912,
      "grad_norm": 0.20398907363414764,
      "learning_rate": 7.616666666666667e-05,
      "loss": 0.2762,
      "step": 67200
    },
    {
      "epoch": 0.6545132726149896,
      "grad_norm": 0.19943884015083313,
      "learning_rate": 7.612500000000001e-05,
      "loss": 0.2763,
      "step": 67300
    },
    {
      "epoch": 0.655485803480688,
      "grad_norm": 0.21186961233615875,
      "learning_rate": 7.608333333333334e-05,
      "loss": 0.2763,
      "step": 67400
    },
    {
      "epoch": 0.6564583343463863,
      "grad_norm": 0.2242312878370285,
      "learning_rate": 7.604166666666667e-05,
      "loss": 0.2768,
      "step": 67500
    },
    {
      "epoch": 0.6574308652120847,
      "grad_norm": 0.20083729922771454,
      "learning_rate": 7.6e-05,
      "loss": 0.2763,
      "step": 67600
    },
    {
      "epoch": 0.658403396077783,
      "grad_norm": 0.19671118259429932,
      "learning_rate": 7.595833333333334e-05,
      "loss": 0.2758,
      "step": 67700
    },
    {
      "epoch": 0.6593759269434813,
      "grad_norm": 0.2237221598625183,
      "learning_rate": 7.591666666666666e-05,
      "loss": 0.2769,
      "step": 67800
    },
    {
      "epoch": 0.6603484578091797,
      "grad_norm": 0.20874585211277008,
      "learning_rate": 7.5875e-05,
      "loss": 0.2751,
      "step": 67900
    },
    {
      "epoch": 0.6613209886748781,
      "grad_norm": 0.2115514725446701,
      "learning_rate": 7.583333333333334e-05,
      "loss": 0.2763,
      "step": 68000
    },
    {
      "epoch": 0.6622935195405765,
      "grad_norm": 0.19853048026561737,
      "learning_rate": 7.579166666666667e-05,
      "loss": 0.2764,
      "step": 68100
    },
    {
      "epoch": 0.6632660504062747,
      "grad_norm": 0.1989164650440216,
      "learning_rate": 7.575e-05,
      "loss": 0.2768,
      "step": 68200
    },
    {
      "epoch": 0.6642385812719731,
      "grad_norm": 0.2057165801525116,
      "learning_rate": 7.570833333333333e-05,
      "loss": 0.275,
      "step": 68300
    },
    {
      "epoch": 0.6652111121376715,
      "grad_norm": 0.21889692544937134,
      "learning_rate": 7.566666666666667e-05,
      "loss": 0.2736,
      "step": 68400
    },
    {
      "epoch": 0.6661836430033699,
      "grad_norm": 0.21076703071594238,
      "learning_rate": 7.5625e-05,
      "loss": 0.2757,
      "step": 68500
    },
    {
      "epoch": 0.6671561738690681,
      "grad_norm": 0.21347014605998993,
      "learning_rate": 7.558333333333335e-05,
      "loss": 0.2737,
      "step": 68600
    },
    {
      "epoch": 0.6681287047347665,
      "grad_norm": 0.2241460233926773,
      "learning_rate": 7.554166666666667e-05,
      "loss": 0.2744,
      "step": 68700
    },
    {
      "epoch": 0.6691012356004649,
      "grad_norm": 0.21294228732585907,
      "learning_rate": 7.55e-05,
      "loss": 0.2751,
      "step": 68800
    },
    {
      "epoch": 0.6700737664661632,
      "grad_norm": 0.21598178148269653,
      "learning_rate": 7.545833333333334e-05,
      "loss": 0.275,
      "step": 68900
    },
    {
      "epoch": 0.6710462973318616,
      "grad_norm": 0.19748874008655548,
      "learning_rate": 7.541666666666667e-05,
      "loss": 0.2749,
      "step": 69000
    },
    {
      "epoch": 0.6720188281975599,
      "grad_norm": 0.20575197041034698,
      "learning_rate": 7.5375e-05,
      "loss": 0.2756,
      "step": 69100
    },
    {
      "epoch": 0.6729913590632582,
      "grad_norm": 0.20226237177848816,
      "learning_rate": 7.533333333333334e-05,
      "loss": 0.2748,
      "step": 69200
    },
    {
      "epoch": 0.6739638899289566,
      "grad_norm": 0.20760081708431244,
      "learning_rate": 7.529166666666668e-05,
      "loss": 0.2779,
      "step": 69300
    },
    {
      "epoch": 0.674936420794655,
      "grad_norm": 0.20090249180793762,
      "learning_rate": 7.525e-05,
      "loss": 0.2766,
      "step": 69400
    },
    {
      "epoch": 0.6759089516603534,
      "grad_norm": 0.20714421570301056,
      "learning_rate": 7.520833333333334e-05,
      "loss": 0.276,
      "step": 69500
    },
    {
      "epoch": 0.6768814825260516,
      "grad_norm": 0.193019300699234,
      "learning_rate": 7.516666666666667e-05,
      "loss": 0.2751,
      "step": 69600
    },
    {
      "epoch": 0.67785401339175,
      "grad_norm": 0.22142887115478516,
      "learning_rate": 7.5125e-05,
      "loss": 0.2751,
      "step": 69700
    },
    {
      "epoch": 0.6788265442574484,
      "grad_norm": 0.19270077347755432,
      "learning_rate": 7.508333333333333e-05,
      "loss": 0.2761,
      "step": 69800
    },
    {
      "epoch": 0.6797990751231467,
      "grad_norm": 0.2031937837600708,
      "learning_rate": 7.504166666666667e-05,
      "loss": 0.2739,
      "step": 69900
    },
    {
      "epoch": 0.6807716059888451,
      "grad_norm": 0.21762758493423462,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.2772,
      "step": 70000
    },
    {
      "epoch": 0.6807716059888451,
      "eval_loss": 0.2754245698451996,
      "eval_runtime": 3091.2806,
      "eval_samples_per_second": 739.095,
      "eval_steps_per_second": 7.391,
      "step": 70000
    },
    {
      "epoch": 0.6817441368545434,
      "grad_norm": 0.19859912991523743,
      "learning_rate": 7.495833333333334e-05,
      "loss": 0.2751,
      "step": 70100
    },
    {
      "epoch": 0.6827166677202418,
      "grad_norm": 0.2082451432943344,
      "learning_rate": 7.491666666666667e-05,
      "loss": 0.2754,
      "step": 70200
    },
    {
      "epoch": 0.6836891985859401,
      "grad_norm": 0.1936638504266739,
      "learning_rate": 7.4875e-05,
      "loss": 0.2762,
      "step": 70300
    },
    {
      "epoch": 0.6846617294516385,
      "grad_norm": 0.23320551216602325,
      "learning_rate": 7.483333333333333e-05,
      "loss": 0.2762,
      "step": 70400
    },
    {
      "epoch": 0.6856342603173369,
      "grad_norm": 0.20427104830741882,
      "learning_rate": 7.479166666666668e-05,
      "loss": 0.2759,
      "step": 70500
    },
    {
      "epoch": 0.6866067911830351,
      "grad_norm": 0.21059656143188477,
      "learning_rate": 7.475000000000001e-05,
      "loss": 0.2757,
      "step": 70600
    },
    {
      "epoch": 0.6875793220487335,
      "grad_norm": 0.19538059830665588,
      "learning_rate": 7.470833333333334e-05,
      "loss": 0.275,
      "step": 70700
    },
    {
      "epoch": 0.6885518529144319,
      "grad_norm": 0.2022228240966797,
      "learning_rate": 7.466666666666667e-05,
      "loss": 0.274,
      "step": 70800
    },
    {
      "epoch": 0.6895243837801303,
      "grad_norm": 0.21563543379306793,
      "learning_rate": 7.4625e-05,
      "loss": 0.2741,
      "step": 70900
    },
    {
      "epoch": 0.6904969146458285,
      "grad_norm": 0.20299533009529114,
      "learning_rate": 7.458333333333333e-05,
      "loss": 0.275,
      "step": 71000
    },
    {
      "epoch": 0.6914694455115269,
      "grad_norm": 0.20503823459148407,
      "learning_rate": 7.454166666666667e-05,
      "loss": 0.2739,
      "step": 71100
    },
    {
      "epoch": 0.6924419763772253,
      "grad_norm": 0.21755138039588928,
      "learning_rate": 7.450000000000001e-05,
      "loss": 0.2756,
      "step": 71200
    },
    {
      "epoch": 0.6934145072429236,
      "grad_norm": 0.19167384505271912,
      "learning_rate": 7.445833333333335e-05,
      "loss": 0.2757,
      "step": 71300
    },
    {
      "epoch": 0.694387038108622,
      "grad_norm": 0.203146293759346,
      "learning_rate": 7.441666666666667e-05,
      "loss": 0.2751,
      "step": 71400
    },
    {
      "epoch": 0.6953595689743203,
      "grad_norm": 0.19873759150505066,
      "learning_rate": 7.4375e-05,
      "loss": 0.2745,
      "step": 71500
    },
    {
      "epoch": 0.6963320998400186,
      "grad_norm": 0.2004411220550537,
      "learning_rate": 7.433333333333333e-05,
      "loss": 0.2731,
      "step": 71600
    },
    {
      "epoch": 0.697304630705717,
      "grad_norm": 0.23569127917289734,
      "learning_rate": 7.429166666666667e-05,
      "loss": 0.2761,
      "step": 71700
    },
    {
      "epoch": 0.6982771615714154,
      "grad_norm": 0.2187211662530899,
      "learning_rate": 7.425e-05,
      "loss": 0.2751,
      "step": 71800
    },
    {
      "epoch": 0.6992496924371138,
      "grad_norm": 0.19898277521133423,
      "learning_rate": 7.420833333333334e-05,
      "loss": 0.2737,
      "step": 71900
    },
    {
      "epoch": 0.700222223302812,
      "grad_norm": 0.2324201613664627,
      "learning_rate": 7.416666666666668e-05,
      "loss": 0.2752,
      "step": 72000
    },
    {
      "epoch": 0.7011947541685104,
      "grad_norm": 0.19704139232635498,
      "learning_rate": 7.4125e-05,
      "loss": 0.2768,
      "step": 72100
    },
    {
      "epoch": 0.7021672850342088,
      "grad_norm": 0.21170824766159058,
      "learning_rate": 7.408333333333334e-05,
      "loss": 0.2744,
      "step": 72200
    },
    {
      "epoch": 0.7031398158999072,
      "grad_norm": 0.19803287088871002,
      "learning_rate": 7.404166666666666e-05,
      "loss": 0.275,
      "step": 72300
    },
    {
      "epoch": 0.7041123467656055,
      "grad_norm": 0.19650347530841827,
      "learning_rate": 7.4e-05,
      "loss": 0.2752,
      "step": 72400
    },
    {
      "epoch": 0.7050848776313038,
      "grad_norm": 0.20494332909584045,
      "learning_rate": 7.395833333333335e-05,
      "loss": 0.2756,
      "step": 72500
    },
    {
      "epoch": 0.7060574084970022,
      "grad_norm": 0.19997479021549225,
      "learning_rate": 7.391666666666667e-05,
      "loss": 0.2758,
      "step": 72600
    },
    {
      "epoch": 0.7070299393627005,
      "grad_norm": 0.19143889844417572,
      "learning_rate": 7.3875e-05,
      "loss": 0.2756,
      "step": 72700
    },
    {
      "epoch": 0.7080024702283989,
      "grad_norm": 0.1963726282119751,
      "learning_rate": 7.383333333333333e-05,
      "loss": 0.2739,
      "step": 72800
    },
    {
      "epoch": 0.7089750010940973,
      "grad_norm": 0.20252008736133575,
      "learning_rate": 7.379166666666667e-05,
      "loss": 0.2744,
      "step": 72900
    },
    {
      "epoch": 0.7099475319597955,
      "grad_norm": 0.20141085982322693,
      "learning_rate": 7.375e-05,
      "loss": 0.2755,
      "step": 73000
    },
    {
      "epoch": 0.7109200628254939,
      "grad_norm": 0.20002038776874542,
      "learning_rate": 7.370833333333334e-05,
      "loss": 0.2737,
      "step": 73100
    },
    {
      "epoch": 0.7118925936911923,
      "grad_norm": 0.19668148458003998,
      "learning_rate": 7.366666666666668e-05,
      "loss": 0.2741,
      "step": 73200
    },
    {
      "epoch": 0.7128651245568907,
      "grad_norm": 0.21955770254135132,
      "learning_rate": 7.3625e-05,
      "loss": 0.2736,
      "step": 73300
    },
    {
      "epoch": 0.7138376554225889,
      "grad_norm": 0.19945743680000305,
      "learning_rate": 7.358333333333334e-05,
      "loss": 0.2756,
      "step": 73400
    },
    {
      "epoch": 0.7148101862882873,
      "grad_norm": 0.2177589237689972,
      "learning_rate": 7.354166666666667e-05,
      "loss": 0.275,
      "step": 73500
    },
    {
      "epoch": 0.7157827171539857,
      "grad_norm": 0.1996801644563675,
      "learning_rate": 7.35e-05,
      "loss": 0.274,
      "step": 73600
    },
    {
      "epoch": 0.716755248019684,
      "grad_norm": 0.19981293380260468,
      "learning_rate": 7.345833333333333e-05,
      "loss": 0.2729,
      "step": 73700
    },
    {
      "epoch": 0.7177277788853824,
      "grad_norm": 0.19230878353118896,
      "learning_rate": 7.341666666666667e-05,
      "loss": 0.2737,
      "step": 73800
    },
    {
      "epoch": 0.7187003097510807,
      "grad_norm": 0.21394047141075134,
      "learning_rate": 7.337500000000001e-05,
      "loss": 0.2773,
      "step": 73900
    },
    {
      "epoch": 0.719672840616779,
      "grad_norm": 0.21512192487716675,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.2745,
      "step": 74000
    },
    {
      "epoch": 0.7206453714824774,
      "grad_norm": 0.21139809489250183,
      "learning_rate": 7.329166666666667e-05,
      "loss": 0.2754,
      "step": 74100
    },
    {
      "epoch": 0.7216179023481758,
      "grad_norm": 0.22974784672260284,
      "learning_rate": 7.325e-05,
      "loss": 0.2757,
      "step": 74200
    },
    {
      "epoch": 0.7225904332138742,
      "grad_norm": 0.2099783569574356,
      "learning_rate": 7.320833333333333e-05,
      "loss": 0.275,
      "step": 74300
    },
    {
      "epoch": 0.7235629640795724,
      "grad_norm": 0.21182513236999512,
      "learning_rate": 7.316666666666668e-05,
      "loss": 0.2761,
      "step": 74400
    },
    {
      "epoch": 0.7245354949452708,
      "grad_norm": 0.19815540313720703,
      "learning_rate": 7.3125e-05,
      "loss": 0.2732,
      "step": 74500
    },
    {
      "epoch": 0.7255080258109692,
      "grad_norm": 0.2045065015554428,
      "learning_rate": 7.308333333333334e-05,
      "loss": 0.2733,
      "step": 74600
    },
    {
      "epoch": 0.7264805566766676,
      "grad_norm": 0.20721113681793213,
      "learning_rate": 7.304166666666668e-05,
      "loss": 0.274,
      "step": 74700
    },
    {
      "epoch": 0.7274530875423659,
      "grad_norm": 0.20726695656776428,
      "learning_rate": 7.3e-05,
      "loss": 0.2753,
      "step": 74800
    },
    {
      "epoch": 0.7284256184080642,
      "grad_norm": 0.19008338451385498,
      "learning_rate": 7.295833333333334e-05,
      "loss": 0.2742,
      "step": 74900
    },
    {
      "epoch": 0.7293981492737626,
      "grad_norm": 0.22048704326152802,
      "learning_rate": 7.291666666666667e-05,
      "loss": 0.2731,
      "step": 75000
    },
    {
      "epoch": 0.7303706801394609,
      "grad_norm": 0.19655869901180267,
      "learning_rate": 7.287500000000001e-05,
      "loss": 0.2761,
      "step": 75100
    },
    {
      "epoch": 0.7313432110051593,
      "grad_norm": 0.2017764300107956,
      "learning_rate": 7.283333333333335e-05,
      "loss": 0.2729,
      "step": 75200
    },
    {
      "epoch": 0.7323157418708576,
      "grad_norm": 0.21481558680534363,
      "learning_rate": 7.279166666666667e-05,
      "loss": 0.274,
      "step": 75300
    },
    {
      "epoch": 0.733288272736556,
      "grad_norm": 0.21290884912014008,
      "learning_rate": 7.275e-05,
      "loss": 0.2735,
      "step": 75400
    },
    {
      "epoch": 0.7342608036022543,
      "grad_norm": 0.2052583247423172,
      "learning_rate": 7.270833333333333e-05,
      "loss": 0.2739,
      "step": 75500
    },
    {
      "epoch": 0.7352333344679527,
      "grad_norm": 0.1980842500925064,
      "learning_rate": 7.266666666666667e-05,
      "loss": 0.2745,
      "step": 75600
    },
    {
      "epoch": 0.7362058653336511,
      "grad_norm": 0.21326085925102234,
      "learning_rate": 7.2625e-05,
      "loss": 0.273,
      "step": 75700
    },
    {
      "epoch": 0.7371783961993493,
      "grad_norm": 0.21365109086036682,
      "learning_rate": 7.258333333333334e-05,
      "loss": 0.2748,
      "step": 75800
    },
    {
      "epoch": 0.7381509270650477,
      "grad_norm": 0.18568934500217438,
      "learning_rate": 7.254166666666668e-05,
      "loss": 0.2745,
      "step": 75900
    },
    {
      "epoch": 0.7391234579307461,
      "grad_norm": 0.20520241558551788,
      "learning_rate": 7.25e-05,
      "loss": 0.2732,
      "step": 76000
    },
    {
      "epoch": 0.7400959887964444,
      "grad_norm": 0.20952288806438446,
      "learning_rate": 7.245833333333334e-05,
      "loss": 0.2743,
      "step": 76100
    },
    {
      "epoch": 0.7410685196621428,
      "grad_norm": 0.20435132086277008,
      "learning_rate": 7.241666666666666e-05,
      "loss": 0.275,
      "step": 76200
    },
    {
      "epoch": 0.7420410505278411,
      "grad_norm": 0.214193657040596,
      "learning_rate": 7.2375e-05,
      "loss": 0.274,
      "step": 76300
    },
    {
      "epoch": 0.7430135813935395,
      "grad_norm": 0.21112211048603058,
      "learning_rate": 7.233333333333335e-05,
      "loss": 0.2744,
      "step": 76400
    },
    {
      "epoch": 0.7439861122592378,
      "grad_norm": 0.19687001407146454,
      "learning_rate": 7.229166666666667e-05,
      "loss": 0.2723,
      "step": 76500
    },
    {
      "epoch": 0.7449586431249362,
      "grad_norm": 0.20619940757751465,
      "learning_rate": 7.225000000000001e-05,
      "loss": 0.2748,
      "step": 76600
    },
    {
      "epoch": 0.7459311739906346,
      "grad_norm": 0.20431557297706604,
      "learning_rate": 7.220833333333333e-05,
      "loss": 0.2717,
      "step": 76700
    },
    {
      "epoch": 0.7469037048563328,
      "grad_norm": 0.23177626729011536,
      "learning_rate": 7.216666666666667e-05,
      "loss": 0.2739,
      "step": 76800
    },
    {
      "epoch": 0.7478762357220312,
      "grad_norm": 0.21184007823467255,
      "learning_rate": 7.2125e-05,
      "loss": 0.2736,
      "step": 76900
    },
    {
      "epoch": 0.7488487665877296,
      "grad_norm": 0.20997057855129242,
      "learning_rate": 7.208333333333334e-05,
      "loss": 0.2737,
      "step": 77000
    },
    {
      "epoch": 0.749821297453428,
      "grad_norm": 0.1926448941230774,
      "learning_rate": 7.204166666666668e-05,
      "loss": 0.2719,
      "step": 77100
    },
    {
      "epoch": 0.7507938283191263,
      "grad_norm": 0.20543186366558075,
      "learning_rate": 7.2e-05,
      "loss": 0.2751,
      "step": 77200
    },
    {
      "epoch": 0.7517663591848246,
      "grad_norm": 0.2065427303314209,
      "learning_rate": 7.195833333333334e-05,
      "loss": 0.2753,
      "step": 77300
    },
    {
      "epoch": 0.752738890050523,
      "grad_norm": 0.21091346442699432,
      "learning_rate": 7.191666666666666e-05,
      "loss": 0.2739,
      "step": 77400
    },
    {
      "epoch": 0.7537114209162213,
      "grad_norm": 0.20991741120815277,
      "learning_rate": 7.1875e-05,
      "loss": 0.2727,
      "step": 77500
    },
    {
      "epoch": 0.7546839517819197,
      "grad_norm": 0.2098870575428009,
      "learning_rate": 7.183333333333334e-05,
      "loss": 0.275,
      "step": 77600
    },
    {
      "epoch": 0.755656482647618,
      "grad_norm": 0.23738038539886475,
      "learning_rate": 7.179166666666667e-05,
      "loss": 0.2741,
      "step": 77700
    },
    {
      "epoch": 0.7566290135133164,
      "grad_norm": 0.20924003422260284,
      "learning_rate": 7.175000000000001e-05,
      "loss": 0.2735,
      "step": 77800
    },
    {
      "epoch": 0.7576015443790147,
      "grad_norm": 0.18433450162410736,
      "learning_rate": 7.170833333333333e-05,
      "loss": 0.2746,
      "step": 77900
    },
    {
      "epoch": 0.7585740752447131,
      "grad_norm": 0.21517588198184967,
      "learning_rate": 7.166666666666667e-05,
      "loss": 0.2741,
      "step": 78000
    },
    {
      "epoch": 0.7595466061104115,
      "grad_norm": 0.19985055923461914,
      "learning_rate": 7.1625e-05,
      "loss": 0.2725,
      "step": 78100
    },
    {
      "epoch": 0.7605191369761097,
      "grad_norm": 0.2120457887649536,
      "learning_rate": 7.158333333333333e-05,
      "loss": 0.2725,
      "step": 78200
    },
    {
      "epoch": 0.7614916678418081,
      "grad_norm": 0.19893920421600342,
      "learning_rate": 7.154166666666668e-05,
      "loss": 0.2725,
      "step": 78300
    },
    {
      "epoch": 0.7624641987075065,
      "grad_norm": 0.2038278579711914,
      "learning_rate": 7.15e-05,
      "loss": 0.2743,
      "step": 78400
    },
    {
      "epoch": 0.7634367295732049,
      "grad_norm": 0.19468972086906433,
      "learning_rate": 7.145833333333334e-05,
      "loss": 0.2729,
      "step": 78500
    },
    {
      "epoch": 0.7644092604389032,
      "grad_norm": 0.21642930805683136,
      "learning_rate": 7.141666666666666e-05,
      "loss": 0.2732,
      "step": 78600
    },
    {
      "epoch": 0.7653817913046015,
      "grad_norm": 0.20321418344974518,
      "learning_rate": 7.1375e-05,
      "loss": 0.2736,
      "step": 78700
    },
    {
      "epoch": 0.7663543221702999,
      "grad_norm": 0.23276935517787933,
      "learning_rate": 7.133333333333334e-05,
      "loss": 0.2741,
      "step": 78800
    },
    {
      "epoch": 0.7673268530359982,
      "grad_norm": 0.2127787321805954,
      "learning_rate": 7.129166666666667e-05,
      "loss": 0.2745,
      "step": 78900
    },
    {
      "epoch": 0.7682993839016966,
      "grad_norm": 0.19755390286445618,
      "learning_rate": 7.125000000000001e-05,
      "loss": 0.2745,
      "step": 79000
    },
    {
      "epoch": 0.769271914767395,
      "grad_norm": 0.2066769152879715,
      "learning_rate": 7.120833333333333e-05,
      "loss": 0.2732,
      "step": 79100
    },
    {
      "epoch": 0.7702444456330932,
      "grad_norm": 0.2069358378648758,
      "learning_rate": 7.116666666666667e-05,
      "loss": 0.2737,
      "step": 79200
    },
    {
      "epoch": 0.7712169764987916,
      "grad_norm": 0.1903837025165558,
      "learning_rate": 7.112500000000001e-05,
      "loss": 0.2717,
      "step": 79300
    },
    {
      "epoch": 0.77218950736449,
      "grad_norm": 0.20352573692798615,
      "learning_rate": 7.108333333333333e-05,
      "loss": 0.2719,
      "step": 79400
    },
    {
      "epoch": 0.7731620382301884,
      "grad_norm": 0.20243313908576965,
      "learning_rate": 7.104166666666667e-05,
      "loss": 0.2722,
      "step": 79500
    },
    {
      "epoch": 0.7741345690958866,
      "grad_norm": 0.22369620203971863,
      "learning_rate": 7.1e-05,
      "loss": 0.2743,
      "step": 79600
    },
    {
      "epoch": 0.775107099961585,
      "grad_norm": 0.19516180455684662,
      "learning_rate": 7.095833333333334e-05,
      "loss": 0.2728,
      "step": 79700
    },
    {
      "epoch": 0.7760796308272834,
      "grad_norm": 0.20127220451831818,
      "learning_rate": 7.091666666666666e-05,
      "loss": 0.272,
      "step": 79800
    },
    {
      "epoch": 0.7770521616929817,
      "grad_norm": 0.21912935376167297,
      "learning_rate": 7.0875e-05,
      "loss": 0.2731,
      "step": 79900
    },
    {
      "epoch": 0.7780246925586801,
      "grad_norm": 0.21549201011657715,
      "learning_rate": 7.083333333333334e-05,
      "loss": 0.2732,
      "step": 80000
    },
    {
      "epoch": 0.7780246925586801,
      "eval_loss": 0.27304399013519287,
      "eval_runtime": 3229.4631,
      "eval_samples_per_second": 707.471,
      "eval_steps_per_second": 7.075,
      "step": 80000
    },
    {
      "epoch": 0.7789972234243784,
      "grad_norm": 0.18813829123973846,
      "learning_rate": 7.079166666666666e-05,
      "loss": 0.274,
      "step": 80100
    },
    {
      "epoch": 0.7799697542900768,
      "grad_norm": 0.19646279513835907,
      "learning_rate": 7.075e-05,
      "loss": 0.2732,
      "step": 80200
    },
    {
      "epoch": 0.7809422851557751,
      "grad_norm": 0.19691339135169983,
      "learning_rate": 7.070833333333334e-05,
      "loss": 0.2729,
      "step": 80300
    },
    {
      "epoch": 0.7819148160214735,
      "grad_norm": 0.19270110130310059,
      "learning_rate": 7.066666666666667e-05,
      "loss": 0.2715,
      "step": 80400
    },
    {
      "epoch": 0.7828873468871719,
      "grad_norm": 0.19744804501533508,
      "learning_rate": 7.062500000000001e-05,
      "loss": 0.2732,
      "step": 80500
    },
    {
      "epoch": 0.7838598777528701,
      "grad_norm": 0.21636927127838135,
      "learning_rate": 7.058333333333333e-05,
      "loss": 0.2731,
      "step": 80600
    },
    {
      "epoch": 0.7848324086185685,
      "grad_norm": 0.209979847073555,
      "learning_rate": 7.054166666666667e-05,
      "loss": 0.2745,
      "step": 80700
    },
    {
      "epoch": 0.7858049394842669,
      "grad_norm": 0.201760932803154,
      "learning_rate": 7.05e-05,
      "loss": 0.272,
      "step": 80800
    },
    {
      "epoch": 0.7867774703499653,
      "grad_norm": 0.2157384306192398,
      "learning_rate": 7.045833333333334e-05,
      "loss": 0.2728,
      "step": 80900
    },
    {
      "epoch": 0.7877500012156636,
      "grad_norm": 0.20003476738929749,
      "learning_rate": 7.041666666666668e-05,
      "loss": 0.2715,
      "step": 81000
    },
    {
      "epoch": 0.7887225320813619,
      "grad_norm": 0.19891174137592316,
      "learning_rate": 7.0375e-05,
      "loss": 0.2712,
      "step": 81100
    },
    {
      "epoch": 0.7896950629470603,
      "grad_norm": 0.20692282915115356,
      "learning_rate": 7.033333333333334e-05,
      "loss": 0.2731,
      "step": 81200
    },
    {
      "epoch": 0.7906675938127586,
      "grad_norm": 0.23222634196281433,
      "learning_rate": 7.029166666666666e-05,
      "loss": 0.2725,
      "step": 81300
    },
    {
      "epoch": 0.791640124678457,
      "grad_norm": 0.19672441482543945,
      "learning_rate": 7.025e-05,
      "loss": 0.2733,
      "step": 81400
    },
    {
      "epoch": 0.7926126555441554,
      "grad_norm": 0.21614620089530945,
      "learning_rate": 7.020833333333334e-05,
      "loss": 0.2713,
      "step": 81500
    },
    {
      "epoch": 0.7935851864098536,
      "grad_norm": 0.22499674558639526,
      "learning_rate": 7.016666666666667e-05,
      "loss": 0.2726,
      "step": 81600
    },
    {
      "epoch": 0.794557717275552,
      "grad_norm": 0.2260386049747467,
      "learning_rate": 7.012500000000001e-05,
      "loss": 0.2741,
      "step": 81700
    },
    {
      "epoch": 0.7955302481412504,
      "grad_norm": 0.20871512591838837,
      "learning_rate": 7.008333333333333e-05,
      "loss": 0.2714,
      "step": 81800
    },
    {
      "epoch": 0.7965027790069488,
      "grad_norm": 0.19957084953784943,
      "learning_rate": 7.004166666666667e-05,
      "loss": 0.2721,
      "step": 81900
    },
    {
      "epoch": 0.797475309872647,
      "grad_norm": 0.18536467850208282,
      "learning_rate": 7e-05,
      "loss": 0.2726,
      "step": 82000
    },
    {
      "epoch": 0.7984478407383454,
      "grad_norm": 0.18504685163497925,
      "learning_rate": 6.995833333333333e-05,
      "loss": 0.2711,
      "step": 82100
    },
    {
      "epoch": 0.7994203716040438,
      "grad_norm": 0.2135026752948761,
      "learning_rate": 6.991666666666668e-05,
      "loss": 0.2733,
      "step": 82200
    },
    {
      "epoch": 0.8003929024697422,
      "grad_norm": 0.20764921605587006,
      "learning_rate": 6.9875e-05,
      "loss": 0.2724,
      "step": 82300
    },
    {
      "epoch": 0.8013654333354405,
      "grad_norm": 0.19810397922992706,
      "learning_rate": 6.983333333333334e-05,
      "loss": 0.2719,
      "step": 82400
    },
    {
      "epoch": 0.8023379642011388,
      "grad_norm": 0.20298978686332703,
      "learning_rate": 6.979166666666666e-05,
      "loss": 0.2745,
      "step": 82500
    },
    {
      "epoch": 0.8033104950668372,
      "grad_norm": 0.1945832222700119,
      "learning_rate": 6.975e-05,
      "loss": 0.2736,
      "step": 82600
    },
    {
      "epoch": 0.8042830259325355,
      "grad_norm": 0.2034040093421936,
      "learning_rate": 6.970833333333334e-05,
      "loss": 0.2718,
      "step": 82700
    },
    {
      "epoch": 0.8052555567982339,
      "grad_norm": 0.18791651725769043,
      "learning_rate": 6.966666666666668e-05,
      "loss": 0.2721,
      "step": 82800
    },
    {
      "epoch": 0.8062280876639323,
      "grad_norm": 0.2143315076828003,
      "learning_rate": 6.962500000000001e-05,
      "loss": 0.2737,
      "step": 82900
    },
    {
      "epoch": 0.8072006185296305,
      "grad_norm": 0.19841569662094116,
      "learning_rate": 6.958333333333334e-05,
      "loss": 0.2726,
      "step": 83000
    },
    {
      "epoch": 0.8081731493953289,
      "grad_norm": 0.2106267809867859,
      "learning_rate": 6.954166666666667e-05,
      "loss": 0.2719,
      "step": 83100
    },
    {
      "epoch": 0.8091456802610273,
      "grad_norm": 0.18850785493850708,
      "learning_rate": 6.95e-05,
      "loss": 0.2714,
      "step": 83200
    },
    {
      "epoch": 0.8101182111267257,
      "grad_norm": 0.20443858206272125,
      "learning_rate": 6.945833333333333e-05,
      "loss": 0.2734,
      "step": 83300
    },
    {
      "epoch": 0.811090741992424,
      "grad_norm": 0.21181483566761017,
      "learning_rate": 6.941666666666667e-05,
      "loss": 0.2725,
      "step": 83400
    },
    {
      "epoch": 0.8120632728581223,
      "grad_norm": 0.21200169622898102,
      "learning_rate": 6.9375e-05,
      "loss": 0.2727,
      "step": 83500
    },
    {
      "epoch": 0.8130358037238207,
      "grad_norm": 0.21668541431427002,
      "learning_rate": 6.933333333333334e-05,
      "loss": 0.272,
      "step": 83600
    },
    {
      "epoch": 0.814008334589519,
      "grad_norm": 0.20588646829128265,
      "learning_rate": 6.929166666666667e-05,
      "loss": 0.2732,
      "step": 83700
    },
    {
      "epoch": 0.8149808654552174,
      "grad_norm": 0.19617712497711182,
      "learning_rate": 6.925e-05,
      "loss": 0.2704,
      "step": 83800
    },
    {
      "epoch": 0.8159533963209157,
      "grad_norm": 0.18802626430988312,
      "learning_rate": 6.920833333333334e-05,
      "loss": 0.2715,
      "step": 83900
    },
    {
      "epoch": 0.816925927186614,
      "grad_norm": 0.1952625811100006,
      "learning_rate": 6.916666666666666e-05,
      "loss": 0.2732,
      "step": 84000
    },
    {
      "epoch": 0.8178984580523124,
      "grad_norm": 0.2028808444738388,
      "learning_rate": 6.9125e-05,
      "loss": 0.2739,
      "step": 84100
    },
    {
      "epoch": 0.8188709889180108,
      "grad_norm": 0.21624061465263367,
      "learning_rate": 6.908333333333334e-05,
      "loss": 0.2742,
      "step": 84200
    },
    {
      "epoch": 0.8198435197837092,
      "grad_norm": 0.20797288417816162,
      "learning_rate": 6.904166666666667e-05,
      "loss": 0.2716,
      "step": 84300
    },
    {
      "epoch": 0.8208160506494074,
      "grad_norm": 0.20899523794651031,
      "learning_rate": 6.9e-05,
      "loss": 0.2711,
      "step": 84400
    },
    {
      "epoch": 0.8217885815151058,
      "grad_norm": 0.20281000435352325,
      "learning_rate": 6.895833333333333e-05,
      "loss": 0.2724,
      "step": 84500
    },
    {
      "epoch": 0.8227611123808042,
      "grad_norm": 0.21095068752765656,
      "learning_rate": 6.891666666666667e-05,
      "loss": 0.2719,
      "step": 84600
    },
    {
      "epoch": 0.8237336432465026,
      "grad_norm": 0.2045532613992691,
      "learning_rate": 6.887500000000001e-05,
      "loss": 0.271,
      "step": 84700
    },
    {
      "epoch": 0.8247061741122009,
      "grad_norm": 0.20235706865787506,
      "learning_rate": 6.883333333333334e-05,
      "loss": 0.2737,
      "step": 84800
    },
    {
      "epoch": 0.8256787049778992,
      "grad_norm": 0.21146327257156372,
      "learning_rate": 6.879166666666667e-05,
      "loss": 0.2722,
      "step": 84900
    },
    {
      "epoch": 0.8266512358435976,
      "grad_norm": 0.19641298055648804,
      "learning_rate": 6.875e-05,
      "loss": 0.2717,
      "step": 85000
    },
    {
      "epoch": 0.8276237667092959,
      "grad_norm": 0.19566822052001953,
      "learning_rate": 6.870833333333334e-05,
      "loss": 0.2705,
      "step": 85100
    },
    {
      "epoch": 0.8285962975749943,
      "grad_norm": 0.20518110692501068,
      "learning_rate": 6.866666666666666e-05,
      "loss": 0.2714,
      "step": 85200
    },
    {
      "epoch": 0.8295688284406927,
      "grad_norm": 0.212514728307724,
      "learning_rate": 6.8625e-05,
      "loss": 0.2706,
      "step": 85300
    },
    {
      "epoch": 0.830541359306391,
      "grad_norm": 0.21985933184623718,
      "learning_rate": 6.858333333333334e-05,
      "loss": 0.2702,
      "step": 85400
    },
    {
      "epoch": 0.8315138901720893,
      "grad_norm": 0.19540241360664368,
      "learning_rate": 6.854166666666667e-05,
      "loss": 0.2717,
      "step": 85500
    },
    {
      "epoch": 0.8324864210377877,
      "grad_norm": 0.20284558832645416,
      "learning_rate": 6.850000000000001e-05,
      "loss": 0.2726,
      "step": 85600
    },
    {
      "epoch": 0.8334589519034861,
      "grad_norm": 0.22903263568878174,
      "learning_rate": 6.845833333333333e-05,
      "loss": 0.2723,
      "step": 85700
    },
    {
      "epoch": 0.8344314827691844,
      "grad_norm": 0.21429434418678284,
      "learning_rate": 6.841666666666667e-05,
      "loss": 0.2713,
      "step": 85800
    },
    {
      "epoch": 0.8354040136348827,
      "grad_norm": 0.2144472748041153,
      "learning_rate": 6.8375e-05,
      "loss": 0.2716,
      "step": 85900
    },
    {
      "epoch": 0.8363765445005811,
      "grad_norm": 0.21586845815181732,
      "learning_rate": 6.833333333333333e-05,
      "loss": 0.271,
      "step": 86000
    },
    {
      "epoch": 0.8373490753662794,
      "grad_norm": 0.22211714088916779,
      "learning_rate": 6.829166666666667e-05,
      "loss": 0.2714,
      "step": 86100
    },
    {
      "epoch": 0.8383216062319778,
      "grad_norm": 0.20439623296260834,
      "learning_rate": 6.825e-05,
      "loss": 0.2718,
      "step": 86200
    },
    {
      "epoch": 0.8392941370976761,
      "grad_norm": 0.19544407725334167,
      "learning_rate": 6.820833333333334e-05,
      "loss": 0.2725,
      "step": 86300
    },
    {
      "epoch": 0.8402666679633745,
      "grad_norm": 0.21405017375946045,
      "learning_rate": 6.816666666666667e-05,
      "loss": 0.2705,
      "step": 86400
    },
    {
      "epoch": 0.8412391988290728,
      "grad_norm": 0.2132761925458908,
      "learning_rate": 6.8125e-05,
      "loss": 0.2727,
      "step": 86500
    },
    {
      "epoch": 0.8422117296947712,
      "grad_norm": 0.19385135173797607,
      "learning_rate": 6.808333333333333e-05,
      "loss": 0.2727,
      "step": 86600
    },
    {
      "epoch": 0.8431842605604696,
      "grad_norm": 0.1924738883972168,
      "learning_rate": 6.804166666666668e-05,
      "loss": 0.2721,
      "step": 86700
    },
    {
      "epoch": 0.8441567914261678,
      "grad_norm": 0.20596849918365479,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.2719,
      "step": 86800
    },
    {
      "epoch": 0.8451293222918662,
      "grad_norm": 0.21860387921333313,
      "learning_rate": 6.795833333333334e-05,
      "loss": 0.2713,
      "step": 86900
    },
    {
      "epoch": 0.8461018531575646,
      "grad_norm": 0.2280261069536209,
      "learning_rate": 6.791666666666667e-05,
      "loss": 0.2707,
      "step": 87000
    },
    {
      "epoch": 0.847074384023263,
      "grad_norm": 0.21118776500225067,
      "learning_rate": 6.7875e-05,
      "loss": 0.2702,
      "step": 87100
    },
    {
      "epoch": 0.8480469148889613,
      "grad_norm": 0.21554067730903625,
      "learning_rate": 6.783333333333333e-05,
      "loss": 0.2716,
      "step": 87200
    },
    {
      "epoch": 0.8490194457546596,
      "grad_norm": 0.20946529507637024,
      "learning_rate": 6.779166666666667e-05,
      "loss": 0.2713,
      "step": 87300
    },
    {
      "epoch": 0.849991976620358,
      "grad_norm": 0.2149290293455124,
      "learning_rate": 6.775000000000001e-05,
      "loss": 0.2715,
      "step": 87400
    },
    {
      "epoch": 0.8509645074860563,
      "grad_norm": 0.20756053924560547,
      "learning_rate": 6.770833333333334e-05,
      "loss": 0.2704,
      "step": 87500
    },
    {
      "epoch": 0.8519370383517547,
      "grad_norm": 0.20354555547237396,
      "learning_rate": 6.766666666666667e-05,
      "loss": 0.2711,
      "step": 87600
    },
    {
      "epoch": 0.8529095692174531,
      "grad_norm": 0.21079130470752716,
      "learning_rate": 6.7625e-05,
      "loss": 0.2725,
      "step": 87700
    },
    {
      "epoch": 0.8538821000831514,
      "grad_norm": 0.21801671385765076,
      "learning_rate": 6.758333333333333e-05,
      "loss": 0.2714,
      "step": 87800
    },
    {
      "epoch": 0.8548546309488497,
      "grad_norm": 0.1901078075170517,
      "learning_rate": 6.754166666666666e-05,
      "loss": 0.2718,
      "step": 87900
    },
    {
      "epoch": 0.8558271618145481,
      "grad_norm": 0.21731343865394592,
      "learning_rate": 6.750000000000001e-05,
      "loss": 0.2682,
      "step": 88000
    },
    {
      "epoch": 0.8567996926802465,
      "grad_norm": 0.20320191979408264,
      "learning_rate": 6.745833333333334e-05,
      "loss": 0.2723,
      "step": 88100
    },
    {
      "epoch": 0.8577722235459447,
      "grad_norm": 0.20559760928153992,
      "learning_rate": 6.741666666666667e-05,
      "loss": 0.2724,
      "step": 88200
    },
    {
      "epoch": 0.8587447544116431,
      "grad_norm": 0.21865813434123993,
      "learning_rate": 6.7375e-05,
      "loss": 0.2721,
      "step": 88300
    },
    {
      "epoch": 0.8597172852773415,
      "grad_norm": 0.21884536743164062,
      "learning_rate": 6.733333333333333e-05,
      "loss": 0.2705,
      "step": 88400
    },
    {
      "epoch": 0.8606898161430399,
      "grad_norm": 0.2313617467880249,
      "learning_rate": 6.729166666666667e-05,
      "loss": 0.2721,
      "step": 88500
    },
    {
      "epoch": 0.8616623470087382,
      "grad_norm": 0.22972635924816132,
      "learning_rate": 6.725000000000001e-05,
      "loss": 0.2716,
      "step": 88600
    },
    {
      "epoch": 0.8626348778744365,
      "grad_norm": 0.22029297053813934,
      "learning_rate": 6.720833333333335e-05,
      "loss": 0.2713,
      "step": 88700
    },
    {
      "epoch": 0.8636074087401349,
      "grad_norm": 0.21298889815807343,
      "learning_rate": 6.716666666666667e-05,
      "loss": 0.2727,
      "step": 88800
    },
    {
      "epoch": 0.8645799396058332,
      "grad_norm": 0.19575326144695282,
      "learning_rate": 6.7125e-05,
      "loss": 0.2715,
      "step": 88900
    },
    {
      "epoch": 0.8655524704715316,
      "grad_norm": 0.22222067415714264,
      "learning_rate": 6.708333333333333e-05,
      "loss": 0.2713,
      "step": 89000
    },
    {
      "epoch": 0.86652500133723,
      "grad_norm": 0.20366407930850983,
      "learning_rate": 6.704166666666667e-05,
      "loss": 0.2722,
      "step": 89100
    },
    {
      "epoch": 0.8674975322029282,
      "grad_norm": 0.21381740272045135,
      "learning_rate": 6.7e-05,
      "loss": 0.2704,
      "step": 89200
    },
    {
      "epoch": 0.8684700630686266,
      "grad_norm": 0.19980554282665253,
      "learning_rate": 6.695833333333334e-05,
      "loss": 0.2727,
      "step": 89300
    },
    {
      "epoch": 0.869442593934325,
      "grad_norm": 0.21995051205158234,
      "learning_rate": 6.691666666666668e-05,
      "loss": 0.2707,
      "step": 89400
    },
    {
      "epoch": 0.8704151248000234,
      "grad_norm": 0.2153463065624237,
      "learning_rate": 6.6875e-05,
      "loss": 0.2718,
      "step": 89500
    },
    {
      "epoch": 0.8713876556657217,
      "grad_norm": 0.21758009493350983,
      "learning_rate": 6.683333333333334e-05,
      "loss": 0.2709,
      "step": 89600
    },
    {
      "epoch": 0.87236018653142,
      "grad_norm": 0.19305212795734406,
      "learning_rate": 6.679166666666667e-05,
      "loss": 0.2698,
      "step": 89700
    },
    {
      "epoch": 0.8733327173971184,
      "grad_norm": 0.21360135078430176,
      "learning_rate": 6.675e-05,
      "loss": 0.2702,
      "step": 89800
    },
    {
      "epoch": 0.8743052482628167,
      "grad_norm": 0.20855408906936646,
      "learning_rate": 6.670833333333333e-05,
      "loss": 0.2727,
      "step": 89900
    },
    {
      "epoch": 0.8752777791285151,
      "grad_norm": 0.22105124592781067,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.2704,
      "step": 90000
    },
    {
      "epoch": 0.8752777791285151,
      "eval_loss": 0.27094852924346924,
      "eval_runtime": 3179.1969,
      "eval_samples_per_second": 718.656,
      "eval_steps_per_second": 7.187,
      "step": 90000
    },
    {
      "epoch": 0.8762503099942135,
      "grad_norm": 0.1958732306957245,
      "learning_rate": 6.6625e-05,
      "loss": 0.2719,
      "step": 90100
    },
    {
      "epoch": 0.8772228408599118,
      "grad_norm": 0.19383525848388672,
      "learning_rate": 6.658333333333334e-05,
      "loss": 0.2711,
      "step": 90200
    },
    {
      "epoch": 0.8781953717256101,
      "grad_norm": 0.20195329189300537,
      "learning_rate": 6.654166666666667e-05,
      "loss": 0.2697,
      "step": 90300
    },
    {
      "epoch": 0.8791679025913085,
      "grad_norm": 0.214803084731102,
      "learning_rate": 6.65e-05,
      "loss": 0.27,
      "step": 90400
    },
    {
      "epoch": 0.8801404334570069,
      "grad_norm": 0.23000821471214294,
      "learning_rate": 6.645833333333333e-05,
      "loss": 0.2711,
      "step": 90500
    },
    {
      "epoch": 0.8811129643227051,
      "grad_norm": 0.1867305189371109,
      "learning_rate": 6.641666666666668e-05,
      "loss": 0.2712,
      "step": 90600
    },
    {
      "epoch": 0.8820854951884035,
      "grad_norm": 0.20946496725082397,
      "learning_rate": 6.6375e-05,
      "loss": 0.271,
      "step": 90700
    },
    {
      "epoch": 0.8830580260541019,
      "grad_norm": 0.21505814790725708,
      "learning_rate": 6.633333333333334e-05,
      "loss": 0.2696,
      "step": 90800
    },
    {
      "epoch": 0.8840305569198003,
      "grad_norm": 0.21783936023712158,
      "learning_rate": 6.629166666666667e-05,
      "loss": 0.2702,
      "step": 90900
    },
    {
      "epoch": 0.8850030877854986,
      "grad_norm": 0.2002098262310028,
      "learning_rate": 6.625e-05,
      "loss": 0.2714,
      "step": 91000
    },
    {
      "epoch": 0.8859756186511969,
      "grad_norm": 0.2013937383890152,
      "learning_rate": 6.620833333333333e-05,
      "loss": 0.2695,
      "step": 91100
    },
    {
      "epoch": 0.8869481495168953,
      "grad_norm": 0.19451528787612915,
      "learning_rate": 6.616666666666667e-05,
      "loss": 0.272,
      "step": 91200
    },
    {
      "epoch": 0.8879206803825936,
      "grad_norm": 0.19014960527420044,
      "learning_rate": 6.612500000000001e-05,
      "loss": 0.2706,
      "step": 91300
    },
    {
      "epoch": 0.888893211248292,
      "grad_norm": 0.19604144990444183,
      "learning_rate": 6.608333333333334e-05,
      "loss": 0.27,
      "step": 91400
    },
    {
      "epoch": 0.8898657421139904,
      "grad_norm": 0.21652038395404816,
      "learning_rate": 6.604166666666667e-05,
      "loss": 0.2694,
      "step": 91500
    },
    {
      "epoch": 0.8908382729796887,
      "grad_norm": 0.19919027388095856,
      "learning_rate": 6.6e-05,
      "loss": 0.2696,
      "step": 91600
    },
    {
      "epoch": 0.891810803845387,
      "grad_norm": 0.22468402981758118,
      "learning_rate": 6.595833333333333e-05,
      "loss": 0.2718,
      "step": 91700
    },
    {
      "epoch": 0.8927833347110854,
      "grad_norm": 0.21416907012462616,
      "learning_rate": 6.591666666666667e-05,
      "loss": 0.273,
      "step": 91800
    },
    {
      "epoch": 0.8937558655767838,
      "grad_norm": 0.2084820717573166,
      "learning_rate": 6.5875e-05,
      "loss": 0.2699,
      "step": 91900
    },
    {
      "epoch": 0.8947283964424821,
      "grad_norm": 0.22735990583896637,
      "learning_rate": 6.583333333333334e-05,
      "loss": 0.2704,
      "step": 92000
    },
    {
      "epoch": 0.8957009273081804,
      "grad_norm": 0.22247454524040222,
      "learning_rate": 6.579166666666668e-05,
      "loss": 0.2718,
      "step": 92100
    },
    {
      "epoch": 0.8966734581738788,
      "grad_norm": 0.20176546275615692,
      "learning_rate": 6.575e-05,
      "loss": 0.2721,
      "step": 92200
    },
    {
      "epoch": 0.8976459890395772,
      "grad_norm": 0.18851611018180847,
      "learning_rate": 6.570833333333334e-05,
      "loss": 0.2699,
      "step": 92300
    },
    {
      "epoch": 0.8986185199052755,
      "grad_norm": 0.21507218480110168,
      "learning_rate": 6.566666666666666e-05,
      "loss": 0.2707,
      "step": 92400
    },
    {
      "epoch": 0.8995910507709739,
      "grad_norm": 0.2280583679676056,
      "learning_rate": 6.562500000000001e-05,
      "loss": 0.2707,
      "step": 92500
    },
    {
      "epoch": 0.9005635816366722,
      "grad_norm": 0.21628805994987488,
      "learning_rate": 6.558333333333335e-05,
      "loss": 0.2683,
      "step": 92600
    },
    {
      "epoch": 0.9015361125023705,
      "grad_norm": 0.20248252153396606,
      "learning_rate": 6.554166666666667e-05,
      "loss": 0.2703,
      "step": 92700
    },
    {
      "epoch": 0.9025086433680689,
      "grad_norm": 0.203867107629776,
      "learning_rate": 6.55e-05,
      "loss": 0.2711,
      "step": 92800
    },
    {
      "epoch": 0.9034811742337673,
      "grad_norm": 0.20547860860824585,
      "learning_rate": 6.545833333333333e-05,
      "loss": 0.27,
      "step": 92900
    },
    {
      "epoch": 0.9044537050994655,
      "grad_norm": 0.21486026048660278,
      "learning_rate": 6.541666666666667e-05,
      "loss": 0.2703,
      "step": 93000
    },
    {
      "epoch": 0.9054262359651639,
      "grad_norm": 0.20960745215415955,
      "learning_rate": 6.5375e-05,
      "loss": 0.2707,
      "step": 93100
    },
    {
      "epoch": 0.9063987668308623,
      "grad_norm": 0.20236004889011383,
      "learning_rate": 6.533333333333334e-05,
      "loss": 0.2704,
      "step": 93200
    },
    {
      "epoch": 0.9073712976965607,
      "grad_norm": 0.20229913294315338,
      "learning_rate": 6.529166666666668e-05,
      "loss": 0.2684,
      "step": 93300
    },
    {
      "epoch": 0.908343828562259,
      "grad_norm": 0.21744729578495026,
      "learning_rate": 6.525e-05,
      "loss": 0.2703,
      "step": 93400
    },
    {
      "epoch": 0.9093163594279573,
      "grad_norm": 0.20574186742305756,
      "learning_rate": 6.520833333333334e-05,
      "loss": 0.2701,
      "step": 93500
    },
    {
      "epoch": 0.9102888902936557,
      "grad_norm": 0.19157885015010834,
      "learning_rate": 6.516666666666666e-05,
      "loss": 0.2694,
      "step": 93600
    },
    {
      "epoch": 0.911261421159354,
      "grad_norm": 0.22899141907691956,
      "learning_rate": 6.5125e-05,
      "loss": 0.2692,
      "step": 93700
    },
    {
      "epoch": 0.9122339520250524,
      "grad_norm": 0.2224147915840149,
      "learning_rate": 6.508333333333333e-05,
      "loss": 0.2708,
      "step": 93800
    },
    {
      "epoch": 0.9132064828907508,
      "grad_norm": 0.23205064237117767,
      "learning_rate": 6.504166666666667e-05,
      "loss": 0.2712,
      "step": 93900
    },
    {
      "epoch": 0.914179013756449,
      "grad_norm": 0.2216743677854538,
      "learning_rate": 6.500000000000001e-05,
      "loss": 0.2693,
      "step": 94000
    },
    {
      "epoch": 0.9151515446221474,
      "grad_norm": 0.19925962388515472,
      "learning_rate": 6.495833333333333e-05,
      "loss": 0.2714,
      "step": 94100
    },
    {
      "epoch": 0.9161240754878458,
      "grad_norm": 0.1941392421722412,
      "learning_rate": 6.491666666666667e-05,
      "loss": 0.2684,
      "step": 94200
    },
    {
      "epoch": 0.9170966063535442,
      "grad_norm": 0.20691239833831787,
      "learning_rate": 6.4875e-05,
      "loss": 0.2698,
      "step": 94300
    },
    {
      "epoch": 0.9180691372192425,
      "grad_norm": 0.19159270823001862,
      "learning_rate": 6.483333333333333e-05,
      "loss": 0.2695,
      "step": 94400
    },
    {
      "epoch": 0.9190416680849408,
      "grad_norm": 0.2168741375207901,
      "learning_rate": 6.479166666666668e-05,
      "loss": 0.2717,
      "step": 94500
    },
    {
      "epoch": 0.9200141989506392,
      "grad_norm": 0.19944503903388977,
      "learning_rate": 6.475e-05,
      "loss": 0.27,
      "step": 94600
    },
    {
      "epoch": 0.9209867298163376,
      "grad_norm": 0.20757223665714264,
      "learning_rate": 6.470833333333334e-05,
      "loss": 0.2709,
      "step": 94700
    },
    {
      "epoch": 0.9219592606820359,
      "grad_norm": 0.20461201667785645,
      "learning_rate": 6.466666666666666e-05,
      "loss": 0.2697,
      "step": 94800
    },
    {
      "epoch": 0.9229317915477342,
      "grad_norm": 0.1866825520992279,
      "learning_rate": 6.4625e-05,
      "loss": 0.27,
      "step": 94900
    },
    {
      "epoch": 0.9239043224134326,
      "grad_norm": 0.2193605750799179,
      "learning_rate": 6.458333333333334e-05,
      "loss": 0.2697,
      "step": 95000
    },
    {
      "epoch": 0.9248768532791309,
      "grad_norm": 0.20325909554958344,
      "learning_rate": 6.454166666666667e-05,
      "loss": 0.2699,
      "step": 95100
    },
    {
      "epoch": 0.9258493841448293,
      "grad_norm": 0.21023574471473694,
      "learning_rate": 6.450000000000001e-05,
      "loss": 0.2701,
      "step": 95200
    },
    {
      "epoch": 0.9268219150105277,
      "grad_norm": 0.21573065221309662,
      "learning_rate": 6.445833333333333e-05,
      "loss": 0.2691,
      "step": 95300
    },
    {
      "epoch": 0.927794445876226,
      "grad_norm": 0.21860936284065247,
      "learning_rate": 6.441666666666667e-05,
      "loss": 0.2697,
      "step": 95400
    },
    {
      "epoch": 0.9287669767419243,
      "grad_norm": 0.20234371721744537,
      "learning_rate": 6.4375e-05,
      "loss": 0.2681,
      "step": 95500
    },
    {
      "epoch": 0.9297395076076227,
      "grad_norm": 0.19452865421772003,
      "learning_rate": 6.433333333333333e-05,
      "loss": 0.2702,
      "step": 95600
    },
    {
      "epoch": 0.9307120384733211,
      "grad_norm": 0.2203240692615509,
      "learning_rate": 6.429166666666667e-05,
      "loss": 0.2706,
      "step": 95700
    },
    {
      "epoch": 0.9316845693390194,
      "grad_norm": 0.20155800879001617,
      "learning_rate": 6.425e-05,
      "loss": 0.2709,
      "step": 95800
    },
    {
      "epoch": 0.9326571002047177,
      "grad_norm": 0.22809618711471558,
      "learning_rate": 6.420833333333334e-05,
      "loss": 0.2699,
      "step": 95900
    },
    {
      "epoch": 0.9336296310704161,
      "grad_norm": 0.2228519320487976,
      "learning_rate": 6.416666666666668e-05,
      "loss": 0.2702,
      "step": 96000
    },
    {
      "epoch": 0.9346021619361145,
      "grad_norm": 0.2137032449245453,
      "learning_rate": 6.4125e-05,
      "loss": 0.2707,
      "step": 96100
    },
    {
      "epoch": 0.9355746928018128,
      "grad_norm": 0.1992332488298416,
      "learning_rate": 6.408333333333334e-05,
      "loss": 0.2687,
      "step": 96200
    },
    {
      "epoch": 0.9365472236675112,
      "grad_norm": 0.2254989594221115,
      "learning_rate": 6.404166666666666e-05,
      "loss": 0.2689,
      "step": 96300
    },
    {
      "epoch": 0.9375197545332095,
      "grad_norm": 0.22553029656410217,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.2707,
      "step": 96400
    },
    {
      "epoch": 0.9384922853989078,
      "grad_norm": 0.20283696055412292,
      "learning_rate": 6.395833333333333e-05,
      "loss": 0.2681,
      "step": 96500
    },
    {
      "epoch": 0.9394648162646062,
      "grad_norm": 0.19092093408107758,
      "learning_rate": 6.391666666666667e-05,
      "loss": 0.2685,
      "step": 96600
    },
    {
      "epoch": 0.9404373471303046,
      "grad_norm": 0.2035970836877823,
      "learning_rate": 6.387500000000001e-05,
      "loss": 0.2691,
      "step": 96700
    },
    {
      "epoch": 0.941409877996003,
      "grad_norm": 0.212959423661232,
      "learning_rate": 6.383333333333333e-05,
      "loss": 0.2702,
      "step": 96800
    },
    {
      "epoch": 0.9423824088617012,
      "grad_norm": 0.2091282308101654,
      "learning_rate": 6.379166666666667e-05,
      "loss": 0.2697,
      "step": 96900
    },
    {
      "epoch": 0.9433549397273996,
      "grad_norm": 0.20970706641674042,
      "learning_rate": 6.375e-05,
      "loss": 0.2707,
      "step": 97000
    },
    {
      "epoch": 0.944327470593098,
      "grad_norm": 0.19953469932079315,
      "learning_rate": 6.370833333333334e-05,
      "loss": 0.2684,
      "step": 97100
    },
    {
      "epoch": 0.9453000014587963,
      "grad_norm": 0.20529456436634064,
      "learning_rate": 6.366666666666668e-05,
      "loss": 0.269,
      "step": 97200
    },
    {
      "epoch": 0.9462725323244946,
      "grad_norm": 0.20027749240398407,
      "learning_rate": 6.3625e-05,
      "loss": 0.2702,
      "step": 97300
    },
    {
      "epoch": 0.947245063190193,
      "grad_norm": 0.2142798751592636,
      "learning_rate": 6.358333333333334e-05,
      "loss": 0.2697,
      "step": 97400
    },
    {
      "epoch": 0.9482175940558913,
      "grad_norm": 0.20749861001968384,
      "learning_rate": 6.354166666666666e-05,
      "loss": 0.2711,
      "step": 97500
    },
    {
      "epoch": 0.9491901249215897,
      "grad_norm": 0.19280050694942474,
      "learning_rate": 6.35e-05,
      "loss": 0.2683,
      "step": 97600
    },
    {
      "epoch": 0.9501626557872881,
      "grad_norm": 0.20526596903800964,
      "learning_rate": 6.345833333333334e-05,
      "loss": 0.2674,
      "step": 97700
    },
    {
      "epoch": 0.9511351866529864,
      "grad_norm": 0.1889866143465042,
      "learning_rate": 6.341666666666667e-05,
      "loss": 0.2688,
      "step": 97800
    },
    {
      "epoch": 0.9521077175186847,
      "grad_norm": 0.22491411864757538,
      "learning_rate": 6.337500000000001e-05,
      "loss": 0.2672,
      "step": 97900
    },
    {
      "epoch": 0.9530802483843831,
      "grad_norm": 0.220502570271492,
      "learning_rate": 6.333333333333333e-05,
      "loss": 0.2679,
      "step": 98000
    },
    {
      "epoch": 0.9540527792500815,
      "grad_norm": 0.21017728745937347,
      "learning_rate": 6.329166666666667e-05,
      "loss": 0.2691,
      "step": 98100
    },
    {
      "epoch": 0.9550253101157798,
      "grad_norm": 0.20214833319187164,
      "learning_rate": 6.324999999999999e-05,
      "loss": 0.2675,
      "step": 98200
    },
    {
      "epoch": 0.9559978409814781,
      "grad_norm": 0.19594670832157135,
      "learning_rate": 6.320833333333334e-05,
      "loss": 0.2682,
      "step": 98300
    },
    {
      "epoch": 0.9569703718471765,
      "grad_norm": 0.20394091308116913,
      "learning_rate": 6.316666666666668e-05,
      "loss": 0.2688,
      "step": 98400
    },
    {
      "epoch": 0.9579429027128749,
      "grad_norm": 0.2098785936832428,
      "learning_rate": 6.3125e-05,
      "loss": 0.2681,
      "step": 98500
    },
    {
      "epoch": 0.9589154335785732,
      "grad_norm": 0.2059049904346466,
      "learning_rate": 6.308333333333334e-05,
      "loss": 0.2693,
      "step": 98600
    },
    {
      "epoch": 0.9598879644442716,
      "grad_norm": 0.21541361510753632,
      "learning_rate": 6.304166666666666e-05,
      "loss": 0.2681,
      "step": 98700
    },
    {
      "epoch": 0.9608604953099699,
      "grad_norm": 0.19650258123874664,
      "learning_rate": 6.3e-05,
      "loss": 0.2691,
      "step": 98800
    },
    {
      "epoch": 0.9618330261756682,
      "grad_norm": 0.23489320278167725,
      "learning_rate": 6.295833333333334e-05,
      "loss": 0.2709,
      "step": 98900
    },
    {
      "epoch": 0.9628055570413666,
      "grad_norm": 0.21813921630382538,
      "learning_rate": 6.291666666666667e-05,
      "loss": 0.2703,
      "step": 99000
    },
    {
      "epoch": 0.963778087907065,
      "grad_norm": 0.22161784768104553,
      "learning_rate": 6.287500000000001e-05,
      "loss": 0.2678,
      "step": 99100
    },
    {
      "epoch": 0.9647506187727632,
      "grad_norm": 0.21279916167259216,
      "learning_rate": 6.283333333333333e-05,
      "loss": 0.2692,
      "step": 99200
    },
    {
      "epoch": 0.9657231496384616,
      "grad_norm": 0.21399156749248505,
      "learning_rate": 6.279166666666667e-05,
      "loss": 0.2713,
      "step": 99300
    },
    {
      "epoch": 0.96669568050416,
      "grad_norm": 0.19980642199516296,
      "learning_rate": 6.275e-05,
      "loss": 0.2689,
      "step": 99400
    },
    {
      "epoch": 0.9676682113698584,
      "grad_norm": 0.20184768736362457,
      "learning_rate": 6.270833333333333e-05,
      "loss": 0.2679,
      "step": 99500
    },
    {
      "epoch": 0.9686407422355567,
      "grad_norm": 0.22194750607013702,
      "learning_rate": 6.266666666666667e-05,
      "loss": 0.2709,
      "step": 99600
    },
    {
      "epoch": 0.969613273101255,
      "grad_norm": 0.19136185944080353,
      "learning_rate": 6.2625e-05,
      "loss": 0.2719,
      "step": 99700
    },
    {
      "epoch": 0.9705858039669534,
      "grad_norm": 0.20235225558280945,
      "learning_rate": 6.258333333333334e-05,
      "loss": 0.2697,
      "step": 99800
    },
    {
      "epoch": 0.9715583348326517,
      "grad_norm": 0.2019340693950653,
      "learning_rate": 6.254166666666666e-05,
      "loss": 0.2699,
      "step": 99900
    },
    {
      "epoch": 0.9725308656983501,
      "grad_norm": 0.2045041173696518,
      "learning_rate": 6.25e-05,
      "loss": 0.2682,
      "step": 100000
    },
    {
      "epoch": 0.9725308656983501,
      "eval_loss": 0.26901477575302124,
      "eval_runtime": 3223.7392,
      "eval_samples_per_second": 708.727,
      "eval_steps_per_second": 7.087,
      "step": 100000
    },
    {
      "epoch": 0.9735033965640485,
      "grad_norm": 0.18965739011764526,
      "learning_rate": 6.245833333333334e-05,
      "loss": 0.2697,
      "step": 100100
    },
    {
      "epoch": 0.9744759274297468,
      "grad_norm": 0.2199510782957077,
      "learning_rate": 6.241666666666666e-05,
      "loss": 0.2693,
      "step": 100200
    },
    {
      "epoch": 0.9754484582954451,
      "grad_norm": 0.20638465881347656,
      "learning_rate": 6.237500000000001e-05,
      "loss": 0.2664,
      "step": 100300
    },
    {
      "epoch": 0.9764209891611435,
      "grad_norm": 0.20524488389492035,
      "learning_rate": 6.233333333333334e-05,
      "loss": 0.2682,
      "step": 100400
    },
    {
      "epoch": 0.9773935200268419,
      "grad_norm": 0.21310356259346008,
      "learning_rate": 6.229166666666667e-05,
      "loss": 0.2676,
      "step": 100500
    },
    {
      "epoch": 0.9783660508925403,
      "grad_norm": 0.2128274291753769,
      "learning_rate": 6.225000000000001e-05,
      "loss": 0.2692,
      "step": 100600
    },
    {
      "epoch": 0.9793385817582385,
      "grad_norm": 0.21065978705883026,
      "learning_rate": 6.220833333333333e-05,
      "loss": 0.2673,
      "step": 100700
    },
    {
      "epoch": 0.9803111126239369,
      "grad_norm": 0.22828349471092224,
      "learning_rate": 6.216666666666667e-05,
      "loss": 0.2687,
      "step": 100800
    },
    {
      "epoch": 0.9812836434896353,
      "grad_norm": 0.20269057154655457,
      "learning_rate": 6.2125e-05,
      "loss": 0.2706,
      "step": 100900
    },
    {
      "epoch": 0.9822561743553336,
      "grad_norm": 0.20880454778671265,
      "learning_rate": 6.208333333333334e-05,
      "loss": 0.2707,
      "step": 101000
    },
    {
      "epoch": 0.983228705221032,
      "grad_norm": 0.21904480457305908,
      "learning_rate": 6.204166666666667e-05,
      "loss": 0.2692,
      "step": 101100
    },
    {
      "epoch": 0.9842012360867303,
      "grad_norm": 0.20012320578098297,
      "learning_rate": 6.2e-05,
      "loss": 0.2697,
      "step": 101200
    },
    {
      "epoch": 0.9851737669524286,
      "grad_norm": 0.20130489766597748,
      "learning_rate": 6.195833333333334e-05,
      "loss": 0.27,
      "step": 101300
    },
    {
      "epoch": 0.986146297818127,
      "grad_norm": 0.2158805876970291,
      "learning_rate": 6.191666666666666e-05,
      "loss": 0.2679,
      "step": 101400
    },
    {
      "epoch": 0.9871188286838254,
      "grad_norm": 0.22174443304538727,
      "learning_rate": 6.1875e-05,
      "loss": 0.2683,
      "step": 101500
    },
    {
      "epoch": 0.9880913595495237,
      "grad_norm": 0.20198044180870056,
      "learning_rate": 6.183333333333334e-05,
      "loss": 0.2674,
      "step": 101600
    },
    {
      "epoch": 0.989063890415222,
      "grad_norm": 0.20968250930309296,
      "learning_rate": 6.179166666666667e-05,
      "loss": 0.2677,
      "step": 101700
    },
    {
      "epoch": 0.9900364212809204,
      "grad_norm": 0.2074027806520462,
      "learning_rate": 6.175000000000001e-05,
      "loss": 0.2695,
      "step": 101800
    },
    {
      "epoch": 0.9910089521466188,
      "grad_norm": 0.21156232059001923,
      "learning_rate": 6.170833333333333e-05,
      "loss": 0.2677,
      "step": 101900
    },
    {
      "epoch": 0.9919814830123171,
      "grad_norm": 0.21420268714427948,
      "learning_rate": 6.166666666666667e-05,
      "loss": 0.2691,
      "step": 102000
    },
    {
      "epoch": 0.9929540138780154,
      "grad_norm": 0.21320052444934845,
      "learning_rate": 6.1625e-05,
      "loss": 0.2681,
      "step": 102100
    },
    {
      "epoch": 0.9939265447437138,
      "grad_norm": 0.22800515592098236,
      "learning_rate": 6.158333333333334e-05,
      "loss": 0.2692,
      "step": 102200
    },
    {
      "epoch": 0.9948990756094122,
      "grad_norm": 0.17984618246555328,
      "learning_rate": 6.154166666666667e-05,
      "loss": 0.2676,
      "step": 102300
    },
    {
      "epoch": 0.9958716064751105,
      "grad_norm": 0.20721188187599182,
      "learning_rate": 6.15e-05,
      "loss": 0.2678,
      "step": 102400
    },
    {
      "epoch": 0.9968441373408089,
      "grad_norm": 0.19117720425128937,
      "learning_rate": 6.145833333333334e-05,
      "loss": 0.2691,
      "step": 102500
    },
    {
      "epoch": 0.9978166682065072,
      "grad_norm": 0.22558659315109253,
      "learning_rate": 6.141666666666666e-05,
      "loss": 0.2694,
      "step": 102600
    },
    {
      "epoch": 0.9987891990722055,
      "grad_norm": 0.20023244619369507,
      "learning_rate": 6.1375e-05,
      "loss": 0.2688,
      "step": 102700
    },
    {
      "epoch": 0.9997617299379039,
      "grad_norm": 0.2018483430147171,
      "learning_rate": 6.133333333333334e-05,
      "loss": 0.2686,
      "step": 102800
    },
    {
      "epoch": 1.0007342608036023,
      "grad_norm": 0.2209031730890274,
      "learning_rate": 6.129166666666667e-05,
      "loss": 0.267,
      "step": 102900
    },
    {
      "epoch": 1.0017067916693005,
      "grad_norm": 0.2213941365480423,
      "learning_rate": 6.125000000000001e-05,
      "loss": 0.2669,
      "step": 103000
    },
    {
      "epoch": 1.002679322534999,
      "grad_norm": 0.2465517371892929,
      "learning_rate": 6.120833333333333e-05,
      "loss": 0.2676,
      "step": 103100
    },
    {
      "epoch": 1.0036518534006973,
      "grad_norm": 0.22683463990688324,
      "learning_rate": 6.116666666666667e-05,
      "loss": 0.2655,
      "step": 103200
    },
    {
      "epoch": 1.0046243842663956,
      "grad_norm": 0.22121934592723846,
      "learning_rate": 6.1125e-05,
      "loss": 0.2646,
      "step": 103300
    },
    {
      "epoch": 1.005596915132094,
      "grad_norm": 0.19997960329055786,
      "learning_rate": 6.108333333333333e-05,
      "loss": 0.2645,
      "step": 103400
    },
    {
      "epoch": 1.0065694459977923,
      "grad_norm": 0.21393923461437225,
      "learning_rate": 6.104166666666667e-05,
      "loss": 0.2664,
      "step": 103500
    },
    {
      "epoch": 1.0075419768634908,
      "grad_norm": 0.21825455129146576,
      "learning_rate": 6.1e-05,
      "loss": 0.2663,
      "step": 103600
    },
    {
      "epoch": 1.008514507729189,
      "grad_norm": 0.2130586802959442,
      "learning_rate": 6.095833333333334e-05,
      "loss": 0.2667,
      "step": 103700
    },
    {
      "epoch": 1.0094870385948873,
      "grad_norm": 0.20904147624969482,
      "learning_rate": 6.0916666666666666e-05,
      "loss": 0.2658,
      "step": 103800
    },
    {
      "epoch": 1.0104595694605858,
      "grad_norm": 0.2265886813402176,
      "learning_rate": 6.0875e-05,
      "loss": 0.2664,
      "step": 103900
    },
    {
      "epoch": 1.011432100326284,
      "grad_norm": 0.2203701287508011,
      "learning_rate": 6.083333333333333e-05,
      "loss": 0.2666,
      "step": 104000
    },
    {
      "epoch": 1.0124046311919825,
      "grad_norm": 0.21870772540569305,
      "learning_rate": 6.079166666666667e-05,
      "loss": 0.2645,
      "step": 104100
    },
    {
      "epoch": 1.0133771620576808,
      "grad_norm": 0.21911142766475677,
      "learning_rate": 6.0750000000000006e-05,
      "loss": 0.2661,
      "step": 104200
    },
    {
      "epoch": 1.014349692923379,
      "grad_norm": 0.2037145048379898,
      "learning_rate": 6.0708333333333336e-05,
      "loss": 0.2655,
      "step": 104300
    },
    {
      "epoch": 1.0153222237890775,
      "grad_norm": 0.20175407826900482,
      "learning_rate": 6.066666666666667e-05,
      "loss": 0.2632,
      "step": 104400
    },
    {
      "epoch": 1.0162947546547758,
      "grad_norm": 0.20845448970794678,
      "learning_rate": 6.0624999999999996e-05,
      "loss": 0.2653,
      "step": 104500
    },
    {
      "epoch": 1.0172672855204743,
      "grad_norm": 0.22398753464221954,
      "learning_rate": 6.058333333333333e-05,
      "loss": 0.265,
      "step": 104600
    },
    {
      "epoch": 1.0182398163861726,
      "grad_norm": 0.23852698504924774,
      "learning_rate": 6.054166666666668e-05,
      "loss": 0.2643,
      "step": 104700
    },
    {
      "epoch": 1.0192123472518708,
      "grad_norm": 0.222347691655159,
      "learning_rate": 6.05e-05,
      "loss": 0.2662,
      "step": 104800
    },
    {
      "epoch": 1.0201848781175693,
      "grad_norm": 0.20686517655849457,
      "learning_rate": 6.045833333333334e-05,
      "loss": 0.2672,
      "step": 104900
    },
    {
      "epoch": 1.0211574089832676,
      "grad_norm": 0.214919775724411,
      "learning_rate": 6.041666666666667e-05,
      "loss": 0.2647,
      "step": 105000
    },
    {
      "epoch": 1.022129939848966,
      "grad_norm": 0.20286881923675537,
      "learning_rate": 6.0375000000000004e-05,
      "loss": 0.2643,
      "step": 105100
    },
    {
      "epoch": 1.0231024707146643,
      "grad_norm": 0.20764058828353882,
      "learning_rate": 6.033333333333334e-05,
      "loss": 0.2652,
      "step": 105200
    },
    {
      "epoch": 1.0240750015803626,
      "grad_norm": 0.19572663307189941,
      "learning_rate": 6.029166666666667e-05,
      "loss": 0.2652,
      "step": 105300
    },
    {
      "epoch": 1.025047532446061,
      "grad_norm": 0.21952323615550995,
      "learning_rate": 6.025000000000001e-05,
      "loss": 0.2665,
      "step": 105400
    },
    {
      "epoch": 1.0260200633117593,
      "grad_norm": 0.2002488225698471,
      "learning_rate": 6.020833333333333e-05,
      "loss": 0.2686,
      "step": 105500
    },
    {
      "epoch": 1.0269925941774578,
      "grad_norm": 0.21996520459651947,
      "learning_rate": 6.0166666666666674e-05,
      "loss": 0.2646,
      "step": 105600
    },
    {
      "epoch": 1.027965125043156,
      "grad_norm": 0.21323008835315704,
      "learning_rate": 6.0125e-05,
      "loss": 0.2669,
      "step": 105700
    },
    {
      "epoch": 1.0289376559088543,
      "grad_norm": 0.21558567881584167,
      "learning_rate": 6.0083333333333335e-05,
      "loss": 0.267,
      "step": 105800
    },
    {
      "epoch": 1.0299101867745528,
      "grad_norm": 0.22140491008758545,
      "learning_rate": 6.004166666666667e-05,
      "loss": 0.2652,
      "step": 105900
    },
    {
      "epoch": 1.030882717640251,
      "grad_norm": 0.20051626861095428,
      "learning_rate": 6e-05,
      "loss": 0.2669,
      "step": 106000
    },
    {
      "epoch": 1.0318552485059496,
      "grad_norm": 0.2146245688199997,
      "learning_rate": 5.995833333333334e-05,
      "loss": 0.2656,
      "step": 106100
    },
    {
      "epoch": 1.0328277793716478,
      "grad_norm": 0.21280227601528168,
      "learning_rate": 5.991666666666667e-05,
      "loss": 0.2643,
      "step": 106200
    },
    {
      "epoch": 1.033800310237346,
      "grad_norm": 0.21874244511127472,
      "learning_rate": 5.9875000000000005e-05,
      "loss": 0.2645,
      "step": 106300
    },
    {
      "epoch": 1.0347728411030446,
      "grad_norm": 0.2338070124387741,
      "learning_rate": 5.983333333333334e-05,
      "loss": 0.2645,
      "step": 106400
    },
    {
      "epoch": 1.0357453719687428,
      "grad_norm": 0.20393994450569153,
      "learning_rate": 5.9791666666666665e-05,
      "loss": 0.2632,
      "step": 106500
    },
    {
      "epoch": 1.0367179028344413,
      "grad_norm": 0.2125510722398758,
      "learning_rate": 5.975000000000001e-05,
      "loss": 0.2644,
      "step": 106600
    },
    {
      "epoch": 1.0376904337001396,
      "grad_norm": 0.20983122289180756,
      "learning_rate": 5.970833333333333e-05,
      "loss": 0.2656,
      "step": 106700
    },
    {
      "epoch": 1.0386629645658378,
      "grad_norm": 0.2035893350839615,
      "learning_rate": 5.966666666666667e-05,
      "loss": 0.2647,
      "step": 106800
    },
    {
      "epoch": 1.0396354954315363,
      "grad_norm": 0.21142908930778503,
      "learning_rate": 5.9625e-05,
      "loss": 0.265,
      "step": 106900
    },
    {
      "epoch": 1.0406080262972346,
      "grad_norm": 0.20809650421142578,
      "learning_rate": 5.9583333333333336e-05,
      "loss": 0.264,
      "step": 107000
    },
    {
      "epoch": 1.0415805571629329,
      "grad_norm": 0.19848555326461792,
      "learning_rate": 5.954166666666667e-05,
      "loss": 0.2665,
      "step": 107100
    },
    {
      "epoch": 1.0425530880286313,
      "grad_norm": 0.2195459008216858,
      "learning_rate": 5.95e-05,
      "loss": 0.2659,
      "step": 107200
    },
    {
      "epoch": 1.0435256188943296,
      "grad_norm": 0.2012062668800354,
      "learning_rate": 5.945833333333334e-05,
      "loss": 0.2643,
      "step": 107300
    },
    {
      "epoch": 1.044498149760028,
      "grad_norm": 0.21059897541999817,
      "learning_rate": 5.941666666666666e-05,
      "loss": 0.2651,
      "step": 107400
    },
    {
      "epoch": 1.0454706806257263,
      "grad_norm": 0.21087993681430817,
      "learning_rate": 5.9375e-05,
      "loss": 0.2661,
      "step": 107500
    },
    {
      "epoch": 1.0464432114914246,
      "grad_norm": 0.22635944187641144,
      "learning_rate": 5.9333333333333343e-05,
      "loss": 0.2648,
      "step": 107600
    },
    {
      "epoch": 1.047415742357123,
      "grad_norm": 0.22877341508865356,
      "learning_rate": 5.929166666666667e-05,
      "loss": 0.2651,
      "step": 107700
    },
    {
      "epoch": 1.0483882732228214,
      "grad_norm": 0.22508063912391663,
      "learning_rate": 5.9250000000000004e-05,
      "loss": 0.265,
      "step": 107800
    },
    {
      "epoch": 1.0493608040885198,
      "grad_norm": 0.2091212123632431,
      "learning_rate": 5.9208333333333334e-05,
      "loss": 0.2654,
      "step": 107900
    },
    {
      "epoch": 1.050333334954218,
      "grad_norm": 0.24024136364459991,
      "learning_rate": 5.916666666666667e-05,
      "loss": 0.2661,
      "step": 108000
    },
    {
      "epoch": 1.0513058658199164,
      "grad_norm": 0.21529017388820648,
      "learning_rate": 5.912500000000001e-05,
      "loss": 0.2666,
      "step": 108100
    },
    {
      "epoch": 1.0522783966856148,
      "grad_norm": 0.21501754224300385,
      "learning_rate": 5.908333333333334e-05,
      "loss": 0.2659,
      "step": 108200
    },
    {
      "epoch": 1.053250927551313,
      "grad_norm": 0.20803771913051605,
      "learning_rate": 5.9041666666666674e-05,
      "loss": 0.2668,
      "step": 108300
    },
    {
      "epoch": 1.0542234584170116,
      "grad_norm": 0.23305073380470276,
      "learning_rate": 5.9e-05,
      "loss": 0.2642,
      "step": 108400
    },
    {
      "epoch": 1.0551959892827099,
      "grad_norm": 0.2287076711654663,
      "learning_rate": 5.8958333333333334e-05,
      "loss": 0.2666,
      "step": 108500
    },
    {
      "epoch": 1.0561685201484081,
      "grad_norm": 0.2279803305864334,
      "learning_rate": 5.8916666666666664e-05,
      "loss": 0.267,
      "step": 108600
    },
    {
      "epoch": 1.0571410510141066,
      "grad_norm": 0.24105708301067352,
      "learning_rate": 5.8875e-05,
      "loss": 0.267,
      "step": 108700
    },
    {
      "epoch": 1.0581135818798049,
      "grad_norm": 0.21997086703777313,
      "learning_rate": 5.883333333333334e-05,
      "loss": 0.2633,
      "step": 108800
    },
    {
      "epoch": 1.0590861127455033,
      "grad_norm": 0.2052147537469864,
      "learning_rate": 5.879166666666667e-05,
      "loss": 0.2659,
      "step": 108900
    },
    {
      "epoch": 1.0600586436112016,
      "grad_norm": 0.23352636396884918,
      "learning_rate": 5.8750000000000005e-05,
      "loss": 0.2633,
      "step": 109000
    },
    {
      "epoch": 1.0610311744768999,
      "grad_norm": 0.19176830351352692,
      "learning_rate": 5.8708333333333335e-05,
      "loss": 0.2662,
      "step": 109100
    },
    {
      "epoch": 1.0620037053425984,
      "grad_norm": 0.2335892766714096,
      "learning_rate": 5.866666666666667e-05,
      "loss": 0.2642,
      "step": 109200
    },
    {
      "epoch": 1.0629762362082966,
      "grad_norm": 0.23785996437072754,
      "learning_rate": 5.862500000000001e-05,
      "loss": 0.2667,
      "step": 109300
    },
    {
      "epoch": 1.063948767073995,
      "grad_norm": 0.21450857818126678,
      "learning_rate": 5.858333333333333e-05,
      "loss": 0.2659,
      "step": 109400
    },
    {
      "epoch": 1.0649212979396934,
      "grad_norm": 0.2203337848186493,
      "learning_rate": 5.8541666666666676e-05,
      "loss": 0.2661,
      "step": 109500
    },
    {
      "epoch": 1.0658938288053916,
      "grad_norm": 0.21851743757724762,
      "learning_rate": 5.85e-05,
      "loss": 0.2646,
      "step": 109600
    },
    {
      "epoch": 1.0668663596710901,
      "grad_norm": 0.2184102088212967,
      "learning_rate": 5.8458333333333336e-05,
      "loss": 0.2653,
      "step": 109700
    },
    {
      "epoch": 1.0678388905367884,
      "grad_norm": 0.20906449854373932,
      "learning_rate": 5.8416666666666666e-05,
      "loss": 0.2648,
      "step": 109800
    },
    {
      "epoch": 1.0688114214024869,
      "grad_norm": 0.21916785836219788,
      "learning_rate": 5.8375e-05,
      "loss": 0.2659,
      "step": 109900
    },
    {
      "epoch": 1.0697839522681851,
      "grad_norm": 0.23839044570922852,
      "learning_rate": 5.833333333333334e-05,
      "loss": 0.2639,
      "step": 110000
    },
    {
      "epoch": 1.0697839522681851,
      "eval_loss": 0.2674519419670105,
      "eval_runtime": 3111.2597,
      "eval_samples_per_second": 734.349,
      "eval_steps_per_second": 7.344,
      "step": 110000
    },
    {
      "epoch": 1.0707564831338834,
      "grad_norm": 0.21889476478099823,
      "learning_rate": 5.829166666666667e-05,
      "loss": 0.2667,
      "step": 110100
    },
    {
      "epoch": 1.0717290139995819,
      "grad_norm": 0.23849976062774658,
      "learning_rate": 5.8250000000000006e-05,
      "loss": 0.2649,
      "step": 110200
    },
    {
      "epoch": 1.0727015448652801,
      "grad_norm": 0.21263949573040009,
      "learning_rate": 5.820833333333333e-05,
      "loss": 0.2648,
      "step": 110300
    },
    {
      "epoch": 1.0736740757309784,
      "grad_norm": 0.19987303018569946,
      "learning_rate": 5.8166666666666667e-05,
      "loss": 0.2644,
      "step": 110400
    },
    {
      "epoch": 1.0746466065966769,
      "grad_norm": 0.21583636105060577,
      "learning_rate": 5.812500000000001e-05,
      "loss": 0.266,
      "step": 110500
    },
    {
      "epoch": 1.0756191374623751,
      "grad_norm": 0.18809199333190918,
      "learning_rate": 5.8083333333333333e-05,
      "loss": 0.2674,
      "step": 110600
    },
    {
      "epoch": 1.0765916683280736,
      "grad_norm": 0.19824309647083282,
      "learning_rate": 5.804166666666667e-05,
      "loss": 0.2657,
      "step": 110700
    },
    {
      "epoch": 1.0775641991937719,
      "grad_norm": 0.20447614789009094,
      "learning_rate": 5.8e-05,
      "loss": 0.2635,
      "step": 110800
    },
    {
      "epoch": 1.0785367300594704,
      "grad_norm": 0.21094518899917603,
      "learning_rate": 5.795833333333334e-05,
      "loss": 0.2654,
      "step": 110900
    },
    {
      "epoch": 1.0795092609251686,
      "grad_norm": 0.22362206876277924,
      "learning_rate": 5.7916666666666674e-05,
      "loss": 0.2643,
      "step": 111000
    },
    {
      "epoch": 1.080481791790867,
      "grad_norm": 0.21690620481967926,
      "learning_rate": 5.7875000000000004e-05,
      "loss": 0.2654,
      "step": 111100
    },
    {
      "epoch": 1.0814543226565654,
      "grad_norm": 0.22189205884933472,
      "learning_rate": 5.783333333333334e-05,
      "loss": 0.2669,
      "step": 111200
    },
    {
      "epoch": 1.0824268535222636,
      "grad_norm": 0.22811740636825562,
      "learning_rate": 5.7791666666666664e-05,
      "loss": 0.2654,
      "step": 111300
    },
    {
      "epoch": 1.083399384387962,
      "grad_norm": 0.20172356069087982,
      "learning_rate": 5.775e-05,
      "loss": 0.2638,
      "step": 111400
    },
    {
      "epoch": 1.0843719152536604,
      "grad_norm": 0.22223344445228577,
      "learning_rate": 5.770833333333333e-05,
      "loss": 0.2656,
      "step": 111500
    },
    {
      "epoch": 1.0853444461193587,
      "grad_norm": 0.21257206797599792,
      "learning_rate": 5.766666666666667e-05,
      "loss": 0.265,
      "step": 111600
    },
    {
      "epoch": 1.0863169769850571,
      "grad_norm": 0.22061198949813843,
      "learning_rate": 5.7625000000000005e-05,
      "loss": 0.2644,
      "step": 111700
    },
    {
      "epoch": 1.0872895078507554,
      "grad_norm": 0.2107248157262802,
      "learning_rate": 5.7583333333333335e-05,
      "loss": 0.2651,
      "step": 111800
    },
    {
      "epoch": 1.0882620387164537,
      "grad_norm": 0.22528639435768127,
      "learning_rate": 5.754166666666667e-05,
      "loss": 0.2658,
      "step": 111900
    },
    {
      "epoch": 1.0892345695821521,
      "grad_norm": 0.20262274146080017,
      "learning_rate": 5.7499999999999995e-05,
      "loss": 0.2656,
      "step": 112000
    },
    {
      "epoch": 1.0902071004478504,
      "grad_norm": 0.22544339299201965,
      "learning_rate": 5.745833333333334e-05,
      "loss": 0.2654,
      "step": 112100
    },
    {
      "epoch": 1.091179631313549,
      "grad_norm": 0.21789123117923737,
      "learning_rate": 5.7416666666666675e-05,
      "loss": 0.2668,
      "step": 112200
    },
    {
      "epoch": 1.0921521621792472,
      "grad_norm": 0.212062805891037,
      "learning_rate": 5.7375e-05,
      "loss": 0.2663,
      "step": 112300
    },
    {
      "epoch": 1.0931246930449454,
      "grad_norm": 0.2210545837879181,
      "learning_rate": 5.7333333333333336e-05,
      "loss": 0.2648,
      "step": 112400
    },
    {
      "epoch": 1.094097223910644,
      "grad_norm": 0.21407635509967804,
      "learning_rate": 5.7291666666666666e-05,
      "loss": 0.2641,
      "step": 112500
    },
    {
      "epoch": 1.0950697547763422,
      "grad_norm": 0.23009264469146729,
      "learning_rate": 5.725e-05,
      "loss": 0.2636,
      "step": 112600
    },
    {
      "epoch": 1.0960422856420406,
      "grad_norm": 0.2352195829153061,
      "learning_rate": 5.720833333333334e-05,
      "loss": 0.2646,
      "step": 112700
    },
    {
      "epoch": 1.097014816507739,
      "grad_norm": 0.22042948007583618,
      "learning_rate": 5.716666666666667e-05,
      "loss": 0.2638,
      "step": 112800
    },
    {
      "epoch": 1.0979873473734372,
      "grad_norm": 0.21805280447006226,
      "learning_rate": 5.7125000000000006e-05,
      "loss": 0.264,
      "step": 112900
    },
    {
      "epoch": 1.0989598782391357,
      "grad_norm": 0.20597249269485474,
      "learning_rate": 5.7083333333333336e-05,
      "loss": 0.2662,
      "step": 113000
    },
    {
      "epoch": 1.099932409104834,
      "grad_norm": 0.20387569069862366,
      "learning_rate": 5.704166666666667e-05,
      "loss": 0.2653,
      "step": 113100
    },
    {
      "epoch": 1.1009049399705324,
      "grad_norm": 0.2288619875907898,
      "learning_rate": 5.6999999999999996e-05,
      "loss": 0.2672,
      "step": 113200
    },
    {
      "epoch": 1.1018774708362307,
      "grad_norm": 0.22677476704120636,
      "learning_rate": 5.695833333333333e-05,
      "loss": 0.2643,
      "step": 113300
    },
    {
      "epoch": 1.102850001701929,
      "grad_norm": 0.22223374247550964,
      "learning_rate": 5.691666666666668e-05,
      "loss": 0.2643,
      "step": 113400
    },
    {
      "epoch": 1.1038225325676274,
      "grad_norm": 0.20806872844696045,
      "learning_rate": 5.6875e-05,
      "loss": 0.264,
      "step": 113500
    },
    {
      "epoch": 1.1047950634333257,
      "grad_norm": 0.20330683887004852,
      "learning_rate": 5.683333333333334e-05,
      "loss": 0.2646,
      "step": 113600
    },
    {
      "epoch": 1.1057675942990242,
      "grad_norm": 0.22808299958705902,
      "learning_rate": 5.679166666666667e-05,
      "loss": 0.2636,
      "step": 113700
    },
    {
      "epoch": 1.1067401251647224,
      "grad_norm": 0.22100549936294556,
      "learning_rate": 5.6750000000000004e-05,
      "loss": 0.2638,
      "step": 113800
    },
    {
      "epoch": 1.1077126560304207,
      "grad_norm": 0.19071777164936066,
      "learning_rate": 5.670833333333334e-05,
      "loss": 0.2659,
      "step": 113900
    },
    {
      "epoch": 1.1086851868961192,
      "grad_norm": 0.23430407047271729,
      "learning_rate": 5.666666666666667e-05,
      "loss": 0.2653,
      "step": 114000
    },
    {
      "epoch": 1.1096577177618174,
      "grad_norm": 0.2217482626438141,
      "learning_rate": 5.662500000000001e-05,
      "loss": 0.2649,
      "step": 114100
    },
    {
      "epoch": 1.110630248627516,
      "grad_norm": 0.1964706927537918,
      "learning_rate": 5.658333333333333e-05,
      "loss": 0.2651,
      "step": 114200
    },
    {
      "epoch": 1.1116027794932142,
      "grad_norm": 0.21453385055065155,
      "learning_rate": 5.654166666666667e-05,
      "loss": 0.2635,
      "step": 114300
    },
    {
      "epoch": 1.1125753103589124,
      "grad_norm": 0.22308579087257385,
      "learning_rate": 5.65e-05,
      "loss": 0.2647,
      "step": 114400
    },
    {
      "epoch": 1.113547841224611,
      "grad_norm": 0.2216210961341858,
      "learning_rate": 5.6458333333333335e-05,
      "loss": 0.2654,
      "step": 114500
    },
    {
      "epoch": 1.1145203720903092,
      "grad_norm": 0.22913192212581635,
      "learning_rate": 5.641666666666667e-05,
      "loss": 0.2652,
      "step": 114600
    },
    {
      "epoch": 1.1154929029560077,
      "grad_norm": 0.20748725533485413,
      "learning_rate": 5.6375e-05,
      "loss": 0.2644,
      "step": 114700
    },
    {
      "epoch": 1.116465433821706,
      "grad_norm": 0.235512375831604,
      "learning_rate": 5.633333333333334e-05,
      "loss": 0.2664,
      "step": 114800
    },
    {
      "epoch": 1.1174379646874042,
      "grad_norm": 0.21943798661231995,
      "learning_rate": 5.629166666666666e-05,
      "loss": 0.2647,
      "step": 114900
    },
    {
      "epoch": 1.1184104955531027,
      "grad_norm": 0.23716431856155396,
      "learning_rate": 5.6250000000000005e-05,
      "loss": 0.2645,
      "step": 115000
    },
    {
      "epoch": 1.119383026418801,
      "grad_norm": 0.2460460066795349,
      "learning_rate": 5.620833333333334e-05,
      "loss": 0.2651,
      "step": 115100
    },
    {
      "epoch": 1.1203555572844994,
      "grad_norm": 0.23145918548107147,
      "learning_rate": 5.6166666666666665e-05,
      "loss": 0.2643,
      "step": 115200
    },
    {
      "epoch": 1.1213280881501977,
      "grad_norm": 0.21134227514266968,
      "learning_rate": 5.6125e-05,
      "loss": 0.2644,
      "step": 115300
    },
    {
      "epoch": 1.122300619015896,
      "grad_norm": 0.20543727278709412,
      "learning_rate": 5.608333333333333e-05,
      "loss": 0.2645,
      "step": 115400
    },
    {
      "epoch": 1.1232731498815944,
      "grad_norm": 0.2073943316936493,
      "learning_rate": 5.604166666666667e-05,
      "loss": 0.2655,
      "step": 115500
    },
    {
      "epoch": 1.1242456807472927,
      "grad_norm": 0.2322847843170166,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.264,
      "step": 115600
    },
    {
      "epoch": 1.125218211612991,
      "grad_norm": 0.20109379291534424,
      "learning_rate": 5.5958333333333336e-05,
      "loss": 0.2637,
      "step": 115700
    },
    {
      "epoch": 1.1261907424786894,
      "grad_norm": 0.20059701800346375,
      "learning_rate": 5.591666666666667e-05,
      "loss": 0.264,
      "step": 115800
    },
    {
      "epoch": 1.1271632733443877,
      "grad_norm": 0.19829818606376648,
      "learning_rate": 5.5875e-05,
      "loss": 0.2642,
      "step": 115900
    },
    {
      "epoch": 1.1281358042100862,
      "grad_norm": 0.19827242195606232,
      "learning_rate": 5.583333333333334e-05,
      "loss": 0.2639,
      "step": 116000
    },
    {
      "epoch": 1.1291083350757845,
      "grad_norm": 0.2077813446521759,
      "learning_rate": 5.579166666666666e-05,
      "loss": 0.2649,
      "step": 116100
    },
    {
      "epoch": 1.130080865941483,
      "grad_norm": 0.23225848376750946,
      "learning_rate": 5.575e-05,
      "loss": 0.263,
      "step": 116200
    },
    {
      "epoch": 1.1310533968071812,
      "grad_norm": 0.22998566925525665,
      "learning_rate": 5.5708333333333343e-05,
      "loss": 0.2651,
      "step": 116300
    },
    {
      "epoch": 1.1320259276728795,
      "grad_norm": 0.23514822125434875,
      "learning_rate": 5.566666666666667e-05,
      "loss": 0.2644,
      "step": 116400
    },
    {
      "epoch": 1.132998458538578,
      "grad_norm": 0.21680788695812225,
      "learning_rate": 5.5625000000000004e-05,
      "loss": 0.2644,
      "step": 116500
    },
    {
      "epoch": 1.1339709894042762,
      "grad_norm": 0.233132466673851,
      "learning_rate": 5.5583333333333334e-05,
      "loss": 0.2651,
      "step": 116600
    },
    {
      "epoch": 1.1349435202699745,
      "grad_norm": 0.21383090317249298,
      "learning_rate": 5.554166666666667e-05,
      "loss": 0.2652,
      "step": 116700
    },
    {
      "epoch": 1.135916051135673,
      "grad_norm": 0.1936347782611847,
      "learning_rate": 5.550000000000001e-05,
      "loss": 0.2651,
      "step": 116800
    },
    {
      "epoch": 1.1368885820013712,
      "grad_norm": 0.23976106941699982,
      "learning_rate": 5.545833333333334e-05,
      "loss": 0.264,
      "step": 116900
    },
    {
      "epoch": 1.1378611128670697,
      "grad_norm": 0.19636473059654236,
      "learning_rate": 5.5416666666666674e-05,
      "loss": 0.2647,
      "step": 117000
    },
    {
      "epoch": 1.138833643732768,
      "grad_norm": 0.22136060893535614,
      "learning_rate": 5.5375e-05,
      "loss": 0.2635,
      "step": 117100
    },
    {
      "epoch": 1.1398061745984662,
      "grad_norm": 0.22746331989765167,
      "learning_rate": 5.5333333333333334e-05,
      "loss": 0.2663,
      "step": 117200
    },
    {
      "epoch": 1.1407787054641647,
      "grad_norm": 0.232222318649292,
      "learning_rate": 5.5291666666666664e-05,
      "loss": 0.2653,
      "step": 117300
    },
    {
      "epoch": 1.141751236329863,
      "grad_norm": 0.21221783757209778,
      "learning_rate": 5.525e-05,
      "loss": 0.2645,
      "step": 117400
    },
    {
      "epoch": 1.1427237671955615,
      "grad_norm": 0.21437859535217285,
      "learning_rate": 5.520833333333334e-05,
      "loss": 0.2645,
      "step": 117500
    },
    {
      "epoch": 1.1436962980612597,
      "grad_norm": 0.22067315876483917,
      "learning_rate": 5.516666666666667e-05,
      "loss": 0.265,
      "step": 117600
    },
    {
      "epoch": 1.144668828926958,
      "grad_norm": 0.22917094826698303,
      "learning_rate": 5.5125000000000005e-05,
      "loss": 0.2635,
      "step": 117700
    },
    {
      "epoch": 1.1456413597926565,
      "grad_norm": 0.2081645429134369,
      "learning_rate": 5.508333333333333e-05,
      "loss": 0.264,
      "step": 117800
    },
    {
      "epoch": 1.1466138906583547,
      "grad_norm": 0.2215350717306137,
      "learning_rate": 5.504166666666667e-05,
      "loss": 0.2642,
      "step": 117900
    },
    {
      "epoch": 1.1475864215240532,
      "grad_norm": 0.2090194970369339,
      "learning_rate": 5.500000000000001e-05,
      "loss": 0.2647,
      "step": 118000
    },
    {
      "epoch": 1.1485589523897515,
      "grad_norm": 0.20987311005592346,
      "learning_rate": 5.495833333333333e-05,
      "loss": 0.2644,
      "step": 118100
    },
    {
      "epoch": 1.1495314832554497,
      "grad_norm": 0.24555478990077972,
      "learning_rate": 5.491666666666667e-05,
      "loss": 0.2654,
      "step": 118200
    },
    {
      "epoch": 1.1505040141211482,
      "grad_norm": 0.23207418620586395,
      "learning_rate": 5.4875e-05,
      "loss": 0.265,
      "step": 118300
    },
    {
      "epoch": 1.1514765449868465,
      "grad_norm": 0.18993782997131348,
      "learning_rate": 5.4833333333333336e-05,
      "loss": 0.2648,
      "step": 118400
    },
    {
      "epoch": 1.152449075852545,
      "grad_norm": 0.20879189670085907,
      "learning_rate": 5.479166666666667e-05,
      "loss": 0.2671,
      "step": 118500
    },
    {
      "epoch": 1.1534216067182432,
      "grad_norm": 0.21362915635108948,
      "learning_rate": 5.475e-05,
      "loss": 0.2646,
      "step": 118600
    },
    {
      "epoch": 1.1543941375839415,
      "grad_norm": 0.20589180290699005,
      "learning_rate": 5.470833333333334e-05,
      "loss": 0.2642,
      "step": 118700
    },
    {
      "epoch": 1.15536666844964,
      "grad_norm": 0.21888722479343414,
      "learning_rate": 5.466666666666666e-05,
      "loss": 0.265,
      "step": 118800
    },
    {
      "epoch": 1.1563391993153382,
      "grad_norm": 0.24139003455638885,
      "learning_rate": 5.4625000000000006e-05,
      "loss": 0.2649,
      "step": 118900
    },
    {
      "epoch": 1.1573117301810365,
      "grad_norm": 0.2167617827653885,
      "learning_rate": 5.458333333333333e-05,
      "loss": 0.2641,
      "step": 119000
    },
    {
      "epoch": 1.158284261046735,
      "grad_norm": 0.21273848414421082,
      "learning_rate": 5.4541666666666667e-05,
      "loss": 0.2639,
      "step": 119100
    },
    {
      "epoch": 1.1592567919124332,
      "grad_norm": 0.2150074541568756,
      "learning_rate": 5.45e-05,
      "loss": 0.2638,
      "step": 119200
    },
    {
      "epoch": 1.1602293227781317,
      "grad_norm": 0.22402015328407288,
      "learning_rate": 5.4458333333333333e-05,
      "loss": 0.2649,
      "step": 119300
    },
    {
      "epoch": 1.16120185364383,
      "grad_norm": 0.22926512360572815,
      "learning_rate": 5.441666666666667e-05,
      "loss": 0.2643,
      "step": 119400
    },
    {
      "epoch": 1.1621743845095285,
      "grad_norm": 0.20333454012870789,
      "learning_rate": 5.4375e-05,
      "loss": 0.2644,
      "step": 119500
    },
    {
      "epoch": 1.1631469153752267,
      "grad_norm": 0.23331910371780396,
      "learning_rate": 5.433333333333334e-05,
      "loss": 0.2635,
      "step": 119600
    },
    {
      "epoch": 1.164119446240925,
      "grad_norm": 0.2231445163488388,
      "learning_rate": 5.4291666666666674e-05,
      "loss": 0.2632,
      "step": 119700
    },
    {
      "epoch": 1.1650919771066235,
      "grad_norm": 0.2507108449935913,
      "learning_rate": 5.4250000000000004e-05,
      "loss": 0.2645,
      "step": 119800
    },
    {
      "epoch": 1.1660645079723218,
      "grad_norm": 0.2302137315273285,
      "learning_rate": 5.420833333333334e-05,
      "loss": 0.2632,
      "step": 119900
    },
    {
      "epoch": 1.16703703883802,
      "grad_norm": 0.22441096603870392,
      "learning_rate": 5.4166666666666664e-05,
      "loss": 0.2632,
      "step": 120000
    },
    {
      "epoch": 1.16703703883802,
      "eval_loss": 0.26578477025032043,
      "eval_runtime": 3164.0123,
      "eval_samples_per_second": 722.105,
      "eval_steps_per_second": 7.221,
      "step": 120000
    },
    {
      "epoch": 1.1680095697037185,
      "grad_norm": 0.20739325881004333,
      "learning_rate": 5.4125e-05,
      "loss": 0.2624,
      "step": 120100
    },
    {
      "epoch": 1.1689821005694168,
      "grad_norm": 0.1974717676639557,
      "learning_rate": 5.4083333333333345e-05,
      "loss": 0.2634,
      "step": 120200
    },
    {
      "epoch": 1.1699546314351152,
      "grad_norm": 0.21625418961048126,
      "learning_rate": 5.404166666666667e-05,
      "loss": 0.2623,
      "step": 120300
    },
    {
      "epoch": 1.1709271623008135,
      "grad_norm": 0.21731169521808624,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.2634,
      "step": 120400
    },
    {
      "epoch": 1.171899693166512,
      "grad_norm": 0.22131307423114777,
      "learning_rate": 5.3958333333333335e-05,
      "loss": 0.2644,
      "step": 120500
    },
    {
      "epoch": 1.1728722240322103,
      "grad_norm": 0.20978067815303802,
      "learning_rate": 5.391666666666667e-05,
      "loss": 0.2644,
      "step": 120600
    },
    {
      "epoch": 1.1738447548979085,
      "grad_norm": 0.22352536022663116,
      "learning_rate": 5.3874999999999995e-05,
      "loss": 0.2641,
      "step": 120700
    },
    {
      "epoch": 1.174817285763607,
      "grad_norm": 0.23125554621219635,
      "learning_rate": 5.383333333333334e-05,
      "loss": 0.265,
      "step": 120800
    },
    {
      "epoch": 1.1757898166293053,
      "grad_norm": 0.21667931973934174,
      "learning_rate": 5.3791666666666675e-05,
      "loss": 0.2628,
      "step": 120900
    },
    {
      "epoch": 1.1767623474950035,
      "grad_norm": 0.22662797570228577,
      "learning_rate": 5.375e-05,
      "loss": 0.2625,
      "step": 121000
    },
    {
      "epoch": 1.177734878360702,
      "grad_norm": 0.23107139766216278,
      "learning_rate": 5.3708333333333336e-05,
      "loss": 0.2628,
      "step": 121100
    },
    {
      "epoch": 1.1787074092264003,
      "grad_norm": 0.2002630978822708,
      "learning_rate": 5.3666666666666666e-05,
      "loss": 0.2641,
      "step": 121200
    },
    {
      "epoch": 1.1796799400920988,
      "grad_norm": 0.21130791306495667,
      "learning_rate": 5.3625e-05,
      "loss": 0.2631,
      "step": 121300
    },
    {
      "epoch": 1.180652470957797,
      "grad_norm": 0.22030708193778992,
      "learning_rate": 5.358333333333334e-05,
      "loss": 0.264,
      "step": 121400
    },
    {
      "epoch": 1.1816250018234953,
      "grad_norm": 0.2396606057882309,
      "learning_rate": 5.354166666666667e-05,
      "loss": 0.2642,
      "step": 121500
    },
    {
      "epoch": 1.1825975326891938,
      "grad_norm": 0.2168000042438507,
      "learning_rate": 5.3500000000000006e-05,
      "loss": 0.2641,
      "step": 121600
    },
    {
      "epoch": 1.183570063554892,
      "grad_norm": 0.24716898798942566,
      "learning_rate": 5.345833333333333e-05,
      "loss": 0.263,
      "step": 121700
    },
    {
      "epoch": 1.1845425944205905,
      "grad_norm": 0.2305288016796112,
      "learning_rate": 5.341666666666667e-05,
      "loss": 0.2633,
      "step": 121800
    },
    {
      "epoch": 1.1855151252862888,
      "grad_norm": 0.2222558856010437,
      "learning_rate": 5.3374999999999996e-05,
      "loss": 0.2634,
      "step": 121900
    },
    {
      "epoch": 1.186487656151987,
      "grad_norm": 0.21447113156318665,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.2636,
      "step": 122000
    },
    {
      "epoch": 1.1874601870176855,
      "grad_norm": 0.2253173440694809,
      "learning_rate": 5.329166666666667e-05,
      "loss": 0.264,
      "step": 122100
    },
    {
      "epoch": 1.1884327178833838,
      "grad_norm": 0.2182534635066986,
      "learning_rate": 5.325e-05,
      "loss": 0.2649,
      "step": 122200
    },
    {
      "epoch": 1.1894052487490823,
      "grad_norm": 0.21320785582065582,
      "learning_rate": 5.320833333333334e-05,
      "loss": 0.2636,
      "step": 122300
    },
    {
      "epoch": 1.1903777796147805,
      "grad_norm": 0.20735014975070953,
      "learning_rate": 5.316666666666667e-05,
      "loss": 0.2662,
      "step": 122400
    },
    {
      "epoch": 1.1913503104804788,
      "grad_norm": 0.21744397282600403,
      "learning_rate": 5.3125000000000004e-05,
      "loss": 0.2642,
      "step": 122500
    },
    {
      "epoch": 1.1923228413461773,
      "grad_norm": 0.22702209651470184,
      "learning_rate": 5.308333333333334e-05,
      "loss": 0.2663,
      "step": 122600
    },
    {
      "epoch": 1.1932953722118755,
      "grad_norm": 0.21056701242923737,
      "learning_rate": 5.3041666666666664e-05,
      "loss": 0.2628,
      "step": 122700
    },
    {
      "epoch": 1.194267903077574,
      "grad_norm": 0.21863850951194763,
      "learning_rate": 5.300000000000001e-05,
      "loss": 0.2626,
      "step": 122800
    },
    {
      "epoch": 1.1952404339432723,
      "grad_norm": 0.23998387157917023,
      "learning_rate": 5.295833333333333e-05,
      "loss": 0.2644,
      "step": 122900
    },
    {
      "epoch": 1.1962129648089705,
      "grad_norm": 0.2314927726984024,
      "learning_rate": 5.291666666666667e-05,
      "loss": 0.2629,
      "step": 123000
    },
    {
      "epoch": 1.197185495674669,
      "grad_norm": 0.21343488991260529,
      "learning_rate": 5.2875000000000005e-05,
      "loss": 0.2646,
      "step": 123100
    },
    {
      "epoch": 1.1981580265403673,
      "grad_norm": 0.23281055688858032,
      "learning_rate": 5.2833333333333335e-05,
      "loss": 0.2638,
      "step": 123200
    },
    {
      "epoch": 1.1991305574060656,
      "grad_norm": 0.20989614725112915,
      "learning_rate": 5.279166666666667e-05,
      "loss": 0.264,
      "step": 123300
    },
    {
      "epoch": 1.200103088271764,
      "grad_norm": 0.22043639421463013,
      "learning_rate": 5.275e-05,
      "loss": 0.264,
      "step": 123400
    },
    {
      "epoch": 1.2010756191374623,
      "grad_norm": 0.21432876586914062,
      "learning_rate": 5.270833333333334e-05,
      "loss": 0.2642,
      "step": 123500
    },
    {
      "epoch": 1.2020481500031608,
      "grad_norm": 0.18011361360549927,
      "learning_rate": 5.266666666666666e-05,
      "loss": 0.2634,
      "step": 123600
    },
    {
      "epoch": 1.203020680868859,
      "grad_norm": 0.22632434964179993,
      "learning_rate": 5.2625000000000005e-05,
      "loss": 0.2652,
      "step": 123700
    },
    {
      "epoch": 1.2039932117345575,
      "grad_norm": 0.19840434193611145,
      "learning_rate": 5.258333333333334e-05,
      "loss": 0.2646,
      "step": 123800
    },
    {
      "epoch": 1.2049657426002558,
      "grad_norm": 0.21960483491420746,
      "learning_rate": 5.2541666666666665e-05,
      "loss": 0.2616,
      "step": 123900
    },
    {
      "epoch": 1.205938273465954,
      "grad_norm": 0.21355454623699188,
      "learning_rate": 5.25e-05,
      "loss": 0.2637,
      "step": 124000
    },
    {
      "epoch": 1.2069108043316525,
      "grad_norm": 0.2178214192390442,
      "learning_rate": 5.245833333333333e-05,
      "loss": 0.2655,
      "step": 124100
    },
    {
      "epoch": 1.2078833351973508,
      "grad_norm": 0.22258220613002777,
      "learning_rate": 5.241666666666667e-05,
      "loss": 0.2654,
      "step": 124200
    },
    {
      "epoch": 1.208855866063049,
      "grad_norm": 0.21174225211143494,
      "learning_rate": 5.2375000000000006e-05,
      "loss": 0.2646,
      "step": 124300
    },
    {
      "epoch": 1.2098283969287476,
      "grad_norm": 0.21822425723075867,
      "learning_rate": 5.2333333333333336e-05,
      "loss": 0.2642,
      "step": 124400
    },
    {
      "epoch": 1.2108009277944458,
      "grad_norm": 0.2249484658241272,
      "learning_rate": 5.229166666666667e-05,
      "loss": 0.2641,
      "step": 124500
    },
    {
      "epoch": 1.2117734586601443,
      "grad_norm": 0.21298468112945557,
      "learning_rate": 5.2249999999999996e-05,
      "loss": 0.2622,
      "step": 124600
    },
    {
      "epoch": 1.2127459895258426,
      "grad_norm": 0.20838724076747894,
      "learning_rate": 5.220833333333334e-05,
      "loss": 0.2639,
      "step": 124700
    },
    {
      "epoch": 1.213718520391541,
      "grad_norm": 0.2342383712530136,
      "learning_rate": 5.216666666666666e-05,
      "loss": 0.2645,
      "step": 124800
    },
    {
      "epoch": 1.2146910512572393,
      "grad_norm": 0.22068092226982117,
      "learning_rate": 5.2125e-05,
      "loss": 0.2634,
      "step": 124900
    },
    {
      "epoch": 1.2156635821229376,
      "grad_norm": 0.22537021338939667,
      "learning_rate": 5.208333333333334e-05,
      "loss": 0.2639,
      "step": 125000
    },
    {
      "epoch": 1.216636112988636,
      "grad_norm": 0.22348907589912415,
      "learning_rate": 5.204166666666667e-05,
      "loss": 0.2632,
      "step": 125100
    },
    {
      "epoch": 1.2176086438543343,
      "grad_norm": 0.2318442165851593,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.2641,
      "step": 125200
    },
    {
      "epoch": 1.2185811747200326,
      "grad_norm": 0.243177130818367,
      "learning_rate": 5.1958333333333334e-05,
      "loss": 0.2643,
      "step": 125300
    },
    {
      "epoch": 1.219553705585731,
      "grad_norm": 0.21991577744483948,
      "learning_rate": 5.191666666666667e-05,
      "loss": 0.2642,
      "step": 125400
    },
    {
      "epoch": 1.2205262364514293,
      "grad_norm": 0.22268567979335785,
      "learning_rate": 5.187500000000001e-05,
      "loss": 0.2636,
      "step": 125500
    },
    {
      "epoch": 1.2214987673171278,
      "grad_norm": 0.23289887607097626,
      "learning_rate": 5.183333333333333e-05,
      "loss": 0.2636,
      "step": 125600
    },
    {
      "epoch": 1.222471298182826,
      "grad_norm": 0.22648142278194427,
      "learning_rate": 5.1791666666666674e-05,
      "loss": 0.2625,
      "step": 125700
    },
    {
      "epoch": 1.2234438290485243,
      "grad_norm": 0.22106648981571198,
      "learning_rate": 5.175e-05,
      "loss": 0.2625,
      "step": 125800
    },
    {
      "epoch": 1.2244163599142228,
      "grad_norm": 0.2120741307735443,
      "learning_rate": 5.1708333333333334e-05,
      "loss": 0.2641,
      "step": 125900
    },
    {
      "epoch": 1.225388890779921,
      "grad_norm": 0.2408904731273651,
      "learning_rate": 5.166666666666667e-05,
      "loss": 0.2624,
      "step": 126000
    },
    {
      "epoch": 1.2263614216456196,
      "grad_norm": 0.21952582895755768,
      "learning_rate": 5.1625e-05,
      "loss": 0.264,
      "step": 126100
    },
    {
      "epoch": 1.2273339525113178,
      "grad_norm": 0.2285124957561493,
      "learning_rate": 5.158333333333334e-05,
      "loss": 0.2644,
      "step": 126200
    },
    {
      "epoch": 1.228306483377016,
      "grad_norm": 0.2240544855594635,
      "learning_rate": 5.154166666666667e-05,
      "loss": 0.2635,
      "step": 126300
    },
    {
      "epoch": 1.2292790142427146,
      "grad_norm": 0.20096932351589203,
      "learning_rate": 5.1500000000000005e-05,
      "loss": 0.2633,
      "step": 126400
    },
    {
      "epoch": 1.2302515451084128,
      "grad_norm": 0.2195463478565216,
      "learning_rate": 5.145833333333333e-05,
      "loss": 0.2627,
      "step": 126500
    },
    {
      "epoch": 1.2312240759741113,
      "grad_norm": 0.2101350575685501,
      "learning_rate": 5.141666666666667e-05,
      "loss": 0.2644,
      "step": 126600
    },
    {
      "epoch": 1.2321966068398096,
      "grad_norm": 0.2264155000448227,
      "learning_rate": 5.137500000000001e-05,
      "loss": 0.263,
      "step": 126700
    },
    {
      "epoch": 1.2331691377055078,
      "grad_norm": 0.23183634877204895,
      "learning_rate": 5.133333333333333e-05,
      "loss": 0.2626,
      "step": 126800
    },
    {
      "epoch": 1.2341416685712063,
      "grad_norm": 0.23092018067836761,
      "learning_rate": 5.129166666666667e-05,
      "loss": 0.2641,
      "step": 126900
    },
    {
      "epoch": 1.2351141994369046,
      "grad_norm": 0.22337284684181213,
      "learning_rate": 5.125e-05,
      "loss": 0.2614,
      "step": 127000
    },
    {
      "epoch": 1.236086730302603,
      "grad_norm": 0.22840873897075653,
      "learning_rate": 5.1208333333333336e-05,
      "loss": 0.2634,
      "step": 127100
    },
    {
      "epoch": 1.2370592611683013,
      "grad_norm": 0.2250540554523468,
      "learning_rate": 5.116666666666667e-05,
      "loss": 0.2625,
      "step": 127200
    },
    {
      "epoch": 1.2380317920339996,
      "grad_norm": 0.2230004519224167,
      "learning_rate": 5.1125e-05,
      "loss": 0.2609,
      "step": 127300
    },
    {
      "epoch": 1.239004322899698,
      "grad_norm": 0.20125725865364075,
      "learning_rate": 5.108333333333334e-05,
      "loss": 0.261,
      "step": 127400
    },
    {
      "epoch": 1.2399768537653963,
      "grad_norm": 0.22114047408103943,
      "learning_rate": 5.104166666666666e-05,
      "loss": 0.2631,
      "step": 127500
    },
    {
      "epoch": 1.2409493846310946,
      "grad_norm": 0.2264653593301773,
      "learning_rate": 5.1000000000000006e-05,
      "loss": 0.2641,
      "step": 127600
    },
    {
      "epoch": 1.241921915496793,
      "grad_norm": 0.21999087929725647,
      "learning_rate": 5.095833333333334e-05,
      "loss": 0.2648,
      "step": 127700
    },
    {
      "epoch": 1.2428944463624914,
      "grad_norm": 0.2202211320400238,
      "learning_rate": 5.0916666666666666e-05,
      "loss": 0.2621,
      "step": 127800
    },
    {
      "epoch": 1.2438669772281898,
      "grad_norm": 0.20314382016658783,
      "learning_rate": 5.0875e-05,
      "loss": 0.2646,
      "step": 127900
    },
    {
      "epoch": 1.244839508093888,
      "grad_norm": 0.21809056401252747,
      "learning_rate": 5.0833333333333333e-05,
      "loss": 0.2633,
      "step": 128000
    },
    {
      "epoch": 1.2458120389595866,
      "grad_norm": 0.20863565802574158,
      "learning_rate": 5.079166666666667e-05,
      "loss": 0.2634,
      "step": 128100
    },
    {
      "epoch": 1.2467845698252848,
      "grad_norm": 0.205046147108078,
      "learning_rate": 5.075e-05,
      "loss": 0.264,
      "step": 128200
    },
    {
      "epoch": 1.2477571006909831,
      "grad_norm": 0.21587106585502625,
      "learning_rate": 5.070833333333334e-05,
      "loss": 0.2631,
      "step": 128300
    },
    {
      "epoch": 1.2487296315566816,
      "grad_norm": 0.21955816447734833,
      "learning_rate": 5.0666666666666674e-05,
      "loss": 0.2608,
      "step": 128400
    },
    {
      "epoch": 1.2497021624223799,
      "grad_norm": 0.21619588136672974,
      "learning_rate": 5.0625e-05,
      "loss": 0.2631,
      "step": 128500
    },
    {
      "epoch": 1.2506746932880781,
      "grad_norm": 0.21650642156600952,
      "learning_rate": 5.058333333333334e-05,
      "loss": 0.2632,
      "step": 128600
    },
    {
      "epoch": 1.2516472241537766,
      "grad_norm": 0.24899524450302124,
      "learning_rate": 5.0541666666666664e-05,
      "loss": 0.2617,
      "step": 128700
    },
    {
      "epoch": 1.2526197550194749,
      "grad_norm": 0.2233569175004959,
      "learning_rate": 5.05e-05,
      "loss": 0.2631,
      "step": 128800
    },
    {
      "epoch": 1.2535922858851734,
      "grad_norm": 0.22256840765476227,
      "learning_rate": 5.045833333333334e-05,
      "loss": 0.2622,
      "step": 128900
    },
    {
      "epoch": 1.2545648167508716,
      "grad_norm": 0.22140151262283325,
      "learning_rate": 5.041666666666667e-05,
      "loss": 0.2635,
      "step": 129000
    },
    {
      "epoch": 1.25553734761657,
      "grad_norm": 0.22540827095508575,
      "learning_rate": 5.0375000000000005e-05,
      "loss": 0.2621,
      "step": 129100
    },
    {
      "epoch": 1.2565098784822684,
      "grad_norm": 0.21826240420341492,
      "learning_rate": 5.0333333333333335e-05,
      "loss": 0.2643,
      "step": 129200
    },
    {
      "epoch": 1.2574824093479666,
      "grad_norm": 0.23308436572551727,
      "learning_rate": 5.029166666666667e-05,
      "loss": 0.2627,
      "step": 129300
    },
    {
      "epoch": 1.258454940213665,
      "grad_norm": 0.21954958140850067,
      "learning_rate": 5.0249999999999995e-05,
      "loss": 0.2631,
      "step": 129400
    },
    {
      "epoch": 1.2594274710793634,
      "grad_norm": 0.2248515486717224,
      "learning_rate": 5.020833333333333e-05,
      "loss": 0.2629,
      "step": 129500
    },
    {
      "epoch": 1.2604000019450616,
      "grad_norm": 0.236212357878685,
      "learning_rate": 5.0166666666666675e-05,
      "loss": 0.2625,
      "step": 129600
    },
    {
      "epoch": 1.2613725328107601,
      "grad_norm": 0.22111739218235016,
      "learning_rate": 5.0125e-05,
      "loss": 0.2647,
      "step": 129700
    },
    {
      "epoch": 1.2623450636764584,
      "grad_norm": 0.21467559039592743,
      "learning_rate": 5.0083333333333335e-05,
      "loss": 0.2615,
      "step": 129800
    },
    {
      "epoch": 1.2633175945421569,
      "grad_norm": 0.23189465701580048,
      "learning_rate": 5.0041666666666666e-05,
      "loss": 0.2635,
      "step": 129900
    },
    {
      "epoch": 1.2642901254078551,
      "grad_norm": 0.23730424046516418,
      "learning_rate": 5e-05,
      "loss": 0.2627,
      "step": 130000
    },
    {
      "epoch": 1.2642901254078551,
      "eval_loss": 0.26443082094192505,
      "eval_runtime": 3088.1416,
      "eval_samples_per_second": 739.846,
      "eval_steps_per_second": 7.399,
      "step": 130000
    },
    {
      "epoch": 1.2652626562735536,
      "grad_norm": 0.20524372160434723,
      "learning_rate": 4.995833333333333e-05,
      "loss": 0.2637,
      "step": 130100
    },
    {
      "epoch": 1.2662351871392519,
      "grad_norm": 0.21611611545085907,
      "learning_rate": 4.991666666666667e-05,
      "loss": 0.2628,
      "step": 130200
    },
    {
      "epoch": 1.2672077180049501,
      "grad_norm": 0.23380272090435028,
      "learning_rate": 4.9875000000000006e-05,
      "loss": 0.2639,
      "step": 130300
    },
    {
      "epoch": 1.2681802488706486,
      "grad_norm": 0.21318690478801727,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.264,
      "step": 130400
    },
    {
      "epoch": 1.2691527797363469,
      "grad_norm": 0.24922022223472595,
      "learning_rate": 4.979166666666667e-05,
      "loss": 0.2627,
      "step": 130500
    },
    {
      "epoch": 1.2701253106020451,
      "grad_norm": 0.2179330438375473,
      "learning_rate": 4.975e-05,
      "loss": 0.2615,
      "step": 130600
    },
    {
      "epoch": 1.2710978414677436,
      "grad_norm": 0.23400700092315674,
      "learning_rate": 4.970833333333333e-05,
      "loss": 0.2626,
      "step": 130700
    },
    {
      "epoch": 1.272070372333442,
      "grad_norm": 0.2329246550798416,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.2638,
      "step": 130800
    },
    {
      "epoch": 1.2730429031991402,
      "grad_norm": 0.22672578692436218,
      "learning_rate": 4.962500000000001e-05,
      "loss": 0.2612,
      "step": 130900
    },
    {
      "epoch": 1.2740154340648386,
      "grad_norm": 0.21625463664531708,
      "learning_rate": 4.958333333333334e-05,
      "loss": 0.2621,
      "step": 131000
    },
    {
      "epoch": 1.2749879649305371,
      "grad_norm": 0.22275733947753906,
      "learning_rate": 4.954166666666667e-05,
      "loss": 0.2608,
      "step": 131100
    },
    {
      "epoch": 1.2759604957962354,
      "grad_norm": 0.25003573298454285,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.2633,
      "step": 131200
    },
    {
      "epoch": 1.2769330266619336,
      "grad_norm": 0.2336827963590622,
      "learning_rate": 4.9458333333333334e-05,
      "loss": 0.2636,
      "step": 131300
    },
    {
      "epoch": 1.2779055575276321,
      "grad_norm": 0.23199693858623505,
      "learning_rate": 4.9416666666666664e-05,
      "loss": 0.2642,
      "step": 131400
    },
    {
      "epoch": 1.2788780883933304,
      "grad_norm": 0.22379322350025177,
      "learning_rate": 4.937500000000001e-05,
      "loss": 0.2641,
      "step": 131500
    },
    {
      "epoch": 1.2798506192590287,
      "grad_norm": 0.21541593968868256,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.2627,
      "step": 131600
    },
    {
      "epoch": 1.2808231501247271,
      "grad_norm": 0.206940159201622,
      "learning_rate": 4.929166666666667e-05,
      "loss": 0.2633,
      "step": 131700
    },
    {
      "epoch": 1.2817956809904254,
      "grad_norm": 0.23940929770469666,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 0.2635,
      "step": 131800
    },
    {
      "epoch": 1.2827682118561237,
      "grad_norm": 0.22072872519493103,
      "learning_rate": 4.9208333333333335e-05,
      "loss": 0.2623,
      "step": 131900
    },
    {
      "epoch": 1.2837407427218221,
      "grad_norm": 0.22605273127555847,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.261,
      "step": 132000
    },
    {
      "epoch": 1.2847132735875204,
      "grad_norm": 0.2256510704755783,
      "learning_rate": 4.9125e-05,
      "loss": 0.2625,
      "step": 132100
    },
    {
      "epoch": 1.285685804453219,
      "grad_norm": 0.1902640014886856,
      "learning_rate": 4.908333333333334e-05,
      "loss": 0.2615,
      "step": 132200
    },
    {
      "epoch": 1.2866583353189172,
      "grad_norm": 0.25477224588394165,
      "learning_rate": 4.904166666666667e-05,
      "loss": 0.2643,
      "step": 132300
    },
    {
      "epoch": 1.2876308661846156,
      "grad_norm": 0.21721717715263367,
      "learning_rate": 4.9e-05,
      "loss": 0.2617,
      "step": 132400
    },
    {
      "epoch": 1.288603397050314,
      "grad_norm": 0.2287362962961197,
      "learning_rate": 4.8958333333333335e-05,
      "loss": 0.263,
      "step": 132500
    },
    {
      "epoch": 1.2895759279160122,
      "grad_norm": 0.22929254174232483,
      "learning_rate": 4.891666666666667e-05,
      "loss": 0.2619,
      "step": 132600
    },
    {
      "epoch": 1.2905484587817107,
      "grad_norm": 0.22312793135643005,
      "learning_rate": 4.8875e-05,
      "loss": 0.2618,
      "step": 132700
    },
    {
      "epoch": 1.291520989647409,
      "grad_norm": 0.2572292685508728,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.2622,
      "step": 132800
    },
    {
      "epoch": 1.2924935205131072,
      "grad_norm": 0.23522113263607025,
      "learning_rate": 4.879166666666667e-05,
      "loss": 0.2626,
      "step": 132900
    },
    {
      "epoch": 1.2934660513788057,
      "grad_norm": 0.2308749407529831,
      "learning_rate": 4.875e-05,
      "loss": 0.262,
      "step": 133000
    },
    {
      "epoch": 1.294438582244504,
      "grad_norm": 0.24702699482440948,
      "learning_rate": 4.8708333333333336e-05,
      "loss": 0.263,
      "step": 133100
    },
    {
      "epoch": 1.2954111131102024,
      "grad_norm": 0.20983408391475677,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.2629,
      "step": 133200
    },
    {
      "epoch": 1.2963836439759007,
      "grad_norm": 0.2355000376701355,
      "learning_rate": 4.8625e-05,
      "loss": 0.262,
      "step": 133300
    },
    {
      "epoch": 1.2973561748415992,
      "grad_norm": 0.21058756113052368,
      "learning_rate": 4.858333333333333e-05,
      "loss": 0.2631,
      "step": 133400
    },
    {
      "epoch": 1.2983287057072974,
      "grad_norm": 0.22026169300079346,
      "learning_rate": 4.854166666666667e-05,
      "loss": 0.2609,
      "step": 133500
    },
    {
      "epoch": 1.2993012365729957,
      "grad_norm": 0.2224881500005722,
      "learning_rate": 4.85e-05,
      "loss": 0.2614,
      "step": 133600
    },
    {
      "epoch": 1.3002737674386942,
      "grad_norm": 0.2135191112756729,
      "learning_rate": 4.845833333333334e-05,
      "loss": 0.263,
      "step": 133700
    },
    {
      "epoch": 1.3012462983043924,
      "grad_norm": 0.22315819561481476,
      "learning_rate": 4.8416666666666673e-05,
      "loss": 0.263,
      "step": 133800
    },
    {
      "epoch": 1.3022188291700907,
      "grad_norm": 0.22145698964595795,
      "learning_rate": 4.8375000000000004e-05,
      "loss": 0.262,
      "step": 133900
    },
    {
      "epoch": 1.3031913600357892,
      "grad_norm": 0.24357423186302185,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.2642,
      "step": 134000
    },
    {
      "epoch": 1.3041638909014874,
      "grad_norm": 0.23985520005226135,
      "learning_rate": 4.829166666666667e-05,
      "loss": 0.2621,
      "step": 134100
    },
    {
      "epoch": 1.305136421767186,
      "grad_norm": 0.234405979514122,
      "learning_rate": 4.825e-05,
      "loss": 0.2637,
      "step": 134200
    },
    {
      "epoch": 1.3061089526328842,
      "grad_norm": 0.23606443405151367,
      "learning_rate": 4.820833333333333e-05,
      "loss": 0.2617,
      "step": 134300
    },
    {
      "epoch": 1.3070814834985827,
      "grad_norm": 0.22963251173496246,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.2632,
      "step": 134400
    },
    {
      "epoch": 1.308054014364281,
      "grad_norm": 0.2324123978614807,
      "learning_rate": 4.8125000000000004e-05,
      "loss": 0.2626,
      "step": 134500
    },
    {
      "epoch": 1.3090265452299792,
      "grad_norm": 0.24182531237602234,
      "learning_rate": 4.8083333333333334e-05,
      "loss": 0.262,
      "step": 134600
    },
    {
      "epoch": 1.3099990760956777,
      "grad_norm": 0.2259395867586136,
      "learning_rate": 4.804166666666667e-05,
      "loss": 0.2616,
      "step": 134700
    },
    {
      "epoch": 1.310971606961376,
      "grad_norm": 0.20904308557510376,
      "learning_rate": 4.8e-05,
      "loss": 0.2635,
      "step": 134800
    },
    {
      "epoch": 1.3119441378270742,
      "grad_norm": 0.23126472532749176,
      "learning_rate": 4.795833333333333e-05,
      "loss": 0.2634,
      "step": 134900
    },
    {
      "epoch": 1.3129166686927727,
      "grad_norm": 0.22835604846477509,
      "learning_rate": 4.791666666666667e-05,
      "loss": 0.2644,
      "step": 135000
    },
    {
      "epoch": 1.313889199558471,
      "grad_norm": 0.22314870357513428,
      "learning_rate": 4.7875000000000005e-05,
      "loss": 0.2611,
      "step": 135100
    },
    {
      "epoch": 1.3148617304241692,
      "grad_norm": 0.2294912189245224,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.2639,
      "step": 135200
    },
    {
      "epoch": 1.3158342612898677,
      "grad_norm": 0.23419815301895142,
      "learning_rate": 4.7791666666666665e-05,
      "loss": 0.2629,
      "step": 135300
    },
    {
      "epoch": 1.3168067921555662,
      "grad_norm": 0.2272844910621643,
      "learning_rate": 4.775e-05,
      "loss": 0.2619,
      "step": 135400
    },
    {
      "epoch": 1.3177793230212644,
      "grad_norm": 0.2299022674560547,
      "learning_rate": 4.770833333333334e-05,
      "loss": 0.2623,
      "step": 135500
    },
    {
      "epoch": 1.3187518538869627,
      "grad_norm": 0.2134765088558197,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.2627,
      "step": 135600
    },
    {
      "epoch": 1.3197243847526612,
      "grad_norm": 0.24266907572746277,
      "learning_rate": 4.7625000000000006e-05,
      "loss": 0.2628,
      "step": 135700
    },
    {
      "epoch": 1.3206969156183594,
      "grad_norm": 0.22334779798984528,
      "learning_rate": 4.7583333333333336e-05,
      "loss": 0.2598,
      "step": 135800
    },
    {
      "epoch": 1.3216694464840577,
      "grad_norm": 0.22351627051830292,
      "learning_rate": 4.7541666666666666e-05,
      "loss": 0.2628,
      "step": 135900
    },
    {
      "epoch": 1.3226419773497562,
      "grad_norm": 0.2184346318244934,
      "learning_rate": 4.75e-05,
      "loss": 0.262,
      "step": 136000
    },
    {
      "epoch": 1.3236145082154545,
      "grad_norm": 0.24006706476211548,
      "learning_rate": 4.745833333333334e-05,
      "loss": 0.2621,
      "step": 136100
    },
    {
      "epoch": 1.3245870390811527,
      "grad_norm": 0.20659680664539337,
      "learning_rate": 4.741666666666667e-05,
      "loss": 0.2608,
      "step": 136200
    },
    {
      "epoch": 1.3255595699468512,
      "grad_norm": 0.21239334344863892,
      "learning_rate": 4.7375e-05,
      "loss": 0.2635,
      "step": 136300
    },
    {
      "epoch": 1.3265321008125495,
      "grad_norm": 0.2108071893453598,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.2618,
      "step": 136400
    },
    {
      "epoch": 1.327504631678248,
      "grad_norm": 0.23146593570709229,
      "learning_rate": 4.7291666666666666e-05,
      "loss": 0.2619,
      "step": 136500
    },
    {
      "epoch": 1.3284771625439462,
      "grad_norm": 0.2562732994556427,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 0.2626,
      "step": 136600
    },
    {
      "epoch": 1.3294496934096447,
      "grad_norm": 0.22266894578933716,
      "learning_rate": 4.720833333333334e-05,
      "loss": 0.2617,
      "step": 136700
    },
    {
      "epoch": 1.330422224275343,
      "grad_norm": 0.24037803709506989,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.2622,
      "step": 136800
    },
    {
      "epoch": 1.3313947551410412,
      "grad_norm": 0.22062690556049347,
      "learning_rate": 4.7125e-05,
      "loss": 0.2629,
      "step": 136900
    },
    {
      "epoch": 1.3323672860067397,
      "grad_norm": 0.25149160623550415,
      "learning_rate": 4.708333333333334e-05,
      "loss": 0.2629,
      "step": 137000
    },
    {
      "epoch": 1.333339816872438,
      "grad_norm": 0.230802983045578,
      "learning_rate": 4.704166666666667e-05,
      "loss": 0.2627,
      "step": 137100
    },
    {
      "epoch": 1.3343123477381362,
      "grad_norm": 0.21930478513240814,
      "learning_rate": 4.7e-05,
      "loss": 0.2616,
      "step": 137200
    },
    {
      "epoch": 1.3352848786038347,
      "grad_norm": 0.2177850902080536,
      "learning_rate": 4.695833333333334e-05,
      "loss": 0.2627,
      "step": 137300
    },
    {
      "epoch": 1.336257409469533,
      "grad_norm": 0.2131715714931488,
      "learning_rate": 4.691666666666667e-05,
      "loss": 0.2606,
      "step": 137400
    },
    {
      "epoch": 1.3372299403352315,
      "grad_norm": 0.23560111224651337,
      "learning_rate": 4.6875e-05,
      "loss": 0.2635,
      "step": 137500
    },
    {
      "epoch": 1.3382024712009297,
      "grad_norm": 0.24784761667251587,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.2633,
      "step": 137600
    },
    {
      "epoch": 1.3391750020666282,
      "grad_norm": 0.20425905287265778,
      "learning_rate": 4.679166666666667e-05,
      "loss": 0.2628,
      "step": 137700
    },
    {
      "epoch": 1.3401475329323265,
      "grad_norm": 0.23215597867965698,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 0.2625,
      "step": 137800
    },
    {
      "epoch": 1.3411200637980247,
      "grad_norm": 0.22165672481060028,
      "learning_rate": 4.6708333333333335e-05,
      "loss": 0.2605,
      "step": 137900
    },
    {
      "epoch": 1.3420925946637232,
      "grad_norm": 0.2419130951166153,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.2615,
      "step": 138000
    },
    {
      "epoch": 1.3430651255294215,
      "grad_norm": 0.21077275276184082,
      "learning_rate": 4.6625e-05,
      "loss": 0.2621,
      "step": 138100
    },
    {
      "epoch": 1.3440376563951197,
      "grad_norm": 0.25074759125709534,
      "learning_rate": 4.658333333333333e-05,
      "loss": 0.2623,
      "step": 138200
    },
    {
      "epoch": 1.3450101872608182,
      "grad_norm": 0.22112835943698883,
      "learning_rate": 4.654166666666667e-05,
      "loss": 0.2632,
      "step": 138300
    },
    {
      "epoch": 1.3459827181265165,
      "grad_norm": 0.21839706599712372,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.2622,
      "step": 138400
    },
    {
      "epoch": 1.346955248992215,
      "grad_norm": 0.26251786947250366,
      "learning_rate": 4.6458333333333335e-05,
      "loss": 0.2617,
      "step": 138500
    },
    {
      "epoch": 1.3479277798579132,
      "grad_norm": 0.22725962102413177,
      "learning_rate": 4.641666666666667e-05,
      "loss": 0.2611,
      "step": 138600
    },
    {
      "epoch": 1.3489003107236117,
      "grad_norm": 0.24244379997253418,
      "learning_rate": 4.6375e-05,
      "loss": 0.262,
      "step": 138700
    },
    {
      "epoch": 1.34987284158931,
      "grad_norm": 0.2388940006494522,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.2619,
      "step": 138800
    },
    {
      "epoch": 1.3508453724550082,
      "grad_norm": 0.22578458487987518,
      "learning_rate": 4.629166666666667e-05,
      "loss": 0.262,
      "step": 138900
    },
    {
      "epoch": 1.3518179033207067,
      "grad_norm": 0.23471467196941376,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.2616,
      "step": 139000
    },
    {
      "epoch": 1.352790434186405,
      "grad_norm": 0.22430351376533508,
      "learning_rate": 4.6208333333333336e-05,
      "loss": 0.2607,
      "step": 139100
    },
    {
      "epoch": 1.3537629650521033,
      "grad_norm": 0.25271308422088623,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.263,
      "step": 139200
    },
    {
      "epoch": 1.3547354959178017,
      "grad_norm": 0.23418138921260834,
      "learning_rate": 4.6125e-05,
      "loss": 0.2627,
      "step": 139300
    },
    {
      "epoch": 1.3557080267835,
      "grad_norm": 0.24102027714252472,
      "learning_rate": 4.608333333333333e-05,
      "loss": 0.2613,
      "step": 139400
    },
    {
      "epoch": 1.3566805576491983,
      "grad_norm": 0.217292919754982,
      "learning_rate": 4.604166666666666e-05,
      "loss": 0.2615,
      "step": 139500
    },
    {
      "epoch": 1.3576530885148967,
      "grad_norm": 0.234689399600029,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.2617,
      "step": 139600
    },
    {
      "epoch": 1.3586256193805952,
      "grad_norm": 0.25268611311912537,
      "learning_rate": 4.595833333333334e-05,
      "loss": 0.2621,
      "step": 139700
    },
    {
      "epoch": 1.3595981502462935,
      "grad_norm": 0.2086258977651596,
      "learning_rate": 4.591666666666667e-05,
      "loss": 0.2619,
      "step": 139800
    },
    {
      "epoch": 1.3605706811119918,
      "grad_norm": 0.24260635673999786,
      "learning_rate": 4.5875000000000004e-05,
      "loss": 0.2623,
      "step": 139900
    },
    {
      "epoch": 1.3615432119776902,
      "grad_norm": 0.2251257598400116,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.2608,
      "step": 140000
    },
    {
      "epoch": 1.3615432119776902,
      "eval_loss": 0.2629941999912262,
      "eval_runtime": 3213.0561,
      "eval_samples_per_second": 711.083,
      "eval_steps_per_second": 7.111,
      "step": 140000
    },
    {
      "epoch": 1.3625157428433885,
      "grad_norm": 0.21033374965190887,
      "learning_rate": 4.579166666666667e-05,
      "loss": 0.2625,
      "step": 140100
    },
    {
      "epoch": 1.3634882737090868,
      "grad_norm": 0.24467723071575165,
      "learning_rate": 4.575e-05,
      "loss": 0.2613,
      "step": 140200
    },
    {
      "epoch": 1.3644608045747852,
      "grad_norm": 0.23335117101669312,
      "learning_rate": 4.570833333333334e-05,
      "loss": 0.2609,
      "step": 140300
    },
    {
      "epoch": 1.3654333354404835,
      "grad_norm": 0.2476382702589035,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.2622,
      "step": 140400
    },
    {
      "epoch": 1.3664058663061818,
      "grad_norm": 0.23948952555656433,
      "learning_rate": 4.5625e-05,
      "loss": 0.2629,
      "step": 140500
    },
    {
      "epoch": 1.3673783971718803,
      "grad_norm": 0.21918325126171112,
      "learning_rate": 4.5583333333333335e-05,
      "loss": 0.2611,
      "step": 140600
    },
    {
      "epoch": 1.3683509280375785,
      "grad_norm": 0.22838051617145538,
      "learning_rate": 4.554166666666667e-05,
      "loss": 0.2626,
      "step": 140700
    },
    {
      "epoch": 1.369323458903277,
      "grad_norm": 0.22279788553714752,
      "learning_rate": 4.55e-05,
      "loss": 0.2623,
      "step": 140800
    },
    {
      "epoch": 1.3702959897689753,
      "grad_norm": 0.21810993552207947,
      "learning_rate": 4.545833333333334e-05,
      "loss": 0.2627,
      "step": 140900
    },
    {
      "epoch": 1.3712685206346737,
      "grad_norm": 0.2502836585044861,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.2615,
      "step": 141000
    },
    {
      "epoch": 1.372241051500372,
      "grad_norm": 0.200208380818367,
      "learning_rate": 4.5375e-05,
      "loss": 0.2605,
      "step": 141100
    },
    {
      "epoch": 1.3732135823660703,
      "grad_norm": 0.21483471989631653,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.2613,
      "step": 141200
    },
    {
      "epoch": 1.3741861132317688,
      "grad_norm": 0.25895798206329346,
      "learning_rate": 4.529166666666667e-05,
      "loss": 0.2611,
      "step": 141300
    },
    {
      "epoch": 1.375158644097467,
      "grad_norm": 0.2370532751083374,
      "learning_rate": 4.525e-05,
      "loss": 0.2636,
      "step": 141400
    },
    {
      "epoch": 1.3761311749631653,
      "grad_norm": 0.22981512546539307,
      "learning_rate": 4.520833333333334e-05,
      "loss": 0.263,
      "step": 141500
    },
    {
      "epoch": 1.3771037058288638,
      "grad_norm": 0.22763840854167938,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.2612,
      "step": 141600
    },
    {
      "epoch": 1.378076236694562,
      "grad_norm": 0.23728010058403015,
      "learning_rate": 4.5125e-05,
      "loss": 0.2598,
      "step": 141700
    },
    {
      "epoch": 1.3790487675602605,
      "grad_norm": 0.23700237274169922,
      "learning_rate": 4.5083333333333336e-05,
      "loss": 0.2622,
      "step": 141800
    },
    {
      "epoch": 1.3800212984259588,
      "grad_norm": 0.2120225876569748,
      "learning_rate": 4.504166666666667e-05,
      "loss": 0.2613,
      "step": 141900
    },
    {
      "epoch": 1.3809938292916573,
      "grad_norm": 0.23462186753749847,
      "learning_rate": 4.5e-05,
      "loss": 0.2613,
      "step": 142000
    },
    {
      "epoch": 1.3819663601573555,
      "grad_norm": 0.21931135654449463,
      "learning_rate": 4.495833333333333e-05,
      "loss": 0.2622,
      "step": 142100
    },
    {
      "epoch": 1.3829388910230538,
      "grad_norm": 0.2362605482339859,
      "learning_rate": 4.491666666666667e-05,
      "loss": 0.2622,
      "step": 142200
    },
    {
      "epoch": 1.3839114218887523,
      "grad_norm": 0.23884552717208862,
      "learning_rate": 4.4875e-05,
      "loss": 0.2623,
      "step": 142300
    },
    {
      "epoch": 1.3848839527544505,
      "grad_norm": 0.2188003957271576,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.2631,
      "step": 142400
    },
    {
      "epoch": 1.3858564836201488,
      "grad_norm": 0.22569598257541656,
      "learning_rate": 4.4791666666666673e-05,
      "loss": 0.2632,
      "step": 142500
    },
    {
      "epoch": 1.3868290144858473,
      "grad_norm": 0.22297711670398712,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 0.2622,
      "step": 142600
    },
    {
      "epoch": 1.3878015453515455,
      "grad_norm": 0.2535697817802429,
      "learning_rate": 4.4708333333333334e-05,
      "loss": 0.2617,
      "step": 142700
    },
    {
      "epoch": 1.388774076217244,
      "grad_norm": 0.23459012806415558,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.2609,
      "step": 142800
    },
    {
      "epoch": 1.3897466070829423,
      "grad_norm": 0.22508969902992249,
      "learning_rate": 4.4625e-05,
      "loss": 0.2606,
      "step": 142900
    },
    {
      "epoch": 1.3907191379486408,
      "grad_norm": 0.22755086421966553,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.2622,
      "step": 143000
    },
    {
      "epoch": 1.391691668814339,
      "grad_norm": 0.23378178477287292,
      "learning_rate": 4.454166666666667e-05,
      "loss": 0.2623,
      "step": 143100
    },
    {
      "epoch": 1.3926641996800373,
      "grad_norm": 0.2449122816324234,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.261,
      "step": 143200
    },
    {
      "epoch": 1.3936367305457358,
      "grad_norm": 0.20280899107456207,
      "learning_rate": 4.4458333333333334e-05,
      "loss": 0.2618,
      "step": 143300
    },
    {
      "epoch": 1.394609261411434,
      "grad_norm": 0.23207096755504608,
      "learning_rate": 4.4416666666666664e-05,
      "loss": 0.2611,
      "step": 143400
    },
    {
      "epoch": 1.3955817922771323,
      "grad_norm": 0.21539488434791565,
      "learning_rate": 4.4375e-05,
      "loss": 0.2614,
      "step": 143500
    },
    {
      "epoch": 1.3965543231428308,
      "grad_norm": 0.227350652217865,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.2607,
      "step": 143600
    },
    {
      "epoch": 1.397526854008529,
      "grad_norm": 0.2771877944469452,
      "learning_rate": 4.429166666666667e-05,
      "loss": 0.2616,
      "step": 143700
    },
    {
      "epoch": 1.3984993848742273,
      "grad_norm": 0.2257964015007019,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 0.2622,
      "step": 143800
    },
    {
      "epoch": 1.3994719157399258,
      "grad_norm": 0.22946156561374664,
      "learning_rate": 4.4208333333333335e-05,
      "loss": 0.2619,
      "step": 143900
    },
    {
      "epoch": 1.4004444466056243,
      "grad_norm": 0.2295530140399933,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.2616,
      "step": 144000
    },
    {
      "epoch": 1.4014169774713225,
      "grad_norm": 0.2361796349287033,
      "learning_rate": 4.4125e-05,
      "loss": 0.2641,
      "step": 144100
    },
    {
      "epoch": 1.4023895083370208,
      "grad_norm": 0.2379601150751114,
      "learning_rate": 4.408333333333334e-05,
      "loss": 0.2599,
      "step": 144200
    },
    {
      "epoch": 1.4033620392027193,
      "grad_norm": 0.24020224809646606,
      "learning_rate": 4.404166666666667e-05,
      "loss": 0.2622,
      "step": 144300
    },
    {
      "epoch": 1.4043345700684176,
      "grad_norm": 0.22975653409957886,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.2619,
      "step": 144400
    },
    {
      "epoch": 1.4053071009341158,
      "grad_norm": 0.21642020344734192,
      "learning_rate": 4.3958333333333336e-05,
      "loss": 0.2619,
      "step": 144500
    },
    {
      "epoch": 1.4062796317998143,
      "grad_norm": 0.2406846135854721,
      "learning_rate": 4.3916666666666666e-05,
      "loss": 0.2616,
      "step": 144600
    },
    {
      "epoch": 1.4072521626655126,
      "grad_norm": 0.2502714991569519,
      "learning_rate": 4.3875e-05,
      "loss": 0.2603,
      "step": 144700
    },
    {
      "epoch": 1.4082246935312108,
      "grad_norm": 0.23656755685806274,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.2617,
      "step": 144800
    },
    {
      "epoch": 1.4091972243969093,
      "grad_norm": 0.2294187694787979,
      "learning_rate": 4.379166666666667e-05,
      "loss": 0.261,
      "step": 144900
    },
    {
      "epoch": 1.4101697552626076,
      "grad_norm": 0.2375468611717224,
      "learning_rate": 4.375e-05,
      "loss": 0.2627,
      "step": 145000
    },
    {
      "epoch": 1.411142286128306,
      "grad_norm": 0.231674462556839,
      "learning_rate": 4.3708333333333336e-05,
      "loss": 0.2595,
      "step": 145100
    },
    {
      "epoch": 1.4121148169940043,
      "grad_norm": 0.2504088878631592,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.2625,
      "step": 145200
    },
    {
      "epoch": 1.4130873478597028,
      "grad_norm": 0.24127037823200226,
      "learning_rate": 4.3625e-05,
      "loss": 0.262,
      "step": 145300
    },
    {
      "epoch": 1.414059878725401,
      "grad_norm": 0.23420879244804382,
      "learning_rate": 4.358333333333334e-05,
      "loss": 0.2618,
      "step": 145400
    },
    {
      "epoch": 1.4150324095910993,
      "grad_norm": 0.22570033371448517,
      "learning_rate": 4.354166666666667e-05,
      "loss": 0.2607,
      "step": 145500
    },
    {
      "epoch": 1.4160049404567978,
      "grad_norm": 0.22140689194202423,
      "learning_rate": 4.35e-05,
      "loss": 0.2617,
      "step": 145600
    },
    {
      "epoch": 1.416977471322496,
      "grad_norm": 0.22297315299510956,
      "learning_rate": 4.345833333333334e-05,
      "loss": 0.2622,
      "step": 145700
    },
    {
      "epoch": 1.4179500021881943,
      "grad_norm": 0.2264527976512909,
      "learning_rate": 4.341666666666667e-05,
      "loss": 0.2626,
      "step": 145800
    },
    {
      "epoch": 1.4189225330538928,
      "grad_norm": 0.22722600400447845,
      "learning_rate": 4.3375000000000004e-05,
      "loss": 0.2617,
      "step": 145900
    },
    {
      "epoch": 1.419895063919591,
      "grad_norm": 0.24257788062095642,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.2612,
      "step": 146000
    },
    {
      "epoch": 1.4208675947852896,
      "grad_norm": 0.22027364373207092,
      "learning_rate": 4.329166666666667e-05,
      "loss": 0.2603,
      "step": 146100
    },
    {
      "epoch": 1.4218401256509878,
      "grad_norm": 0.22954276204109192,
      "learning_rate": 4.325e-05,
      "loss": 0.2619,
      "step": 146200
    },
    {
      "epoch": 1.4228126565166863,
      "grad_norm": 0.22258742153644562,
      "learning_rate": 4.320833333333333e-05,
      "loss": 0.2603,
      "step": 146300
    },
    {
      "epoch": 1.4237851873823846,
      "grad_norm": 0.2370394617319107,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.2609,
      "step": 146400
    },
    {
      "epoch": 1.4247577182480828,
      "grad_norm": 0.242767333984375,
      "learning_rate": 4.3125000000000005e-05,
      "loss": 0.2609,
      "step": 146500
    },
    {
      "epoch": 1.4257302491137813,
      "grad_norm": 0.24553245306015015,
      "learning_rate": 4.3083333333333335e-05,
      "loss": 0.2611,
      "step": 146600
    },
    {
      "epoch": 1.4267027799794796,
      "grad_norm": 0.23909413814544678,
      "learning_rate": 4.304166666666667e-05,
      "loss": 0.2602,
      "step": 146700
    },
    {
      "epoch": 1.4276753108451778,
      "grad_norm": 0.22441351413726807,
      "learning_rate": 4.3e-05,
      "loss": 0.2625,
      "step": 146800
    },
    {
      "epoch": 1.4286478417108763,
      "grad_norm": 0.2402324676513672,
      "learning_rate": 4.295833333333333e-05,
      "loss": 0.2612,
      "step": 146900
    },
    {
      "epoch": 1.4296203725765746,
      "grad_norm": 0.22174319624900818,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.2609,
      "step": 147000
    },
    {
      "epoch": 1.430592903442273,
      "grad_norm": 0.2588229477405548,
      "learning_rate": 4.2875000000000005e-05,
      "loss": 0.2606,
      "step": 147100
    },
    {
      "epoch": 1.4315654343079713,
      "grad_norm": 0.2368621826171875,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.2592,
      "step": 147200
    },
    {
      "epoch": 1.4325379651736698,
      "grad_norm": 0.2155609428882599,
      "learning_rate": 4.2791666666666666e-05,
      "loss": 0.2629,
      "step": 147300
    },
    {
      "epoch": 1.433510496039368,
      "grad_norm": 0.20576165616512299,
      "learning_rate": 4.275e-05,
      "loss": 0.261,
      "step": 147400
    },
    {
      "epoch": 1.4344830269050663,
      "grad_norm": 0.2400285005569458,
      "learning_rate": 4.270833333333333e-05,
      "loss": 0.2594,
      "step": 147500
    },
    {
      "epoch": 1.4354555577707648,
      "grad_norm": 0.21199101209640503,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.2615,
      "step": 147600
    },
    {
      "epoch": 1.436428088636463,
      "grad_norm": 0.22457900643348694,
      "learning_rate": 4.2625000000000006e-05,
      "loss": 0.2604,
      "step": 147700
    },
    {
      "epoch": 1.4374006195021614,
      "grad_norm": 0.2276991456747055,
      "learning_rate": 4.2583333333333336e-05,
      "loss": 0.2605,
      "step": 147800
    },
    {
      "epoch": 1.4383731503678598,
      "grad_norm": 0.2521013021469116,
      "learning_rate": 4.2541666666666666e-05,
      "loss": 0.2592,
      "step": 147900
    },
    {
      "epoch": 1.439345681233558,
      "grad_norm": 0.2049698680639267,
      "learning_rate": 4.25e-05,
      "loss": 0.2616,
      "step": 148000
    },
    {
      "epoch": 1.4403182120992564,
      "grad_norm": 0.22299841046333313,
      "learning_rate": 4.245833333333333e-05,
      "loss": 0.2606,
      "step": 148100
    },
    {
      "epoch": 1.4412907429649549,
      "grad_norm": 0.2545938193798065,
      "learning_rate": 4.241666666666667e-05,
      "loss": 0.2602,
      "step": 148200
    },
    {
      "epoch": 1.4422632738306533,
      "grad_norm": 0.2393825799226761,
      "learning_rate": 4.237500000000001e-05,
      "loss": 0.2612,
      "step": 148300
    },
    {
      "epoch": 1.4432358046963516,
      "grad_norm": 0.2584700882434845,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.2613,
      "step": 148400
    },
    {
      "epoch": 1.4442083355620499,
      "grad_norm": 0.2426891326904297,
      "learning_rate": 4.229166666666667e-05,
      "loss": 0.2606,
      "step": 148500
    },
    {
      "epoch": 1.4451808664277483,
      "grad_norm": 0.24660255014896393,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 0.2615,
      "step": 148600
    },
    {
      "epoch": 1.4461533972934466,
      "grad_norm": 0.22458966076374054,
      "learning_rate": 4.2208333333333334e-05,
      "loss": 0.261,
      "step": 148700
    },
    {
      "epoch": 1.4471259281591449,
      "grad_norm": 0.23737846314907074,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.2608,
      "step": 148800
    },
    {
      "epoch": 1.4480984590248434,
      "grad_norm": 0.2432878017425537,
      "learning_rate": 4.2125e-05,
      "loss": 0.2619,
      "step": 148900
    },
    {
      "epoch": 1.4490709898905416,
      "grad_norm": 0.21231672167778015,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.261,
      "step": 149000
    },
    {
      "epoch": 1.4500435207562399,
      "grad_norm": 0.21722419559955597,
      "learning_rate": 4.204166666666667e-05,
      "loss": 0.2601,
      "step": 149100
    },
    {
      "epoch": 1.4510160516219384,
      "grad_norm": 0.26029396057128906,
      "learning_rate": 4.2e-05,
      "loss": 0.2616,
      "step": 149200
    },
    {
      "epoch": 1.4519885824876366,
      "grad_norm": 0.22875970602035522,
      "learning_rate": 4.1958333333333335e-05,
      "loss": 0.2602,
      "step": 149300
    },
    {
      "epoch": 1.452961113353335,
      "grad_norm": 0.23335523903369904,
      "learning_rate": 4.191666666666667e-05,
      "loss": 0.2592,
      "step": 149400
    },
    {
      "epoch": 1.4539336442190334,
      "grad_norm": 0.2538594901561737,
      "learning_rate": 4.1875e-05,
      "loss": 0.2594,
      "step": 149500
    },
    {
      "epoch": 1.4549061750847319,
      "grad_norm": 0.25725042819976807,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.2609,
      "step": 149600
    },
    {
      "epoch": 1.4558787059504301,
      "grad_norm": 0.23043887317180634,
      "learning_rate": 4.179166666666667e-05,
      "loss": 0.2604,
      "step": 149700
    },
    {
      "epoch": 1.4568512368161284,
      "grad_norm": 0.2372676432132721,
      "learning_rate": 4.175e-05,
      "loss": 0.2617,
      "step": 149800
    },
    {
      "epoch": 1.4578237676818269,
      "grad_norm": 0.23257318139076233,
      "learning_rate": 4.1708333333333335e-05,
      "loss": 0.2617,
      "step": 149900
    },
    {
      "epoch": 1.4587962985475251,
      "grad_norm": 0.24698182940483093,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.2615,
      "step": 150000
    },
    {
      "epoch": 1.4587962985475251,
      "eval_loss": 0.26159802079200745,
      "eval_runtime": 3009.0489,
      "eval_samples_per_second": 759.293,
      "eval_steps_per_second": 7.593,
      "step": 150000
    },
    {
      "epoch": 1.4597688294132234,
      "grad_norm": 0.22812284529209137,
      "learning_rate": 4.1625e-05,
      "loss": 0.2611,
      "step": 150100
    },
    {
      "epoch": 1.4607413602789219,
      "grad_norm": 0.2356364130973816,
      "learning_rate": 4.158333333333333e-05,
      "loss": 0.2595,
      "step": 150200
    },
    {
      "epoch": 1.4617138911446201,
      "grad_norm": 0.23982474207878113,
      "learning_rate": 4.154166666666667e-05,
      "loss": 0.2607,
      "step": 150300
    },
    {
      "epoch": 1.4626864220103186,
      "grad_norm": 0.21842750906944275,
      "learning_rate": 4.15e-05,
      "loss": 0.2615,
      "step": 150400
    },
    {
      "epoch": 1.4636589528760169,
      "grad_norm": 0.25572434067726135,
      "learning_rate": 4.1458333333333336e-05,
      "loss": 0.2599,
      "step": 150500
    },
    {
      "epoch": 1.4646314837417154,
      "grad_norm": 0.23998577892780304,
      "learning_rate": 4.141666666666667e-05,
      "loss": 0.2624,
      "step": 150600
    },
    {
      "epoch": 1.4656040146074136,
      "grad_norm": 0.2342551052570343,
      "learning_rate": 4.1375e-05,
      "loss": 0.2608,
      "step": 150700
    },
    {
      "epoch": 1.466576545473112,
      "grad_norm": 0.2399446964263916,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.2606,
      "step": 150800
    },
    {
      "epoch": 1.4675490763388104,
      "grad_norm": 0.2467675656080246,
      "learning_rate": 4.129166666666667e-05,
      "loss": 0.2598,
      "step": 150900
    },
    {
      "epoch": 1.4685216072045086,
      "grad_norm": 0.24078090488910675,
      "learning_rate": 4.125e-05,
      "loss": 0.2596,
      "step": 151000
    },
    {
      "epoch": 1.469494138070207,
      "grad_norm": 0.2394653856754303,
      "learning_rate": 4.120833333333334e-05,
      "loss": 0.2605,
      "step": 151100
    },
    {
      "epoch": 1.4704666689359054,
      "grad_norm": 0.25507640838623047,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.2628,
      "step": 151200
    },
    {
      "epoch": 1.4714391998016036,
      "grad_norm": 0.2509083151817322,
      "learning_rate": 4.1125000000000004e-05,
      "loss": 0.2626,
      "step": 151300
    },
    {
      "epoch": 1.4724117306673021,
      "grad_norm": 0.21824051439762115,
      "learning_rate": 4.1083333333333334e-05,
      "loss": 0.2611,
      "step": 151400
    },
    {
      "epoch": 1.4733842615330004,
      "grad_norm": 0.22760656476020813,
      "learning_rate": 4.104166666666667e-05,
      "loss": 0.2615,
      "step": 151500
    },
    {
      "epoch": 1.4743567923986989,
      "grad_norm": 0.23792222142219543,
      "learning_rate": 4.1e-05,
      "loss": 0.2595,
      "step": 151600
    },
    {
      "epoch": 1.4753293232643971,
      "grad_norm": 0.2324260175228119,
      "learning_rate": 4.095833333333334e-05,
      "loss": 0.2613,
      "step": 151700
    },
    {
      "epoch": 1.4763018541300954,
      "grad_norm": 0.27629441022872925,
      "learning_rate": 4.091666666666667e-05,
      "loss": 0.2595,
      "step": 151800
    },
    {
      "epoch": 1.4772743849957939,
      "grad_norm": 0.23525744676589966,
      "learning_rate": 4.0875000000000004e-05,
      "loss": 0.2611,
      "step": 151900
    },
    {
      "epoch": 1.4782469158614922,
      "grad_norm": 0.20956476032733917,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.2594,
      "step": 152000
    },
    {
      "epoch": 1.4792194467271904,
      "grad_norm": 0.23272103071212769,
      "learning_rate": 4.0791666666666664e-05,
      "loss": 0.2592,
      "step": 152100
    },
    {
      "epoch": 1.480191977592889,
      "grad_norm": 0.23893727362155914,
      "learning_rate": 4.075e-05,
      "loss": 0.2621,
      "step": 152200
    },
    {
      "epoch": 1.4811645084585872,
      "grad_norm": 0.23821067810058594,
      "learning_rate": 4.070833333333334e-05,
      "loss": 0.2621,
      "step": 152300
    },
    {
      "epoch": 1.4821370393242854,
      "grad_norm": 0.23532070219516754,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.2602,
      "step": 152400
    },
    {
      "epoch": 1.483109570189984,
      "grad_norm": 0.23653867840766907,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 0.2591,
      "step": 152500
    },
    {
      "epoch": 1.4840821010556824,
      "grad_norm": 0.22608204185962677,
      "learning_rate": 4.0583333333333335e-05,
      "loss": 0.2609,
      "step": 152600
    },
    {
      "epoch": 1.4850546319213807,
      "grad_norm": 0.23346763849258423,
      "learning_rate": 4.0541666666666665e-05,
      "loss": 0.2611,
      "step": 152700
    },
    {
      "epoch": 1.486027162787079,
      "grad_norm": 0.2399185746908188,
      "learning_rate": 4.05e-05,
      "loss": 0.2605,
      "step": 152800
    },
    {
      "epoch": 1.4869996936527774,
      "grad_norm": 0.22503843903541565,
      "learning_rate": 4.045833333333334e-05,
      "loss": 0.2602,
      "step": 152900
    },
    {
      "epoch": 1.4879722245184757,
      "grad_norm": 0.23172934353351593,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.2619,
      "step": 153000
    },
    {
      "epoch": 1.488944755384174,
      "grad_norm": 0.24234983325004578,
      "learning_rate": 4.0375e-05,
      "loss": 0.2595,
      "step": 153100
    },
    {
      "epoch": 1.4899172862498724,
      "grad_norm": 0.2415904551744461,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.2605,
      "step": 153200
    },
    {
      "epoch": 1.4908898171155707,
      "grad_norm": 0.26663708686828613,
      "learning_rate": 4.0291666666666666e-05,
      "loss": 0.2603,
      "step": 153300
    },
    {
      "epoch": 1.491862347981269,
      "grad_norm": 0.2594946026802063,
      "learning_rate": 4.025e-05,
      "loss": 0.2605,
      "step": 153400
    },
    {
      "epoch": 1.4928348788469674,
      "grad_norm": 0.23649877309799194,
      "learning_rate": 4.020833333333334e-05,
      "loss": 0.261,
      "step": 153500
    },
    {
      "epoch": 1.4938074097126657,
      "grad_norm": 0.21869716048240662,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.2596,
      "step": 153600
    },
    {
      "epoch": 1.4947799405783642,
      "grad_norm": 0.23375561833381653,
      "learning_rate": 4.0125e-05,
      "loss": 0.2602,
      "step": 153700
    },
    {
      "epoch": 1.4957524714440624,
      "grad_norm": 0.2503579258918762,
      "learning_rate": 4.0083333333333336e-05,
      "loss": 0.262,
      "step": 153800
    },
    {
      "epoch": 1.496725002309761,
      "grad_norm": 0.2465110421180725,
      "learning_rate": 4.0041666666666666e-05,
      "loss": 0.2597,
      "step": 153900
    },
    {
      "epoch": 1.4976975331754592,
      "grad_norm": 0.2392175942659378,
      "learning_rate": 4e-05,
      "loss": 0.2596,
      "step": 154000
    },
    {
      "epoch": 1.4986700640411574,
      "grad_norm": 0.23020821809768677,
      "learning_rate": 3.995833333333333e-05,
      "loss": 0.2607,
      "step": 154100
    },
    {
      "epoch": 1.499642594906856,
      "grad_norm": 0.25065693259239197,
      "learning_rate": 3.991666666666667e-05,
      "loss": 0.2606,
      "step": 154200
    },
    {
      "epoch": 1.5006151257725542,
      "grad_norm": 0.2835197150707245,
      "learning_rate": 3.9875e-05,
      "loss": 0.2591,
      "step": 154300
    },
    {
      "epoch": 1.5015876566382524,
      "grad_norm": 0.2341475933790207,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.2593,
      "step": 154400
    },
    {
      "epoch": 1.502560187503951,
      "grad_norm": 0.2417604774236679,
      "learning_rate": 3.979166666666667e-05,
      "loss": 0.2605,
      "step": 154500
    },
    {
      "epoch": 1.5035327183696494,
      "grad_norm": 0.2467944324016571,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 0.26,
      "step": 154600
    },
    {
      "epoch": 1.5045052492353475,
      "grad_norm": 0.22969196736812592,
      "learning_rate": 3.9708333333333334e-05,
      "loss": 0.26,
      "step": 154700
    },
    {
      "epoch": 1.505477780101046,
      "grad_norm": 0.2174936830997467,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.2601,
      "step": 154800
    },
    {
      "epoch": 1.5064503109667444,
      "grad_norm": 0.2359195202589035,
      "learning_rate": 3.9625e-05,
      "loss": 0.2608,
      "step": 154900
    },
    {
      "epoch": 1.5074228418324427,
      "grad_norm": 0.24359294772148132,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.2605,
      "step": 155000
    },
    {
      "epoch": 1.508395372698141,
      "grad_norm": 0.24685369431972504,
      "learning_rate": 3.9541666666666675e-05,
      "loss": 0.2593,
      "step": 155100
    },
    {
      "epoch": 1.5093679035638394,
      "grad_norm": 0.22780939936637878,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.2593,
      "step": 155200
    },
    {
      "epoch": 1.5103404344295377,
      "grad_norm": 0.236886128783226,
      "learning_rate": 3.9458333333333335e-05,
      "loss": 0.2604,
      "step": 155300
    },
    {
      "epoch": 1.511312965295236,
      "grad_norm": 0.24258936941623688,
      "learning_rate": 3.941666666666667e-05,
      "loss": 0.2587,
      "step": 155400
    },
    {
      "epoch": 1.5122854961609344,
      "grad_norm": 0.24072134494781494,
      "learning_rate": 3.9375e-05,
      "loss": 0.2613,
      "step": 155500
    },
    {
      "epoch": 1.513258027026633,
      "grad_norm": 0.25170767307281494,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.2598,
      "step": 155600
    },
    {
      "epoch": 1.514230557892331,
      "grad_norm": 0.21415360271930695,
      "learning_rate": 3.929166666666667e-05,
      "loss": 0.2584,
      "step": 155700
    },
    {
      "epoch": 1.5152030887580294,
      "grad_norm": 0.25753486156463623,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 0.2598,
      "step": 155800
    },
    {
      "epoch": 1.516175619623728,
      "grad_norm": 0.25799375772476196,
      "learning_rate": 3.9208333333333335e-05,
      "loss": 0.26,
      "step": 155900
    },
    {
      "epoch": 1.5171481504894262,
      "grad_norm": 0.22554263472557068,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.2606,
      "step": 156000
    },
    {
      "epoch": 1.5181206813551245,
      "grad_norm": 0.2351863533258438,
      "learning_rate": 3.9125e-05,
      "loss": 0.2607,
      "step": 156100
    },
    {
      "epoch": 1.519093212220823,
      "grad_norm": 0.2561335563659668,
      "learning_rate": 3.908333333333333e-05,
      "loss": 0.2591,
      "step": 156200
    },
    {
      "epoch": 1.5200657430865212,
      "grad_norm": 0.23376435041427612,
      "learning_rate": 3.904166666666667e-05,
      "loss": 0.2594,
      "step": 156300
    },
    {
      "epoch": 1.5210382739522195,
      "grad_norm": 0.22708098590373993,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.2576,
      "step": 156400
    },
    {
      "epoch": 1.522010804817918,
      "grad_norm": 0.2528015077114105,
      "learning_rate": 3.8958333333333336e-05,
      "loss": 0.2585,
      "step": 156500
    },
    {
      "epoch": 1.5229833356836162,
      "grad_norm": 0.24811598658561707,
      "learning_rate": 3.8916666666666666e-05,
      "loss": 0.2596,
      "step": 156600
    },
    {
      "epoch": 1.5239558665493145,
      "grad_norm": 0.23531600832939148,
      "learning_rate": 3.8875e-05,
      "loss": 0.2602,
      "step": 156700
    },
    {
      "epoch": 1.524928397415013,
      "grad_norm": 0.2513745129108429,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.2594,
      "step": 156800
    },
    {
      "epoch": 1.5259009282807114,
      "grad_norm": 0.24793323874473572,
      "learning_rate": 3.879166666666667e-05,
      "loss": 0.2581,
      "step": 156900
    },
    {
      "epoch": 1.5268734591464097,
      "grad_norm": 0.25833213329315186,
      "learning_rate": 3.875e-05,
      "loss": 0.2606,
      "step": 157000
    },
    {
      "epoch": 1.527845990012108,
      "grad_norm": 0.23070454597473145,
      "learning_rate": 3.870833333333334e-05,
      "loss": 0.2573,
      "step": 157100
    },
    {
      "epoch": 1.5288185208778065,
      "grad_norm": 0.2414993941783905,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.2597,
      "step": 157200
    },
    {
      "epoch": 1.5297910517435047,
      "grad_norm": 0.2281898707151413,
      "learning_rate": 3.8625e-05,
      "loss": 0.2611,
      "step": 157300
    },
    {
      "epoch": 1.530763582609203,
      "grad_norm": 0.24078664183616638,
      "learning_rate": 3.8583333333333334e-05,
      "loss": 0.2595,
      "step": 157400
    },
    {
      "epoch": 1.5317361134749015,
      "grad_norm": 0.24665586650371552,
      "learning_rate": 3.854166666666667e-05,
      "loss": 0.2587,
      "step": 157500
    },
    {
      "epoch": 1.5327086443405997,
      "grad_norm": 0.23488660156726837,
      "learning_rate": 3.85e-05,
      "loss": 0.259,
      "step": 157600
    },
    {
      "epoch": 1.533681175206298,
      "grad_norm": 0.2507287859916687,
      "learning_rate": 3.845833333333334e-05,
      "loss": 0.2587,
      "step": 157700
    },
    {
      "epoch": 1.5346537060719965,
      "grad_norm": 0.24483969807624817,
      "learning_rate": 3.841666666666667e-05,
      "loss": 0.2601,
      "step": 157800
    },
    {
      "epoch": 1.535626236937695,
      "grad_norm": 0.24098648130893707,
      "learning_rate": 3.8375e-05,
      "loss": 0.2587,
      "step": 157900
    },
    {
      "epoch": 1.536598767803393,
      "grad_norm": 0.24336360394954681,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.2599,
      "step": 158000
    },
    {
      "epoch": 1.5375712986690915,
      "grad_norm": 0.2517784833908081,
      "learning_rate": 3.829166666666667e-05,
      "loss": 0.2603,
      "step": 158100
    },
    {
      "epoch": 1.53854382953479,
      "grad_norm": 0.25306153297424316,
      "learning_rate": 3.825e-05,
      "loss": 0.2595,
      "step": 158200
    },
    {
      "epoch": 1.5395163604004882,
      "grad_norm": 0.2092384546995163,
      "learning_rate": 3.820833333333334e-05,
      "loss": 0.2607,
      "step": 158300
    },
    {
      "epoch": 1.5404888912661865,
      "grad_norm": 0.2322668433189392,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.2608,
      "step": 158400
    },
    {
      "epoch": 1.541461422131885,
      "grad_norm": 0.2663743793964386,
      "learning_rate": 3.8125e-05,
      "loss": 0.2596,
      "step": 158500
    },
    {
      "epoch": 1.5424339529975832,
      "grad_norm": 0.25591790676116943,
      "learning_rate": 3.8083333333333335e-05,
      "loss": 0.2588,
      "step": 158600
    },
    {
      "epoch": 1.5434064838632815,
      "grad_norm": 0.2740025818347931,
      "learning_rate": 3.804166666666667e-05,
      "loss": 0.2604,
      "step": 158700
    },
    {
      "epoch": 1.54437901472898,
      "grad_norm": 0.24081414937973022,
      "learning_rate": 3.8e-05,
      "loss": 0.2586,
      "step": 158800
    },
    {
      "epoch": 1.5453515455946785,
      "grad_norm": 0.2269275188446045,
      "learning_rate": 3.795833333333333e-05,
      "loss": 0.2573,
      "step": 158900
    },
    {
      "epoch": 1.5463240764603765,
      "grad_norm": 0.23337413370609283,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.2601,
      "step": 159000
    },
    {
      "epoch": 1.547296607326075,
      "grad_norm": 0.2489895224571228,
      "learning_rate": 3.7875e-05,
      "loss": 0.2593,
      "step": 159100
    },
    {
      "epoch": 1.5482691381917735,
      "grad_norm": 0.26835349202156067,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.26,
      "step": 159200
    },
    {
      "epoch": 1.5492416690574717,
      "grad_norm": 0.23117519915103912,
      "learning_rate": 3.779166666666667e-05,
      "loss": 0.2595,
      "step": 159300
    },
    {
      "epoch": 1.55021419992317,
      "grad_norm": 0.24558036029338837,
      "learning_rate": 3.775e-05,
      "loss": 0.2602,
      "step": 159400
    },
    {
      "epoch": 1.5511867307888685,
      "grad_norm": 0.23719927668571472,
      "learning_rate": 3.770833333333333e-05,
      "loss": 0.2587,
      "step": 159500
    },
    {
      "epoch": 1.5521592616545667,
      "grad_norm": 0.25016844272613525,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.2597,
      "step": 159600
    },
    {
      "epoch": 1.553131792520265,
      "grad_norm": 0.21742424368858337,
      "learning_rate": 3.7625e-05,
      "loss": 0.2597,
      "step": 159700
    },
    {
      "epoch": 1.5541043233859635,
      "grad_norm": 0.24252036213874817,
      "learning_rate": 3.7583333333333337e-05,
      "loss": 0.2601,
      "step": 159800
    },
    {
      "epoch": 1.555076854251662,
      "grad_norm": 0.24797828495502472,
      "learning_rate": 3.754166666666667e-05,
      "loss": 0.2582,
      "step": 159900
    },
    {
      "epoch": 1.55604938511736,
      "grad_norm": 0.2527284026145935,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.2584,
      "step": 160000
    },
    {
      "epoch": 1.55604938511736,
      "eval_loss": 0.2602594494819641,
      "eval_runtime": 2960.8018,
      "eval_samples_per_second": 771.666,
      "eval_steps_per_second": 7.717,
      "step": 160000
    },
    {
      "epoch": 1.5570219159830585,
      "grad_norm": 0.2282237410545349,
      "learning_rate": 3.7458333333333334e-05,
      "loss": 0.2611,
      "step": 160100
    },
    {
      "epoch": 1.557994446848757,
      "grad_norm": 0.25566625595092773,
      "learning_rate": 3.7416666666666664e-05,
      "loss": 0.2597,
      "step": 160200
    },
    {
      "epoch": 1.5589669777144552,
      "grad_norm": 0.22976118326187134,
      "learning_rate": 3.737500000000001e-05,
      "loss": 0.2588,
      "step": 160300
    },
    {
      "epoch": 1.5599395085801535,
      "grad_norm": 0.24327391386032104,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.2605,
      "step": 160400
    },
    {
      "epoch": 1.560912039445852,
      "grad_norm": 0.24165818095207214,
      "learning_rate": 3.729166666666667e-05,
      "loss": 0.2615,
      "step": 160500
    },
    {
      "epoch": 1.5618845703115503,
      "grad_norm": 0.24525311589241028,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 0.2596,
      "step": 160600
    },
    {
      "epoch": 1.5628571011772485,
      "grad_norm": 0.25632572174072266,
      "learning_rate": 3.7208333333333334e-05,
      "loss": 0.2577,
      "step": 160700
    },
    {
      "epoch": 1.563829632042947,
      "grad_norm": 0.2467278391122818,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.2605,
      "step": 160800
    },
    {
      "epoch": 1.5648021629086453,
      "grad_norm": 0.2462191879749298,
      "learning_rate": 3.7125e-05,
      "loss": 0.2593,
      "step": 160900
    },
    {
      "epoch": 1.5657746937743435,
      "grad_norm": 0.24258621037006378,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.2592,
      "step": 161000
    },
    {
      "epoch": 1.566747224640042,
      "grad_norm": 0.23997484147548676,
      "learning_rate": 3.704166666666667e-05,
      "loss": 0.2589,
      "step": 161100
    },
    {
      "epoch": 1.5677197555057405,
      "grad_norm": 0.2518363296985626,
      "learning_rate": 3.7e-05,
      "loss": 0.2581,
      "step": 161200
    },
    {
      "epoch": 1.5686922863714388,
      "grad_norm": 0.23216305673122406,
      "learning_rate": 3.6958333333333335e-05,
      "loss": 0.2577,
      "step": 161300
    },
    {
      "epoch": 1.569664817237137,
      "grad_norm": 0.22775006294250488,
      "learning_rate": 3.6916666666666665e-05,
      "loss": 0.2604,
      "step": 161400
    },
    {
      "epoch": 1.5706373481028355,
      "grad_norm": 0.22994095087051392,
      "learning_rate": 3.6875e-05,
      "loss": 0.2584,
      "step": 161500
    },
    {
      "epoch": 1.5716098789685338,
      "grad_norm": 0.2401806116104126,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.2576,
      "step": 161600
    },
    {
      "epoch": 1.572582409834232,
      "grad_norm": 0.23369228839874268,
      "learning_rate": 3.679166666666667e-05,
      "loss": 0.2579,
      "step": 161700
    },
    {
      "epoch": 1.5735549406999305,
      "grad_norm": 0.2533378303050995,
      "learning_rate": 3.675e-05,
      "loss": 0.2586,
      "step": 161800
    },
    {
      "epoch": 1.5745274715656288,
      "grad_norm": 0.24184922873973846,
      "learning_rate": 3.6708333333333336e-05,
      "loss": 0.2577,
      "step": 161900
    },
    {
      "epoch": 1.575500002431327,
      "grad_norm": 0.24825355410575867,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.2601,
      "step": 162000
    },
    {
      "epoch": 1.5764725332970255,
      "grad_norm": 0.25205111503601074,
      "learning_rate": 3.6625e-05,
      "loss": 0.2598,
      "step": 162100
    },
    {
      "epoch": 1.577445064162724,
      "grad_norm": 0.2580556273460388,
      "learning_rate": 3.658333333333334e-05,
      "loss": 0.2597,
      "step": 162200
    },
    {
      "epoch": 1.578417595028422,
      "grad_norm": 0.2505266070365906,
      "learning_rate": 3.654166666666667e-05,
      "loss": 0.2581,
      "step": 162300
    },
    {
      "epoch": 1.5793901258941205,
      "grad_norm": 0.24095548689365387,
      "learning_rate": 3.65e-05,
      "loss": 0.2582,
      "step": 162400
    },
    {
      "epoch": 1.580362656759819,
      "grad_norm": 0.24181489646434784,
      "learning_rate": 3.6458333333333336e-05,
      "loss": 0.2572,
      "step": 162500
    },
    {
      "epoch": 1.5813351876255173,
      "grad_norm": 0.22157783806324005,
      "learning_rate": 3.641666666666667e-05,
      "loss": 0.2603,
      "step": 162600
    },
    {
      "epoch": 1.5823077184912155,
      "grad_norm": 0.22610703110694885,
      "learning_rate": 3.6375e-05,
      "loss": 0.2572,
      "step": 162700
    },
    {
      "epoch": 1.583280249356914,
      "grad_norm": 0.251787006855011,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.2576,
      "step": 162800
    },
    {
      "epoch": 1.5842527802226123,
      "grad_norm": 0.24746692180633545,
      "learning_rate": 3.629166666666667e-05,
      "loss": 0.2608,
      "step": 162900
    },
    {
      "epoch": 1.5852253110883106,
      "grad_norm": 0.2364295870065689,
      "learning_rate": 3.625e-05,
      "loss": 0.2591,
      "step": 163000
    },
    {
      "epoch": 1.586197841954009,
      "grad_norm": 0.2461637556552887,
      "learning_rate": 3.620833333333333e-05,
      "loss": 0.2589,
      "step": 163100
    },
    {
      "epoch": 1.5871703728197075,
      "grad_norm": 0.2531013786792755,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.2591,
      "step": 163200
    },
    {
      "epoch": 1.5881429036854056,
      "grad_norm": 0.2578372359275818,
      "learning_rate": 3.6125000000000004e-05,
      "loss": 0.2588,
      "step": 163300
    },
    {
      "epoch": 1.589115434551104,
      "grad_norm": 0.2391522228717804,
      "learning_rate": 3.6083333333333334e-05,
      "loss": 0.258,
      "step": 163400
    },
    {
      "epoch": 1.5900879654168025,
      "grad_norm": 0.22695821523666382,
      "learning_rate": 3.604166666666667e-05,
      "loss": 0.2597,
      "step": 163500
    },
    {
      "epoch": 1.5910604962825008,
      "grad_norm": 0.2640249729156494,
      "learning_rate": 3.6e-05,
      "loss": 0.2603,
      "step": 163600
    },
    {
      "epoch": 1.592033027148199,
      "grad_norm": 0.26752549409866333,
      "learning_rate": 3.595833333333333e-05,
      "loss": 0.2572,
      "step": 163700
    },
    {
      "epoch": 1.5930055580138975,
      "grad_norm": 0.24165502190589905,
      "learning_rate": 3.591666666666667e-05,
      "loss": 0.2574,
      "step": 163800
    },
    {
      "epoch": 1.5939780888795958,
      "grad_norm": 0.22180619835853577,
      "learning_rate": 3.5875000000000005e-05,
      "loss": 0.2615,
      "step": 163900
    },
    {
      "epoch": 1.594950619745294,
      "grad_norm": 0.25281789898872375,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.2571,
      "step": 164000
    },
    {
      "epoch": 1.5959231506109925,
      "grad_norm": 0.28045186400413513,
      "learning_rate": 3.5791666666666665e-05,
      "loss": 0.2603,
      "step": 164100
    },
    {
      "epoch": 1.596895681476691,
      "grad_norm": 0.2311592400074005,
      "learning_rate": 3.575e-05,
      "loss": 0.2591,
      "step": 164200
    },
    {
      "epoch": 1.597868212342389,
      "grad_norm": 0.24977616965770721,
      "learning_rate": 3.570833333333333e-05,
      "loss": 0.2581,
      "step": 164300
    },
    {
      "epoch": 1.5988407432080876,
      "grad_norm": 0.2503441572189331,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.2598,
      "step": 164400
    },
    {
      "epoch": 1.599813274073786,
      "grad_norm": 0.24895043671131134,
      "learning_rate": 3.5625000000000005e-05,
      "loss": 0.2597,
      "step": 164500
    },
    {
      "epoch": 1.6007858049394843,
      "grad_norm": 0.23910103738307953,
      "learning_rate": 3.5583333333333335e-05,
      "loss": 0.2601,
      "step": 164600
    },
    {
      "epoch": 1.6017583358051826,
      "grad_norm": 0.2440255731344223,
      "learning_rate": 3.5541666666666665e-05,
      "loss": 0.26,
      "step": 164700
    },
    {
      "epoch": 1.602730866670881,
      "grad_norm": 0.2561860680580139,
      "learning_rate": 3.55e-05,
      "loss": 0.2594,
      "step": 164800
    },
    {
      "epoch": 1.6037033975365793,
      "grad_norm": 0.23414424061775208,
      "learning_rate": 3.545833333333333e-05,
      "loss": 0.2577,
      "step": 164900
    },
    {
      "epoch": 1.6046759284022776,
      "grad_norm": 0.24752767384052277,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.2576,
      "step": 165000
    },
    {
      "epoch": 1.605648459267976,
      "grad_norm": 0.22554419934749603,
      "learning_rate": 3.5375e-05,
      "loss": 0.2599,
      "step": 165100
    },
    {
      "epoch": 1.6066209901336743,
      "grad_norm": 0.2527083456516266,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.2587,
      "step": 165200
    },
    {
      "epoch": 1.6075935209993726,
      "grad_norm": 0.2570473849773407,
      "learning_rate": 3.5291666666666666e-05,
      "loss": 0.2585,
      "step": 165300
    },
    {
      "epoch": 1.608566051865071,
      "grad_norm": 0.24368728697299957,
      "learning_rate": 3.525e-05,
      "loss": 0.2577,
      "step": 165400
    },
    {
      "epoch": 1.6095385827307696,
      "grad_norm": 0.2273639291524887,
      "learning_rate": 3.520833333333334e-05,
      "loss": 0.2585,
      "step": 165500
    },
    {
      "epoch": 1.6105111135964678,
      "grad_norm": 0.2402883619070053,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.2584,
      "step": 165600
    },
    {
      "epoch": 1.611483644462166,
      "grad_norm": 0.26894569396972656,
      "learning_rate": 3.5125e-05,
      "loss": 0.2595,
      "step": 165700
    },
    {
      "epoch": 1.6124561753278646,
      "grad_norm": 0.24953703582286835,
      "learning_rate": 3.508333333333334e-05,
      "loss": 0.2592,
      "step": 165800
    },
    {
      "epoch": 1.6134287061935628,
      "grad_norm": 0.23741436004638672,
      "learning_rate": 3.504166666666667e-05,
      "loss": 0.2583,
      "step": 165900
    },
    {
      "epoch": 1.614401237059261,
      "grad_norm": 0.25256651639938354,
      "learning_rate": 3.5e-05,
      "loss": 0.259,
      "step": 166000
    },
    {
      "epoch": 1.6153737679249596,
      "grad_norm": 0.24206286668777466,
      "learning_rate": 3.495833333333334e-05,
      "loss": 0.2577,
      "step": 166100
    },
    {
      "epoch": 1.6163462987906578,
      "grad_norm": 0.2402055412530899,
      "learning_rate": 3.491666666666667e-05,
      "loss": 0.2561,
      "step": 166200
    },
    {
      "epoch": 1.617318829656356,
      "grad_norm": 0.2495090663433075,
      "learning_rate": 3.4875e-05,
      "loss": 0.259,
      "step": 166300
    },
    {
      "epoch": 1.6182913605220546,
      "grad_norm": 0.2469640076160431,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.2589,
      "step": 166400
    },
    {
      "epoch": 1.619263891387753,
      "grad_norm": 0.27243638038635254,
      "learning_rate": 3.479166666666667e-05,
      "loss": 0.258,
      "step": 166500
    },
    {
      "epoch": 1.620236422253451,
      "grad_norm": 0.2468772530555725,
      "learning_rate": 3.475e-05,
      "loss": 0.258,
      "step": 166600
    },
    {
      "epoch": 1.6212089531191496,
      "grad_norm": 0.2506564259529114,
      "learning_rate": 3.4708333333333334e-05,
      "loss": 0.2594,
      "step": 166700
    },
    {
      "epoch": 1.622181483984848,
      "grad_norm": 0.2467881590127945,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.2592,
      "step": 166800
    },
    {
      "epoch": 1.6231540148505463,
      "grad_norm": 0.23986120522022247,
      "learning_rate": 3.4625e-05,
      "loss": 0.2591,
      "step": 166900
    },
    {
      "epoch": 1.6241265457162446,
      "grad_norm": 0.27190589904785156,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.2586,
      "step": 167000
    },
    {
      "epoch": 1.625099076581943,
      "grad_norm": 0.2610282003879547,
      "learning_rate": 3.454166666666667e-05,
      "loss": 0.2558,
      "step": 167100
    },
    {
      "epoch": 1.6260716074476413,
      "grad_norm": 0.2515981197357178,
      "learning_rate": 3.45e-05,
      "loss": 0.2593,
      "step": 167200
    },
    {
      "epoch": 1.6270441383133396,
      "grad_norm": 0.2714163661003113,
      "learning_rate": 3.4458333333333335e-05,
      "loss": 0.2594,
      "step": 167300
    },
    {
      "epoch": 1.628016669179038,
      "grad_norm": 0.2781410217285156,
      "learning_rate": 3.441666666666667e-05,
      "loss": 0.2593,
      "step": 167400
    },
    {
      "epoch": 1.6289892000447366,
      "grad_norm": 0.2419556826353073,
      "learning_rate": 3.4375e-05,
      "loss": 0.258,
      "step": 167500
    },
    {
      "epoch": 1.6299617309104346,
      "grad_norm": 0.250426322221756,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.2583,
      "step": 167600
    },
    {
      "epoch": 1.630934261776133,
      "grad_norm": 0.2434384822845459,
      "learning_rate": 3.429166666666667e-05,
      "loss": 0.2584,
      "step": 167700
    },
    {
      "epoch": 1.6319067926418316,
      "grad_norm": 0.233004629611969,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.2582,
      "step": 167800
    },
    {
      "epoch": 1.6328793235075298,
      "grad_norm": 0.26695066690444946,
      "learning_rate": 3.4208333333333336e-05,
      "loss": 0.2584,
      "step": 167900
    },
    {
      "epoch": 1.633851854373228,
      "grad_norm": 0.24340611696243286,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.2572,
      "step": 168000
    },
    {
      "epoch": 1.6348243852389266,
      "grad_norm": 0.24978169798851013,
      "learning_rate": 3.4125e-05,
      "loss": 0.2577,
      "step": 168100
    },
    {
      "epoch": 1.6357969161046249,
      "grad_norm": 0.27598118782043457,
      "learning_rate": 3.408333333333333e-05,
      "loss": 0.2591,
      "step": 168200
    },
    {
      "epoch": 1.6367694469703231,
      "grad_norm": 0.2726825177669525,
      "learning_rate": 3.404166666666666e-05,
      "loss": 0.26,
      "step": 168300
    },
    {
      "epoch": 1.6377419778360216,
      "grad_norm": 0.21848106384277344,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.2576,
      "step": 168400
    },
    {
      "epoch": 1.63871450870172,
      "grad_norm": 0.2401365339756012,
      "learning_rate": 3.3958333333333337e-05,
      "loss": 0.2591,
      "step": 168500
    },
    {
      "epoch": 1.6396870395674181,
      "grad_norm": 0.2445928305387497,
      "learning_rate": 3.391666666666667e-05,
      "loss": 0.2572,
      "step": 168600
    },
    {
      "epoch": 1.6406595704331166,
      "grad_norm": 0.24482163786888123,
      "learning_rate": 3.3875000000000003e-05,
      "loss": 0.2596,
      "step": 168700
    },
    {
      "epoch": 1.641632101298815,
      "grad_norm": 0.23232218623161316,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.2576,
      "step": 168800
    },
    {
      "epoch": 1.6426046321645134,
      "grad_norm": 0.22750148177146912,
      "learning_rate": 3.3791666666666664e-05,
      "loss": 0.2584,
      "step": 168900
    },
    {
      "epoch": 1.6435771630302116,
      "grad_norm": 0.27902963757514954,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.2588,
      "step": 169000
    },
    {
      "epoch": 1.64454969389591,
      "grad_norm": 0.2182673215866089,
      "learning_rate": 3.370833333333334e-05,
      "loss": 0.2581,
      "step": 169100
    },
    {
      "epoch": 1.6455222247616084,
      "grad_norm": 0.24837133288383484,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.2593,
      "step": 169200
    },
    {
      "epoch": 1.6464947556273066,
      "grad_norm": 0.23515893518924713,
      "learning_rate": 3.3625000000000004e-05,
      "loss": 0.2565,
      "step": 169300
    },
    {
      "epoch": 1.6474672864930051,
      "grad_norm": 0.2803076207637787,
      "learning_rate": 3.3583333333333334e-05,
      "loss": 0.2588,
      "step": 169400
    },
    {
      "epoch": 1.6484398173587034,
      "grad_norm": 0.2564719319343567,
      "learning_rate": 3.3541666666666664e-05,
      "loss": 0.2587,
      "step": 169500
    },
    {
      "epoch": 1.6494123482244016,
      "grad_norm": 0.2869248688220978,
      "learning_rate": 3.35e-05,
      "loss": 0.2587,
      "step": 169600
    },
    {
      "epoch": 1.6503848790901001,
      "grad_norm": 0.27637073397636414,
      "learning_rate": 3.345833333333334e-05,
      "loss": 0.2575,
      "step": 169700
    },
    {
      "epoch": 1.6513574099557986,
      "grad_norm": 0.2353079915046692,
      "learning_rate": 3.341666666666667e-05,
      "loss": 0.2585,
      "step": 169800
    },
    {
      "epoch": 1.6523299408214969,
      "grad_norm": 0.2587924301624298,
      "learning_rate": 3.3375e-05,
      "loss": 0.2569,
      "step": 169900
    },
    {
      "epoch": 1.6533024716871951,
      "grad_norm": 0.27139487862586975,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.2583,
      "step": 170000
    },
    {
      "epoch": 1.6533024716871951,
      "eval_loss": 0.2589271366596222,
      "eval_runtime": 3042.6043,
      "eval_samples_per_second": 750.919,
      "eval_steps_per_second": 7.509,
      "step": 170000
    },
    {
      "epoch": 1.6542750025528936,
      "grad_norm": 0.2510834336280823,
      "learning_rate": 3.329166666666667e-05,
      "loss": 0.2582,
      "step": 170100
    },
    {
      "epoch": 1.6552475334185919,
      "grad_norm": 0.237276092171669,
      "learning_rate": 3.325e-05,
      "loss": 0.2582,
      "step": 170200
    },
    {
      "epoch": 1.6562200642842901,
      "grad_norm": 0.2413438856601715,
      "learning_rate": 3.320833333333334e-05,
      "loss": 0.2594,
      "step": 170300
    },
    {
      "epoch": 1.6571925951499886,
      "grad_norm": 0.2594917118549347,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.2574,
      "step": 170400
    },
    {
      "epoch": 1.6581651260156869,
      "grad_norm": 0.2588779032230377,
      "learning_rate": 3.3125e-05,
      "loss": 0.2574,
      "step": 170500
    },
    {
      "epoch": 1.6591376568813851,
      "grad_norm": 0.2685862183570862,
      "learning_rate": 3.3083333333333336e-05,
      "loss": 0.2579,
      "step": 170600
    },
    {
      "epoch": 1.6601101877470836,
      "grad_norm": 0.2605993449687958,
      "learning_rate": 3.304166666666667e-05,
      "loss": 0.2577,
      "step": 170700
    },
    {
      "epoch": 1.6610827186127821,
      "grad_norm": 0.2655169665813446,
      "learning_rate": 3.3e-05,
      "loss": 0.2579,
      "step": 170800
    },
    {
      "epoch": 1.6620552494784804,
      "grad_norm": 0.2398274540901184,
      "learning_rate": 3.295833333333333e-05,
      "loss": 0.257,
      "step": 170900
    },
    {
      "epoch": 1.6630277803441786,
      "grad_norm": 0.2728515863418579,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.2577,
      "step": 171000
    },
    {
      "epoch": 1.6640003112098771,
      "grad_norm": 0.2508768141269684,
      "learning_rate": 3.2875e-05,
      "loss": 0.2573,
      "step": 171100
    },
    {
      "epoch": 1.6649728420755754,
      "grad_norm": 0.26232171058654785,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.2603,
      "step": 171200
    },
    {
      "epoch": 1.6659453729412737,
      "grad_norm": 0.2554837763309479,
      "learning_rate": 3.279166666666667e-05,
      "loss": 0.2579,
      "step": 171300
    },
    {
      "epoch": 1.6669179038069721,
      "grad_norm": 0.22550258040428162,
      "learning_rate": 3.275e-05,
      "loss": 0.2563,
      "step": 171400
    },
    {
      "epoch": 1.6678904346726704,
      "grad_norm": 0.22977323830127716,
      "learning_rate": 3.270833333333333e-05,
      "loss": 0.2576,
      "step": 171500
    },
    {
      "epoch": 1.6688629655383687,
      "grad_norm": 0.23356004059314728,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.2576,
      "step": 171600
    },
    {
      "epoch": 1.6698354964040671,
      "grad_norm": 0.26801449060440063,
      "learning_rate": 3.2625e-05,
      "loss": 0.2588,
      "step": 171700
    },
    {
      "epoch": 1.6708080272697656,
      "grad_norm": 0.24796639382839203,
      "learning_rate": 3.258333333333333e-05,
      "loss": 0.2601,
      "step": 171800
    },
    {
      "epoch": 1.6717805581354637,
      "grad_norm": 0.25361841917037964,
      "learning_rate": 3.254166666666667e-05,
      "loss": 0.2566,
      "step": 171900
    },
    {
      "epoch": 1.6727530890011622,
      "grad_norm": 0.24300189316272736,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.258,
      "step": 172000
    },
    {
      "epoch": 1.6737256198668606,
      "grad_norm": 0.23235616087913513,
      "learning_rate": 3.2458333333333334e-05,
      "loss": 0.2567,
      "step": 172100
    },
    {
      "epoch": 1.674698150732559,
      "grad_norm": 0.22705203294754028,
      "learning_rate": 3.2416666666666664e-05,
      "loss": 0.258,
      "step": 172200
    },
    {
      "epoch": 1.6756706815982572,
      "grad_norm": 0.22240623831748962,
      "learning_rate": 3.2375e-05,
      "loss": 0.2579,
      "step": 172300
    },
    {
      "epoch": 1.6766432124639556,
      "grad_norm": 0.252461314201355,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.2573,
      "step": 172400
    },
    {
      "epoch": 1.677615743329654,
      "grad_norm": 0.2549903690814972,
      "learning_rate": 3.229166666666667e-05,
      "loss": 0.258,
      "step": 172500
    },
    {
      "epoch": 1.6785882741953522,
      "grad_norm": 0.2578549087047577,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.2571,
      "step": 172600
    },
    {
      "epoch": 1.6795608050610507,
      "grad_norm": 0.2604607045650482,
      "learning_rate": 3.2208333333333335e-05,
      "loss": 0.2597,
      "step": 172700
    },
    {
      "epoch": 1.6805333359267491,
      "grad_norm": 0.2397976964712143,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.2584,
      "step": 172800
    },
    {
      "epoch": 1.6815058667924472,
      "grad_norm": 0.2566029727458954,
      "learning_rate": 3.2125e-05,
      "loss": 0.2577,
      "step": 172900
    },
    {
      "epoch": 1.6824783976581457,
      "grad_norm": 0.25285959243774414,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.2577,
      "step": 173000
    },
    {
      "epoch": 1.6834509285238441,
      "grad_norm": 0.26601818203926086,
      "learning_rate": 3.204166666666667e-05,
      "loss": 0.257,
      "step": 173100
    },
    {
      "epoch": 1.6844234593895424,
      "grad_norm": 0.23079438507556915,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.2578,
      "step": 173200
    },
    {
      "epoch": 1.6853959902552407,
      "grad_norm": 0.267267107963562,
      "learning_rate": 3.1958333333333335e-05,
      "loss": 0.2583,
      "step": 173300
    },
    {
      "epoch": 1.6863685211209392,
      "grad_norm": 0.24836458265781403,
      "learning_rate": 3.1916666666666665e-05,
      "loss": 0.2572,
      "step": 173400
    },
    {
      "epoch": 1.6873410519866374,
      "grad_norm": 0.2637424170970917,
      "learning_rate": 3.1875e-05,
      "loss": 0.2568,
      "step": 173500
    },
    {
      "epoch": 1.6883135828523357,
      "grad_norm": 0.23090624809265137,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.2599,
      "step": 173600
    },
    {
      "epoch": 1.6892861137180342,
      "grad_norm": 0.25575411319732666,
      "learning_rate": 3.179166666666667e-05,
      "loss": 0.2583,
      "step": 173700
    },
    {
      "epoch": 1.6902586445837324,
      "grad_norm": 0.2513868510723114,
      "learning_rate": 3.175e-05,
      "loss": 0.2579,
      "step": 173800
    },
    {
      "epoch": 1.6912311754494307,
      "grad_norm": 0.2405688464641571,
      "learning_rate": 3.1708333333333336e-05,
      "loss": 0.2581,
      "step": 173900
    },
    {
      "epoch": 1.6922037063151292,
      "grad_norm": 0.2612170875072479,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.2563,
      "step": 174000
    },
    {
      "epoch": 1.6931762371808277,
      "grad_norm": 0.25713932514190674,
      "learning_rate": 3.1624999999999996e-05,
      "loss": 0.2585,
      "step": 174100
    },
    {
      "epoch": 1.694148768046526,
      "grad_norm": 0.2697269022464752,
      "learning_rate": 3.158333333333334e-05,
      "loss": 0.257,
      "step": 174200
    },
    {
      "epoch": 1.6951212989122242,
      "grad_norm": 0.2567676603794098,
      "learning_rate": 3.154166666666667e-05,
      "loss": 0.2572,
      "step": 174300
    },
    {
      "epoch": 1.6960938297779227,
      "grad_norm": 0.26609477400779724,
      "learning_rate": 3.15e-05,
      "loss": 0.2579,
      "step": 174400
    },
    {
      "epoch": 1.697066360643621,
      "grad_norm": 0.26842421293258667,
      "learning_rate": 3.145833333333334e-05,
      "loss": 0.2591,
      "step": 174500
    },
    {
      "epoch": 1.6980388915093192,
      "grad_norm": 0.24353624880313873,
      "learning_rate": 3.141666666666667e-05,
      "loss": 0.2587,
      "step": 174600
    },
    {
      "epoch": 1.6990114223750177,
      "grad_norm": 0.2574882507324219,
      "learning_rate": 3.1375e-05,
      "loss": 0.2581,
      "step": 174700
    },
    {
      "epoch": 1.699983953240716,
      "grad_norm": 0.2545328736305237,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.2571,
      "step": 174800
    },
    {
      "epoch": 1.7009564841064142,
      "grad_norm": 0.2308449000120163,
      "learning_rate": 3.129166666666667e-05,
      "loss": 0.2578,
      "step": 174900
    },
    {
      "epoch": 1.7019290149721127,
      "grad_norm": 0.26092979311943054,
      "learning_rate": 3.125e-05,
      "loss": 0.2575,
      "step": 175000
    },
    {
      "epoch": 1.7029015458378112,
      "grad_norm": 0.27713507413864136,
      "learning_rate": 3.120833333333333e-05,
      "loss": 0.2568,
      "step": 175100
    },
    {
      "epoch": 1.7038740767035094,
      "grad_norm": 0.24617242813110352,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.2559,
      "step": 175200
    },
    {
      "epoch": 1.7048466075692077,
      "grad_norm": 0.26288262009620667,
      "learning_rate": 3.1125000000000004e-05,
      "loss": 0.2559,
      "step": 175300
    },
    {
      "epoch": 1.7058191384349062,
      "grad_norm": 0.2567991614341736,
      "learning_rate": 3.1083333333333334e-05,
      "loss": 0.256,
      "step": 175400
    },
    {
      "epoch": 1.7067916693006044,
      "grad_norm": 0.24407057464122772,
      "learning_rate": 3.104166666666667e-05,
      "loss": 0.2566,
      "step": 175500
    },
    {
      "epoch": 1.7077642001663027,
      "grad_norm": 0.2781731188297272,
      "learning_rate": 3.1e-05,
      "loss": 0.2576,
      "step": 175600
    },
    {
      "epoch": 1.7087367310320012,
      "grad_norm": 0.2692856192588806,
      "learning_rate": 3.095833333333333e-05,
      "loss": 0.2592,
      "step": 175700
    },
    {
      "epoch": 1.7097092618976995,
      "grad_norm": 0.22376537322998047,
      "learning_rate": 3.091666666666667e-05,
      "loss": 0.2584,
      "step": 175800
    },
    {
      "epoch": 1.7106817927633977,
      "grad_norm": 0.24746595323085785,
      "learning_rate": 3.0875000000000005e-05,
      "loss": 0.2587,
      "step": 175900
    },
    {
      "epoch": 1.7116543236290962,
      "grad_norm": 0.2713605761528015,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.2583,
      "step": 176000
    },
    {
      "epoch": 1.7126268544947947,
      "grad_norm": 0.2291872650384903,
      "learning_rate": 3.079166666666667e-05,
      "loss": 0.2555,
      "step": 176100
    },
    {
      "epoch": 1.7135993853604927,
      "grad_norm": 0.2904232442378998,
      "learning_rate": 3.075e-05,
      "loss": 0.2579,
      "step": 176200
    },
    {
      "epoch": 1.7145719162261912,
      "grad_norm": 0.25773680210113525,
      "learning_rate": 3.070833333333333e-05,
      "loss": 0.258,
      "step": 176300
    },
    {
      "epoch": 1.7155444470918897,
      "grad_norm": 0.28549712896347046,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.2562,
      "step": 176400
    },
    {
      "epoch": 1.716516977957588,
      "grad_norm": 0.2514145076274872,
      "learning_rate": 3.0625000000000006e-05,
      "loss": 0.2568,
      "step": 176500
    },
    {
      "epoch": 1.7174895088232862,
      "grad_norm": 0.2418389469385147,
      "learning_rate": 3.0583333333333336e-05,
      "loss": 0.2555,
      "step": 176600
    },
    {
      "epoch": 1.7184620396889847,
      "grad_norm": 0.24726684391498566,
      "learning_rate": 3.0541666666666666e-05,
      "loss": 0.2581,
      "step": 176700
    },
    {
      "epoch": 1.719434570554683,
      "grad_norm": 0.24973952770233154,
      "learning_rate": 3.05e-05,
      "loss": 0.2582,
      "step": 176800
    },
    {
      "epoch": 1.7204071014203812,
      "grad_norm": 0.2502679228782654,
      "learning_rate": 3.0458333333333333e-05,
      "loss": 0.2563,
      "step": 176900
    },
    {
      "epoch": 1.7213796322860797,
      "grad_norm": 0.2471766173839569,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.2572,
      "step": 177000
    },
    {
      "epoch": 1.7223521631517782,
      "grad_norm": 0.28305283188819885,
      "learning_rate": 3.0375000000000003e-05,
      "loss": 0.2565,
      "step": 177100
    },
    {
      "epoch": 1.7233246940174762,
      "grad_norm": 0.2435758262872696,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.2565,
      "step": 177200
    },
    {
      "epoch": 1.7242972248831747,
      "grad_norm": 0.2512734830379486,
      "learning_rate": 3.0291666666666667e-05,
      "loss": 0.2591,
      "step": 177300
    },
    {
      "epoch": 1.7252697557488732,
      "grad_norm": 0.2648010849952698,
      "learning_rate": 3.025e-05,
      "loss": 0.2563,
      "step": 177400
    },
    {
      "epoch": 1.7262422866145715,
      "grad_norm": 0.25005635619163513,
      "learning_rate": 3.0208333333333334e-05,
      "loss": 0.2582,
      "step": 177500
    },
    {
      "epoch": 1.7272148174802697,
      "grad_norm": 0.2719656527042389,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.2554,
      "step": 177600
    },
    {
      "epoch": 1.7281873483459682,
      "grad_norm": 0.23124517500400543,
      "learning_rate": 3.0125000000000004e-05,
      "loss": 0.256,
      "step": 177700
    },
    {
      "epoch": 1.7291598792116665,
      "grad_norm": 0.26456424593925476,
      "learning_rate": 3.0083333333333337e-05,
      "loss": 0.2562,
      "step": 177800
    },
    {
      "epoch": 1.7301324100773647,
      "grad_norm": 0.2494199573993683,
      "learning_rate": 3.0041666666666667e-05,
      "loss": 0.2578,
      "step": 177900
    },
    {
      "epoch": 1.7311049409430632,
      "grad_norm": 0.270216703414917,
      "learning_rate": 3e-05,
      "loss": 0.2584,
      "step": 178000
    },
    {
      "epoch": 1.7320774718087615,
      "grad_norm": 0.2676747143268585,
      "learning_rate": 2.9958333333333334e-05,
      "loss": 0.2573,
      "step": 178100
    },
    {
      "epoch": 1.7330500026744597,
      "grad_norm": 0.2777582108974457,
      "learning_rate": 2.991666666666667e-05,
      "loss": 0.2562,
      "step": 178200
    },
    {
      "epoch": 1.7340225335401582,
      "grad_norm": 0.2460622936487198,
      "learning_rate": 2.9875000000000004e-05,
      "loss": 0.2557,
      "step": 178300
    },
    {
      "epoch": 1.7349950644058567,
      "grad_norm": 0.2599807381629944,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.257,
      "step": 178400
    },
    {
      "epoch": 1.735967595271555,
      "grad_norm": 0.28101643919944763,
      "learning_rate": 2.9791666666666668e-05,
      "loss": 0.2579,
      "step": 178500
    },
    {
      "epoch": 1.7369401261372532,
      "grad_norm": 0.29834678769111633,
      "learning_rate": 2.975e-05,
      "loss": 0.2569,
      "step": 178600
    },
    {
      "epoch": 1.7379126570029517,
      "grad_norm": 0.2534823715686798,
      "learning_rate": 2.970833333333333e-05,
      "loss": 0.2559,
      "step": 178700
    },
    {
      "epoch": 1.73888518786865,
      "grad_norm": 0.2601371705532074,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.2573,
      "step": 178800
    },
    {
      "epoch": 1.7398577187343482,
      "grad_norm": 0.2606101334095001,
      "learning_rate": 2.9625000000000002e-05,
      "loss": 0.2556,
      "step": 178900
    },
    {
      "epoch": 1.7408302496000467,
      "grad_norm": 0.23536111414432526,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.2572,
      "step": 179000
    },
    {
      "epoch": 1.741802780465745,
      "grad_norm": 0.23584412038326263,
      "learning_rate": 2.954166666666667e-05,
      "loss": 0.2573,
      "step": 179100
    },
    {
      "epoch": 1.7427753113314433,
      "grad_norm": 0.274497389793396,
      "learning_rate": 2.95e-05,
      "loss": 0.257,
      "step": 179200
    },
    {
      "epoch": 1.7437478421971417,
      "grad_norm": 0.2574503719806671,
      "learning_rate": 2.9458333333333332e-05,
      "loss": 0.2567,
      "step": 179300
    },
    {
      "epoch": 1.7447203730628402,
      "grad_norm": 0.26360201835632324,
      "learning_rate": 2.941666666666667e-05,
      "loss": 0.2573,
      "step": 179400
    },
    {
      "epoch": 1.7456929039285385,
      "grad_norm": 0.23800651729106903,
      "learning_rate": 2.9375000000000003e-05,
      "loss": 0.257,
      "step": 179500
    },
    {
      "epoch": 1.7466654347942367,
      "grad_norm": 0.2755768895149231,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.2573,
      "step": 179600
    },
    {
      "epoch": 1.7476379656599352,
      "grad_norm": 0.25524765253067017,
      "learning_rate": 2.9291666666666666e-05,
      "loss": 0.2589,
      "step": 179700
    },
    {
      "epoch": 1.7486104965256335,
      "grad_norm": 0.2927190363407135,
      "learning_rate": 2.925e-05,
      "loss": 0.2556,
      "step": 179800
    },
    {
      "epoch": 1.7495830273913318,
      "grad_norm": 0.25400614738464355,
      "learning_rate": 2.9208333333333333e-05,
      "loss": 0.2565,
      "step": 179900
    },
    {
      "epoch": 1.7505555582570302,
      "grad_norm": 0.2714146375656128,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.2578,
      "step": 180000
    },
    {
      "epoch": 1.7505555582570302,
      "eval_loss": 0.2575218379497528,
      "eval_runtime": 2965.4378,
      "eval_samples_per_second": 770.46,
      "eval_steps_per_second": 7.705,
      "step": 180000
    },
    {
      "epoch": 1.751532951777057,
      "grad_norm": 0.2778613567352295,
      "learning_rate": 2.9125000000000003e-05,
      "loss": 0.2568,
      "step": 180100
    },
    {
      "epoch": 1.7525054826427553,
      "grad_norm": 0.26013386249542236,
      "learning_rate": 2.9083333333333333e-05,
      "loss": 0.2562,
      "step": 180200
    },
    {
      "epoch": 1.7534780135084538,
      "grad_norm": 0.2495071440935135,
      "learning_rate": 2.9041666666666667e-05,
      "loss": 0.2576,
      "step": 180300
    },
    {
      "epoch": 1.754450544374152,
      "grad_norm": 0.2528427839279175,
      "learning_rate": 2.9e-05,
      "loss": 0.2572,
      "step": 180400
    },
    {
      "epoch": 1.7554230752398503,
      "grad_norm": 0.23451752960681915,
      "learning_rate": 2.8958333333333337e-05,
      "loss": 0.2583,
      "step": 180500
    },
    {
      "epoch": 1.7563956061055488,
      "grad_norm": 0.2767198383808136,
      "learning_rate": 2.891666666666667e-05,
      "loss": 0.2574,
      "step": 180600
    },
    {
      "epoch": 1.7573681369712473,
      "grad_norm": 0.22493354976177216,
      "learning_rate": 2.8875e-05,
      "loss": 0.2561,
      "step": 180700
    },
    {
      "epoch": 1.7583406678369453,
      "grad_norm": 0.25641947984695435,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.2557,
      "step": 180800
    },
    {
      "epoch": 1.7593131987026438,
      "grad_norm": 0.25835710763931274,
      "learning_rate": 2.8791666666666667e-05,
      "loss": 0.2566,
      "step": 180900
    },
    {
      "epoch": 1.7602857295683423,
      "grad_norm": 0.26505059003829956,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.2571,
      "step": 181000
    },
    {
      "epoch": 1.7612582604340405,
      "grad_norm": 0.28651663661003113,
      "learning_rate": 2.8708333333333338e-05,
      "loss": 0.2574,
      "step": 181100
    },
    {
      "epoch": 1.7622307912997388,
      "grad_norm": 0.25488919019699097,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.2571,
      "step": 181200
    },
    {
      "epoch": 1.7632033221654373,
      "grad_norm": 0.26782482862472534,
      "learning_rate": 2.8625e-05,
      "loss": 0.2572,
      "step": 181300
    },
    {
      "epoch": 1.7641758530311356,
      "grad_norm": 0.25720539689064026,
      "learning_rate": 2.8583333333333335e-05,
      "loss": 0.2552,
      "step": 181400
    },
    {
      "epoch": 1.7651483838968338,
      "grad_norm": 0.27012744545936584,
      "learning_rate": 2.8541666666666668e-05,
      "loss": 0.2565,
      "step": 181500
    },
    {
      "epoch": 1.7661209147625323,
      "grad_norm": 0.2754051089286804,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.2575,
      "step": 181600
    },
    {
      "epoch": 1.7670934456282308,
      "grad_norm": 0.247574582695961,
      "learning_rate": 2.845833333333334e-05,
      "loss": 0.2573,
      "step": 181700
    },
    {
      "epoch": 1.7680659764939288,
      "grad_norm": 0.26030460000038147,
      "learning_rate": 2.841666666666667e-05,
      "loss": 0.2559,
      "step": 181800
    },
    {
      "epoch": 1.7690385073596273,
      "grad_norm": 0.2615581154823303,
      "learning_rate": 2.8375000000000002e-05,
      "loss": 0.2575,
      "step": 181900
    },
    {
      "epoch": 1.7700110382253258,
      "grad_norm": 0.2712562084197998,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.256,
      "step": 182000
    },
    {
      "epoch": 1.770983569091024,
      "grad_norm": 0.2394152134656906,
      "learning_rate": 2.8291666666666665e-05,
      "loss": 0.256,
      "step": 182100
    },
    {
      "epoch": 1.7719560999567223,
      "grad_norm": 0.2593921720981598,
      "learning_rate": 2.825e-05,
      "loss": 0.2564,
      "step": 182200
    },
    {
      "epoch": 1.7729286308224208,
      "grad_norm": 0.26164907217025757,
      "learning_rate": 2.8208333333333336e-05,
      "loss": 0.2564,
      "step": 182300
    },
    {
      "epoch": 1.773901161688119,
      "grad_norm": 0.2598407566547394,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.2567,
      "step": 182400
    },
    {
      "epoch": 1.7748736925538173,
      "grad_norm": 0.2678469121456146,
      "learning_rate": 2.8125000000000003e-05,
      "loss": 0.2564,
      "step": 182500
    },
    {
      "epoch": 1.7758462234195158,
      "grad_norm": 0.28842541575431824,
      "learning_rate": 2.8083333333333333e-05,
      "loss": 0.256,
      "step": 182600
    },
    {
      "epoch": 1.776818754285214,
      "grad_norm": 0.25273966789245605,
      "learning_rate": 2.8041666666666666e-05,
      "loss": 0.2575,
      "step": 182700
    },
    {
      "epoch": 1.7777912851509123,
      "grad_norm": 0.26083749532699585,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.2571,
      "step": 182800
    },
    {
      "epoch": 1.7787638160166108,
      "grad_norm": 0.26632755994796753,
      "learning_rate": 2.7958333333333336e-05,
      "loss": 0.256,
      "step": 182900
    },
    {
      "epoch": 1.7797363468823093,
      "grad_norm": 0.29051950573921204,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.254,
      "step": 183000
    },
    {
      "epoch": 1.7807088777480076,
      "grad_norm": 0.287948876619339,
      "learning_rate": 2.7875e-05,
      "loss": 0.2556,
      "step": 183100
    },
    {
      "epoch": 1.7816814086137058,
      "grad_norm": 0.2231159806251526,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.2589,
      "step": 183200
    },
    {
      "epoch": 1.7826539394794043,
      "grad_norm": 0.2593683898448944,
      "learning_rate": 2.7791666666666667e-05,
      "loss": 0.2581,
      "step": 183300
    },
    {
      "epoch": 1.7836264703451026,
      "grad_norm": 0.2747757136821747,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.2564,
      "step": 183400
    },
    {
      "epoch": 1.7845990012108008,
      "grad_norm": 0.24962107837200165,
      "learning_rate": 2.7708333333333337e-05,
      "loss": 0.2565,
      "step": 183500
    },
    {
      "epoch": 1.7855715320764993,
      "grad_norm": 0.254769504070282,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.2576,
      "step": 183600
    },
    {
      "epoch": 1.7865440629421976,
      "grad_norm": 0.2724938988685608,
      "learning_rate": 2.7625e-05,
      "loss": 0.2558,
      "step": 183700
    },
    {
      "epoch": 1.7875165938078958,
      "grad_norm": 0.2572990953922272,
      "learning_rate": 2.7583333333333334e-05,
      "loss": 0.2561,
      "step": 183800
    },
    {
      "epoch": 1.7884891246735943,
      "grad_norm": 0.25451552867889404,
      "learning_rate": 2.7541666666666664e-05,
      "loss": 0.2574,
      "step": 183900
    },
    {
      "epoch": 1.7894616555392928,
      "grad_norm": 0.2477487027645111,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.2568,
      "step": 184000
    },
    {
      "epoch": 1.790434186404991,
      "grad_norm": 0.25404632091522217,
      "learning_rate": 2.7458333333333334e-05,
      "loss": 0.2566,
      "step": 184100
    },
    {
      "epoch": 1.7914067172706893,
      "grad_norm": 0.25341424345970154,
      "learning_rate": 2.7416666666666668e-05,
      "loss": 0.2542,
      "step": 184200
    },
    {
      "epoch": 1.7923792481363878,
      "grad_norm": 0.27103084325790405,
      "learning_rate": 2.7375e-05,
      "loss": 0.2559,
      "step": 184300
    },
    {
      "epoch": 1.793351779002086,
      "grad_norm": 0.2555638253688812,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.2545,
      "step": 184400
    },
    {
      "epoch": 1.7943243098677844,
      "grad_norm": 0.28522664308547974,
      "learning_rate": 2.7291666666666665e-05,
      "loss": 0.2565,
      "step": 184500
    },
    {
      "epoch": 1.7952968407334828,
      "grad_norm": 0.26585713028907776,
      "learning_rate": 2.725e-05,
      "loss": 0.256,
      "step": 184600
    },
    {
      "epoch": 1.796269371599181,
      "grad_norm": 0.2566606402397156,
      "learning_rate": 2.7208333333333335e-05,
      "loss": 0.256,
      "step": 184700
    },
    {
      "epoch": 1.7972419024648794,
      "grad_norm": 0.245478093624115,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.2564,
      "step": 184800
    },
    {
      "epoch": 1.7982144333305778,
      "grad_norm": 0.2661529779434204,
      "learning_rate": 2.7125000000000002e-05,
      "loss": 0.2576,
      "step": 184900
    },
    {
      "epoch": 1.7991869641962763,
      "grad_norm": 0.28990909457206726,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.2573,
      "step": 185000
    },
    {
      "epoch": 1.8001594950619744,
      "grad_norm": 0.2582120895385742,
      "learning_rate": 2.7041666666666672e-05,
      "loss": 0.2573,
      "step": 185100
    },
    {
      "epoch": 1.8011320259276729,
      "grad_norm": 0.28104934096336365,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.2554,
      "step": 185200
    },
    {
      "epoch": 1.8021045567933713,
      "grad_norm": 0.2791210412979126,
      "learning_rate": 2.6958333333333336e-05,
      "loss": 0.2563,
      "step": 185300
    },
    {
      "epoch": 1.8030770876590696,
      "grad_norm": 0.26537320017814636,
      "learning_rate": 2.691666666666667e-05,
      "loss": 0.256,
      "step": 185400
    },
    {
      "epoch": 1.8040496185247679,
      "grad_norm": 0.2863003611564636,
      "learning_rate": 2.6875e-05,
      "loss": 0.2568,
      "step": 185500
    },
    {
      "epoch": 1.8050221493904663,
      "grad_norm": 0.2637844979763031,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.2557,
      "step": 185600
    },
    {
      "epoch": 1.8059946802561646,
      "grad_norm": 0.26743125915527344,
      "learning_rate": 2.679166666666667e-05,
      "loss": 0.2557,
      "step": 185700
    },
    {
      "epoch": 1.8069672111218629,
      "grad_norm": 0.30988457798957825,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.2543,
      "step": 185800
    },
    {
      "epoch": 1.8079397419875614,
      "grad_norm": 0.24994131922721863,
      "learning_rate": 2.6708333333333337e-05,
      "loss": 0.2575,
      "step": 185900
    },
    {
      "epoch": 1.8089122728532598,
      "grad_norm": 0.23733122646808624,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.256,
      "step": 186000
    },
    {
      "epoch": 1.8098848037189579,
      "grad_norm": 0.2681843340396881,
      "learning_rate": 2.6625e-05,
      "loss": 0.2554,
      "step": 186100
    },
    {
      "epoch": 1.8108573345846564,
      "grad_norm": 0.25651878118515015,
      "learning_rate": 2.6583333333333333e-05,
      "loss": 0.2553,
      "step": 186200
    },
    {
      "epoch": 1.8118298654503548,
      "grad_norm": 0.2670724093914032,
      "learning_rate": 2.654166666666667e-05,
      "loss": 0.2546,
      "step": 186300
    },
    {
      "epoch": 1.812802396316053,
      "grad_norm": 0.26319095492362976,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.2562,
      "step": 186400
    },
    {
      "epoch": 1.8137749271817514,
      "grad_norm": 0.2556197941303253,
      "learning_rate": 2.6458333333333334e-05,
      "loss": 0.257,
      "step": 186500
    },
    {
      "epoch": 1.8147474580474499,
      "grad_norm": 0.2605140805244446,
      "learning_rate": 2.6416666666666667e-05,
      "loss": 0.2568,
      "step": 186600
    },
    {
      "epoch": 1.8157199889131481,
      "grad_norm": 0.2668505012989044,
      "learning_rate": 2.6375e-05,
      "loss": 0.2546,
      "step": 186700
    },
    {
      "epoch": 1.8166925197788464,
      "grad_norm": 0.25666725635528564,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.2571,
      "step": 186800
    },
    {
      "epoch": 1.8176650506445449,
      "grad_norm": 0.2723137438297272,
      "learning_rate": 2.629166666666667e-05,
      "loss": 0.2555,
      "step": 186900
    },
    {
      "epoch": 1.8186375815102431,
      "grad_norm": 0.26759451627731323,
      "learning_rate": 2.625e-05,
      "loss": 0.2548,
      "step": 187000
    },
    {
      "epoch": 1.8196101123759414,
      "grad_norm": 0.2953483462333679,
      "learning_rate": 2.6208333333333335e-05,
      "loss": 0.2562,
      "step": 187100
    },
    {
      "epoch": 1.8205826432416399,
      "grad_norm": 0.23453256487846375,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.2566,
      "step": 187200
    },
    {
      "epoch": 1.8215551741073384,
      "grad_norm": 0.2385493814945221,
      "learning_rate": 2.6124999999999998e-05,
      "loss": 0.2555,
      "step": 187300
    },
    {
      "epoch": 1.8225277049730366,
      "grad_norm": 0.25589120388031006,
      "learning_rate": 2.608333333333333e-05,
      "loss": 0.2548,
      "step": 187400
    },
    {
      "epoch": 1.8235002358387349,
      "grad_norm": 0.2461543083190918,
      "learning_rate": 2.604166666666667e-05,
      "loss": 0.255,
      "step": 187500
    },
    {
      "epoch": 1.8244727667044334,
      "grad_norm": 0.24769999086856842,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.2561,
      "step": 187600
    },
    {
      "epoch": 1.8254452975701316,
      "grad_norm": 0.24695666134357452,
      "learning_rate": 2.5958333333333335e-05,
      "loss": 0.2571,
      "step": 187700
    },
    {
      "epoch": 1.82641782843583,
      "grad_norm": 0.2544550895690918,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.2562,
      "step": 187800
    },
    {
      "epoch": 1.8273903593015284,
      "grad_norm": 0.282351016998291,
      "learning_rate": 2.5875e-05,
      "loss": 0.256,
      "step": 187900
    },
    {
      "epoch": 1.8283628901672266,
      "grad_norm": 0.2769288718700409,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.2556,
      "step": 188000
    },
    {
      "epoch": 1.829335421032925,
      "grad_norm": 0.25955742597579956,
      "learning_rate": 2.579166666666667e-05,
      "loss": 0.2552,
      "step": 188100
    },
    {
      "epoch": 1.8303079518986234,
      "grad_norm": 0.2819537818431854,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.2564,
      "step": 188200
    },
    {
      "epoch": 1.8312804827643219,
      "grad_norm": 0.2805129885673523,
      "learning_rate": 2.5708333333333336e-05,
      "loss": 0.257,
      "step": 188300
    },
    {
      "epoch": 1.8322530136300201,
      "grad_norm": 0.28925344347953796,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.2558,
      "step": 188400
    },
    {
      "epoch": 1.8332255444957184,
      "grad_norm": 0.2773892283439636,
      "learning_rate": 2.5625e-05,
      "loss": 0.2557,
      "step": 188500
    },
    {
      "epoch": 1.8341980753614169,
      "grad_norm": 0.2580546438694,
      "learning_rate": 2.5583333333333336e-05,
      "loss": 0.2571,
      "step": 188600
    },
    {
      "epoch": 1.8351706062271151,
      "grad_norm": 0.26120197772979736,
      "learning_rate": 2.554166666666667e-05,
      "loss": 0.2564,
      "step": 188700
    },
    {
      "epoch": 1.8361431370928134,
      "grad_norm": 0.26020151376724243,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.2557,
      "step": 188800
    },
    {
      "epoch": 1.8371156679585119,
      "grad_norm": 0.2723587155342102,
      "learning_rate": 2.5458333333333333e-05,
      "loss": 0.254,
      "step": 188900
    },
    {
      "epoch": 1.8380881988242102,
      "grad_norm": 0.2669619917869568,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.2547,
      "step": 189000
    },
    {
      "epoch": 1.8390607296899084,
      "grad_norm": 0.28495001792907715,
      "learning_rate": 2.5375e-05,
      "loss": 0.2556,
      "step": 189100
    },
    {
      "epoch": 1.840033260555607,
      "grad_norm": 0.2526612877845764,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.2555,
      "step": 189200
    },
    {
      "epoch": 1.8410057914213054,
      "grad_norm": 0.2596222758293152,
      "learning_rate": 2.529166666666667e-05,
      "loss": 0.2547,
      "step": 189300
    },
    {
      "epoch": 1.8419783222870034,
      "grad_norm": 0.2686727046966553,
      "learning_rate": 2.525e-05,
      "loss": 0.2558,
      "step": 189400
    },
    {
      "epoch": 1.842950853152702,
      "grad_norm": 0.26009148359298706,
      "learning_rate": 2.5208333333333334e-05,
      "loss": 0.2551,
      "step": 189500
    },
    {
      "epoch": 1.8439233840184004,
      "grad_norm": 0.2747553884983063,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.2566,
      "step": 189600
    },
    {
      "epoch": 1.8448959148840987,
      "grad_norm": 0.2734721601009369,
      "learning_rate": 2.5124999999999997e-05,
      "loss": 0.254,
      "step": 189700
    },
    {
      "epoch": 1.845868445749797,
      "grad_norm": 0.2442283183336258,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.255,
      "step": 189800
    },
    {
      "epoch": 1.8468409766154954,
      "grad_norm": 0.2622552812099457,
      "learning_rate": 2.5041666666666668e-05,
      "loss": 0.2541,
      "step": 189900
    },
    {
      "epoch": 1.8478135074811937,
      "grad_norm": 0.25620657205581665,
      "learning_rate": 2.5e-05,
      "loss": 0.2554,
      "step": 190000
    },
    {
      "epoch": 1.8478135074811937,
      "eval_loss": 0.2561706006526947,
      "eval_runtime": 2967.9209,
      "eval_samples_per_second": 769.815,
      "eval_steps_per_second": 7.698,
      "step": 190000
    },
    {
      "epoch": 1.848786038346892,
      "grad_norm": 0.2696678638458252,
      "learning_rate": 2.4958333333333335e-05,
      "loss": 0.2579,
      "step": 190100
    },
    {
      "epoch": 1.8497585692125904,
      "grad_norm": 0.26091042160987854,
      "learning_rate": 2.4916666666666668e-05,
      "loss": 0.2558,
      "step": 190200
    },
    {
      "epoch": 1.850731100078289,
      "grad_norm": 0.2345784604549408,
      "learning_rate": 2.4875e-05,
      "loss": 0.2566,
      "step": 190300
    },
    {
      "epoch": 1.851703630943987,
      "grad_norm": 0.27357155084609985,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.2553,
      "step": 190400
    },
    {
      "epoch": 1.8526761618096854,
      "grad_norm": 0.2645758390426636,
      "learning_rate": 2.479166666666667e-05,
      "loss": 0.2559,
      "step": 190500
    },
    {
      "epoch": 1.853648692675384,
      "grad_norm": 0.26334795355796814,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 0.253,
      "step": 190600
    },
    {
      "epoch": 1.8546212235410822,
      "grad_norm": 0.24977289140224457,
      "learning_rate": 2.4708333333333332e-05,
      "loss": 0.2559,
      "step": 190700
    },
    {
      "epoch": 1.8555937544067804,
      "grad_norm": 0.27679362893104553,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.2545,
      "step": 190800
    },
    {
      "epoch": 1.856566285272479,
      "grad_norm": 0.28392720222473145,
      "learning_rate": 2.4625000000000002e-05,
      "loss": 0.2551,
      "step": 190900
    },
    {
      "epoch": 1.8575388161381772,
      "grad_norm": 0.24575166404247284,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.2549,
      "step": 191000
    },
    {
      "epoch": 1.8585113470038754,
      "grad_norm": 0.24188750982284546,
      "learning_rate": 2.454166666666667e-05,
      "loss": 0.255,
      "step": 191100
    },
    {
      "epoch": 1.859483877869574,
      "grad_norm": 0.24527132511138916,
      "learning_rate": 2.45e-05,
      "loss": 0.2559,
      "step": 191200
    },
    {
      "epoch": 1.8604564087352722,
      "grad_norm": 0.2852931618690491,
      "learning_rate": 2.4458333333333336e-05,
      "loss": 0.2557,
      "step": 191300
    },
    {
      "epoch": 1.8614289396009704,
      "grad_norm": 0.2836942970752716,
      "learning_rate": 2.441666666666667e-05,
      "loss": 0.2542,
      "step": 191400
    },
    {
      "epoch": 1.862401470466669,
      "grad_norm": 0.2704940438270569,
      "learning_rate": 2.4375e-05,
      "loss": 0.2546,
      "step": 191500
    },
    {
      "epoch": 1.8633740013323674,
      "grad_norm": 0.28126752376556396,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.2576,
      "step": 191600
    },
    {
      "epoch": 1.8643465321980657,
      "grad_norm": 0.2708323895931244,
      "learning_rate": 2.4291666666666666e-05,
      "loss": 0.2549,
      "step": 191700
    },
    {
      "epoch": 1.865319063063764,
      "grad_norm": 0.2899998426437378,
      "learning_rate": 2.425e-05,
      "loss": 0.255,
      "step": 191800
    },
    {
      "epoch": 1.8662915939294624,
      "grad_norm": 0.27285274863243103,
      "learning_rate": 2.4208333333333337e-05,
      "loss": 0.255,
      "step": 191900
    },
    {
      "epoch": 1.8672641247951607,
      "grad_norm": 0.26507624983787537,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.2547,
      "step": 192000
    },
    {
      "epoch": 1.868236655660859,
      "grad_norm": 0.2592616081237793,
      "learning_rate": 2.4125e-05,
      "loss": 0.2552,
      "step": 192100
    },
    {
      "epoch": 1.8692091865265574,
      "grad_norm": 0.25993964076042175,
      "learning_rate": 2.4083333333333337e-05,
      "loss": 0.2555,
      "step": 192200
    },
    {
      "epoch": 1.8701817173922557,
      "grad_norm": 0.2976095676422119,
      "learning_rate": 2.4041666666666667e-05,
      "loss": 0.2545,
      "step": 192300
    },
    {
      "epoch": 1.871154248257954,
      "grad_norm": 0.2750246822834015,
      "learning_rate": 2.4e-05,
      "loss": 0.2549,
      "step": 192400
    },
    {
      "epoch": 1.8721267791236524,
      "grad_norm": 0.28511717915534973,
      "learning_rate": 2.3958333333333334e-05,
      "loss": 0.2558,
      "step": 192500
    },
    {
      "epoch": 1.873099309989351,
      "grad_norm": 0.27566665410995483,
      "learning_rate": 2.3916666666666668e-05,
      "loss": 0.2566,
      "step": 192600
    },
    {
      "epoch": 1.8740718408550492,
      "grad_norm": 0.2564448118209839,
      "learning_rate": 2.3875e-05,
      "loss": 0.2543,
      "step": 192700
    },
    {
      "epoch": 1.8750443717207474,
      "grad_norm": 0.27197226881980896,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.2556,
      "step": 192800
    },
    {
      "epoch": 1.876016902586446,
      "grad_norm": 0.26383790373802185,
      "learning_rate": 2.3791666666666668e-05,
      "loss": 0.2545,
      "step": 192900
    },
    {
      "epoch": 1.8769894334521442,
      "grad_norm": 0.27389010787010193,
      "learning_rate": 2.375e-05,
      "loss": 0.2557,
      "step": 193000
    },
    {
      "epoch": 1.8779619643178425,
      "grad_norm": 0.2927761673927307,
      "learning_rate": 2.3708333333333335e-05,
      "loss": 0.2547,
      "step": 193100
    },
    {
      "epoch": 1.878934495183541,
      "grad_norm": 0.3164970576763153,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.2544,
      "step": 193200
    },
    {
      "epoch": 1.8799070260492392,
      "grad_norm": 0.28882113099098206,
      "learning_rate": 2.3624999999999998e-05,
      "loss": 0.2533,
      "step": 193300
    },
    {
      "epoch": 1.8808795569149375,
      "grad_norm": 0.30941158533096313,
      "learning_rate": 2.3583333333333335e-05,
      "loss": 0.253,
      "step": 193400
    },
    {
      "epoch": 1.881852087780636,
      "grad_norm": 0.29668647050857544,
      "learning_rate": 2.354166666666667e-05,
      "loss": 0.2554,
      "step": 193500
    },
    {
      "epoch": 1.8828246186463344,
      "grad_norm": 0.28361237049102783,
      "learning_rate": 2.35e-05,
      "loss": 0.255,
      "step": 193600
    },
    {
      "epoch": 1.8837971495120325,
      "grad_norm": 0.2861749827861786,
      "learning_rate": 2.3458333333333335e-05,
      "loss": 0.2538,
      "step": 193700
    },
    {
      "epoch": 1.884769680377731,
      "grad_norm": 0.259604275226593,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.2546,
      "step": 193800
    },
    {
      "epoch": 1.8857422112434294,
      "grad_norm": 0.26736992597579956,
      "learning_rate": 2.3375000000000002e-05,
      "loss": 0.2561,
      "step": 193900
    },
    {
      "epoch": 1.8867147421091277,
      "grad_norm": 0.27444830536842346,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.2538,
      "step": 194000
    },
    {
      "epoch": 1.887687272974826,
      "grad_norm": 0.26842835545539856,
      "learning_rate": 2.3291666666666666e-05,
      "loss": 0.2567,
      "step": 194100
    },
    {
      "epoch": 1.8886598038405245,
      "grad_norm": 0.26450321078300476,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 0.2551,
      "step": 194200
    },
    {
      "epoch": 1.8896323347062227,
      "grad_norm": 0.2737228274345398,
      "learning_rate": 2.3208333333333336e-05,
      "loss": 0.2562,
      "step": 194300
    },
    {
      "epoch": 1.890604865571921,
      "grad_norm": 0.2692013084888458,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.2565,
      "step": 194400
    },
    {
      "epoch": 1.8915773964376195,
      "grad_norm": 0.24079804122447968,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 0.2534,
      "step": 194500
    },
    {
      "epoch": 1.892549927303318,
      "grad_norm": 0.28076910972595215,
      "learning_rate": 2.3083333333333333e-05,
      "loss": 0.2552,
      "step": 194600
    },
    {
      "epoch": 1.893522458169016,
      "grad_norm": 0.2872772812843323,
      "learning_rate": 2.3041666666666667e-05,
      "loss": 0.254,
      "step": 194700
    },
    {
      "epoch": 1.8944949890347145,
      "grad_norm": 0.2534789741039276,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.2541,
      "step": 194800
    },
    {
      "epoch": 1.895467519900413,
      "grad_norm": 0.27233660221099854,
      "learning_rate": 2.2958333333333333e-05,
      "loss": 0.2549,
      "step": 194900
    },
    {
      "epoch": 1.8964400507661112,
      "grad_norm": 0.2712058424949646,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.2534,
      "step": 195000
    },
    {
      "epoch": 1.8974125816318095,
      "grad_norm": 0.2993268072605133,
      "learning_rate": 2.2875e-05,
      "loss": 0.2533,
      "step": 195100
    },
    {
      "epoch": 1.898385112497508,
      "grad_norm": 0.2697032690048218,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.2527,
      "step": 195200
    },
    {
      "epoch": 1.8993576433632062,
      "grad_norm": 0.27405327558517456,
      "learning_rate": 2.2791666666666667e-05,
      "loss": 0.2569,
      "step": 195300
    },
    {
      "epoch": 1.9003301742289045,
      "grad_norm": 0.28061556816101074,
      "learning_rate": 2.275e-05,
      "loss": 0.2534,
      "step": 195400
    },
    {
      "epoch": 1.901302705094603,
      "grad_norm": 0.26600632071495056,
      "learning_rate": 2.2708333333333334e-05,
      "loss": 0.2544,
      "step": 195500
    },
    {
      "epoch": 1.9022752359603012,
      "grad_norm": 0.27502885460853577,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.2522,
      "step": 195600
    },
    {
      "epoch": 1.9032477668259995,
      "grad_norm": 0.2783049941062927,
      "learning_rate": 2.2625e-05,
      "loss": 0.2565,
      "step": 195700
    },
    {
      "epoch": 1.904220297691698,
      "grad_norm": 0.2907143235206604,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.2543,
      "step": 195800
    },
    {
      "epoch": 1.9051928285573965,
      "grad_norm": 0.2897476851940155,
      "learning_rate": 2.2541666666666668e-05,
      "loss": 0.2539,
      "step": 195900
    },
    {
      "epoch": 1.9061653594230947,
      "grad_norm": 0.28947463631629944,
      "learning_rate": 2.25e-05,
      "loss": 0.2568,
      "step": 196000
    },
    {
      "epoch": 1.907137890288793,
      "grad_norm": 0.26398488879203796,
      "learning_rate": 2.2458333333333335e-05,
      "loss": 0.2564,
      "step": 196100
    },
    {
      "epoch": 1.9081104211544915,
      "grad_norm": 0.2667842209339142,
      "learning_rate": 2.2416666666666665e-05,
      "loss": 0.2546,
      "step": 196200
    },
    {
      "epoch": 1.9090829520201897,
      "grad_norm": 0.2767062783241272,
      "learning_rate": 2.2375000000000002e-05,
      "loss": 0.2553,
      "step": 196300
    },
    {
      "epoch": 1.910055482885888,
      "grad_norm": 0.2506953775882721,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.2536,
      "step": 196400
    },
    {
      "epoch": 1.9110280137515865,
      "grad_norm": 0.25816747546195984,
      "learning_rate": 2.229166666666667e-05,
      "loss": 0.2529,
      "step": 196500
    },
    {
      "epoch": 1.9120005446172847,
      "grad_norm": 0.2676454484462738,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.2554,
      "step": 196600
    },
    {
      "epoch": 1.912973075482983,
      "grad_norm": 0.2590232491493225,
      "learning_rate": 2.2208333333333332e-05,
      "loss": 0.2548,
      "step": 196700
    },
    {
      "epoch": 1.9139456063486815,
      "grad_norm": 0.3019530475139618,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.254,
      "step": 196800
    },
    {
      "epoch": 1.91491813721438,
      "grad_norm": 0.28231045603752136,
      "learning_rate": 2.2125000000000002e-05,
      "loss": 0.253,
      "step": 196900
    },
    {
      "epoch": 1.9158906680800782,
      "grad_norm": 0.2818187475204468,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.2552,
      "step": 197000
    },
    {
      "epoch": 1.9168631989457765,
      "grad_norm": 0.31594210863113403,
      "learning_rate": 2.204166666666667e-05,
      "loss": 0.2545,
      "step": 197100
    },
    {
      "epoch": 1.917835729811475,
      "grad_norm": 0.2862415313720703,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.2529,
      "step": 197200
    },
    {
      "epoch": 1.9188082606771732,
      "grad_norm": 0.26047447323799133,
      "learning_rate": 2.1958333333333333e-05,
      "loss": 0.2542,
      "step": 197300
    },
    {
      "epoch": 1.9197807915428715,
      "grad_norm": 0.2728036344051361,
      "learning_rate": 2.191666666666667e-05,
      "loss": 0.254,
      "step": 197400
    },
    {
      "epoch": 1.92075332240857,
      "grad_norm": 0.26290163397789,
      "learning_rate": 2.1875e-05,
      "loss": 0.2542,
      "step": 197500
    },
    {
      "epoch": 1.9217258532742683,
      "grad_norm": 0.2947406470775604,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.2552,
      "step": 197600
    },
    {
      "epoch": 1.9226983841399665,
      "grad_norm": 0.29073357582092285,
      "learning_rate": 2.179166666666667e-05,
      "loss": 0.2546,
      "step": 197700
    },
    {
      "epoch": 1.923670915005665,
      "grad_norm": 0.26243361830711365,
      "learning_rate": 2.175e-05,
      "loss": 0.2538,
      "step": 197800
    },
    {
      "epoch": 1.9246434458713635,
      "grad_norm": 0.26739221811294556,
      "learning_rate": 2.1708333333333334e-05,
      "loss": 0.2542,
      "step": 197900
    },
    {
      "epoch": 1.9256159767370615,
      "grad_norm": 0.2702265977859497,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.255,
      "step": 198000
    },
    {
      "epoch": 1.92658850760276,
      "grad_norm": 0.27034398913383484,
      "learning_rate": 2.1625e-05,
      "loss": 0.2549,
      "step": 198100
    },
    {
      "epoch": 1.9275610384684585,
      "grad_norm": 0.2865122854709625,
      "learning_rate": 2.1583333333333334e-05,
      "loss": 0.2557,
      "step": 198200
    },
    {
      "epoch": 1.9285335693341568,
      "grad_norm": 0.25714829564094543,
      "learning_rate": 2.1541666666666667e-05,
      "loss": 0.255,
      "step": 198300
    },
    {
      "epoch": 1.929506100199855,
      "grad_norm": 0.250568151473999,
      "learning_rate": 2.15e-05,
      "loss": 0.2521,
      "step": 198400
    },
    {
      "epoch": 1.9304786310655535,
      "grad_norm": 0.2833842635154724,
      "learning_rate": 2.1458333333333334e-05,
      "loss": 0.2526,
      "step": 198500
    },
    {
      "epoch": 1.9314511619312518,
      "grad_norm": 0.28501683473587036,
      "learning_rate": 2.1416666666666668e-05,
      "loss": 0.254,
      "step": 198600
    },
    {
      "epoch": 1.93242369279695,
      "grad_norm": 0.26882097125053406,
      "learning_rate": 2.1375e-05,
      "loss": 0.254,
      "step": 198700
    },
    {
      "epoch": 1.9333962236626485,
      "grad_norm": 0.26785922050476074,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.2534,
      "step": 198800
    },
    {
      "epoch": 1.934368754528347,
      "grad_norm": 0.26982975006103516,
      "learning_rate": 2.1291666666666668e-05,
      "loss": 0.254,
      "step": 198900
    },
    {
      "epoch": 1.935341285394045,
      "grad_norm": 0.2855360209941864,
      "learning_rate": 2.125e-05,
      "loss": 0.2545,
      "step": 199000
    },
    {
      "epoch": 1.9363138162597435,
      "grad_norm": 0.29419851303100586,
      "learning_rate": 2.1208333333333335e-05,
      "loss": 0.2538,
      "step": 199100
    },
    {
      "epoch": 1.937286347125442,
      "grad_norm": 0.28609392046928406,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.2557,
      "step": 199200
    },
    {
      "epoch": 1.9382588779911403,
      "grad_norm": 0.2826031446456909,
      "learning_rate": 2.1125000000000002e-05,
      "loss": 0.2545,
      "step": 199300
    },
    {
      "epoch": 1.9392314088568385,
      "grad_norm": 0.26521211862564087,
      "learning_rate": 2.1083333333333335e-05,
      "loss": 0.254,
      "step": 199400
    },
    {
      "epoch": 1.940203939722537,
      "grad_norm": 0.2636483907699585,
      "learning_rate": 2.104166666666667e-05,
      "loss": 0.2552,
      "step": 199500
    },
    {
      "epoch": 1.9411764705882353,
      "grad_norm": 0.28584107756614685,
      "learning_rate": 2.1e-05,
      "loss": 0.2537,
      "step": 199600
    },
    {
      "epoch": 1.9421490014539335,
      "grad_norm": 0.25932958722114563,
      "learning_rate": 2.0958333333333336e-05,
      "loss": 0.2545,
      "step": 199700
    },
    {
      "epoch": 1.943121532319632,
      "grad_norm": 0.2898203730583191,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.2559,
      "step": 199800
    },
    {
      "epoch": 1.9440940631853303,
      "grad_norm": 0.2727896571159363,
      "learning_rate": 2.0875e-05,
      "loss": 0.2534,
      "step": 199900
    },
    {
      "epoch": 1.9450665940510286,
      "grad_norm": 0.2891833186149597,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.2562,
      "step": 200000
    },
    {
      "epoch": 1.9450665940510286,
      "eval_loss": 0.2547934949398041,
      "eval_runtime": 2958.6879,
      "eval_samples_per_second": 772.217,
      "eval_steps_per_second": 7.722,
      "step": 200000
    },
    {
      "epoch": 1.946039124916727,
      "grad_norm": 0.26477286219596863,
      "learning_rate": 2.0791666666666666e-05,
      "loss": 0.2541,
      "step": 200100
    },
    {
      "epoch": 1.9470116557824255,
      "grad_norm": 0.26634299755096436,
      "learning_rate": 2.075e-05,
      "loss": 0.2533,
      "step": 200200
    },
    {
      "epoch": 1.9479841866481238,
      "grad_norm": 0.2766646444797516,
      "learning_rate": 2.0708333333333336e-05,
      "loss": 0.2526,
      "step": 200300
    },
    {
      "epoch": 1.948956717513822,
      "grad_norm": 0.2682519853115082,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.2532,
      "step": 200400
    },
    {
      "epoch": 1.9499292483795205,
      "grad_norm": 0.27436643838882446,
      "learning_rate": 2.0625e-05,
      "loss": 0.253,
      "step": 200500
    },
    {
      "epoch": 1.9509017792452188,
      "grad_norm": 0.2620948851108551,
      "learning_rate": 2.0583333333333333e-05,
      "loss": 0.2535,
      "step": 200600
    },
    {
      "epoch": 1.951874310110917,
      "grad_norm": 0.24678447842597961,
      "learning_rate": 2.0541666666666667e-05,
      "loss": 0.2521,
      "step": 200700
    },
    {
      "epoch": 1.9528468409766155,
      "grad_norm": 0.2765863835811615,
      "learning_rate": 2.05e-05,
      "loss": 0.2534,
      "step": 200800
    },
    {
      "epoch": 1.9538193718423138,
      "grad_norm": 0.2809138000011444,
      "learning_rate": 2.0458333333333334e-05,
      "loss": 0.253,
      "step": 200900
    },
    {
      "epoch": 1.954791902708012,
      "grad_norm": 0.28698989748954773,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.2532,
      "step": 201000
    },
    {
      "epoch": 1.9557644335737105,
      "grad_norm": 0.2785381078720093,
      "learning_rate": 2.0375e-05,
      "loss": 0.2538,
      "step": 201100
    },
    {
      "epoch": 1.956736964439409,
      "grad_norm": 0.3022438883781433,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.2539,
      "step": 201200
    },
    {
      "epoch": 1.9577094953051073,
      "grad_norm": 0.2514653205871582,
      "learning_rate": 2.0291666666666667e-05,
      "loss": 0.2546,
      "step": 201300
    },
    {
      "epoch": 1.9586820261708056,
      "grad_norm": 0.27419671416282654,
      "learning_rate": 2.025e-05,
      "loss": 0.2529,
      "step": 201400
    },
    {
      "epoch": 1.959654557036504,
      "grad_norm": 0.30120211839675903,
      "learning_rate": 2.0208333333333334e-05,
      "loss": 0.2549,
      "step": 201500
    },
    {
      "epoch": 1.9606270879022023,
      "grad_norm": 0.2872224450111389,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.2537,
      "step": 201600
    },
    {
      "epoch": 1.9615996187679006,
      "grad_norm": 0.2809911370277405,
      "learning_rate": 2.0125e-05,
      "loss": 0.2545,
      "step": 201700
    },
    {
      "epoch": 1.962572149633599,
      "grad_norm": 0.27518579363822937,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.2532,
      "step": 201800
    },
    {
      "epoch": 1.9635446804992973,
      "grad_norm": 0.26653391122817993,
      "learning_rate": 2.0041666666666668e-05,
      "loss": 0.255,
      "step": 201900
    },
    {
      "epoch": 1.9645172113649956,
      "grad_norm": 0.2779908776283264,
      "learning_rate": 2e-05,
      "loss": 0.253,
      "step": 202000
    },
    {
      "epoch": 1.965489742230694,
      "grad_norm": 0.28605449199676514,
      "learning_rate": 1.9958333333333335e-05,
      "loss": 0.2538,
      "step": 202100
    },
    {
      "epoch": 1.9664622730963925,
      "grad_norm": 0.31078657507896423,
      "learning_rate": 1.9916666666666665e-05,
      "loss": 0.2539,
      "step": 202200
    },
    {
      "epoch": 1.9674348039620906,
      "grad_norm": 0.2799663245677948,
      "learning_rate": 1.9875000000000002e-05,
      "loss": 0.2538,
      "step": 202300
    },
    {
      "epoch": 1.968407334827789,
      "grad_norm": 0.28488412499427795,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.2521,
      "step": 202400
    },
    {
      "epoch": 1.9693798656934876,
      "grad_norm": 0.3224577307701111,
      "learning_rate": 1.9791666666666665e-05,
      "loss": 0.2543,
      "step": 202500
    },
    {
      "epoch": 1.9703523965591858,
      "grad_norm": 0.283641517162323,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.2533,
      "step": 202600
    },
    {
      "epoch": 1.971324927424884,
      "grad_norm": 0.3275993764400482,
      "learning_rate": 1.9708333333333336e-05,
      "loss": 0.2537,
      "step": 202700
    },
    {
      "epoch": 1.9722974582905826,
      "grad_norm": 0.3043596148490906,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.2546,
      "step": 202800
    },
    {
      "epoch": 1.9732699891562808,
      "grad_norm": 0.2968596816062927,
      "learning_rate": 1.9625000000000003e-05,
      "loss": 0.2543,
      "step": 202900
    },
    {
      "epoch": 1.974242520021979,
      "grad_norm": 0.28363147377967834,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.2535,
      "step": 203000
    },
    {
      "epoch": 1.9752150508876776,
      "grad_norm": 0.2786106765270233,
      "learning_rate": 1.9541666666666666e-05,
      "loss": 0.2522,
      "step": 203100
    },
    {
      "epoch": 1.976187581753376,
      "grad_norm": 0.2754920423030853,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.2534,
      "step": 203200
    },
    {
      "epoch": 1.977160112619074,
      "grad_norm": 0.2752825617790222,
      "learning_rate": 1.9458333333333333e-05,
      "loss": 0.2543,
      "step": 203300
    },
    {
      "epoch": 1.9781326434847726,
      "grad_norm": 0.2692679166793823,
      "learning_rate": 1.9416666666666667e-05,
      "loss": 0.2548,
      "step": 203400
    },
    {
      "epoch": 1.979105174350471,
      "grad_norm": 0.2871459722518921,
      "learning_rate": 1.9375e-05,
      "loss": 0.2539,
      "step": 203500
    },
    {
      "epoch": 1.9800777052161693,
      "grad_norm": 0.2694208025932312,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.2535,
      "step": 203600
    },
    {
      "epoch": 1.9810502360818676,
      "grad_norm": 0.26592594385147095,
      "learning_rate": 1.9291666666666667e-05,
      "loss": 0.2524,
      "step": 203700
    },
    {
      "epoch": 1.982022766947566,
      "grad_norm": 0.3099307715892792,
      "learning_rate": 1.925e-05,
      "loss": 0.2517,
      "step": 203800
    },
    {
      "epoch": 1.9829952978132643,
      "grad_norm": 0.2871834933757782,
      "learning_rate": 1.9208333333333334e-05,
      "loss": 0.2561,
      "step": 203900
    },
    {
      "epoch": 1.9839678286789626,
      "grad_norm": 0.3081578314304352,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.2534,
      "step": 204000
    },
    {
      "epoch": 1.984940359544661,
      "grad_norm": 0.2725760340690613,
      "learning_rate": 1.9125e-05,
      "loss": 0.2537,
      "step": 204100
    },
    {
      "epoch": 1.9859128904103593,
      "grad_norm": 0.3179336190223694,
      "learning_rate": 1.9083333333333334e-05,
      "loss": 0.2521,
      "step": 204200
    },
    {
      "epoch": 1.9868854212760576,
      "grad_norm": 0.2655251622200012,
      "learning_rate": 1.9041666666666668e-05,
      "loss": 0.2531,
      "step": 204300
    },
    {
      "epoch": 1.987857952141756,
      "grad_norm": 0.29347679018974304,
      "learning_rate": 1.9e-05,
      "loss": 0.2537,
      "step": 204400
    },
    {
      "epoch": 1.9888304830074546,
      "grad_norm": 0.28225427865982056,
      "learning_rate": 1.8958333333333334e-05,
      "loss": 0.2526,
      "step": 204500
    },
    {
      "epoch": 1.9898030138731528,
      "grad_norm": 0.27961835265159607,
      "learning_rate": 1.8916666666666668e-05,
      "loss": 0.2536,
      "step": 204600
    },
    {
      "epoch": 1.990775544738851,
      "grad_norm": 0.2793445885181427,
      "learning_rate": 1.8875e-05,
      "loss": 0.2534,
      "step": 204700
    },
    {
      "epoch": 1.9917480756045496,
      "grad_norm": 0.2902885377407074,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.2531,
      "step": 204800
    },
    {
      "epoch": 1.9927206064702478,
      "grad_norm": 0.2693851590156555,
      "learning_rate": 1.8791666666666668e-05,
      "loss": 0.2527,
      "step": 204900
    },
    {
      "epoch": 1.993693137335946,
      "grad_norm": 0.2899640202522278,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.2536,
      "step": 205000
    },
    {
      "epoch": 1.9946656682016446,
      "grad_norm": 0.33555400371551514,
      "learning_rate": 1.8708333333333332e-05,
      "loss": 0.2544,
      "step": 205100
    },
    {
      "epoch": 1.9956381990673429,
      "grad_norm": 0.2921668589115143,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.2539,
      "step": 205200
    },
    {
      "epoch": 1.9966107299330411,
      "grad_norm": 0.2865349352359772,
      "learning_rate": 1.8625000000000002e-05,
      "loss": 0.2514,
      "step": 205300
    },
    {
      "epoch": 1.9975832607987396,
      "grad_norm": 0.2882469594478607,
      "learning_rate": 1.8583333333333332e-05,
      "loss": 0.2521,
      "step": 205400
    },
    {
      "epoch": 1.998555791664438,
      "grad_norm": 0.2717474400997162,
      "learning_rate": 1.854166666666667e-05,
      "loss": 0.2541,
      "step": 205500
    },
    {
      "epoch": 1.9995283225301363,
      "grad_norm": 0.29316800832748413,
      "learning_rate": 1.85e-05,
      "loss": 0.2516,
      "step": 205600
    },
    {
      "epoch": 2.0005008533958346,
      "grad_norm": 0.27325931191444397,
      "learning_rate": 1.8458333333333333e-05,
      "loss": 0.2513,
      "step": 205700
    },
    {
      "epoch": 2.001473384261533,
      "grad_norm": 0.2674468755722046,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.2505,
      "step": 205800
    },
    {
      "epoch": 2.002445915127231,
      "grad_norm": 0.280081182718277,
      "learning_rate": 1.8375e-05,
      "loss": 0.249,
      "step": 205900
    },
    {
      "epoch": 2.0034184459929296,
      "grad_norm": 0.29550522565841675,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.25,
      "step": 206000
    },
    {
      "epoch": 2.004390976858628,
      "grad_norm": 0.3125355839729309,
      "learning_rate": 1.829166666666667e-05,
      "loss": 0.2474,
      "step": 206100
    },
    {
      "epoch": 2.0053635077243266,
      "grad_norm": 0.2814922034740448,
      "learning_rate": 1.825e-05,
      "loss": 0.2501,
      "step": 206200
    },
    {
      "epoch": 2.0063360385900246,
      "grad_norm": 0.27168112993240356,
      "learning_rate": 1.8208333333333337e-05,
      "loss": 0.2495,
      "step": 206300
    },
    {
      "epoch": 2.007308569455723,
      "grad_norm": 0.3135957717895508,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.2496,
      "step": 206400
    },
    {
      "epoch": 2.0082811003214216,
      "grad_norm": 0.27537065744400024,
      "learning_rate": 1.8125e-05,
      "loss": 0.2473,
      "step": 206500
    },
    {
      "epoch": 2.0092536311871196,
      "grad_norm": 0.29439350962638855,
      "learning_rate": 1.8083333333333337e-05,
      "loss": 0.2486,
      "step": 206600
    },
    {
      "epoch": 2.010226162052818,
      "grad_norm": 0.30192938446998596,
      "learning_rate": 1.8041666666666667e-05,
      "loss": 0.2469,
      "step": 206700
    },
    {
      "epoch": 2.0111986929185166,
      "grad_norm": 0.2880089581012726,
      "learning_rate": 1.8e-05,
      "loss": 0.2491,
      "step": 206800
    },
    {
      "epoch": 2.0121712237842146,
      "grad_norm": 0.30210721492767334,
      "learning_rate": 1.7958333333333334e-05,
      "loss": 0.2489,
      "step": 206900
    },
    {
      "epoch": 2.013143754649913,
      "grad_norm": 0.30438193678855896,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.2492,
      "step": 207000
    },
    {
      "epoch": 2.0141162855156116,
      "grad_norm": 0.2965029180049896,
      "learning_rate": 1.7875e-05,
      "loss": 0.2495,
      "step": 207100
    },
    {
      "epoch": 2.01508881638131,
      "grad_norm": 0.30525586009025574,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.2487,
      "step": 207200
    },
    {
      "epoch": 2.016061347247008,
      "grad_norm": 0.3061729073524475,
      "learning_rate": 1.7791666666666668e-05,
      "loss": 0.2494,
      "step": 207300
    },
    {
      "epoch": 2.0170338781127066,
      "grad_norm": 0.29013675451278687,
      "learning_rate": 1.775e-05,
      "loss": 0.2494,
      "step": 207400
    },
    {
      "epoch": 2.018006408978405,
      "grad_norm": 0.2762332856655121,
      "learning_rate": 1.7708333333333335e-05,
      "loss": 0.247,
      "step": 207500
    },
    {
      "epoch": 2.018978939844103,
      "grad_norm": 0.29003745317459106,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.2494,
      "step": 207600
    },
    {
      "epoch": 2.0199514707098016,
      "grad_norm": 0.320432186126709,
      "learning_rate": 1.7625e-05,
      "loss": 0.2489,
      "step": 207700
    },
    {
      "epoch": 2.0209240015755,
      "grad_norm": 0.27339455485343933,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.2493,
      "step": 207800
    },
    {
      "epoch": 2.021896532441198,
      "grad_norm": 0.3184228837490082,
      "learning_rate": 1.754166666666667e-05,
      "loss": 0.249,
      "step": 207900
    },
    {
      "epoch": 2.0228690633068966,
      "grad_norm": 0.27557048201560974,
      "learning_rate": 1.75e-05,
      "loss": 0.248,
      "step": 208000
    },
    {
      "epoch": 2.023841594172595,
      "grad_norm": 0.28914201259613037,
      "learning_rate": 1.7458333333333335e-05,
      "loss": 0.2476,
      "step": 208100
    },
    {
      "epoch": 2.0248141250382936,
      "grad_norm": 0.30392715334892273,
      "learning_rate": 1.741666666666667e-05,
      "loss": 0.2494,
      "step": 208200
    },
    {
      "epoch": 2.0257866559039917,
      "grad_norm": 0.2815219759941101,
      "learning_rate": 1.7375e-05,
      "loss": 0.2493,
      "step": 208300
    },
    {
      "epoch": 2.02675918676969,
      "grad_norm": 0.319579154253006,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.2492,
      "step": 208400
    },
    {
      "epoch": 2.0277317176353886,
      "grad_norm": 0.29127007722854614,
      "learning_rate": 1.7291666666666666e-05,
      "loss": 0.2496,
      "step": 208500
    },
    {
      "epoch": 2.0287042485010867,
      "grad_norm": 0.34408333897590637,
      "learning_rate": 1.725e-05,
      "loss": 0.2497,
      "step": 208600
    },
    {
      "epoch": 2.029676779366785,
      "grad_norm": 0.3133526146411896,
      "learning_rate": 1.7208333333333336e-05,
      "loss": 0.2486,
      "step": 208700
    },
    {
      "epoch": 2.0306493102324836,
      "grad_norm": 0.2928318381309509,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.2494,
      "step": 208800
    },
    {
      "epoch": 2.0316218410981817,
      "grad_norm": 0.31063422560691833,
      "learning_rate": 1.7125000000000003e-05,
      "loss": 0.2507,
      "step": 208900
    },
    {
      "epoch": 2.03259437196388,
      "grad_norm": 0.30836090445518494,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.2483,
      "step": 209000
    },
    {
      "epoch": 2.0335669028295786,
      "grad_norm": 0.3187820613384247,
      "learning_rate": 1.7041666666666666e-05,
      "loss": 0.2492,
      "step": 209100
    },
    {
      "epoch": 2.034539433695277,
      "grad_norm": 0.3130931258201599,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.2474,
      "step": 209200
    },
    {
      "epoch": 2.035511964560975,
      "grad_norm": 0.2704743444919586,
      "learning_rate": 1.6958333333333333e-05,
      "loss": 0.2502,
      "step": 209300
    },
    {
      "epoch": 2.0364844954266736,
      "grad_norm": 0.3115565776824951,
      "learning_rate": 1.6916666666666667e-05,
      "loss": 0.2472,
      "step": 209400
    },
    {
      "epoch": 2.037457026292372,
      "grad_norm": 0.31182441115379333,
      "learning_rate": 1.6875000000000004e-05,
      "loss": 0.2482,
      "step": 209500
    },
    {
      "epoch": 2.03842955715807,
      "grad_norm": 0.31142058968544006,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.2487,
      "step": 209600
    },
    {
      "epoch": 2.0394020880237687,
      "grad_norm": 0.31786587834358215,
      "learning_rate": 1.6791666666666667e-05,
      "loss": 0.2489,
      "step": 209700
    },
    {
      "epoch": 2.040374618889467,
      "grad_norm": 0.28598812222480774,
      "learning_rate": 1.675e-05,
      "loss": 0.2497,
      "step": 209800
    },
    {
      "epoch": 2.041347149755165,
      "grad_norm": 0.28783655166625977,
      "learning_rate": 1.6708333333333334e-05,
      "loss": 0.2492,
      "step": 209900
    },
    {
      "epoch": 2.0423196806208637,
      "grad_norm": 0.31710153818130493,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.2485,
      "step": 210000
    },
    {
      "epoch": 2.0423196806208637,
      "eval_loss": 0.2535785436630249,
      "eval_runtime": 2925.4912,
      "eval_samples_per_second": 780.98,
      "eval_steps_per_second": 7.81,
      "step": 210000
    },
    {
      "epoch": 2.043292211486562,
      "grad_norm": 0.30933934450149536,
      "learning_rate": 1.6625e-05,
      "loss": 0.2483,
      "step": 210100
    },
    {
      "epoch": 2.04426474235226,
      "grad_norm": 0.27825772762298584,
      "learning_rate": 1.6583333333333334e-05,
      "loss": 0.2483,
      "step": 210200
    },
    {
      "epoch": 2.0452372732179587,
      "grad_norm": 0.2881488502025604,
      "learning_rate": 1.6541666666666668e-05,
      "loss": 0.2477,
      "step": 210300
    },
    {
      "epoch": 2.046209804083657,
      "grad_norm": 0.29424014687538147,
      "learning_rate": 1.65e-05,
      "loss": 0.2503,
      "step": 210400
    },
    {
      "epoch": 2.0471823349493556,
      "grad_norm": 0.2964059114456177,
      "learning_rate": 1.6458333333333335e-05,
      "loss": 0.2504,
      "step": 210500
    },
    {
      "epoch": 2.0481548658150537,
      "grad_norm": 0.27854210138320923,
      "learning_rate": 1.6416666666666665e-05,
      "loss": 0.2504,
      "step": 210600
    },
    {
      "epoch": 2.049127396680752,
      "grad_norm": 0.3110392391681671,
      "learning_rate": 1.6375e-05,
      "loss": 0.2499,
      "step": 210700
    },
    {
      "epoch": 2.0500999275464507,
      "grad_norm": 0.28958994150161743,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.2475,
      "step": 210800
    },
    {
      "epoch": 2.0510724584121487,
      "grad_norm": 0.3175606429576874,
      "learning_rate": 1.6291666666666665e-05,
      "loss": 0.2491,
      "step": 210900
    },
    {
      "epoch": 2.052044989277847,
      "grad_norm": 0.25909894704818726,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.2481,
      "step": 211000
    },
    {
      "epoch": 2.0530175201435457,
      "grad_norm": 0.2973978519439697,
      "learning_rate": 1.6208333333333332e-05,
      "loss": 0.2484,
      "step": 211100
    },
    {
      "epoch": 2.0539900510092437,
      "grad_norm": 0.33961713314056396,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.2487,
      "step": 211200
    },
    {
      "epoch": 2.054962581874942,
      "grad_norm": 0.30893632769584656,
      "learning_rate": 1.6125000000000002e-05,
      "loss": 0.2489,
      "step": 211300
    },
    {
      "epoch": 2.0559351127406407,
      "grad_norm": 0.31616154313087463,
      "learning_rate": 1.6083333333333332e-05,
      "loss": 0.2493,
      "step": 211400
    },
    {
      "epoch": 2.056907643606339,
      "grad_norm": 0.3147698640823364,
      "learning_rate": 1.604166666666667e-05,
      "loss": 0.2493,
      "step": 211500
    },
    {
      "epoch": 2.057880174472037,
      "grad_norm": 0.3062461018562317,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.2482,
      "step": 211600
    },
    {
      "epoch": 2.0588527053377357,
      "grad_norm": 0.27492350339889526,
      "learning_rate": 1.5958333333333333e-05,
      "loss": 0.2475,
      "step": 211700
    },
    {
      "epoch": 2.059825236203434,
      "grad_norm": 0.3154521584510803,
      "learning_rate": 1.591666666666667e-05,
      "loss": 0.2492,
      "step": 211800
    },
    {
      "epoch": 2.060797767069132,
      "grad_norm": 0.30625906586647034,
      "learning_rate": 1.5875e-05,
      "loss": 0.2492,
      "step": 211900
    },
    {
      "epoch": 2.0617702979348307,
      "grad_norm": 0.2992630898952484,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.2483,
      "step": 212000
    },
    {
      "epoch": 2.062742828800529,
      "grad_norm": 0.32895341515541077,
      "learning_rate": 1.579166666666667e-05,
      "loss": 0.2468,
      "step": 212100
    },
    {
      "epoch": 2.063715359666227,
      "grad_norm": 0.287463515996933,
      "learning_rate": 1.575e-05,
      "loss": 0.2483,
      "step": 212200
    },
    {
      "epoch": 2.0646878905319257,
      "grad_norm": 0.29701393842697144,
      "learning_rate": 1.5708333333333333e-05,
      "loss": 0.2489,
      "step": 212300
    },
    {
      "epoch": 2.065660421397624,
      "grad_norm": 0.31081414222717285,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.2502,
      "step": 212400
    },
    {
      "epoch": 2.0666329522633227,
      "grad_norm": 0.27912530303001404,
      "learning_rate": 1.5625e-05,
      "loss": 0.2486,
      "step": 212500
    },
    {
      "epoch": 2.0676054831290207,
      "grad_norm": 0.30057641863822937,
      "learning_rate": 1.5583333333333334e-05,
      "loss": 0.2474,
      "step": 212600
    },
    {
      "epoch": 2.068578013994719,
      "grad_norm": 0.30765530467033386,
      "learning_rate": 1.5541666666666667e-05,
      "loss": 0.2479,
      "step": 212700
    },
    {
      "epoch": 2.0695505448604177,
      "grad_norm": 0.3374115228652954,
      "learning_rate": 1.55e-05,
      "loss": 0.2479,
      "step": 212800
    },
    {
      "epoch": 2.0705230757261157,
      "grad_norm": 0.29188641905784607,
      "learning_rate": 1.5458333333333334e-05,
      "loss": 0.2509,
      "step": 212900
    },
    {
      "epoch": 2.071495606591814,
      "grad_norm": 0.294227659702301,
      "learning_rate": 1.5416666666666668e-05,
      "loss": 0.2488,
      "step": 213000
    },
    {
      "epoch": 2.0724681374575127,
      "grad_norm": 0.3350706994533539,
      "learning_rate": 1.5375e-05,
      "loss": 0.2505,
      "step": 213100
    },
    {
      "epoch": 2.0734406683232107,
      "grad_norm": 0.3083857297897339,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.2488,
      "step": 213200
    },
    {
      "epoch": 2.074413199188909,
      "grad_norm": 0.3083394467830658,
      "learning_rate": 1.5291666666666668e-05,
      "loss": 0.249,
      "step": 213300
    },
    {
      "epoch": 2.0753857300546077,
      "grad_norm": 0.3097505569458008,
      "learning_rate": 1.525e-05,
      "loss": 0.2484,
      "step": 213400
    },
    {
      "epoch": 2.0763582609203057,
      "grad_norm": 0.3062841594219208,
      "learning_rate": 1.5208333333333333e-05,
      "loss": 0.2497,
      "step": 213500
    },
    {
      "epoch": 2.077330791786004,
      "grad_norm": 0.26912999153137207,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.2485,
      "step": 213600
    },
    {
      "epoch": 2.0783033226517027,
      "grad_norm": 0.278125524520874,
      "learning_rate": 1.5125e-05,
      "loss": 0.2483,
      "step": 213700
    },
    {
      "epoch": 2.079275853517401,
      "grad_norm": 0.31042981147766113,
      "learning_rate": 1.5083333333333335e-05,
      "loss": 0.2481,
      "step": 213800
    },
    {
      "epoch": 2.0802483843830992,
      "grad_norm": 0.31285008788108826,
      "learning_rate": 1.5041666666666669e-05,
      "loss": 0.2483,
      "step": 213900
    },
    {
      "epoch": 2.0812209152487977,
      "grad_norm": 0.31824445724487305,
      "learning_rate": 1.5e-05,
      "loss": 0.2495,
      "step": 214000
    },
    {
      "epoch": 2.082193446114496,
      "grad_norm": 0.32611891627311707,
      "learning_rate": 1.4958333333333336e-05,
      "loss": 0.2488,
      "step": 214100
    },
    {
      "epoch": 2.0831659769801942,
      "grad_norm": 0.3137377202510834,
      "learning_rate": 1.4916666666666667e-05,
      "loss": 0.2472,
      "step": 214200
    },
    {
      "epoch": 2.0841385078458927,
      "grad_norm": 0.30624881386756897,
      "learning_rate": 1.4875e-05,
      "loss": 0.2477,
      "step": 214300
    },
    {
      "epoch": 2.085111038711591,
      "grad_norm": 0.2623519003391266,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.2469,
      "step": 214400
    },
    {
      "epoch": 2.0860835695772897,
      "grad_norm": 0.32282960414886475,
      "learning_rate": 1.4791666666666668e-05,
      "loss": 0.249,
      "step": 214500
    },
    {
      "epoch": 2.0870561004429877,
      "grad_norm": 0.3146914541721344,
      "learning_rate": 1.475e-05,
      "loss": 0.2478,
      "step": 214600
    },
    {
      "epoch": 2.088028631308686,
      "grad_norm": 0.33039674162864685,
      "learning_rate": 1.4708333333333335e-05,
      "loss": 0.247,
      "step": 214700
    },
    {
      "epoch": 2.0890011621743847,
      "grad_norm": 0.3125225603580475,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.2491,
      "step": 214800
    },
    {
      "epoch": 2.0899736930400827,
      "grad_norm": 0.29042765498161316,
      "learning_rate": 1.4625e-05,
      "loss": 0.2493,
      "step": 214900
    },
    {
      "epoch": 2.090946223905781,
      "grad_norm": 0.29781365394592285,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.2483,
      "step": 215000
    },
    {
      "epoch": 2.0919187547714797,
      "grad_norm": 0.30982935428619385,
      "learning_rate": 1.4541666666666667e-05,
      "loss": 0.2483,
      "step": 215100
    },
    {
      "epoch": 2.0928912856371777,
      "grad_norm": 0.3071763217449188,
      "learning_rate": 1.45e-05,
      "loss": 0.2502,
      "step": 215200
    },
    {
      "epoch": 2.0938638165028762,
      "grad_norm": 0.3194619417190552,
      "learning_rate": 1.4458333333333335e-05,
      "loss": 0.2498,
      "step": 215300
    },
    {
      "epoch": 2.0948363473685747,
      "grad_norm": 0.29090651869773865,
      "learning_rate": 1.4416666666666667e-05,
      "loss": 0.2499,
      "step": 215400
    },
    {
      "epoch": 2.0958088782342728,
      "grad_norm": 0.2999190390110016,
      "learning_rate": 1.4374999999999999e-05,
      "loss": 0.2479,
      "step": 215500
    },
    {
      "epoch": 2.0967814090999712,
      "grad_norm": 0.27172666788101196,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.2477,
      "step": 215600
    },
    {
      "epoch": 2.0977539399656697,
      "grad_norm": 0.30170938372612,
      "learning_rate": 1.4291666666666667e-05,
      "loss": 0.2472,
      "step": 215700
    },
    {
      "epoch": 2.098726470831368,
      "grad_norm": 0.28032392263412476,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.2455,
      "step": 215800
    },
    {
      "epoch": 2.0996990016970662,
      "grad_norm": 0.29666730761528015,
      "learning_rate": 1.4208333333333334e-05,
      "loss": 0.2493,
      "step": 215900
    },
    {
      "epoch": 2.1006715325627647,
      "grad_norm": 0.30363115668296814,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.2487,
      "step": 216000
    },
    {
      "epoch": 2.101644063428463,
      "grad_norm": 0.29960983991622925,
      "learning_rate": 1.4125e-05,
      "loss": 0.2479,
      "step": 216100
    },
    {
      "epoch": 2.1026165942941613,
      "grad_norm": 0.33000215888023376,
      "learning_rate": 1.4083333333333335e-05,
      "loss": 0.2488,
      "step": 216200
    },
    {
      "epoch": 2.1035891251598597,
      "grad_norm": 0.29538866877555847,
      "learning_rate": 1.4041666666666666e-05,
      "loss": 0.2485,
      "step": 216300
    },
    {
      "epoch": 2.1045616560255582,
      "grad_norm": 0.33314207196235657,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.2492,
      "step": 216400
    },
    {
      "epoch": 2.1055341868912563,
      "grad_norm": 0.27540600299835205,
      "learning_rate": 1.3958333333333335e-05,
      "loss": 0.2474,
      "step": 216500
    },
    {
      "epoch": 2.1065067177569547,
      "grad_norm": 0.30907225608825684,
      "learning_rate": 1.3916666666666667e-05,
      "loss": 0.2488,
      "step": 216600
    },
    {
      "epoch": 2.1074792486226532,
      "grad_norm": 0.2966636121273041,
      "learning_rate": 1.3875000000000002e-05,
      "loss": 0.2486,
      "step": 216700
    },
    {
      "epoch": 2.1084517794883517,
      "grad_norm": 0.3173702657222748,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.2486,
      "step": 216800
    },
    {
      "epoch": 2.1094243103540498,
      "grad_norm": 0.30652758479118347,
      "learning_rate": 1.3791666666666667e-05,
      "loss": 0.2485,
      "step": 216900
    },
    {
      "epoch": 2.1103968412197482,
      "grad_norm": 0.2913925349712372,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.248,
      "step": 217000
    },
    {
      "epoch": 2.1113693720854467,
      "grad_norm": 0.30345651507377625,
      "learning_rate": 1.3708333333333334e-05,
      "loss": 0.2481,
      "step": 217100
    },
    {
      "epoch": 2.1123419029511448,
      "grad_norm": 0.3234368562698364,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.2476,
      "step": 217200
    },
    {
      "epoch": 2.1133144338168433,
      "grad_norm": 0.2933773100376129,
      "learning_rate": 1.3625e-05,
      "loss": 0.2478,
      "step": 217300
    },
    {
      "epoch": 2.1142869646825417,
      "grad_norm": 0.3127010762691498,
      "learning_rate": 1.3583333333333334e-05,
      "loss": 0.2497,
      "step": 217400
    },
    {
      "epoch": 2.1152594955482398,
      "grad_norm": 0.2962621748447418,
      "learning_rate": 1.3541666666666666e-05,
      "loss": 0.2486,
      "step": 217500
    },
    {
      "epoch": 2.1162320264139383,
      "grad_norm": 0.2950209081172943,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.2471,
      "step": 217600
    },
    {
      "epoch": 2.1172045572796367,
      "grad_norm": 0.32156428694725037,
      "learning_rate": 1.3458333333333335e-05,
      "loss": 0.248,
      "step": 217700
    },
    {
      "epoch": 2.118177088145335,
      "grad_norm": 0.31029874086380005,
      "learning_rate": 1.3416666666666666e-05,
      "loss": 0.2484,
      "step": 217800
    },
    {
      "epoch": 2.1191496190110333,
      "grad_norm": 0.334564208984375,
      "learning_rate": 1.3375000000000002e-05,
      "loss": 0.2499,
      "step": 217900
    },
    {
      "epoch": 2.1201221498767318,
      "grad_norm": 0.3043030798435211,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.2479,
      "step": 218000
    },
    {
      "epoch": 2.1210946807424302,
      "grad_norm": 0.312703937292099,
      "learning_rate": 1.3291666666666667e-05,
      "loss": 0.249,
      "step": 218100
    },
    {
      "epoch": 2.1220672116081283,
      "grad_norm": 0.3085457384586334,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.2484,
      "step": 218200
    },
    {
      "epoch": 2.1230397424738268,
      "grad_norm": 0.31501859426498413,
      "learning_rate": 1.3208333333333334e-05,
      "loss": 0.2485,
      "step": 218300
    },
    {
      "epoch": 2.1240122733395252,
      "grad_norm": 0.3012644350528717,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.2475,
      "step": 218400
    },
    {
      "epoch": 2.1249848042052233,
      "grad_norm": 0.32700634002685547,
      "learning_rate": 1.3125e-05,
      "loss": 0.2486,
      "step": 218500
    },
    {
      "epoch": 2.1259573350709218,
      "grad_norm": 0.32168370485305786,
      "learning_rate": 1.3083333333333334e-05,
      "loss": 0.2481,
      "step": 218600
    },
    {
      "epoch": 2.1269298659366203,
      "grad_norm": 0.3047489821910858,
      "learning_rate": 1.3041666666666666e-05,
      "loss": 0.2485,
      "step": 218700
    },
    {
      "epoch": 2.1279023968023187,
      "grad_norm": 0.31240198016166687,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.2482,
      "step": 218800
    },
    {
      "epoch": 2.128874927668017,
      "grad_norm": 0.3251364827156067,
      "learning_rate": 1.2958333333333333e-05,
      "loss": 0.2485,
      "step": 218900
    },
    {
      "epoch": 2.1298474585337153,
      "grad_norm": 0.286909282207489,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.2478,
      "step": 219000
    },
    {
      "epoch": 2.1308199893994137,
      "grad_norm": 0.29971182346343994,
      "learning_rate": 1.2875000000000001e-05,
      "loss": 0.2487,
      "step": 219100
    },
    {
      "epoch": 2.131792520265112,
      "grad_norm": 0.3186820447444916,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.2484,
      "step": 219200
    },
    {
      "epoch": 2.1327650511308103,
      "grad_norm": 0.3073074221611023,
      "learning_rate": 1.2791666666666668e-05,
      "loss": 0.2497,
      "step": 219300
    },
    {
      "epoch": 2.1337375819965088,
      "grad_norm": 0.3020171523094177,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 0.248,
      "step": 219400
    },
    {
      "epoch": 2.134710112862207,
      "grad_norm": 0.3296918272972107,
      "learning_rate": 1.2708333333333333e-05,
      "loss": 0.2481,
      "step": 219500
    },
    {
      "epoch": 2.1356826437279053,
      "grad_norm": 0.30289915204048157,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.2491,
      "step": 219600
    },
    {
      "epoch": 2.1366551745936038,
      "grad_norm": 0.31616151332855225,
      "learning_rate": 1.2625e-05,
      "loss": 0.2487,
      "step": 219700
    },
    {
      "epoch": 2.137627705459302,
      "grad_norm": 0.2885904908180237,
      "learning_rate": 1.2583333333333334e-05,
      "loss": 0.2482,
      "step": 219800
    },
    {
      "epoch": 2.1386002363250003,
      "grad_norm": 0.3061118721961975,
      "learning_rate": 1.2541666666666669e-05,
      "loss": 0.2488,
      "step": 219900
    },
    {
      "epoch": 2.1395727671906988,
      "grad_norm": 0.3153454065322876,
      "learning_rate": 1.25e-05,
      "loss": 0.2488,
      "step": 220000
    },
    {
      "epoch": 2.1395727671906988,
      "eval_loss": 0.25221091508865356,
      "eval_runtime": 2921.4896,
      "eval_samples_per_second": 782.05,
      "eval_steps_per_second": 7.821,
      "step": 220000
    },
    {
      "epoch": 2.1405452980563973,
      "grad_norm": 0.31252261996269226,
      "learning_rate": 1.2458333333333334e-05,
      "loss": 0.2469,
      "step": 220100
    },
    {
      "epoch": 2.1415178289220953,
      "grad_norm": 0.28906068205833435,
      "learning_rate": 1.2416666666666667e-05,
      "loss": 0.2484,
      "step": 220200
    },
    {
      "epoch": 2.142490359787794,
      "grad_norm": 0.2851819396018982,
      "learning_rate": 1.2375000000000001e-05,
      "loss": 0.2469,
      "step": 220300
    },
    {
      "epoch": 2.1434628906534923,
      "grad_norm": 0.31213125586509705,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.2485,
      "step": 220400
    },
    {
      "epoch": 2.1444354215191903,
      "grad_norm": 0.30992719531059265,
      "learning_rate": 1.2291666666666666e-05,
      "loss": 0.2475,
      "step": 220500
    },
    {
      "epoch": 2.145407952384889,
      "grad_norm": 0.2997044324874878,
      "learning_rate": 1.225e-05,
      "loss": 0.2472,
      "step": 220600
    },
    {
      "epoch": 2.1463804832505873,
      "grad_norm": 0.3300841152667999,
      "learning_rate": 1.2208333333333335e-05,
      "loss": 0.2469,
      "step": 220700
    },
    {
      "epoch": 2.1473530141162853,
      "grad_norm": 0.3001660406589508,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.2493,
      "step": 220800
    },
    {
      "epoch": 2.148325544981984,
      "grad_norm": 0.2778239846229553,
      "learning_rate": 1.2125e-05,
      "loss": 0.2478,
      "step": 220900
    },
    {
      "epoch": 2.1492980758476823,
      "grad_norm": 0.33045443892478943,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.2485,
      "step": 221000
    },
    {
      "epoch": 2.1502706067133808,
      "grad_norm": 0.31361499428749084,
      "learning_rate": 1.2041666666666669e-05,
      "loss": 0.2481,
      "step": 221100
    },
    {
      "epoch": 2.151243137579079,
      "grad_norm": 0.3046347498893738,
      "learning_rate": 1.2e-05,
      "loss": 0.2501,
      "step": 221200
    },
    {
      "epoch": 2.1522156684447773,
      "grad_norm": 0.3057398498058319,
      "learning_rate": 1.1958333333333334e-05,
      "loss": 0.2472,
      "step": 221300
    },
    {
      "epoch": 2.153188199310476,
      "grad_norm": 0.3289368450641632,
      "learning_rate": 1.1916666666666667e-05,
      "loss": 0.2479,
      "step": 221400
    },
    {
      "epoch": 2.154160730176174,
      "grad_norm": 0.29946595430374146,
      "learning_rate": 1.1875e-05,
      "loss": 0.2473,
      "step": 221500
    },
    {
      "epoch": 2.1551332610418723,
      "grad_norm": 0.30618947744369507,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.2484,
      "step": 221600
    },
    {
      "epoch": 2.156105791907571,
      "grad_norm": 0.3101297914981842,
      "learning_rate": 1.1791666666666668e-05,
      "loss": 0.2474,
      "step": 221700
    },
    {
      "epoch": 2.157078322773269,
      "grad_norm": 0.2866537272930145,
      "learning_rate": 1.175e-05,
      "loss": 0.2489,
      "step": 221800
    },
    {
      "epoch": 2.1580508536389673,
      "grad_norm": 0.34481704235076904,
      "learning_rate": 1.1708333333333334e-05,
      "loss": 0.2475,
      "step": 221900
    },
    {
      "epoch": 2.159023384504666,
      "grad_norm": 0.3070971965789795,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.2489,
      "step": 222000
    },
    {
      "epoch": 2.159995915370364,
      "grad_norm": 0.3096024692058563,
      "learning_rate": 1.1625000000000001e-05,
      "loss": 0.2484,
      "step": 222100
    },
    {
      "epoch": 2.1609684462360623,
      "grad_norm": 0.3218632936477661,
      "learning_rate": 1.1583333333333333e-05,
      "loss": 0.247,
      "step": 222200
    },
    {
      "epoch": 2.161940977101761,
      "grad_norm": 0.3094685971736908,
      "learning_rate": 1.1541666666666667e-05,
      "loss": 0.2496,
      "step": 222300
    },
    {
      "epoch": 2.1629135079674593,
      "grad_norm": 0.29715171456336975,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.2477,
      "step": 222400
    },
    {
      "epoch": 2.1638860388331573,
      "grad_norm": 0.3057979345321655,
      "learning_rate": 1.1458333333333333e-05,
      "loss": 0.247,
      "step": 222500
    },
    {
      "epoch": 2.164858569698856,
      "grad_norm": 0.30945903062820435,
      "learning_rate": 1.1416666666666667e-05,
      "loss": 0.2471,
      "step": 222600
    },
    {
      "epoch": 2.1658311005645543,
      "grad_norm": 0.3314204216003418,
      "learning_rate": 1.1375e-05,
      "loss": 0.2473,
      "step": 222700
    },
    {
      "epoch": 2.1668036314302523,
      "grad_norm": 0.3329413831233978,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.2474,
      "step": 222800
    },
    {
      "epoch": 2.167776162295951,
      "grad_norm": 0.2966429591178894,
      "learning_rate": 1.1291666666666667e-05,
      "loss": 0.2486,
      "step": 222900
    },
    {
      "epoch": 2.1687486931616493,
      "grad_norm": 0.31958723068237305,
      "learning_rate": 1.125e-05,
      "loss": 0.2463,
      "step": 223000
    },
    {
      "epoch": 2.169721224027348,
      "grad_norm": 0.330168217420578,
      "learning_rate": 1.1208333333333332e-05,
      "loss": 0.2468,
      "step": 223100
    },
    {
      "epoch": 2.170693754893046,
      "grad_norm": 0.3027903735637665,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.2469,
      "step": 223200
    },
    {
      "epoch": 2.1716662857587443,
      "grad_norm": 0.29167771339416504,
      "learning_rate": 1.1125000000000001e-05,
      "loss": 0.2482,
      "step": 223300
    },
    {
      "epoch": 2.172638816624443,
      "grad_norm": 0.33859094977378845,
      "learning_rate": 1.1083333333333335e-05,
      "loss": 0.2492,
      "step": 223400
    },
    {
      "epoch": 2.173611347490141,
      "grad_norm": 0.3252139985561371,
      "learning_rate": 1.1041666666666666e-05,
      "loss": 0.248,
      "step": 223500
    },
    {
      "epoch": 2.1745838783558393,
      "grad_norm": 0.306603342294693,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.2471,
      "step": 223600
    },
    {
      "epoch": 2.175556409221538,
      "grad_norm": 0.3303920328617096,
      "learning_rate": 1.0958333333333335e-05,
      "loss": 0.246,
      "step": 223700
    },
    {
      "epoch": 2.176528940087236,
      "grad_norm": 0.28996363282203674,
      "learning_rate": 1.0916666666666667e-05,
      "loss": 0.2476,
      "step": 223800
    },
    {
      "epoch": 2.1775014709529343,
      "grad_norm": 0.3376007080078125,
      "learning_rate": 1.0875e-05,
      "loss": 0.2473,
      "step": 223900
    },
    {
      "epoch": 2.178474001818633,
      "grad_norm": 0.3144984841346741,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.2479,
      "step": 224000
    },
    {
      "epoch": 2.179446532684331,
      "grad_norm": 0.3130110502243042,
      "learning_rate": 1.0791666666666667e-05,
      "loss": 0.248,
      "step": 224100
    },
    {
      "epoch": 2.1804190635500293,
      "grad_norm": 0.30094918608665466,
      "learning_rate": 1.075e-05,
      "loss": 0.247,
      "step": 224200
    },
    {
      "epoch": 2.181391594415728,
      "grad_norm": 0.31690657138824463,
      "learning_rate": 1.0708333333333334e-05,
      "loss": 0.248,
      "step": 224300
    },
    {
      "epoch": 2.1823641252814263,
      "grad_norm": 0.3219717741012573,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.2468,
      "step": 224400
    },
    {
      "epoch": 2.1833366561471244,
      "grad_norm": 0.30979493260383606,
      "learning_rate": 1.0625e-05,
      "loss": 0.2474,
      "step": 224500
    },
    {
      "epoch": 2.184309187012823,
      "grad_norm": 0.31973615288734436,
      "learning_rate": 1.0583333333333334e-05,
      "loss": 0.247,
      "step": 224600
    },
    {
      "epoch": 2.1852817178785213,
      "grad_norm": 0.3317258358001709,
      "learning_rate": 1.0541666666666668e-05,
      "loss": 0.2485,
      "step": 224700
    },
    {
      "epoch": 2.1862542487442194,
      "grad_norm": 0.33185431361198425,
      "learning_rate": 1.05e-05,
      "loss": 0.2476,
      "step": 224800
    },
    {
      "epoch": 2.187226779609918,
      "grad_norm": 0.3265644311904907,
      "learning_rate": 1.0458333333333335e-05,
      "loss": 0.2467,
      "step": 224900
    },
    {
      "epoch": 2.1881993104756163,
      "grad_norm": 0.33303534984588623,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.2458,
      "step": 225000
    },
    {
      "epoch": 2.1891718413413144,
      "grad_norm": 0.338879257440567,
      "learning_rate": 1.0375e-05,
      "loss": 0.2473,
      "step": 225100
    },
    {
      "epoch": 2.190144372207013,
      "grad_norm": 0.2981417179107666,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.2486,
      "step": 225200
    },
    {
      "epoch": 2.1911169030727113,
      "grad_norm": 0.30237770080566406,
      "learning_rate": 1.0291666666666667e-05,
      "loss": 0.2474,
      "step": 225300
    },
    {
      "epoch": 2.19208943393841,
      "grad_norm": 0.3092253506183624,
      "learning_rate": 1.025e-05,
      "loss": 0.2468,
      "step": 225400
    },
    {
      "epoch": 2.193061964804108,
      "grad_norm": 0.3253352642059326,
      "learning_rate": 1.0208333333333334e-05,
      "loss": 0.2464,
      "step": 225500
    },
    {
      "epoch": 2.1940344956698064,
      "grad_norm": 0.2872909605503082,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.2475,
      "step": 225600
    },
    {
      "epoch": 2.195007026535505,
      "grad_norm": 0.3420272767543793,
      "learning_rate": 1.0125e-05,
      "loss": 0.2468,
      "step": 225700
    },
    {
      "epoch": 2.195979557401203,
      "grad_norm": 0.32429105043411255,
      "learning_rate": 1.0083333333333334e-05,
      "loss": 0.2465,
      "step": 225800
    },
    {
      "epoch": 2.1969520882669014,
      "grad_norm": 0.3027268350124359,
      "learning_rate": 1.0041666666666667e-05,
      "loss": 0.2459,
      "step": 225900
    },
    {
      "epoch": 2.1979246191326,
      "grad_norm": 0.29998913407325745,
      "learning_rate": 1e-05,
      "loss": 0.2479,
      "step": 226000
    },
    {
      "epoch": 2.198897149998298,
      "grad_norm": 0.30436229705810547,
      "learning_rate": 9.958333333333333e-06,
      "loss": 0.247,
      "step": 226100
    },
    {
      "epoch": 2.1998696808639964,
      "grad_norm": 0.29511770606040955,
      "learning_rate": 9.916666666666668e-06,
      "loss": 0.2473,
      "step": 226200
    },
    {
      "epoch": 2.200842211729695,
      "grad_norm": 0.30779898166656494,
      "learning_rate": 9.875000000000001e-06,
      "loss": 0.246,
      "step": 226300
    },
    {
      "epoch": 2.201814742595393,
      "grad_norm": 0.313155859708786,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.2482,
      "step": 226400
    },
    {
      "epoch": 2.2027872734610914,
      "grad_norm": 0.3194786012172699,
      "learning_rate": 9.791666666666666e-06,
      "loss": 0.2478,
      "step": 226500
    },
    {
      "epoch": 2.20375980432679,
      "grad_norm": 0.301060289144516,
      "learning_rate": 9.750000000000002e-06,
      "loss": 0.2455,
      "step": 226600
    },
    {
      "epoch": 2.2047323351924883,
      "grad_norm": 0.3113769292831421,
      "learning_rate": 9.708333333333333e-06,
      "loss": 0.2496,
      "step": 226700
    },
    {
      "epoch": 2.2057048660581864,
      "grad_norm": 0.3020261526107788,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.2473,
      "step": 226800
    },
    {
      "epoch": 2.206677396923885,
      "grad_norm": 0.33892202377319336,
      "learning_rate": 9.625e-06,
      "loss": 0.2456,
      "step": 226900
    },
    {
      "epoch": 2.2076499277895834,
      "grad_norm": 0.29672670364379883,
      "learning_rate": 9.583333333333334e-06,
      "loss": 0.2459,
      "step": 227000
    },
    {
      "epoch": 2.2086224586552814,
      "grad_norm": 0.3162482678890228,
      "learning_rate": 9.541666666666667e-06,
      "loss": 0.2451,
      "step": 227100
    },
    {
      "epoch": 2.20959498952098,
      "grad_norm": 0.322364866733551,
      "learning_rate": 9.5e-06,
      "loss": 0.2469,
      "step": 227200
    },
    {
      "epoch": 2.2105675203866784,
      "grad_norm": 0.3267417550086975,
      "learning_rate": 9.458333333333334e-06,
      "loss": 0.2476,
      "step": 227300
    },
    {
      "epoch": 2.211540051252377,
      "grad_norm": 0.32186710834503174,
      "learning_rate": 9.416666666666667e-06,
      "loss": 0.2473,
      "step": 227400
    },
    {
      "epoch": 2.212512582118075,
      "grad_norm": 0.29964229464530945,
      "learning_rate": 9.375000000000001e-06,
      "loss": 0.2479,
      "step": 227500
    },
    {
      "epoch": 2.2134851129837734,
      "grad_norm": 0.29319116473197937,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.2462,
      "step": 227600
    },
    {
      "epoch": 2.214457643849472,
      "grad_norm": 0.3189988136291504,
      "learning_rate": 9.291666666666666e-06,
      "loss": 0.2469,
      "step": 227700
    },
    {
      "epoch": 2.21543017471517,
      "grad_norm": 0.30919989943504333,
      "learning_rate": 9.25e-06,
      "loss": 0.2475,
      "step": 227800
    },
    {
      "epoch": 2.2164027055808684,
      "grad_norm": 0.34083059430122375,
      "learning_rate": 9.208333333333335e-06,
      "loss": 0.2481,
      "step": 227900
    },
    {
      "epoch": 2.217375236446567,
      "grad_norm": 0.31584182381629944,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.2468,
      "step": 228000
    },
    {
      "epoch": 2.218347767312265,
      "grad_norm": 0.3015693426132202,
      "learning_rate": 9.125e-06,
      "loss": 0.2476,
      "step": 228100
    },
    {
      "epoch": 2.2193202981779634,
      "grad_norm": 0.312208354473114,
      "learning_rate": 9.083333333333333e-06,
      "loss": 0.2471,
      "step": 228200
    },
    {
      "epoch": 2.220292829043662,
      "grad_norm": 0.29653576016426086,
      "learning_rate": 9.041666666666668e-06,
      "loss": 0.2465,
      "step": 228300
    },
    {
      "epoch": 2.22126535990936,
      "grad_norm": 0.30242517590522766,
      "learning_rate": 9e-06,
      "loss": 0.2459,
      "step": 228400
    },
    {
      "epoch": 2.2222378907750584,
      "grad_norm": 0.2890695333480835,
      "learning_rate": 8.958333333333334e-06,
      "loss": 0.247,
      "step": 228500
    },
    {
      "epoch": 2.223210421640757,
      "grad_norm": 0.2899690270423889,
      "learning_rate": 8.916666666666667e-06,
      "loss": 0.2463,
      "step": 228600
    },
    {
      "epoch": 2.2241829525064554,
      "grad_norm": 0.29637610912323,
      "learning_rate": 8.875e-06,
      "loss": 0.2472,
      "step": 228700
    },
    {
      "epoch": 2.2251554833721534,
      "grad_norm": 0.3379298150539398,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.2469,
      "step": 228800
    },
    {
      "epoch": 2.226128014237852,
      "grad_norm": 0.3467751741409302,
      "learning_rate": 8.791666666666667e-06,
      "loss": 0.2467,
      "step": 228900
    },
    {
      "epoch": 2.2271005451035504,
      "grad_norm": 0.33346372842788696,
      "learning_rate": 8.75e-06,
      "loss": 0.2473,
      "step": 229000
    },
    {
      "epoch": 2.2280730759692484,
      "grad_norm": 0.3303225040435791,
      "learning_rate": 8.708333333333334e-06,
      "loss": 0.2455,
      "step": 229100
    },
    {
      "epoch": 2.229045606834947,
      "grad_norm": 0.34702107310295105,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.2466,
      "step": 229200
    },
    {
      "epoch": 2.2300181377006454,
      "grad_norm": 0.35803237557411194,
      "learning_rate": 8.625e-06,
      "loss": 0.2469,
      "step": 229300
    },
    {
      "epoch": 2.2309906685663434,
      "grad_norm": 0.3057006001472473,
      "learning_rate": 8.583333333333333e-06,
      "loss": 0.2468,
      "step": 229400
    },
    {
      "epoch": 2.231963199432042,
      "grad_norm": 0.3063690662384033,
      "learning_rate": 8.541666666666666e-06,
      "loss": 0.2456,
      "step": 229500
    },
    {
      "epoch": 2.2329357302977404,
      "grad_norm": 0.3172744810581207,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.2474,
      "step": 229600
    },
    {
      "epoch": 2.233908261163439,
      "grad_norm": 0.3176746964454651,
      "learning_rate": 8.458333333333333e-06,
      "loss": 0.2471,
      "step": 229700
    },
    {
      "epoch": 2.234880792029137,
      "grad_norm": 0.3285086452960968,
      "learning_rate": 8.416666666666667e-06,
      "loss": 0.2468,
      "step": 229800
    },
    {
      "epoch": 2.2358533228948354,
      "grad_norm": 0.3435315191745758,
      "learning_rate": 8.375e-06,
      "loss": 0.2469,
      "step": 229900
    },
    {
      "epoch": 2.236825853760534,
      "grad_norm": 0.28521907329559326,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.246,
      "step": 230000
    },
    {
      "epoch": 2.236825853760534,
      "eval_loss": 0.2510048449039459,
      "eval_runtime": 2915.4967,
      "eval_samples_per_second": 783.657,
      "eval_steps_per_second": 7.837,
      "step": 230000
    },
    {
      "epoch": 2.237798384626232,
      "grad_norm": 0.31881946325302124,
      "learning_rate": 8.291666666666667e-06,
      "loss": 0.2466,
      "step": 230100
    },
    {
      "epoch": 2.2387709154919304,
      "grad_norm": 0.339891642332077,
      "learning_rate": 8.25e-06,
      "loss": 0.2476,
      "step": 230200
    },
    {
      "epoch": 2.239743446357629,
      "grad_norm": 0.33108532428741455,
      "learning_rate": 8.208333333333332e-06,
      "loss": 0.2468,
      "step": 230300
    },
    {
      "epoch": 2.240715977223327,
      "grad_norm": 0.3360237181186676,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.2472,
      "step": 230400
    },
    {
      "epoch": 2.2416885080890254,
      "grad_norm": 0.3187093436717987,
      "learning_rate": 8.125000000000001e-06,
      "loss": 0.2464,
      "step": 230500
    },
    {
      "epoch": 2.242661038954724,
      "grad_norm": 0.29948657751083374,
      "learning_rate": 8.083333333333333e-06,
      "loss": 0.2469,
      "step": 230600
    },
    {
      "epoch": 2.243633569820422,
      "grad_norm": 0.31514570116996765,
      "learning_rate": 8.041666666666666e-06,
      "loss": 0.2464,
      "step": 230700
    },
    {
      "epoch": 2.2446061006861204,
      "grad_norm": 0.3489866554737091,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.2472,
      "step": 230800
    },
    {
      "epoch": 2.245578631551819,
      "grad_norm": 0.3014008104801178,
      "learning_rate": 7.958333333333335e-06,
      "loss": 0.2472,
      "step": 230900
    },
    {
      "epoch": 2.2465511624175174,
      "grad_norm": 0.346516489982605,
      "learning_rate": 7.916666666666667e-06,
      "loss": 0.2461,
      "step": 231000
    },
    {
      "epoch": 2.2475236932832154,
      "grad_norm": 0.3178921341896057,
      "learning_rate": 7.875e-06,
      "loss": 0.2447,
      "step": 231100
    },
    {
      "epoch": 2.248496224148914,
      "grad_norm": 0.36581411957740784,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.2477,
      "step": 231200
    },
    {
      "epoch": 2.2494687550146124,
      "grad_norm": 0.301627516746521,
      "learning_rate": 7.791666666666667e-06,
      "loss": 0.2476,
      "step": 231300
    },
    {
      "epoch": 2.2504412858803104,
      "grad_norm": 0.29882460832595825,
      "learning_rate": 7.75e-06,
      "loss": 0.2468,
      "step": 231400
    },
    {
      "epoch": 2.251413816746009,
      "grad_norm": 0.3142513930797577,
      "learning_rate": 7.708333333333334e-06,
      "loss": 0.2481,
      "step": 231500
    },
    {
      "epoch": 2.2523863476117074,
      "grad_norm": 0.32526835799217224,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.2469,
      "step": 231600
    },
    {
      "epoch": 2.253358878477406,
      "grad_norm": 0.2966165542602539,
      "learning_rate": 7.625e-06,
      "loss": 0.2468,
      "step": 231700
    },
    {
      "epoch": 2.254331409343104,
      "grad_norm": 0.36824047565460205,
      "learning_rate": 7.583333333333334e-06,
      "loss": 0.2465,
      "step": 231800
    },
    {
      "epoch": 2.2553039402088024,
      "grad_norm": 0.3559720516204834,
      "learning_rate": 7.541666666666668e-06,
      "loss": 0.2473,
      "step": 231900
    },
    {
      "epoch": 2.256276471074501,
      "grad_norm": 0.30645689368247986,
      "learning_rate": 7.5e-06,
      "loss": 0.2469,
      "step": 232000
    },
    {
      "epoch": 2.257249001940199,
      "grad_norm": 0.3080826699733734,
      "learning_rate": 7.458333333333334e-06,
      "loss": 0.2451,
      "step": 232100
    },
    {
      "epoch": 2.2582215328058974,
      "grad_norm": 0.3094707429409027,
      "learning_rate": 7.416666666666668e-06,
      "loss": 0.2459,
      "step": 232200
    },
    {
      "epoch": 2.259194063671596,
      "grad_norm": 0.3291703164577484,
      "learning_rate": 7.375e-06,
      "loss": 0.2461,
      "step": 232300
    },
    {
      "epoch": 2.260166594537294,
      "grad_norm": 0.3301514983177185,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.2465,
      "step": 232400
    },
    {
      "epoch": 2.2611391254029924,
      "grad_norm": 0.3468194603919983,
      "learning_rate": 7.2916666666666674e-06,
      "loss": 0.2467,
      "step": 232500
    },
    {
      "epoch": 2.262111656268691,
      "grad_norm": 0.3208886384963989,
      "learning_rate": 7.25e-06,
      "loss": 0.2467,
      "step": 232600
    },
    {
      "epoch": 2.263084187134389,
      "grad_norm": 0.3046521544456482,
      "learning_rate": 7.2083333333333335e-06,
      "loss": 0.2466,
      "step": 232700
    },
    {
      "epoch": 2.2640567180000875,
      "grad_norm": 0.3051392436027527,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.2469,
      "step": 232800
    },
    {
      "epoch": 2.265029248865786,
      "grad_norm": 0.3433142304420471,
      "learning_rate": 7.1249999999999995e-06,
      "loss": 0.2479,
      "step": 232900
    },
    {
      "epoch": 2.266001779731484,
      "grad_norm": 0.308707594871521,
      "learning_rate": 7.083333333333334e-06,
      "loss": 0.2472,
      "step": 233000
    },
    {
      "epoch": 2.2669743105971825,
      "grad_norm": 0.40542224049568176,
      "learning_rate": 7.041666666666667e-06,
      "loss": 0.246,
      "step": 233100
    },
    {
      "epoch": 2.267946841462881,
      "grad_norm": 0.36521682143211365,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.246,
      "step": 233200
    },
    {
      "epoch": 2.2689193723285794,
      "grad_norm": 0.30538877844810486,
      "learning_rate": 6.958333333333333e-06,
      "loss": 0.2457,
      "step": 233300
    },
    {
      "epoch": 2.2698919031942775,
      "grad_norm": 0.3158034086227417,
      "learning_rate": 6.916666666666667e-06,
      "loss": 0.2462,
      "step": 233400
    },
    {
      "epoch": 2.270864434059976,
      "grad_norm": 0.3420775830745697,
      "learning_rate": 6.875000000000001e-06,
      "loss": 0.2478,
      "step": 233500
    },
    {
      "epoch": 2.2718369649256744,
      "grad_norm": 0.320035457611084,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.248,
      "step": 233600
    },
    {
      "epoch": 2.272809495791373,
      "grad_norm": 0.30825820565223694,
      "learning_rate": 6.791666666666667e-06,
      "loss": 0.2445,
      "step": 233700
    },
    {
      "epoch": 2.273782026657071,
      "grad_norm": 0.3615835905075073,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.2467,
      "step": 233800
    },
    {
      "epoch": 2.2747545575227694,
      "grad_norm": 0.3599953353404999,
      "learning_rate": 6.708333333333333e-06,
      "loss": 0.247,
      "step": 233900
    },
    {
      "epoch": 2.275727088388468,
      "grad_norm": 0.33799439668655396,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.2453,
      "step": 234000
    },
    {
      "epoch": 2.276699619254166,
      "grad_norm": 0.27204975485801697,
      "learning_rate": 6.625000000000001e-06,
      "loss": 0.247,
      "step": 234100
    },
    {
      "epoch": 2.2776721501198645,
      "grad_norm": 0.3404309153556824,
      "learning_rate": 6.583333333333333e-06,
      "loss": 0.2457,
      "step": 234200
    },
    {
      "epoch": 2.278644680985563,
      "grad_norm": 0.35391390323638916,
      "learning_rate": 6.541666666666667e-06,
      "loss": 0.2463,
      "step": 234300
    },
    {
      "epoch": 2.279617211851261,
      "grad_norm": 0.3285747766494751,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.2449,
      "step": 234400
    },
    {
      "epoch": 2.2805897427169595,
      "grad_norm": 0.2991334795951843,
      "learning_rate": 6.458333333333334e-06,
      "loss": 0.2476,
      "step": 234500
    },
    {
      "epoch": 2.281562273582658,
      "grad_norm": 0.31093356013298035,
      "learning_rate": 6.4166666666666665e-06,
      "loss": 0.2476,
      "step": 234600
    },
    {
      "epoch": 2.282534804448356,
      "grad_norm": 0.3385816216468811,
      "learning_rate": 6.375000000000001e-06,
      "loss": 0.2483,
      "step": 234700
    },
    {
      "epoch": 2.2835073353140545,
      "grad_norm": 0.31683430075645447,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.2447,
      "step": 234800
    },
    {
      "epoch": 2.284479866179753,
      "grad_norm": 0.3044261932373047,
      "learning_rate": 6.291666666666667e-06,
      "loss": 0.2461,
      "step": 234900
    },
    {
      "epoch": 2.285452397045451,
      "grad_norm": 0.3199518322944641,
      "learning_rate": 6.25e-06,
      "loss": 0.2445,
      "step": 235000
    },
    {
      "epoch": 2.2864249279111495,
      "grad_norm": 0.33286869525909424,
      "learning_rate": 6.208333333333334e-06,
      "loss": 0.2476,
      "step": 235100
    },
    {
      "epoch": 2.287397458776848,
      "grad_norm": 0.3290618062019348,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.2456,
      "step": 235200
    },
    {
      "epoch": 2.2883699896425465,
      "grad_norm": 0.3469233810901642,
      "learning_rate": 6.125e-06,
      "loss": 0.2458,
      "step": 235300
    },
    {
      "epoch": 2.2893425205082445,
      "grad_norm": 0.339144229888916,
      "learning_rate": 6.083333333333334e-06,
      "loss": 0.2483,
      "step": 235400
    },
    {
      "epoch": 2.290315051373943,
      "grad_norm": 0.3334217965602875,
      "learning_rate": 6.041666666666667e-06,
      "loss": 0.2471,
      "step": 235500
    },
    {
      "epoch": 2.2912875822396415,
      "grad_norm": 0.32132688164711,
      "learning_rate": 6e-06,
      "loss": 0.2471,
      "step": 235600
    },
    {
      "epoch": 2.2922601131053395,
      "grad_norm": 0.32143333554267883,
      "learning_rate": 5.958333333333334e-06,
      "loss": 0.2469,
      "step": 235700
    },
    {
      "epoch": 2.293232643971038,
      "grad_norm": 0.3259904682636261,
      "learning_rate": 5.916666666666667e-06,
      "loss": 0.2455,
      "step": 235800
    },
    {
      "epoch": 2.2942051748367365,
      "grad_norm": 0.3191240727901459,
      "learning_rate": 5.875e-06,
      "loss": 0.2469,
      "step": 235900
    },
    {
      "epoch": 2.295177705702435,
      "grad_norm": 0.32073190808296204,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.2456,
      "step": 236000
    },
    {
      "epoch": 2.296150236568133,
      "grad_norm": 0.29285067319869995,
      "learning_rate": 5.7916666666666666e-06,
      "loss": 0.2462,
      "step": 236100
    },
    {
      "epoch": 2.2971227674338315,
      "grad_norm": 0.32781243324279785,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.2465,
      "step": 236200
    },
    {
      "epoch": 2.29809529829953,
      "grad_norm": 0.313693642616272,
      "learning_rate": 5.7083333333333335e-06,
      "loss": 0.2489,
      "step": 236300
    },
    {
      "epoch": 2.299067829165228,
      "grad_norm": 0.34612175822257996,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.2472,
      "step": 236400
    },
    {
      "epoch": 2.3000403600309265,
      "grad_norm": 0.32701238989830017,
      "learning_rate": 5.625e-06,
      "loss": 0.2448,
      "step": 236500
    },
    {
      "epoch": 2.301012890896625,
      "grad_norm": 0.3659310042858124,
      "learning_rate": 5.583333333333334e-06,
      "loss": 0.2457,
      "step": 236600
    },
    {
      "epoch": 2.301985421762323,
      "grad_norm": 0.3472748100757599,
      "learning_rate": 5.541666666666667e-06,
      "loss": 0.2455,
      "step": 236700
    },
    {
      "epoch": 2.3029579526280215,
      "grad_norm": 0.32035624980926514,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.2466,
      "step": 236800
    },
    {
      "epoch": 2.30393048349372,
      "grad_norm": 0.3137545883655548,
      "learning_rate": 5.458333333333333e-06,
      "loss": 0.2458,
      "step": 236900
    },
    {
      "epoch": 2.304903014359418,
      "grad_norm": 0.3086320161819458,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.2452,
      "step": 237000
    },
    {
      "epoch": 2.3058755452251165,
      "grad_norm": 0.31941360235214233,
      "learning_rate": 5.375e-06,
      "loss": 0.2468,
      "step": 237100
    },
    {
      "epoch": 2.306848076090815,
      "grad_norm": 0.3329659104347229,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.2466,
      "step": 237200
    },
    {
      "epoch": 2.3078206069565135,
      "grad_norm": 0.32081934809684753,
      "learning_rate": 5.291666666666667e-06,
      "loss": 0.2448,
      "step": 237300
    },
    {
      "epoch": 2.3087931378222115,
      "grad_norm": 0.33552271127700806,
      "learning_rate": 5.25e-06,
      "loss": 0.2454,
      "step": 237400
    },
    {
      "epoch": 2.30976566868791,
      "grad_norm": 0.3292774260044098,
      "learning_rate": 5.208333333333334e-06,
      "loss": 0.2453,
      "step": 237500
    },
    {
      "epoch": 2.3107381995536085,
      "grad_norm": 0.3467322885990143,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.2459,
      "step": 237600
    },
    {
      "epoch": 2.3117107304193065,
      "grad_norm": 0.33790484070777893,
      "learning_rate": 5.125e-06,
      "loss": 0.2449,
      "step": 237700
    },
    {
      "epoch": 2.312683261285005,
      "grad_norm": 0.3495756685733795,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.2458,
      "step": 237800
    },
    {
      "epoch": 2.3136557921507035,
      "grad_norm": 0.31035253405570984,
      "learning_rate": 5.041666666666667e-06,
      "loss": 0.2463,
      "step": 237900
    },
    {
      "epoch": 2.314628323016402,
      "grad_norm": 0.32762840390205383,
      "learning_rate": 5e-06,
      "loss": 0.2467,
      "step": 238000
    },
    {
      "epoch": 2.3156008538821,
      "grad_norm": 0.34365054965019226,
      "learning_rate": 4.958333333333334e-06,
      "loss": 0.2465,
      "step": 238100
    },
    {
      "epoch": 2.3165733847477985,
      "grad_norm": 0.3180239200592041,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 0.2457,
      "step": 238200
    },
    {
      "epoch": 2.317545915613497,
      "grad_norm": 0.31155359745025635,
      "learning_rate": 4.875000000000001e-06,
      "loss": 0.2452,
      "step": 238300
    },
    {
      "epoch": 2.318518446479195,
      "grad_norm": 0.3502615690231323,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.2457,
      "step": 238400
    },
    {
      "epoch": 2.3194909773448935,
      "grad_norm": 0.3468746840953827,
      "learning_rate": 4.791666666666667e-06,
      "loss": 0.2448,
      "step": 238500
    },
    {
      "epoch": 2.320463508210592,
      "grad_norm": 0.33167314529418945,
      "learning_rate": 4.75e-06,
      "loss": 0.2465,
      "step": 238600
    },
    {
      "epoch": 2.32143603907629,
      "grad_norm": 0.3132151663303375,
      "learning_rate": 4.708333333333334e-06,
      "loss": 0.2457,
      "step": 238700
    },
    {
      "epoch": 2.3224085699419885,
      "grad_norm": 0.3249841630458832,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.2437,
      "step": 238800
    },
    {
      "epoch": 2.323381100807687,
      "grad_norm": 0.3314627707004547,
      "learning_rate": 4.625e-06,
      "loss": 0.2457,
      "step": 238900
    },
    {
      "epoch": 2.324353631673385,
      "grad_norm": 0.32007378339767456,
      "learning_rate": 4.583333333333333e-06,
      "loss": 0.2451,
      "step": 239000
    },
    {
      "epoch": 2.3253261625390835,
      "grad_norm": 0.334957093000412,
      "learning_rate": 4.541666666666667e-06,
      "loss": 0.2459,
      "step": 239100
    },
    {
      "epoch": 2.326298693404782,
      "grad_norm": 0.3347564935684204,
      "learning_rate": 4.5e-06,
      "loss": 0.2467,
      "step": 239200
    },
    {
      "epoch": 2.32727122427048,
      "grad_norm": 0.31139832735061646,
      "learning_rate": 4.4583333333333336e-06,
      "loss": 0.2455,
      "step": 239300
    },
    {
      "epoch": 2.3282437551361785,
      "grad_norm": 0.3325159549713135,
      "learning_rate": 4.416666666666667e-06,
      "loss": 0.2453,
      "step": 239400
    },
    {
      "epoch": 2.329216286001877,
      "grad_norm": 0.303748220205307,
      "learning_rate": 4.375e-06,
      "loss": 0.2457,
      "step": 239500
    },
    {
      "epoch": 2.3301888168675755,
      "grad_norm": 0.34924963116645813,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.2472,
      "step": 239600
    },
    {
      "epoch": 2.3311613477332735,
      "grad_norm": 0.3151227831840515,
      "learning_rate": 4.2916666666666665e-06,
      "loss": 0.2457,
      "step": 239700
    },
    {
      "epoch": 2.332133878598972,
      "grad_norm": 0.3254547715187073,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.2451,
      "step": 239800
    },
    {
      "epoch": 2.3331064094646705,
      "grad_norm": 0.31405484676361084,
      "learning_rate": 4.208333333333333e-06,
      "loss": 0.2447,
      "step": 239900
    },
    {
      "epoch": 2.3340789403303686,
      "grad_norm": 0.32478949427604675,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.2445,
      "step": 240000
    },
    {
      "epoch": 2.3340789403303686,
      "eval_loss": 0.24981917440891266,
      "eval_runtime": 2913.0945,
      "eval_samples_per_second": 784.303,
      "eval_steps_per_second": 7.843,
      "step": 240000
    },
    {
      "epoch": 2.335051471196067,
      "grad_norm": 0.37571918964385986,
      "learning_rate": 4.125e-06,
      "loss": 0.2449,
      "step": 240100
    },
    {
      "epoch": 2.3360240020617655,
      "grad_norm": 0.3265098035335541,
      "learning_rate": 4.083333333333334e-06,
      "loss": 0.2458,
      "step": 240200
    },
    {
      "epoch": 2.336996532927464,
      "grad_norm": 0.32392528653144836,
      "learning_rate": 4.041666666666666e-06,
      "loss": 0.2459,
      "step": 240300
    },
    {
      "epoch": 2.337969063793162,
      "grad_norm": 0.3282138407230377,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.2468,
      "step": 240400
    },
    {
      "epoch": 2.3389415946588605,
      "grad_norm": 0.3098198175430298,
      "learning_rate": 3.958333333333333e-06,
      "loss": 0.2451,
      "step": 240500
    },
    {
      "epoch": 2.339914125524559,
      "grad_norm": 0.3361474275588989,
      "learning_rate": 3.916666666666667e-06,
      "loss": 0.2447,
      "step": 240600
    },
    {
      "epoch": 2.340886656390257,
      "grad_norm": 0.33555662631988525,
      "learning_rate": 3.875e-06,
      "loss": 0.2455,
      "step": 240700
    },
    {
      "epoch": 2.3418591872559555,
      "grad_norm": 0.3086563050746918,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.2451,
      "step": 240800
    },
    {
      "epoch": 2.342831718121654,
      "grad_norm": 0.32097259163856506,
      "learning_rate": 3.791666666666667e-06,
      "loss": 0.2461,
      "step": 240900
    },
    {
      "epoch": 2.343804248987352,
      "grad_norm": 0.3141642212867737,
      "learning_rate": 3.75e-06,
      "loss": 0.246,
      "step": 241000
    },
    {
      "epoch": 2.3447767798530506,
      "grad_norm": 0.3243330419063568,
      "learning_rate": 3.708333333333334e-06,
      "loss": 0.2451,
      "step": 241100
    },
    {
      "epoch": 2.345749310718749,
      "grad_norm": 0.3094371259212494,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.2474,
      "step": 241200
    },
    {
      "epoch": 2.346721841584447,
      "grad_norm": 0.3331696093082428,
      "learning_rate": 3.625e-06,
      "loss": 0.2454,
      "step": 241300
    },
    {
      "epoch": 2.3476943724501456,
      "grad_norm": 0.3241218030452728,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 0.2448,
      "step": 241400
    },
    {
      "epoch": 2.348666903315844,
      "grad_norm": 0.33641764521598816,
      "learning_rate": 3.541666666666667e-06,
      "loss": 0.2446,
      "step": 241500
    },
    {
      "epoch": 2.3496394341815425,
      "grad_norm": 0.31200361251831055,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.2454,
      "step": 241600
    },
    {
      "epoch": 2.3506119650472406,
      "grad_norm": 0.33255308866500854,
      "learning_rate": 3.4583333333333334e-06,
      "loss": 0.2468,
      "step": 241700
    },
    {
      "epoch": 2.351584495912939,
      "grad_norm": 0.34982529282569885,
      "learning_rate": 3.4166666666666664e-06,
      "loss": 0.2452,
      "step": 241800
    },
    {
      "epoch": 2.3525570267786375,
      "grad_norm": 0.3326180577278137,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 0.2448,
      "step": 241900
    },
    {
      "epoch": 2.3535295576443356,
      "grad_norm": 0.35538432002067566,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.2438,
      "step": 242000
    },
    {
      "epoch": 2.354502088510034,
      "grad_norm": 0.33465299010276794,
      "learning_rate": 3.2916666666666664e-06,
      "loss": 0.2444,
      "step": 242100
    },
    {
      "epoch": 2.3554746193757325,
      "grad_norm": 0.3557606637477875,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 0.2444,
      "step": 242200
    },
    {
      "epoch": 2.356447150241431,
      "grad_norm": 0.32647624611854553,
      "learning_rate": 3.2083333333333332e-06,
      "loss": 0.2463,
      "step": 242300
    },
    {
      "epoch": 2.357419681107129,
      "grad_norm": 0.3607323467731476,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.2466,
      "step": 242400
    },
    {
      "epoch": 2.3583922119728276,
      "grad_norm": 0.34995752573013306,
      "learning_rate": 3.125e-06,
      "loss": 0.2455,
      "step": 242500
    },
    {
      "epoch": 2.359364742838526,
      "grad_norm": 0.3206184506416321,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.2461,
      "step": 242600
    },
    {
      "epoch": 2.360337273704224,
      "grad_norm": 0.346466988325119,
      "learning_rate": 3.041666666666667e-06,
      "loss": 0.2459,
      "step": 242700
    },
    {
      "epoch": 2.3613098045699226,
      "grad_norm": 0.3171654939651489,
      "learning_rate": 3e-06,
      "loss": 0.2432,
      "step": 242800
    },
    {
      "epoch": 2.362282335435621,
      "grad_norm": 0.3381092846393585,
      "learning_rate": 2.9583333333333335e-06,
      "loss": 0.2451,
      "step": 242900
    },
    {
      "epoch": 2.363254866301319,
      "grad_norm": 0.33785879611968994,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.2461,
      "step": 243000
    },
    {
      "epoch": 2.3642273971670176,
      "grad_norm": 0.3443366289138794,
      "learning_rate": 2.8750000000000004e-06,
      "loss": 0.245,
      "step": 243100
    },
    {
      "epoch": 2.365199928032716,
      "grad_norm": 0.3115454614162445,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.2457,
      "step": 243200
    },
    {
      "epoch": 2.366172458898414,
      "grad_norm": 0.31264495849609375,
      "learning_rate": 2.791666666666667e-06,
      "loss": 0.2451,
      "step": 243300
    },
    {
      "epoch": 2.3671449897641126,
      "grad_norm": 0.3095508813858032,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.2445,
      "step": 243400
    },
    {
      "epoch": 2.368117520629811,
      "grad_norm": 0.318979412317276,
      "learning_rate": 2.7083333333333334e-06,
      "loss": 0.244,
      "step": 243500
    },
    {
      "epoch": 2.369090051495509,
      "grad_norm": 0.3393004834651947,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.2461,
      "step": 243600
    },
    {
      "epoch": 2.3700625823612076,
      "grad_norm": 0.3079603314399719,
      "learning_rate": 2.625e-06,
      "loss": 0.2442,
      "step": 243700
    },
    {
      "epoch": 2.371035113226906,
      "grad_norm": 0.31056085228919983,
      "learning_rate": 2.5833333333333333e-06,
      "loss": 0.245,
      "step": 243800
    },
    {
      "epoch": 2.3720076440926046,
      "grad_norm": 0.3399510681629181,
      "learning_rate": 2.5416666666666668e-06,
      "loss": 0.2459,
      "step": 243900
    },
    {
      "epoch": 2.3729801749583026,
      "grad_norm": 0.3105454742908478,
      "learning_rate": 2.5e-06,
      "loss": 0.2445,
      "step": 244000
    },
    {
      "epoch": 2.373952705824001,
      "grad_norm": 0.319153755903244,
      "learning_rate": 2.4583333333333332e-06,
      "loss": 0.2431,
      "step": 244100
    },
    {
      "epoch": 2.3749252366896996,
      "grad_norm": 0.3412023186683655,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.2458,
      "step": 244200
    },
    {
      "epoch": 2.3758977675553976,
      "grad_norm": 0.3044845461845398,
      "learning_rate": 2.375e-06,
      "loss": 0.2457,
      "step": 244300
    },
    {
      "epoch": 2.376870298421096,
      "grad_norm": 0.33099129796028137,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.2445,
      "step": 244400
    },
    {
      "epoch": 2.3778428292867946,
      "grad_norm": 0.31082218885421753,
      "learning_rate": 2.2916666666666666e-06,
      "loss": 0.2457,
      "step": 244500
    },
    {
      "epoch": 2.378815360152493,
      "grad_norm": 0.334079772233963,
      "learning_rate": 2.25e-06,
      "loss": 0.2437,
      "step": 244600
    },
    {
      "epoch": 2.379787891018191,
      "grad_norm": 0.3264886736869812,
      "learning_rate": 2.2083333333333335e-06,
      "loss": 0.2436,
      "step": 244700
    },
    {
      "epoch": 2.3807604218838896,
      "grad_norm": 0.3181847035884857,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.2449,
      "step": 244800
    },
    {
      "epoch": 2.381732952749588,
      "grad_norm": 0.3177994191646576,
      "learning_rate": 2.1250000000000004e-06,
      "loss": 0.2457,
      "step": 244900
    },
    {
      "epoch": 2.382705483615286,
      "grad_norm": 0.3353228271007538,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.2454,
      "step": 245000
    },
    {
      "epoch": 2.3836780144809846,
      "grad_norm": 0.3536393642425537,
      "learning_rate": 2.041666666666667e-06,
      "loss": 0.2443,
      "step": 245100
    },
    {
      "epoch": 2.384650545346683,
      "grad_norm": 0.3637482225894928,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.2445,
      "step": 245200
    },
    {
      "epoch": 2.385623076212381,
      "grad_norm": 0.35153329372406006,
      "learning_rate": 1.9583333333333334e-06,
      "loss": 0.2457,
      "step": 245300
    },
    {
      "epoch": 2.3865956070780796,
      "grad_norm": 0.3165295720100403,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.2456,
      "step": 245400
    },
    {
      "epoch": 2.387568137943778,
      "grad_norm": 0.34375739097595215,
      "learning_rate": 1.875e-06,
      "loss": 0.2452,
      "step": 245500
    },
    {
      "epoch": 2.388540668809476,
      "grad_norm": 0.3526320457458496,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.245,
      "step": 245600
    },
    {
      "epoch": 2.3895131996751746,
      "grad_norm": 0.31446272134780884,
      "learning_rate": 1.7916666666666667e-06,
      "loss": 0.2459,
      "step": 245700
    },
    {
      "epoch": 2.390485730540873,
      "grad_norm": 0.3186013698577881,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.2444,
      "step": 245800
    },
    {
      "epoch": 2.3914582614065716,
      "grad_norm": 0.3423629701137543,
      "learning_rate": 1.7083333333333332e-06,
      "loss": 0.2465,
      "step": 245900
    },
    {
      "epoch": 2.3924307922722696,
      "grad_norm": 0.28528571128845215,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.2445,
      "step": 246000
    },
    {
      "epoch": 2.393403323137968,
      "grad_norm": 0.30391010642051697,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 0.2426,
      "step": 246100
    },
    {
      "epoch": 2.3943758540036666,
      "grad_norm": 0.34280508756637573,
      "learning_rate": 1.5833333333333336e-06,
      "loss": 0.2448,
      "step": 246200
    },
    {
      "epoch": 2.3953483848693646,
      "grad_norm": 0.3514806926250458,
      "learning_rate": 1.5416666666666668e-06,
      "loss": 0.2459,
      "step": 246300
    },
    {
      "epoch": 2.396320915735063,
      "grad_norm": 0.3155166208744049,
      "learning_rate": 1.5e-06,
      "loss": 0.2452,
      "step": 246400
    },
    {
      "epoch": 2.3972934466007616,
      "grad_norm": 0.3280225098133087,
      "learning_rate": 1.4583333333333335e-06,
      "loss": 0.2448,
      "step": 246500
    },
    {
      "epoch": 2.39826597746646,
      "grad_norm": 0.3533461093902588,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 0.2443,
      "step": 246600
    },
    {
      "epoch": 2.399238508332158,
      "grad_norm": 0.3118663430213928,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 0.2448,
      "step": 246700
    },
    {
      "epoch": 2.4002110391978566,
      "grad_norm": 0.2936462461948395,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.2443,
      "step": 246800
    },
    {
      "epoch": 2.401183570063555,
      "grad_norm": 0.33887696266174316,
      "learning_rate": 1.2916666666666667e-06,
      "loss": 0.2448,
      "step": 246900
    },
    {
      "epoch": 2.402156100929253,
      "grad_norm": 0.3094359040260315,
      "learning_rate": 1.25e-06,
      "loss": 0.2476,
      "step": 247000
    },
    {
      "epoch": 2.4031286317949516,
      "grad_norm": 0.3140760660171509,
      "learning_rate": 1.2083333333333333e-06,
      "loss": 0.2447,
      "step": 247100
    },
    {
      "epoch": 2.40410116266065,
      "grad_norm": 0.28727808594703674,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.2457,
      "step": 247200
    },
    {
      "epoch": 2.405073693526348,
      "grad_norm": 0.3265096843242645,
      "learning_rate": 1.125e-06,
      "loss": 0.2449,
      "step": 247300
    },
    {
      "epoch": 2.4060462243920466,
      "grad_norm": 0.3235945701599121,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 0.2466,
      "step": 247400
    },
    {
      "epoch": 2.407018755257745,
      "grad_norm": 0.32351261377334595,
      "learning_rate": 1.0416666666666667e-06,
      "loss": 0.2458,
      "step": 247500
    },
    {
      "epoch": 2.407991286123443,
      "grad_norm": 0.3156859576702118,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.2445,
      "step": 247600
    },
    {
      "epoch": 2.4089638169891416,
      "grad_norm": 0.32230448722839355,
      "learning_rate": 9.583333333333334e-07,
      "loss": 0.2461,
      "step": 247700
    },
    {
      "epoch": 2.40993634785484,
      "grad_norm": 0.3206712603569031,
      "learning_rate": 9.166666666666667e-07,
      "loss": 0.2446,
      "step": 247800
    },
    {
      "epoch": 2.410908878720538,
      "grad_norm": 0.31927254796028137,
      "learning_rate": 8.750000000000001e-07,
      "loss": 0.244,
      "step": 247900
    },
    {
      "epoch": 2.4118814095862366,
      "grad_norm": 0.321722149848938,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.2455,
      "step": 248000
    },
    {
      "epoch": 2.412853940451935,
      "grad_norm": 0.32051169872283936,
      "learning_rate": 7.916666666666668e-07,
      "loss": 0.2452,
      "step": 248100
    },
    {
      "epoch": 2.4138264713176336,
      "grad_norm": 0.3561771512031555,
      "learning_rate": 7.5e-07,
      "loss": 0.2446,
      "step": 248200
    },
    {
      "epoch": 2.4147990021833317,
      "grad_norm": 0.30285361409187317,
      "learning_rate": 7.083333333333334e-07,
      "loss": 0.2443,
      "step": 248300
    },
    {
      "epoch": 2.41577153304903,
      "grad_norm": 0.3076382279396057,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.245,
      "step": 248400
    },
    {
      "epoch": 2.4167440639147286,
      "grad_norm": 0.32523536682128906,
      "learning_rate": 6.25e-07,
      "loss": 0.2455,
      "step": 248500
    },
    {
      "epoch": 2.4177165947804267,
      "grad_norm": 0.31538090109825134,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.2461,
      "step": 248600
    },
    {
      "epoch": 2.418689125646125,
      "grad_norm": 0.3333967328071594,
      "learning_rate": 5.416666666666667e-07,
      "loss": 0.2456,
      "step": 248700
    },
    {
      "epoch": 2.4196616565118236,
      "grad_norm": 0.3182865381240845,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.2436,
      "step": 248800
    },
    {
      "epoch": 2.420634187377522,
      "grad_norm": 0.32444703578948975,
      "learning_rate": 4.583333333333334e-07,
      "loss": 0.2457,
      "step": 248900
    },
    {
      "epoch": 2.42160671824322,
      "grad_norm": 0.3150453567504883,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.244,
      "step": 249000
    },
    {
      "epoch": 2.4225792491089186,
      "grad_norm": 0.3153004050254822,
      "learning_rate": 3.75e-07,
      "loss": 0.244,
      "step": 249100
    },
    {
      "epoch": 2.423551779974617,
      "grad_norm": 0.3373783528804779,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.2459,
      "step": 249200
    },
    {
      "epoch": 2.424524310840315,
      "grad_norm": 0.3226148188114166,
      "learning_rate": 2.916666666666667e-07,
      "loss": 0.2453,
      "step": 249300
    },
    {
      "epoch": 2.4254968417060137,
      "grad_norm": 0.37006932497024536,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 0.2444,
      "step": 249400
    },
    {
      "epoch": 2.426469372571712,
      "grad_norm": 0.31041720509529114,
      "learning_rate": 2.0833333333333333e-07,
      "loss": 0.2445,
      "step": 249500
    },
    {
      "epoch": 2.42744190343741,
      "grad_norm": 0.3306181728839874,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.2437,
      "step": 249600
    },
    {
      "epoch": 2.4284144343031087,
      "grad_norm": 0.31956684589385986,
      "learning_rate": 1.2500000000000002e-07,
      "loss": 0.2466,
      "step": 249700
    },
    {
      "epoch": 2.429386965168807,
      "grad_norm": 0.3294055163860321,
      "learning_rate": 8.333333333333334e-08,
      "loss": 0.2446,
      "step": 249800
    },
    {
      "epoch": 2.430359496034505,
      "grad_norm": 0.2986252009868622,
      "learning_rate": 4.166666666666667e-08,
      "loss": 0.2442,
      "step": 249900
    },
    {
      "epoch": 2.4313320269002037,
      "grad_norm": 0.3157273232936859,
      "learning_rate": 0.0,
      "loss": 0.2456,
      "step": 250000
    },
    {
      "epoch": 2.4313320269002037,
      "eval_loss": 0.24910381436347961,
      "eval_runtime": 2913.0133,
      "eval_samples_per_second": 784.325,
      "eval_steps_per_second": 7.843,
      "step": 250000
    },
    {
      "epoch": 2.4313320269002037,
      "step": 250000,
      "total_flos": 0.0,
      "train_loss": 0.06998486274719239,
      "train_runtime": 61496.59,
      "train_samples_per_second": 813.053,
      "train_steps_per_second": 4.065
    }
  ],
  "logging_steps": 100,
  "max_steps": 250000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 100,
  "trial_name": null,
  "trial_params": null
}
