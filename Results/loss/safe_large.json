{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.069783952268185,
  "eval_steps": 10000,
  "global_step": 250000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 9.725308656983501e-06,
      "grad_norm": 64.6833267211914,
      "learning_rate": 1e-08,
      "loss": 7.69719893060506,
      "step": 1
    },
    {
      "epoch": 0.0009725308656983501,
      "grad_norm": 5.014211177825928,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 5.599122208645382,
      "step": 100
    },
    {
      "epoch": 0.0019450617313967002,
      "grad_norm": 5.1709465980529785,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.120113300262653,
      "step": 200
    },
    {
      "epoch": 0.00291759259709505,
      "grad_norm": 5.046287536621094,
      "learning_rate": 3e-06,
      "loss": 2.439298326065981,
      "step": 300
    },
    {
      "epoch": 0.0038901234627934005,
      "grad_norm": 8.534097671508789,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.1535803120156887,
      "step": 400
    },
    {
      "epoch": 0.00486265432849175,
      "grad_norm": 6.799100875854492,
      "learning_rate": 5e-06,
      "loss": 1.9190666799116107,
      "step": 500
    },
    {
      "epoch": 0.0058351851941901,
      "grad_norm": 9.85946273803711,
      "learning_rate": 6e-06,
      "loss": 1.704557896568564,
      "step": 600
    },
    {
      "epoch": 0.006807716059888451,
      "grad_norm": 7.399956226348877,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.536550551181868,
      "step": 700
    },
    {
      "epoch": 0.007780246925586801,
      "grad_norm": 12.306794166564941,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.41364286124138,
      "step": 800
    },
    {
      "epoch": 0.008752777791285152,
      "grad_norm": 8.758864402770996,
      "learning_rate": 9e-06,
      "loss": 1.3147355853211746,
      "step": 900
    },
    {
      "epoch": 0.0097253086569835,
      "grad_norm": 8.193803787231445,
      "learning_rate": 1e-05,
      "loss": 1.249226417133771,
      "step": 1000
    },
    {
      "epoch": 0.010697839522681852,
      "grad_norm": 8.987944602966309,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.1842189925552762,
      "step": 1100
    },
    {
      "epoch": 0.0116703703883802,
      "grad_norm": 9.413154602050781,
      "learning_rate": 1.2e-05,
      "loss": 1.1286118069415085,
      "step": 1200
    },
    {
      "epoch": 0.012642901254078551,
      "grad_norm": 5.977198600769043,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.0866037099273866,
      "step": 1300
    },
    {
      "epoch": 0.013615432119776902,
      "grad_norm": 7.044963359832764,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.0465963909379574,
      "step": 1400
    },
    {
      "epoch": 0.014587962985475251,
      "grad_norm": 7.949780464172363,
      "learning_rate": 1.5e-05,
      "loss": 1.0187876409388257,
      "step": 1500
    },
    {
      "epoch": 0.015560493851173602,
      "grad_norm": 9.82184886932373,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.9842811527686217,
      "step": 1600
    },
    {
      "epoch": 0.01653302471687195,
      "grad_norm": 5.2906813621521,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.9561741116451541,
      "step": 1700
    },
    {
      "epoch": 0.017505555582570304,
      "grad_norm": 6.0557661056518555,
      "learning_rate": 1.8e-05,
      "loss": 0.9287676762694828,
      "step": 1800
    },
    {
      "epoch": 0.018478086448268653,
      "grad_norm": 6.862719535827637,
      "learning_rate": 1.9e-05,
      "loss": 0.9087600488488361,
      "step": 1900
    },
    {
      "epoch": 0.019450617313967,
      "grad_norm": 5.2972588539123535,
      "learning_rate": 2e-05,
      "loss": 0.887253393959055,
      "step": 2000
    },
    {
      "epoch": 0.02042314817966535,
      "grad_norm": 4.700634002685547,
      "learning_rate": 2.1e-05,
      "loss": 0.8659472781292397,
      "step": 2100
    },
    {
      "epoch": 0.021395679045363703,
      "grad_norm": 3.98956036567688,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.8495402564563893,
      "step": 2200
    },
    {
      "epoch": 0.022368209911062052,
      "grad_norm": 5.060997486114502,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.8318341058933286,
      "step": 2300
    },
    {
      "epoch": 0.0233407407767604,
      "grad_norm": 3.2420268058776855,
      "learning_rate": 2.4e-05,
      "loss": 0.8172274134916429,
      "step": 2400
    },
    {
      "epoch": 0.024313271642458754,
      "grad_norm": 4.312952995300293,
      "learning_rate": 2.5e-05,
      "loss": 0.8039206752420734,
      "step": 2500
    },
    {
      "epoch": 0.025285802508157103,
      "grad_norm": 3.3811893463134766,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.7901144871671079,
      "step": 2600
    },
    {
      "epoch": 0.026258333373855452,
      "grad_norm": 4.512073993682861,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.7793075571900249,
      "step": 2700
    },
    {
      "epoch": 0.027230864239553804,
      "grad_norm": 3.416520833969116,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.7699003826908957,
      "step": 2800
    },
    {
      "epoch": 0.028203395105252153,
      "grad_norm": 2.6381702423095703,
      "learning_rate": 2.9e-05,
      "loss": 0.759193993163108,
      "step": 2900
    },
    {
      "epoch": 0.029175925970950502,
      "grad_norm": 2.693751811981201,
      "learning_rate": 3e-05,
      "loss": 0.752686150396341,
      "step": 3000
    },
    {
      "epoch": 0.03014845683664885,
      "grad_norm": 3.016892433166504,
      "learning_rate": 3.1e-05,
      "loss": 0.7415804694267556,
      "step": 3100
    },
    {
      "epoch": 0.031120987702347204,
      "grad_norm": 2.381049394607544,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.7338735852975562,
      "step": 3200
    },
    {
      "epoch": 0.032093518568045556,
      "grad_norm": 2.168074131011963,
      "learning_rate": 3.3e-05,
      "loss": 0.7271664566463107,
      "step": 3300
    },
    {
      "epoch": 0.0330660494337439,
      "grad_norm": 2.006967782974243,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.7180606478583829,
      "step": 3400
    },
    {
      "epoch": 0.034038580299442255,
      "grad_norm": 2.4366564750671387,
      "learning_rate": 3.5e-05,
      "loss": 0.7125533413729218,
      "step": 3500
    },
    {
      "epoch": 0.03501111116514061,
      "grad_norm": 1.921441912651062,
      "learning_rate": 3.6e-05,
      "loss": 0.7093450373486586,
      "step": 3600
    },
    {
      "epoch": 0.03598364203083895,
      "grad_norm": 1.495890498161316,
      "learning_rate": 3.7e-05,
      "loss": 0.6998400607692858,
      "step": 3700
    },
    {
      "epoch": 0.036956172896537305,
      "grad_norm": 1.4245883226394653,
      "learning_rate": 3.8e-05,
      "loss": 0.6972317053398196,
      "step": 3800
    },
    {
      "epoch": 0.03792870376223565,
      "grad_norm": 1.6564841270446777,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.6902258062454543,
      "step": 3900
    },
    {
      "epoch": 0.038901234627934,
      "grad_norm": 1.5618265867233276,
      "learning_rate": 4e-05,
      "loss": 0.6844194347789542,
      "step": 4000
    },
    {
      "epoch": 0.039873765493632356,
      "grad_norm": 1.6293443441390991,
      "learning_rate": 4.1e-05,
      "loss": 0.6794127687745346,
      "step": 4100
    },
    {
      "epoch": 0.0408462963593307,
      "grad_norm": 1.6215664148330688,
      "learning_rate": 4.2e-05,
      "loss": 0.6790035575179068,
      "step": 4200
    },
    {
      "epoch": 0.041818827225029054,
      "grad_norm": 1.0775853395462036,
      "learning_rate": 4.3e-05,
      "loss": 0.6726978821056405,
      "step": 4300
    },
    {
      "epoch": 0.042791358090727406,
      "grad_norm": 1.3254491090774536,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.6684910980081874,
      "step": 4400
    },
    {
      "epoch": 0.04376388895642575,
      "grad_norm": 1.1923505067825317,
      "learning_rate": 4.5e-05,
      "loss": 0.6638846806938035,
      "step": 4500
    },
    {
      "epoch": 0.044736419822124104,
      "grad_norm": 1.317108154296875,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6605775603785369,
      "step": 4600
    },
    {
      "epoch": 0.04570895068782246,
      "grad_norm": 1.184264063835144,
      "learning_rate": 4.7e-05,
      "loss": 0.6564710541470453,
      "step": 4700
    },
    {
      "epoch": 0.0466814815535208,
      "grad_norm": 1.2269423007965088,
      "learning_rate": 4.8e-05,
      "loss": 0.6532640616501209,
      "step": 4800
    },
    {
      "epoch": 0.047654012419219155,
      "grad_norm": 1.3164294958114624,
      "learning_rate": 4.9e-05,
      "loss": 0.6476587919221586,
      "step": 4900
    },
    {
      "epoch": 0.04862654328491751,
      "grad_norm": 1.2390860319137573,
      "learning_rate": 5e-05,
      "loss": 0.6461508296730282,
      "step": 5000
    },
    {
      "epoch": 0.04959907415061585,
      "grad_norm": 1.2215869426727295,
      "learning_rate": 5.1000000000000006e-05,
      "loss": 0.6443431216712526,
      "step": 5100
    },
    {
      "epoch": 0.050571605016314206,
      "grad_norm": 1.0996086597442627,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.638638281246201,
      "step": 5200
    },
    {
      "epoch": 0.05154413588201256,
      "grad_norm": 0.9687982201576233,
      "learning_rate": 5.300000000000001e-05,
      "loss": 0.6362311692669133,
      "step": 5300
    },
    {
      "epoch": 0.052516666747710904,
      "grad_norm": 0.8808282613754272,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.6359225484754536,
      "step": 5400
    },
    {
      "epoch": 0.053489197613409256,
      "grad_norm": 0.9376997351646423,
      "learning_rate": 5.500000000000001e-05,
      "loss": 0.6325163048272959,
      "step": 5500
    },
    {
      "epoch": 0.05446172847910761,
      "grad_norm": 0.8782524466514587,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.6293100000486266,
      "step": 5600
    },
    {
      "epoch": 0.055434259344805954,
      "grad_norm": 1.4556019306182861,
      "learning_rate": 5.6999999999999996e-05,
      "loss": 0.6237056847902425,
      "step": 5700
    },
    {
      "epoch": 0.05640679021050431,
      "grad_norm": 1.1644368171691895,
      "learning_rate": 5.8e-05,
      "loss": 0.6227977378237398,
      "step": 5800
    },
    {
      "epoch": 0.05737932107620266,
      "grad_norm": 0.7296381592750549,
      "learning_rate": 5.9e-05,
      "loss": 0.6215900617765499,
      "step": 5900
    },
    {
      "epoch": 0.058351851941901005,
      "grad_norm": 0.9351217150688171,
      "learning_rate": 6e-05,
      "loss": 0.6183840862690451,
      "step": 6000
    },
    {
      "epoch": 0.05932438280759936,
      "grad_norm": 0.7304871082305908,
      "learning_rate": 6.1e-05,
      "loss": 0.6181756572050991,
      "step": 6100
    },
    {
      "epoch": 0.0602969136732977,
      "grad_norm": 0.9230391979217529,
      "learning_rate": 6.2e-05,
      "loss": 0.6138707653748446,
      "step": 6200
    },
    {
      "epoch": 0.061269444538996055,
      "grad_norm": 0.8013409376144409,
      "learning_rate": 6.3e-05,
      "loss": 0.6125633671936163,
      "step": 6300
    },
    {
      "epoch": 0.06224197540469441,
      "grad_norm": 1.3381892442703247,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.6111560940520707,
      "step": 6400
    },
    {
      "epoch": 0.06321450627039275,
      "grad_norm": 0.8822233080863953,
      "learning_rate": 6.500000000000001e-05,
      "loss": 0.60854994348901,
      "step": 6500
    },
    {
      "epoch": 0.06418703713609111,
      "grad_norm": 1.0786831378936768,
      "learning_rate": 6.6e-05,
      "loss": 0.6047449655203074,
      "step": 6600
    },
    {
      "epoch": 0.06515956800178946,
      "grad_norm": 0.6606298685073853,
      "learning_rate": 6.7e-05,
      "loss": 0.6063350665454245,
      "step": 6700
    },
    {
      "epoch": 0.0661320988674878,
      "grad_norm": 1.0113170146942139,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.6014312639497396,
      "step": 6800
    },
    {
      "epoch": 0.06710462973318616,
      "grad_norm": 0.8099570274353027,
      "learning_rate": 6.9e-05,
      "loss": 0.6009233795030784,
      "step": 6900
    },
    {
      "epoch": 0.06807716059888451,
      "grad_norm": 0.7136301398277283,
      "learning_rate": 7e-05,
      "loss": 0.5998160924682348,
      "step": 7000
    },
    {
      "epoch": 0.06904969146458285,
      "grad_norm": 0.6670240759849548,
      "learning_rate": 7.1e-05,
      "loss": 0.5966109074893906,
      "step": 7100
    },
    {
      "epoch": 0.07002222233028121,
      "grad_norm": 0.6445488929748535,
      "learning_rate": 7.2e-05,
      "loss": 0.5946046110467003,
      "step": 7200
    },
    {
      "epoch": 0.07099475319597956,
      "grad_norm": 0.7234918475151062,
      "learning_rate": 7.3e-05,
      "loss": 0.5947961389135302,
      "step": 7300
    },
    {
      "epoch": 0.0719672840616779,
      "grad_norm": 0.7653946280479431,
      "learning_rate": 7.4e-05,
      "loss": 0.5922904371039975,
      "step": 7400
    },
    {
      "epoch": 0.07293981492737625,
      "grad_norm": 0.6087489724159241,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.5899845963614843,
      "step": 7500
    },
    {
      "epoch": 0.07391234579307461,
      "grad_norm": 0.670348584651947,
      "learning_rate": 7.6e-05,
      "loss": 0.5884779748156186,
      "step": 7600
    },
    {
      "epoch": 0.07488487665877296,
      "grad_norm": 0.8336578607559204,
      "learning_rate": 7.7e-05,
      "loss": 0.5882700042305092,
      "step": 7700
    },
    {
      "epoch": 0.0758574075244713,
      "grad_norm": 0.7040469646453857,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.5870631228799695,
      "step": 7800
    },
    {
      "epoch": 0.07682993839016966,
      "grad_norm": 0.6776348948478699,
      "learning_rate": 7.900000000000001e-05,
      "loss": 0.5849572626868388,
      "step": 7900
    },
    {
      "epoch": 0.077802469255868,
      "grad_norm": 0.6432666182518005,
      "learning_rate": 8e-05,
      "loss": 0.5844496825034056,
      "step": 8000
    },
    {
      "epoch": 0.07877500012156635,
      "grad_norm": 0.7013528347015381,
      "learning_rate": 8.1e-05,
      "loss": 0.5844415535346981,
      "step": 8100
    },
    {
      "epoch": 0.07974753098726471,
      "grad_norm": 0.5644661784172058,
      "learning_rate": 8.2e-05,
      "loss": 0.582435703066876,
      "step": 8200
    },
    {
      "epoch": 0.08072006185296306,
      "grad_norm": 0.7084136605262756,
      "learning_rate": 8.3e-05,
      "loss": 0.5799304847441167,
      "step": 8300
    },
    {
      "epoch": 0.0816925927186614,
      "grad_norm": 1.1265673637390137,
      "learning_rate": 8.4e-05,
      "loss": 0.5816204343322846,
      "step": 8400
    },
    {
      "epoch": 0.08266512358435976,
      "grad_norm": 0.6951225399971008,
      "learning_rate": 8.5e-05,
      "loss": 0.5764184851025638,
      "step": 8500
    },
    {
      "epoch": 0.08363765445005811,
      "grad_norm": 0.611601710319519,
      "learning_rate": 8.6e-05,
      "loss": 0.5761108257273315,
      "step": 8600
    },
    {
      "epoch": 0.08461018531575645,
      "grad_norm": 0.5386055111885071,
      "learning_rate": 8.7e-05,
      "loss": 0.575203899918238,
      "step": 8700
    },
    {
      "epoch": 0.08558271618145481,
      "grad_norm": 0.533259928226471,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.5760947984186647,
      "step": 8800
    },
    {
      "epoch": 0.08655524704715316,
      "grad_norm": 0.6646748781204224,
      "learning_rate": 8.900000000000001e-05,
      "loss": 0.5747883922189194,
      "step": 8900
    },
    {
      "epoch": 0.0875277779128515,
      "grad_norm": 0.5444666743278503,
      "learning_rate": 9e-05,
      "loss": 0.5729826473401628,
      "step": 9000
    },
    {
      "epoch": 0.08850030877854986,
      "grad_norm": 0.5413150787353516,
      "learning_rate": 9.1e-05,
      "loss": 0.5712768260482667,
      "step": 9100
    },
    {
      "epoch": 0.08947283964424821,
      "grad_norm": 0.5529418587684631,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.5722676008983406,
      "step": 9200
    },
    {
      "epoch": 0.09044537050994655,
      "grad_norm": 0.7584314942359924,
      "learning_rate": 9.300000000000001e-05,
      "loss": 0.5705618365689668,
      "step": 9300
    },
    {
      "epoch": 0.09141790137564491,
      "grad_norm": 0.6077556610107422,
      "learning_rate": 9.4e-05,
      "loss": 0.5669586008198435,
      "step": 9400
    },
    {
      "epoch": 0.09239043224134326,
      "grad_norm": 0.5240262150764465,
      "learning_rate": 9.5e-05,
      "loss": 0.569747017977233,
      "step": 9500
    },
    {
      "epoch": 0.0933629631070416,
      "grad_norm": 0.5175920128822327,
      "learning_rate": 9.6e-05,
      "loss": 0.5684408257342782,
      "step": 9600
    },
    {
      "epoch": 0.09433549397273996,
      "grad_norm": 0.5856274962425232,
      "learning_rate": 9.7e-05,
      "loss": 0.5649376344437089,
      "step": 9700
    },
    {
      "epoch": 0.09530802483843831,
      "grad_norm": 0.5137988328933716,
      "learning_rate": 9.8e-05,
      "loss": 0.5652293665420206,
      "step": 9800
    },
    {
      "epoch": 0.09628055570413666,
      "grad_norm": 0.5384404063224792,
      "learning_rate": 9.900000000000001e-05,
      "loss": 0.5631243913519777,
      "step": 9900
    },
    {
      "epoch": 0.09725308656983501,
      "grad_norm": 0.5194739103317261,
      "learning_rate": 0.0001,
      "loss": 0.5637157233233047,
      "step": 10000
    },
    {
      "epoch": 0.09725308656983501,
      "eval_accuracy": 0.6481953157704002,
      "eval_loss": 0.5479408044399364,
      "eval_runtime": 3705.0107,
      "eval_samples_per_second": 616.665,
      "eval_steps_per_second": 6.167,
      "step": 10000
    },
    {
      "epoch": 0.09822561743553336,
      "grad_norm": 0.4869207441806793,
      "learning_rate": 9.9989898989899e-05,
      "loss": 0.563508161201159,
      "step": 10100
    },
    {
      "epoch": 0.0991981483012317,
      "grad_norm": 0.48197534680366516,
      "learning_rate": 9.997979797979799e-05,
      "loss": 0.5595059896786688,
      "step": 10200
    },
    {
      "epoch": 0.10017067916693007,
      "grad_norm": 0.5691006183624268,
      "learning_rate": 9.996969696969698e-05,
      "loss": 0.5595980621627836,
      "step": 10300
    },
    {
      "epoch": 0.10114321003262841,
      "grad_norm": 0.47706490755081177,
      "learning_rate": 9.995959595959596e-05,
      "loss": 0.559290709829439,
      "step": 10400
    },
    {
      "epoch": 0.10211574089832676,
      "grad_norm": 0.47567734122276306,
      "learning_rate": 9.994949494949496e-05,
      "loss": 0.5578849705080015,
      "step": 10500
    },
    {
      "epoch": 0.10308827176402512,
      "grad_norm": 0.5282366871833801,
      "learning_rate": 9.993939393939394e-05,
      "loss": 0.5575776501292424,
      "step": 10600
    },
    {
      "epoch": 0.10406080262972346,
      "grad_norm": 0.45336759090423584,
      "learning_rate": 9.992929292929294e-05,
      "loss": 0.5554730139356726,
      "step": 10700
    },
    {
      "epoch": 0.10503333349542181,
      "grad_norm": 0.4558686316013336,
      "learning_rate": 9.991919191919193e-05,
      "loss": 0.5543669356177621,
      "step": 10800
    },
    {
      "epoch": 0.10600586436112017,
      "grad_norm": 0.530484139919281,
      "learning_rate": 9.990909090909092e-05,
      "loss": 0.5542593734956164,
      "step": 10900
    },
    {
      "epoch": 0.10697839522681851,
      "grad_norm": 0.5788471102714539,
      "learning_rate": 9.98989898989899e-05,
      "loss": 0.5537524254572458,
      "step": 11000
    },
    {
      "epoch": 0.10795092609251686,
      "grad_norm": 0.46602970361709595,
      "learning_rate": 9.98888888888889e-05,
      "loss": 0.5491518141521302,
      "step": 11100
    },
    {
      "epoch": 0.10892345695821522,
      "grad_norm": 0.4274083375930786,
      "learning_rate": 9.987878787878788e-05,
      "loss": 0.5494437060233699,
      "step": 11200
    },
    {
      "epoch": 0.10989598782391356,
      "grad_norm": 0.6842690706253052,
      "learning_rate": 9.986868686868687e-05,
      "loss": 0.5478385724497288,
      "step": 11300
    },
    {
      "epoch": 0.11086851868961191,
      "grad_norm": 0.4098871648311615,
      "learning_rate": 9.985858585858587e-05,
      "loss": 0.5478309491970715,
      "step": 11400
    },
    {
      "epoch": 0.11184104955531027,
      "grad_norm": 0.5007039904594421,
      "learning_rate": 9.984848484848486e-05,
      "loss": 0.547324124809054,
      "step": 11500
    },
    {
      "epoch": 0.11281358042100861,
      "grad_norm": 0.43691399693489075,
      "learning_rate": 9.983838383838384e-05,
      "loss": 0.5463181201256232,
      "step": 11600
    },
    {
      "epoch": 0.11378611128670696,
      "grad_norm": 0.41120219230651855,
      "learning_rate": 9.982828282828284e-05,
      "loss": 0.5458113304708507,
      "step": 11700
    },
    {
      "epoch": 0.11475864215240532,
      "grad_norm": 0.5135898590087891,
      "learning_rate": 9.981818181818182e-05,
      "loss": 0.5437071777640542,
      "step": 11800
    },
    {
      "epoch": 0.11573117301810366,
      "grad_norm": 0.48171988129615784,
      "learning_rate": 9.980808080808081e-05,
      "loss": 0.5437994461436719,
      "step": 11900
    },
    {
      "epoch": 0.11670370388380201,
      "grad_norm": 0.40800270438194275,
      "learning_rate": 9.97979797979798e-05,
      "loss": 0.5427935456599767,
      "step": 12000
    },
    {
      "epoch": 0.11767623474950037,
      "grad_norm": 0.44523152709007263,
      "learning_rate": 9.97878787878788e-05,
      "loss": 0.5428858237649031,
      "step": 12100
    },
    {
      "epoch": 0.11864876561519871,
      "grad_norm": 0.4116867184638977,
      "learning_rate": 9.977777777777779e-05,
      "loss": 0.5413808110629553,
      "step": 12200
    },
    {
      "epoch": 0.11962129648089706,
      "grad_norm": 0.36469656229019165,
      "learning_rate": 9.976767676767678e-05,
      "loss": 0.5427708843014762,
      "step": 12300
    },
    {
      "epoch": 0.1205938273465954,
      "grad_norm": 0.4034406244754791,
      "learning_rate": 9.975757575757576e-05,
      "loss": 0.5400679819915071,
      "step": 12400
    },
    {
      "epoch": 0.12156635821229377,
      "grad_norm": 0.5052666664123535,
      "learning_rate": 9.974747474747475e-05,
      "loss": 0.5379641127079914,
      "step": 12500
    },
    {
      "epoch": 0.12253888907799211,
      "grad_norm": 0.363688588142395,
      "learning_rate": 9.973737373737374e-05,
      "loss": 0.5388550501096528,
      "step": 12600
    },
    {
      "epoch": 0.12351141994369046,
      "grad_norm": 0.4446203112602234,
      "learning_rate": 9.972727272727273e-05,
      "loss": 0.5352539025232314,
      "step": 12700
    },
    {
      "epoch": 0.12448395080938882,
      "grad_norm": 0.42950063943862915,
      "learning_rate": 9.971717171717173e-05,
      "loss": 0.5364443189262148,
      "step": 12800
    },
    {
      "epoch": 0.12545648167508716,
      "grad_norm": 0.3937179446220398,
      "learning_rate": 9.970707070707072e-05,
      "loss": 0.5362372111149163,
      "step": 12900
    },
    {
      "epoch": 0.1264290125407855,
      "grad_norm": 0.3817354738712311,
      "learning_rate": 9.96969696969697e-05,
      "loss": 0.5350319149896877,
      "step": 13000
    },
    {
      "epoch": 0.12740154340648385,
      "grad_norm": 0.44148460030555725,
      "learning_rate": 9.968686868686869e-05,
      "loss": 0.5347250141885306,
      "step": 13100
    },
    {
      "epoch": 0.12837407427218223,
      "grad_norm": 0.37998032569885254,
      "learning_rate": 9.967676767676768e-05,
      "loss": 0.5351168379826098,
      "step": 13200
    },
    {
      "epoch": 0.12934660513788057,
      "grad_norm": 0.4094431400299072,
      "learning_rate": 9.966666666666667e-05,
      "loss": 0.5338117919367467,
      "step": 13300
    },
    {
      "epoch": 0.13031913600357892,
      "grad_norm": 0.43352606892585754,
      "learning_rate": 9.965656565656566e-05,
      "loss": 0.5331056649922927,
      "step": 13400
    },
    {
      "epoch": 0.13129166686927726,
      "grad_norm": 0.40545547008514404,
      "learning_rate": 9.964646464646466e-05,
      "loss": 0.5297046216056067,
      "step": 13500
    },
    {
      "epoch": 0.1322641977349756,
      "grad_norm": 0.36964157223701477,
      "learning_rate": 9.963636363636363e-05,
      "loss": 0.5303959257903376,
      "step": 13600
    },
    {
      "epoch": 0.13323672860067395,
      "grad_norm": 0.42794084548950195,
      "learning_rate": 9.962626262626264e-05,
      "loss": 0.5293904462735743,
      "step": 13700
    },
    {
      "epoch": 0.13420925946637233,
      "grad_norm": 0.4231896996498108,
      "learning_rate": 9.961616161616162e-05,
      "loss": 0.5299819269031908,
      "step": 13800
    },
    {
      "epoch": 0.13518179033207067,
      "grad_norm": 0.3509965240955353,
      "learning_rate": 9.960606060606061e-05,
      "loss": 0.5297749357955963,
      "step": 13900
    },
    {
      "epoch": 0.13615432119776902,
      "grad_norm": 0.4618797302246094,
      "learning_rate": 9.95959595959596e-05,
      "loss": 0.5289691172823597,
      "step": 14000
    },
    {
      "epoch": 0.13712685206346736,
      "grad_norm": 0.36853182315826416,
      "learning_rate": 9.95858585858586e-05,
      "loss": 0.5288619497298797,
      "step": 14100
    },
    {
      "epoch": 0.1380993829291657,
      "grad_norm": 0.36360597610473633,
      "learning_rate": 9.957575757575757e-05,
      "loss": 0.5270581387982158,
      "step": 14200
    },
    {
      "epoch": 0.13907191379486405,
      "grad_norm": 0.34857162833213806,
      "learning_rate": 9.956565656565658e-05,
      "loss": 0.5263521924666369,
      "step": 14300
    },
    {
      "epoch": 0.14004444466056243,
      "grad_norm": 0.3484053611755371,
      "learning_rate": 9.955555555555556e-05,
      "loss": 0.5247480661571068,
      "step": 14400
    },
    {
      "epoch": 0.14101697552626077,
      "grad_norm": 0.37412428855895996,
      "learning_rate": 9.954545454545455e-05,
      "loss": 0.5258385450799317,
      "step": 14500
    },
    {
      "epoch": 0.14198950639195912,
      "grad_norm": 0.38621488213539124,
      "learning_rate": 9.953535353535354e-05,
      "loss": 0.5250328474383884,
      "step": 14600
    },
    {
      "epoch": 0.14296203725765746,
      "grad_norm": 0.3595302402973175,
      "learning_rate": 9.952525252525253e-05,
      "loss": 0.5245265593316767,
      "step": 14700
    },
    {
      "epoch": 0.1439345681233558,
      "grad_norm": 0.4069189131259918,
      "learning_rate": 9.951515151515151e-05,
      "loss": 0.5228227525680026,
      "step": 14800
    },
    {
      "epoch": 0.14490709898905416,
      "grad_norm": 0.3591151535511017,
      "learning_rate": 9.950505050505052e-05,
      "loss": 0.5226158878894205,
      "step": 14900
    },
    {
      "epoch": 0.1458796298547525,
      "grad_norm": 0.35127997398376465,
      "learning_rate": 9.94949494949495e-05,
      "loss": 0.5219100707655138,
      "step": 15000
    },
    {
      "epoch": 0.14685216072045087,
      "grad_norm": 0.4552753269672394,
      "learning_rate": 9.948484848484849e-05,
      "loss": 0.5236990283721987,
      "step": 15100
    },
    {
      "epoch": 0.14782469158614922,
      "grad_norm": 0.3603357970714569,
      "learning_rate": 9.947474747474748e-05,
      "loss": 0.5212968054445056,
      "step": 15200
    },
    {
      "epoch": 0.14879722245184757,
      "grad_norm": 0.3783835172653198,
      "learning_rate": 9.946464646464647e-05,
      "loss": 0.5212895475855879,
      "step": 15300
    },
    {
      "epoch": 0.1497697533175459,
      "grad_norm": 0.3617851138114929,
      "learning_rate": 9.945454545454545e-05,
      "loss": 0.5194861409488983,
      "step": 15400
    },
    {
      "epoch": 0.15074228418324426,
      "grad_norm": 0.34237611293792725,
      "learning_rate": 9.944444444444446e-05,
      "loss": 0.5175829996742022,
      "step": 15500
    },
    {
      "epoch": 0.1517148150489426,
      "grad_norm": 0.3839002549648285,
      "learning_rate": 9.943434343434343e-05,
      "loss": 0.5183740593368867,
      "step": 15600
    },
    {
      "epoch": 0.15268734591464098,
      "grad_norm": 0.33859142661094666,
      "learning_rate": 9.942424242424243e-05,
      "loss": 0.5172692411411121,
      "step": 15700
    },
    {
      "epoch": 0.15365987678033932,
      "grad_norm": 0.3450720012187958,
      "learning_rate": 9.941414141414142e-05,
      "loss": 0.5198563314885349,
      "step": 15800
    },
    {
      "epoch": 0.15463240764603767,
      "grad_norm": 0.33444857597351074,
      "learning_rate": 9.940404040404041e-05,
      "loss": 0.5172548365696613,
      "step": 15900
    },
    {
      "epoch": 0.155604938511736,
      "grad_norm": 0.3944435119628906,
      "learning_rate": 9.939393939393939e-05,
      "loss": 0.5183451890634736,
      "step": 16000
    },
    {
      "epoch": 0.15657746937743436,
      "grad_norm": 0.3838315010070801,
      "learning_rate": 9.93838383838384e-05,
      "loss": 0.5169411030445079,
      "step": 16100
    },
    {
      "epoch": 0.1575500002431327,
      "grad_norm": 0.35751157999038696,
      "learning_rate": 9.937373737373737e-05,
      "loss": 0.5149384063553503,
      "step": 16200
    },
    {
      "epoch": 0.15852253110883108,
      "grad_norm": 0.3415018916130066,
      "learning_rate": 9.936363636363636e-05,
      "loss": 0.5168269332697947,
      "step": 16300
    },
    {
      "epoch": 0.15949506197452942,
      "grad_norm": 0.3791143596172333,
      "learning_rate": 9.935353535353536e-05,
      "loss": 0.5159217871921853,
      "step": 16400
    },
    {
      "epoch": 0.16046759284022777,
      "grad_norm": 0.3431517481803894,
      "learning_rate": 9.934343434343435e-05,
      "loss": 0.5167127690524007,
      "step": 16500
    },
    {
      "epoch": 0.16144012370592611,
      "grad_norm": 0.3628230690956116,
      "learning_rate": 9.933333333333334e-05,
      "loss": 0.5134131844620132,
      "step": 16600
    },
    {
      "epoch": 0.16241265457162446,
      "grad_norm": 0.3537401854991913,
      "learning_rate": 9.932323232323233e-05,
      "loss": 0.5124083551516001,
      "step": 16700
    },
    {
      "epoch": 0.1633851854373228,
      "grad_norm": 0.3556720018386841,
      "learning_rate": 9.931313131313131e-05,
      "loss": 0.5130995857018512,
      "step": 16800
    },
    {
      "epoch": 0.16435771630302118,
      "grad_norm": 0.3310394585132599,
      "learning_rate": 9.93030303030303e-05,
      "loss": 0.5119950231427612,
      "step": 16900
    },
    {
      "epoch": 0.16533024716871952,
      "grad_norm": 0.34654271602630615,
      "learning_rate": 9.92929292929293e-05,
      "loss": 0.5120876569161186,
      "step": 17000
    },
    {
      "epoch": 0.16630277803441787,
      "grad_norm": 0.3370843827724457,
      "learning_rate": 9.928282828282829e-05,
      "loss": 0.5112824260893908,
      "step": 17100
    },
    {
      "epoch": 0.16727530890011622,
      "grad_norm": 0.35077980160713196,
      "learning_rate": 9.927272727272728e-05,
      "loss": 0.5103774564566672,
      "step": 17200
    },
    {
      "epoch": 0.16824783976581456,
      "grad_norm": 0.3419266939163208,
      "learning_rate": 9.926262626262627e-05,
      "loss": 0.511068666166971,
      "step": 17300
    },
    {
      "epoch": 0.1692203706315129,
      "grad_norm": 0.3700767755508423,
      "learning_rate": 9.925252525252525e-05,
      "loss": 0.5112610651435907,
      "step": 17400
    },
    {
      "epoch": 0.17019290149721128,
      "grad_norm": 0.3383312523365021,
      "learning_rate": 9.924242424242425e-05,
      "loss": 0.5088597800135182,
      "step": 17500
    },
    {
      "epoch": 0.17116543236290963,
      "grad_norm": 0.32818201184272766,
      "learning_rate": 9.923232323232323e-05,
      "loss": 0.5091519594760281,
      "step": 17600
    },
    {
      "epoch": 0.17213796322860797,
      "grad_norm": 0.37795937061309814,
      "learning_rate": 9.922222222222222e-05,
      "loss": 0.5086460978935676,
      "step": 17700
    },
    {
      "epoch": 0.17311049409430632,
      "grad_norm": 0.3350246250629425,
      "learning_rate": 9.921212121212122e-05,
      "loss": 0.5090380244980525,
      "step": 17800
    },
    {
      "epoch": 0.17408302496000466,
      "grad_norm": 0.3349106013774872,
      "learning_rate": 9.920202020202021e-05,
      "loss": 0.5079336703384337,
      "step": 17900
    },
    {
      "epoch": 0.175055555825703,
      "grad_norm": 0.3279871344566345,
      "learning_rate": 9.919191919191919e-05,
      "loss": 0.5064303470615328,
      "step": 18000
    },
    {
      "epoch": 0.17602808669140138,
      "grad_norm": 0.3685106933116913,
      "learning_rate": 9.918181818181819e-05,
      "loss": 0.5070217846219252,
      "step": 18100
    },
    {
      "epoch": 0.17700061755709973,
      "grad_norm": 0.3524211347103119,
      "learning_rate": 9.917171717171717e-05,
      "loss": 0.5068152283745605,
      "step": 18200
    },
    {
      "epoch": 0.17797314842279807,
      "grad_norm": 0.3426039516925812,
      "learning_rate": 9.916161616161616e-05,
      "loss": 0.5069079149424505,
      "step": 18300
    },
    {
      "epoch": 0.17894567928849642,
      "grad_norm": 0.32101109623908997,
      "learning_rate": 9.915151515151515e-05,
      "loss": 0.5067013656417348,
      "step": 18400
    },
    {
      "epoch": 0.17991821015419476,
      "grad_norm": 0.33992740511894226,
      "learning_rate": 9.914141414141415e-05,
      "loss": 0.5061955929763821,
      "step": 18500
    },
    {
      "epoch": 0.1808907410198931,
      "grad_norm": 0.31938713788986206,
      "learning_rate": 9.913131313131314e-05,
      "loss": 0.5055900926196717,
      "step": 18600
    },
    {
      "epoch": 0.18186327188559148,
      "grad_norm": 0.3322224020957947,
      "learning_rate": 9.912121212121213e-05,
      "loss": 0.5044859079582895,
      "step": 18700
    },
    {
      "epoch": 0.18283580275128983,
      "grad_norm": 0.3281875550746918,
      "learning_rate": 9.911111111111112e-05,
      "loss": 0.5063739180420453,
      "step": 18800
    },
    {
      "epoch": 0.18380833361698817,
      "grad_norm": 0.3483811020851135,
      "learning_rate": 9.91010101010101e-05,
      "loss": 0.5044718534979504,
      "step": 18900
    },
    {
      "epoch": 0.18478086448268652,
      "grad_norm": 0.3325604796409607,
      "learning_rate": 9.909090909090911e-05,
      "loss": 0.502968785857734,
      "step": 19000
    },
    {
      "epoch": 0.18575339534838486,
      "grad_norm": 0.3591443598270416,
      "learning_rate": 9.908080808080809e-05,
      "loss": 0.5033607180195382,
      "step": 19100
    },
    {
      "epoch": 0.1867259262140832,
      "grad_norm": 0.3413999080657959,
      "learning_rate": 9.907070707070708e-05,
      "loss": 0.5020571738391976,
      "step": 19200
    },
    {
      "epoch": 0.18769845707978158,
      "grad_norm": 0.32104068994522095,
      "learning_rate": 9.906060606060607e-05,
      "loss": 0.5025488392496785,
      "step": 19300
    },
    {
      "epoch": 0.18867098794547993,
      "grad_norm": 0.3611776828765869,
      "learning_rate": 9.905050505050506e-05,
      "loss": 0.5026415688867926,
      "step": 19400
    },
    {
      "epoch": 0.18964351881117827,
      "grad_norm": 0.2919318675994873,
      "learning_rate": 9.904040404040404e-05,
      "loss": 0.5011386304542482,
      "step": 19500
    },
    {
      "epoch": 0.19061604967687662,
      "grad_norm": 0.2990705668926239,
      "learning_rate": 9.903030303030305e-05,
      "loss": 0.5012313767633201,
      "step": 19600
    },
    {
      "epoch": 0.19158858054257497,
      "grad_norm": 0.3495672047138214,
      "learning_rate": 9.902020202020202e-05,
      "loss": 0.5012243939917044,
      "step": 19700
    },
    {
      "epoch": 0.1925611114082733,
      "grad_norm": 0.3173944652080536,
      "learning_rate": 9.901010101010102e-05,
      "loss": 0.5002201620931087,
      "step": 19800
    },
    {
      "epoch": 0.19353364227397166,
      "grad_norm": 0.3821360468864441,
      "learning_rate": 9.900000000000001e-05,
      "loss": 0.49991402264468654,
      "step": 19900
    },
    {
      "epoch": 0.19450617313967003,
      "grad_norm": 0.34445855021476746,
      "learning_rate": 9.8989898989899e-05,
      "loss": 0.4994084472641665,
      "step": 20000
    },
    {
      "epoch": 0.19450617313967003,
      "eval_accuracy": 0.6562236448088693,
      "eval_loss": 0.489191249219197,
      "eval_runtime": 3695.864,
      "eval_samples_per_second": 618.191,
      "eval_steps_per_second": 6.182,
      "step": 20000
    },
    {
      "epoch": 0.19547870400536838,
      "grad_norm": 0.3012506663799286,
      "learning_rate": 9.897979797979798e-05,
      "loss": 0.49850400279811025,
      "step": 20100
    },
    {
      "epoch": 0.19645123487106672,
      "grad_norm": 0.3103438913822174,
      "learning_rate": 9.896969696969698e-05,
      "loss": 0.4988959349599143,
      "step": 20200
    },
    {
      "epoch": 0.19742376573676507,
      "grad_norm": 0.3613319993019104,
      "learning_rate": 9.895959595959596e-05,
      "loss": 0.49888898414288424,
      "step": 20300
    },
    {
      "epoch": 0.1983962966024634,
      "grad_norm": 0.35598891973495483,
      "learning_rate": 9.894949494949495e-05,
      "loss": 0.500178348823203,
      "step": 20400
    },
    {
      "epoch": 0.19936882746816176,
      "grad_norm": 0.3173132538795471,
      "learning_rate": 9.893939393939395e-05,
      "loss": 0.49947337363315997,
      "step": 20500
    },
    {
      "epoch": 0.20034135833386013,
      "grad_norm": 0.32309943437576294,
      "learning_rate": 9.892929292929294e-05,
      "loss": 0.4986687040956748,
      "step": 20600
    },
    {
      "epoch": 0.20131388919955848,
      "grad_norm": 0.301580548286438,
      "learning_rate": 9.891919191919192e-05,
      "loss": 0.49577009620414253,
      "step": 20700
    },
    {
      "epoch": 0.20228642006525682,
      "grad_norm": 0.3452286124229431,
      "learning_rate": 9.890909090909092e-05,
      "loss": 0.49865480801893375,
      "step": 20800
    },
    {
      "epoch": 0.20325895093095517,
      "grad_norm": 0.3631146252155304,
      "learning_rate": 9.88989898989899e-05,
      "loss": 0.4966536673791613,
      "step": 20900
    },
    {
      "epoch": 0.2042314817966535,
      "grad_norm": 0.3286854028701782,
      "learning_rate": 9.888888888888889e-05,
      "loss": 0.4968461636088676,
      "step": 21000
    },
    {
      "epoch": 0.20520401266235186,
      "grad_norm": 0.32509687542915344,
      "learning_rate": 9.887878787878788e-05,
      "loss": 0.4981354296467698,
      "step": 21100
    },
    {
      "epoch": 0.20617654352805023,
      "grad_norm": 0.3086968660354614,
      "learning_rate": 9.886868686868688e-05,
      "loss": 0.4958352629271929,
      "step": 21200
    },
    {
      "epoch": 0.20714907439374858,
      "grad_norm": 0.2993854284286499,
      "learning_rate": 9.885858585858587e-05,
      "loss": 0.49463190491704656,
      "step": 21300
    },
    {
      "epoch": 0.20812160525944692,
      "grad_norm": 0.3019491136074066,
      "learning_rate": 9.884848484848486e-05,
      "loss": 0.4954226339192369,
      "step": 21400
    },
    {
      "epoch": 0.20909413612514527,
      "grad_norm": 0.2863098680973053,
      "learning_rate": 9.883838383838384e-05,
      "loss": 0.4948175225748727,
      "step": 21500
    },
    {
      "epoch": 0.21006666699084361,
      "grad_norm": 0.2972549498081207,
      "learning_rate": 9.882828282828283e-05,
      "loss": 0.4948106273310349,
      "step": 21600
    },
    {
      "epoch": 0.21103919785654196,
      "grad_norm": 0.30514654517173767,
      "learning_rate": 9.881818181818182e-05,
      "loss": 0.49420554099460734,
      "step": 21700
    },
    {
      "epoch": 0.21201172872224033,
      "grad_norm": 0.30161213874816895,
      "learning_rate": 9.880808080808081e-05,
      "loss": 0.4942983512128502,
      "step": 21800
    },
    {
      "epoch": 0.21298425958793868,
      "grad_norm": 0.32228222489356995,
      "learning_rate": 9.87979797979798e-05,
      "loss": 0.4925966353905371,
      "step": 21900
    },
    {
      "epoch": 0.21395679045363702,
      "grad_norm": 0.3295589089393616,
      "learning_rate": 9.87878787878788e-05,
      "loss": 0.4935867141863772,
      "step": 22000
    },
    {
      "epoch": 0.21492932131933537,
      "grad_norm": 0.3072807788848877,
      "learning_rate": 9.877777777777778e-05,
      "loss": 0.4945767651956211,
      "step": 22100
    },
    {
      "epoch": 0.21590185218503372,
      "grad_norm": 0.3076956570148468,
      "learning_rate": 9.876767676767677e-05,
      "loss": 0.49177850880453033,
      "step": 22200
    },
    {
      "epoch": 0.21687438305073206,
      "grad_norm": 0.32337865233421326,
      "learning_rate": 9.875757575757576e-05,
      "loss": 0.49356607847073686,
      "step": 22300
    },
    {
      "epoch": 0.21784691391643043,
      "grad_norm": 0.31160208582878113,
      "learning_rate": 9.874747474747475e-05,
      "loss": 0.49126635772602834,
      "step": 22400
    },
    {
      "epoch": 0.21881944478212878,
      "grad_norm": 0.32169902324676514,
      "learning_rate": 9.873737373737374e-05,
      "loss": 0.493153571723804,
      "step": 22500
    },
    {
      "epoch": 0.21979197564782713,
      "grad_norm": 0.3112693428993225,
      "learning_rate": 9.872727272727274e-05,
      "loss": 0.4921498385946096,
      "step": 22600
    },
    {
      "epoch": 0.22076450651352547,
      "grad_norm": 0.2704222798347473,
      "learning_rate": 9.871717171717172e-05,
      "loss": 0.49234234871761384,
      "step": 22700
    },
    {
      "epoch": 0.22173703737922382,
      "grad_norm": 0.3098367750644684,
      "learning_rate": 9.870707070707072e-05,
      "loss": 0.49044150539442033,
      "step": 22800
    },
    {
      "epoch": 0.22270956824492216,
      "grad_norm": 0.3064345717430115,
      "learning_rate": 9.86969696969697e-05,
      "loss": 0.49043466989176426,
      "step": 22900
    },
    {
      "epoch": 0.22368209911062054,
      "grad_norm": 0.3143150508403778,
      "learning_rate": 9.868686868686869e-05,
      "loss": 0.4914246389305495,
      "step": 23000
    },
    {
      "epoch": 0.22465462997631888,
      "grad_norm": 0.3030448853969574,
      "learning_rate": 9.867676767676768e-05,
      "loss": 0.4898229244975662,
      "step": 23100
    },
    {
      "epoch": 0.22562716084201723,
      "grad_norm": 0.26225656270980835,
      "learning_rate": 9.866666666666668e-05,
      "loss": 0.4897164196554046,
      "step": 23200
    },
    {
      "epoch": 0.22659969170771557,
      "grad_norm": 0.28908073902130127,
      "learning_rate": 9.865656565656565e-05,
      "loss": 0.4909057093119136,
      "step": 23300
    },
    {
      "epoch": 0.22757222257341392,
      "grad_norm": 0.29368892312049866,
      "learning_rate": 9.864646464646466e-05,
      "loss": 0.4898024429975346,
      "step": 23400
    },
    {
      "epoch": 0.22854475343911226,
      "grad_norm": 0.3080741763114929,
      "learning_rate": 9.863636363636364e-05,
      "loss": 0.4908920244133034,
      "step": 23500
    },
    {
      "epoch": 0.22951728430481064,
      "grad_norm": 0.31463754177093506,
      "learning_rate": 9.862626262626263e-05,
      "loss": 0.4888917396006927,
      "step": 23600
    },
    {
      "epoch": 0.23048981517050898,
      "grad_norm": 0.30542680621147156,
      "learning_rate": 9.861616161616162e-05,
      "loss": 0.48938327858216113,
      "step": 23700
    },
    {
      "epoch": 0.23146234603620733,
      "grad_norm": 0.3309226334095001,
      "learning_rate": 9.860606060606061e-05,
      "loss": 0.4885791022567579,
      "step": 23800
    },
    {
      "epoch": 0.23243487690190567,
      "grad_norm": 0.3118945062160492,
      "learning_rate": 9.859595959595959e-05,
      "loss": 0.4886719597122143,
      "step": 23900
    },
    {
      "epoch": 0.23340740776760402,
      "grad_norm": 0.2865544259548187,
      "learning_rate": 9.85858585858586e-05,
      "loss": 0.4891634806323952,
      "step": 24000
    },
    {
      "epoch": 0.23437993863330236,
      "grad_norm": 0.32899510860443115,
      "learning_rate": 9.857575757575758e-05,
      "loss": 0.4884590056010831,
      "step": 24100
    },
    {
      "epoch": 0.23535246949900074,
      "grad_norm": 0.2931799590587616,
      "learning_rate": 9.856565656565657e-05,
      "loss": 0.48914984297099845,
      "step": 24200
    },
    {
      "epoch": 0.23632500036469908,
      "grad_norm": 0.30272164940834045,
      "learning_rate": 9.855555555555556e-05,
      "loss": 0.4875484258545942,
      "step": 24300
    },
    {
      "epoch": 0.23729753123039743,
      "grad_norm": 0.2983759641647339,
      "learning_rate": 9.854545454545455e-05,
      "loss": 0.4867443412249582,
      "step": 24400
    },
    {
      "epoch": 0.23827006209609577,
      "grad_norm": 0.31024160981178284,
      "learning_rate": 9.853535353535353e-05,
      "loss": 0.4854419807536141,
      "step": 24500
    },
    {
      "epoch": 0.23924259296179412,
      "grad_norm": 0.27710944414138794,
      "learning_rate": 9.852525252525254e-05,
      "loss": 0.48623247912705625,
      "step": 24600
    },
    {
      "epoch": 0.24021512382749247,
      "grad_norm": 0.2777211666107178,
      "learning_rate": 9.851515151515151e-05,
      "loss": 0.4856277595736981,
      "step": 24700
    },
    {
      "epoch": 0.2411876546931908,
      "grad_norm": 0.284925252199173,
      "learning_rate": 9.85050505050505e-05,
      "loss": 0.4868168547240339,
      "step": 24800
    },
    {
      "epoch": 0.24216018555888918,
      "grad_norm": 0.30468645691871643,
      "learning_rate": 9.84949494949495e-05,
      "loss": 0.48631179756353227,
      "step": 24900
    },
    {
      "epoch": 0.24313271642458753,
      "grad_norm": 0.3314826786518097,
      "learning_rate": 9.848484848484849e-05,
      "loss": 0.48530849095858897,
      "step": 25000
    },
    {
      "epoch": 0.24410524729028588,
      "grad_norm": 0.28756392002105713,
      "learning_rate": 9.847474747474747e-05,
      "loss": 0.4861985864263867,
      "step": 25100
    },
    {
      "epoch": 0.24507777815598422,
      "grad_norm": 0.3060457110404968,
      "learning_rate": 9.846464646464647e-05,
      "loss": 0.48539460877514595,
      "step": 25200
    },
    {
      "epoch": 0.24605030902168257,
      "grad_norm": 0.26673054695129395,
      "learning_rate": 9.845454545454545e-05,
      "loss": 0.48528819285009195,
      "step": 25300
    },
    {
      "epoch": 0.2470228398873809,
      "grad_norm": 0.28506630659103394,
      "learning_rate": 9.844444444444444e-05,
      "loss": 0.4853810739241551,
      "step": 25400
    },
    {
      "epoch": 0.2479953707530793,
      "grad_norm": 0.3043409585952759,
      "learning_rate": 9.843434343434344e-05,
      "loss": 0.4855735979404575,
      "step": 25500
    },
    {
      "epoch": 0.24896790161877763,
      "grad_norm": 0.28708434104919434,
      "learning_rate": 9.842424242424243e-05,
      "loss": 0.4859654050625789,
      "step": 25600
    },
    {
      "epoch": 0.24994043248447598,
      "grad_norm": 0.30161648988723755,
      "learning_rate": 9.841414141414142e-05,
      "loss": 0.4850618428209508,
      "step": 25700
    },
    {
      "epoch": 0.2509129633501743,
      "grad_norm": 0.29326337575912476,
      "learning_rate": 9.840404040404041e-05,
      "loss": 0.48385938092853076,
      "step": 25800
    },
    {
      "epoch": 0.2518854942158727,
      "grad_norm": 0.28967681527137756,
      "learning_rate": 9.839393939393939e-05,
      "loss": 0.4834540736886637,
      "step": 25900
    },
    {
      "epoch": 0.252858025081571,
      "grad_norm": 0.2858356237411499,
      "learning_rate": 9.838383838383838e-05,
      "loss": 0.4831484163376849,
      "step": 26000
    },
    {
      "epoch": 0.2538305559472694,
      "grad_norm": 0.30548012256622314,
      "learning_rate": 9.837373737373737e-05,
      "loss": 0.4830420420925252,
      "step": 26100
    },
    {
      "epoch": 0.2548030868129677,
      "grad_norm": 0.2695578336715698,
      "learning_rate": 9.836363636363637e-05,
      "loss": 0.48323457861279584,
      "step": 26200
    },
    {
      "epoch": 0.2557756176786661,
      "grad_norm": 0.2981511950492859,
      "learning_rate": 9.835353535353536e-05,
      "loss": 0.4834271095757473,
      "step": 26300
    },
    {
      "epoch": 0.25674814854436445,
      "grad_norm": 0.2770542800426483,
      "learning_rate": 9.834343434343435e-05,
      "loss": 0.4832211021136569,
      "step": 26400
    },
    {
      "epoch": 0.25772067941006277,
      "grad_norm": 0.3288051187992096,
      "learning_rate": 9.833333333333333e-05,
      "loss": 0.4836128911744908,
      "step": 26500
    },
    {
      "epoch": 0.25869321027576114,
      "grad_norm": 0.2797970473766327,
      "learning_rate": 9.832323232323233e-05,
      "loss": 0.48211169079353655,
      "step": 26600
    },
    {
      "epoch": 0.25966574114145946,
      "grad_norm": 0.2864944636821747,
      "learning_rate": 9.831313131313131e-05,
      "loss": 0.4813079354350651,
      "step": 26700
    },
    {
      "epoch": 0.26063827200715783,
      "grad_norm": 0.27549320459365845,
      "learning_rate": 9.83030303030303e-05,
      "loss": 0.48080308528470495,
      "step": 26800
    },
    {
      "epoch": 0.26161080287285615,
      "grad_norm": 0.2898082137107849,
      "learning_rate": 9.82929292929293e-05,
      "loss": 0.48258965324411984,
      "step": 26900
    },
    {
      "epoch": 0.2625833337385545,
      "grad_norm": 0.29356178641319275,
      "learning_rate": 9.828282828282829e-05,
      "loss": 0.4814870496400594,
      "step": 27000
    },
    {
      "epoch": 0.2635558646042529,
      "grad_norm": 0.2997514307498932,
      "learning_rate": 9.827272727272728e-05,
      "loss": 0.4829746873834265,
      "step": 27100
    },
    {
      "epoch": 0.2645283954699512,
      "grad_norm": 0.28983330726623535,
      "learning_rate": 9.826262626262627e-05,
      "loss": 0.48047739935521205,
      "step": 27200
    },
    {
      "epoch": 0.2655009263356496,
      "grad_norm": 0.290416419506073,
      "learning_rate": 9.825252525252527e-05,
      "loss": 0.47957411220088597,
      "step": 27300
    },
    {
      "epoch": 0.2664734572013479,
      "grad_norm": 0.3044191896915436,
      "learning_rate": 9.824242424242424e-05,
      "loss": 0.48136057179257585,
      "step": 27400
    },
    {
      "epoch": 0.2674459880670463,
      "grad_norm": 0.29515430331230164,
      "learning_rate": 9.823232323232325e-05,
      "loss": 0.4810550047480346,
      "step": 27500
    },
    {
      "epoch": 0.26841851893274465,
      "grad_norm": 0.3711446523666382,
      "learning_rate": 9.822222222222223e-05,
      "loss": 0.4796536640447698,
      "step": 27600
    },
    {
      "epoch": 0.26939104979844297,
      "grad_norm": 0.276926189661026,
      "learning_rate": 9.821212121212122e-05,
      "loss": 0.47855120770966897,
      "step": 27700
    },
    {
      "epoch": 0.27036358066414135,
      "grad_norm": 0.27185869216918945,
      "learning_rate": 9.820202020202021e-05,
      "loss": 0.480138353630284,
      "step": 27800
    },
    {
      "epoch": 0.27133611152983966,
      "grad_norm": 0.2993779182434082,
      "learning_rate": 9.81919191919192e-05,
      "loss": 0.48122739320742763,
      "step": 27900
    },
    {
      "epoch": 0.27230864239553804,
      "grad_norm": 0.2687966227531433,
      "learning_rate": 9.818181818181818e-05,
      "loss": 0.4792284616020501,
      "step": 28000
    },
    {
      "epoch": 0.27328117326123635,
      "grad_norm": 0.27421486377716064,
      "learning_rate": 9.817171717171719e-05,
      "loss": 0.4799190447246105,
      "step": 28100
    },
    {
      "epoch": 0.2742537041269347,
      "grad_norm": 0.2681407630443573,
      "learning_rate": 9.816161616161617e-05,
      "loss": 0.4771233210817599,
      "step": 28200
    },
    {
      "epoch": 0.2752262349926331,
      "grad_norm": 0.2605465054512024,
      "learning_rate": 9.815151515151516e-05,
      "loss": 0.4794076230442578,
      "step": 28300
    },
    {
      "epoch": 0.2761987658583314,
      "grad_norm": 0.3410974144935608,
      "learning_rate": 9.814141414141415e-05,
      "loss": 0.4767115895808601,
      "step": 28400
    },
    {
      "epoch": 0.2771712967240298,
      "grad_norm": 0.30448341369628906,
      "learning_rate": 9.813131313131314e-05,
      "loss": 0.47879662510949655,
      "step": 28500
    },
    {
      "epoch": 0.2781438275897281,
      "grad_norm": 0.2811489403247833,
      "learning_rate": 9.812121212121212e-05,
      "loss": 0.47789352273603497,
      "step": 28600
    },
    {
      "epoch": 0.2791163584554265,
      "grad_norm": 0.27394992113113403,
      "learning_rate": 9.811111111111113e-05,
      "loss": 0.477090046632855,
      "step": 28700
    },
    {
      "epoch": 0.28008888932112486,
      "grad_norm": 0.2578553259372711,
      "learning_rate": 9.81010101010101e-05,
      "loss": 0.47768099098116557,
      "step": 28800
    },
    {
      "epoch": 0.2810614201868232,
      "grad_norm": 0.2639196813106537,
      "learning_rate": 9.80909090909091e-05,
      "loss": 0.47667834291855127,
      "step": 28900
    },
    {
      "epoch": 0.28203395105252155,
      "grad_norm": 0.28673768043518066,
      "learning_rate": 9.808080808080809e-05,
      "loss": 0.477070081963512,
      "step": 29000
    },
    {
      "epoch": 0.28300648191821987,
      "grad_norm": 0.271944522857666,
      "learning_rate": 9.807070707070708e-05,
      "loss": 0.47736221418880853,
      "step": 29100
    },
    {
      "epoch": 0.28397901278391824,
      "grad_norm": 0.365427702665329,
      "learning_rate": 9.806060606060606e-05,
      "loss": 0.4774551494467341,
      "step": 29200
    },
    {
      "epoch": 0.28495154364961656,
      "grad_norm": 0.2975412607192993,
      "learning_rate": 9.805050505050506e-05,
      "loss": 0.4766517455887042,
      "step": 29300
    },
    {
      "epoch": 0.28592407451531493,
      "grad_norm": 0.2815972566604614,
      "learning_rate": 9.804040404040404e-05,
      "loss": 0.4767446877932789,
      "step": 29400
    },
    {
      "epoch": 0.2868966053810133,
      "grad_norm": 0.26909855008125305,
      "learning_rate": 9.803030303030303e-05,
      "loss": 0.47733557795772685,
      "step": 29500
    },
    {
      "epoch": 0.2878691362467116,
      "grad_norm": 0.27005821466445923,
      "learning_rate": 9.802020202020203e-05,
      "loss": 0.47633303131618854,
      "step": 29600
    },
    {
      "epoch": 0.28884166711241,
      "grad_norm": 0.2712157666683197,
      "learning_rate": 9.801010101010102e-05,
      "loss": 0.47652556088981013,
      "step": 29700
    },
    {
      "epoch": 0.2898141979781083,
      "grad_norm": 0.2723917067050934,
      "learning_rate": 9.8e-05,
      "loss": 0.4750251232509206,
      "step": 29800
    },
    {
      "epoch": 0.2907867288438067,
      "grad_norm": 0.295829713344574,
      "learning_rate": 9.7989898989899e-05,
      "loss": 0.4750184961477357,
      "step": 29900
    },
    {
      "epoch": 0.291759259709505,
      "grad_norm": 0.2564955949783325,
      "learning_rate": 9.797979797979798e-05,
      "loss": 0.476804366663587,
      "step": 30000
    },
    {
      "epoch": 0.291759259709505,
      "eval_accuracy": 0.6594136752359833,
      "eval_loss": 0.4663422392821813,
      "eval_runtime": 3705.9713,
      "eval_samples_per_second": 616.505,
      "eval_steps_per_second": 6.165,
      "step": 30000
    },
    {
      "epoch": 0.2927317905752034,
      "grad_norm": 0.2932312786579132,
      "learning_rate": 9.796969696969697e-05,
      "loss": 0.47610064187037143,
      "step": 30100
    },
    {
      "epoch": 0.29370432144090175,
      "grad_norm": 0.27669188380241394,
      "learning_rate": 9.795959595959596e-05,
      "loss": 0.47519777568297716,
      "step": 30200
    },
    {
      "epoch": 0.29467685230660007,
      "grad_norm": 0.296373188495636,
      "learning_rate": 9.794949494949496e-05,
      "loss": 0.4752907248342009,
      "step": 30300
    },
    {
      "epoch": 0.29564938317229844,
      "grad_norm": 0.2947692275047302,
      "learning_rate": 9.793939393939394e-05,
      "loss": 0.4757819817817182,
      "step": 30400
    },
    {
      "epoch": 0.29662191403799676,
      "grad_norm": 0.27609577775001526,
      "learning_rate": 9.792929292929294e-05,
      "loss": 0.4751778860374439,
      "step": 30500
    },
    {
      "epoch": 0.29759444490369513,
      "grad_norm": 0.27138379216194153,
      "learning_rate": 9.791919191919192e-05,
      "loss": 0.47457380696512713,
      "step": 30600
    },
    {
      "epoch": 0.2985669757693935,
      "grad_norm": 0.27890023589134216,
      "learning_rate": 9.790909090909091e-05,
      "loss": 0.4724761424285346,
      "step": 30700
    },
    {
      "epoch": 0.2995395066350918,
      "grad_norm": 0.26584309339523315,
      "learning_rate": 9.78989898989899e-05,
      "loss": 0.47376398718204316,
      "step": 30800
    },
    {
      "epoch": 0.3005120375007902,
      "grad_norm": 0.26898208260536194,
      "learning_rate": 9.78888888888889e-05,
      "loss": 0.4730603818711911,
      "step": 30900
    },
    {
      "epoch": 0.3014845683664885,
      "grad_norm": 0.27267986536026,
      "learning_rate": 9.787878787878789e-05,
      "loss": 0.4734520583963105,
      "step": 31000
    },
    {
      "epoch": 0.3024570992321869,
      "grad_norm": 0.27584534883499146,
      "learning_rate": 9.786868686868688e-05,
      "loss": 0.4744411313173694,
      "step": 31100
    },
    {
      "epoch": 0.3034296300978852,
      "grad_norm": 0.26593902707099915,
      "learning_rate": 9.785858585858586e-05,
      "loss": 0.4751314768645327,
      "step": 31200
    },
    {
      "epoch": 0.3044021609635836,
      "grad_norm": 0.2763272821903229,
      "learning_rate": 9.784848484848485e-05,
      "loss": 0.47363136988613747,
      "step": 31300
    },
    {
      "epoch": 0.30537469182928195,
      "grad_norm": 0.262298047542572,
      "learning_rate": 9.783838383838384e-05,
      "loss": 0.47322650584247916,
      "step": 31400
    },
    {
      "epoch": 0.30634722269498027,
      "grad_norm": 0.27080458402633667,
      "learning_rate": 9.782828282828283e-05,
      "loss": 0.4729212152745697,
      "step": 31500
    },
    {
      "epoch": 0.30731975356067864,
      "grad_norm": 0.2551056444644928,
      "learning_rate": 9.781818181818183e-05,
      "loss": 0.4720185672119555,
      "step": 31600
    },
    {
      "epoch": 0.30829228442637696,
      "grad_norm": 0.27582862973213196,
      "learning_rate": 9.780808080808082e-05,
      "loss": 0.4713150633221793,
      "step": 31700
    },
    {
      "epoch": 0.30926481529207533,
      "grad_norm": 0.252853125333786,
      "learning_rate": 9.77979797979798e-05,
      "loss": 0.47310053371104577,
      "step": 31800
    },
    {
      "epoch": 0.3102373461577737,
      "grad_norm": 0.2719995081424713,
      "learning_rate": 9.77878787878788e-05,
      "loss": 0.4734921588309623,
      "step": 31900
    },
    {
      "epoch": 0.311209877023472,
      "grad_norm": 0.29760828614234924,
      "learning_rate": 9.777777777777778e-05,
      "loss": 0.47318688493501065,
      "step": 32000
    },
    {
      "epoch": 0.3121824078891704,
      "grad_norm": 0.29419174790382385,
      "learning_rate": 9.776767676767677e-05,
      "loss": 0.47168697107345875,
      "step": 32100
    },
    {
      "epoch": 0.3131549387548687,
      "grad_norm": 0.26911312341690063,
      "learning_rate": 9.775757575757576e-05,
      "loss": 0.472476809515242,
      "step": 32200
    },
    {
      "epoch": 0.3141274696205671,
      "grad_norm": 0.26586151123046875,
      "learning_rate": 9.774747474747476e-05,
      "loss": 0.4722711132630286,
      "step": 32300
    },
    {
      "epoch": 0.3151000004862654,
      "grad_norm": 0.29049018025398254,
      "learning_rate": 9.773737373737373e-05,
      "loss": 0.471766772996708,
      "step": 32400
    },
    {
      "epoch": 0.3160725313519638,
      "grad_norm": 0.28226813673973083,
      "learning_rate": 9.772727272727274e-05,
      "loss": 0.4723574797696213,
      "step": 32500
    },
    {
      "epoch": 0.31704506221766215,
      "grad_norm": 0.2757214307785034,
      "learning_rate": 9.771717171717172e-05,
      "loss": 0.4721517932427166,
      "step": 32600
    },
    {
      "epoch": 0.3180175930833605,
      "grad_norm": 0.31578803062438965,
      "learning_rate": 9.770707070707071e-05,
      "loss": 0.47055247262499283,
      "step": 32700
    },
    {
      "epoch": 0.31899012394905885,
      "grad_norm": 0.2665267288684845,
      "learning_rate": 9.76969696969697e-05,
      "loss": 0.47174043686086486,
      "step": 32800
    },
    {
      "epoch": 0.31996265481475716,
      "grad_norm": 0.2719144821166992,
      "learning_rate": 9.76868686868687e-05,
      "loss": 0.47043979499049354,
      "step": 32900
    },
    {
      "epoch": 0.32093518568045554,
      "grad_norm": 0.3065173625946045,
      "learning_rate": 9.767676767676767e-05,
      "loss": 0.47103047814480015,
      "step": 33000
    },
    {
      "epoch": 0.3219077165461539,
      "grad_norm": 0.2911020517349243,
      "learning_rate": 9.766666666666668e-05,
      "loss": 0.47072528344064746,
      "step": 33100
    },
    {
      "epoch": 0.32288024741185223,
      "grad_norm": 0.27579495310783386,
      "learning_rate": 9.765656565656566e-05,
      "loss": 0.47101732952749586,
      "step": 33200
    },
    {
      "epoch": 0.3238527782775506,
      "grad_norm": 0.26728343963623047,
      "learning_rate": 9.764646464646465e-05,
      "loss": 0.47061260580614817,
      "step": 33300
    },
    {
      "epoch": 0.3248253091432489,
      "grad_norm": 0.27552711963653564,
      "learning_rate": 9.763636363636364e-05,
      "loss": 0.470207893199439,
      "step": 33400
    },
    {
      "epoch": 0.3257978400089473,
      "grad_norm": 0.2563963830471039,
      "learning_rate": 9.762626262626263e-05,
      "loss": 0.4689073805367398,
      "step": 33500
    },
    {
      "epoch": 0.3267703708746456,
      "grad_norm": 0.29092809557914734,
      "learning_rate": 9.761616161616161e-05,
      "loss": 0.469398501329936,
      "step": 33600
    },
    {
      "epoch": 0.327742901740344,
      "grad_norm": 0.29263776540756226,
      "learning_rate": 9.760606060606062e-05,
      "loss": 0.4699891400256887,
      "step": 33700
    },
    {
      "epoch": 0.32871543260604236,
      "grad_norm": 0.27077430486679077,
      "learning_rate": 9.75959595959596e-05,
      "loss": 0.4689872755450862,
      "step": 33800
    },
    {
      "epoch": 0.3296879634717407,
      "grad_norm": 0.2751825749874115,
      "learning_rate": 9.758585858585859e-05,
      "loss": 0.4701750772293707,
      "step": 33900
    },
    {
      "epoch": 0.33066049433743905,
      "grad_norm": 0.2643389403820038,
      "learning_rate": 9.757575757575758e-05,
      "loss": 0.4697704035238961,
      "step": 34000
    },
    {
      "epoch": 0.33163302520313737,
      "grad_norm": 0.27313804626464844,
      "learning_rate": 9.756565656565657e-05,
      "loss": 0.4699628983642726,
      "step": 34100
    },
    {
      "epoch": 0.33260555606883574,
      "grad_norm": 0.2655849754810333,
      "learning_rate": 9.755555555555555e-05,
      "loss": 0.4685629900600399,
      "step": 34200
    },
    {
      "epoch": 0.33357808693453406,
      "grad_norm": 0.26939183473587036,
      "learning_rate": 9.754545454545455e-05,
      "loss": 0.4688550194749306,
      "step": 34300
    },
    {
      "epoch": 0.33455061780023243,
      "grad_norm": 0.26603934168815613,
      "learning_rate": 9.753535353535353e-05,
      "loss": 0.46785325363674446,
      "step": 34400
    },
    {
      "epoch": 0.3355231486659308,
      "grad_norm": 0.27809104323387146,
      "learning_rate": 9.752525252525253e-05,
      "loss": 0.46914049125312685,
      "step": 34500
    },
    {
      "epoch": 0.3364956795316291,
      "grad_norm": 0.2814273536205292,
      "learning_rate": 9.751515151515152e-05,
      "loss": 0.4690344226605249,
      "step": 34600
    },
    {
      "epoch": 0.3374682103973275,
      "grad_norm": 0.2609437108039856,
      "learning_rate": 9.750505050505051e-05,
      "loss": 0.4690278747491391,
      "step": 34700
    },
    {
      "epoch": 0.3384407412630258,
      "grad_norm": 0.299285352230072,
      "learning_rate": 9.74949494949495e-05,
      "loss": 0.46772761216580533,
      "step": 34800
    },
    {
      "epoch": 0.3394132721287242,
      "grad_norm": 0.29191309213638306,
      "learning_rate": 9.748484848484849e-05,
      "loss": 0.4672235066962223,
      "step": 34900
    },
    {
      "epoch": 0.34038580299442256,
      "grad_norm": 0.25994178652763367,
      "learning_rate": 9.747474747474747e-05,
      "loss": 0.467117470058206,
      "step": 35000
    },
    {
      "epoch": 0.3413583338601209,
      "grad_norm": 0.2590861916542053,
      "learning_rate": 9.746464646464646e-05,
      "loss": 0.46671289916313713,
      "step": 35100
    },
    {
      "epoch": 0.34233086472581925,
      "grad_norm": 0.30838853120803833,
      "learning_rate": 9.745454545454546e-05,
      "loss": 0.4675024708535973,
      "step": 35200
    },
    {
      "epoch": 0.34330339559151757,
      "grad_norm": 0.2571333646774292,
      "learning_rate": 9.744444444444445e-05,
      "loss": 0.4666003576829591,
      "step": 35300
    },
    {
      "epoch": 0.34427592645721594,
      "grad_norm": 0.25429078936576843,
      "learning_rate": 9.743434343434344e-05,
      "loss": 0.46838499030595127,
      "step": 35400
    },
    {
      "epoch": 0.34524845732291426,
      "grad_norm": 0.261874794960022,
      "learning_rate": 9.742424242424243e-05,
      "loss": 0.46678634212385456,
      "step": 35500
    },
    {
      "epoch": 0.34622098818861263,
      "grad_norm": 0.25487953424453735,
      "learning_rate": 9.741414141414141e-05,
      "loss": 0.4664813085819597,
      "step": 35600
    },
    {
      "epoch": 0.347193519054311,
      "grad_norm": 0.2624712884426117,
      "learning_rate": 9.740404040404042e-05,
      "loss": 0.4662757873853022,
      "step": 35700
    },
    {
      "epoch": 0.3481660499200093,
      "grad_norm": 0.24206966161727905,
      "learning_rate": 9.739393939393941e-05,
      "loss": 0.46646828222567854,
      "step": 35800
    },
    {
      "epoch": 0.3491385807857077,
      "grad_norm": 0.26030799746513367,
      "learning_rate": 9.738383838383839e-05,
      "loss": 0.4665612702781368,
      "step": 35900
    },
    {
      "epoch": 0.350111111651406,
      "grad_norm": 0.25378915667533875,
      "learning_rate": 9.737373737373738e-05,
      "loss": 0.46496275825036143,
      "step": 36000
    },
    {
      "epoch": 0.3510836425171044,
      "grad_norm": 0.2565891742706299,
      "learning_rate": 9.736363636363637e-05,
      "loss": 0.4663492442393176,
      "step": 36100
    },
    {
      "epoch": 0.35205617338280276,
      "grad_norm": 0.2513543963432312,
      "learning_rate": 9.735353535353536e-05,
      "loss": 0.46604424126267846,
      "step": 36200
    },
    {
      "epoch": 0.3530287042485011,
      "grad_norm": 0.25087830424308777,
      "learning_rate": 9.734343434343435e-05,
      "loss": 0.46613722931513685,
      "step": 36300
    },
    {
      "epoch": 0.35400123511419945,
      "grad_norm": 0.26052793860435486,
      "learning_rate": 9.733333333333335e-05,
      "loss": 0.46563324888523655,
      "step": 36400
    },
    {
      "epoch": 0.35497376597989777,
      "grad_norm": 0.2748456299304962,
      "learning_rate": 9.732323232323232e-05,
      "loss": 0.4650297894540142,
      "step": 36500
    },
    {
      "epoch": 0.35594629684559614,
      "grad_norm": 0.27905774116516113,
      "learning_rate": 9.731313131313132e-05,
      "loss": 0.4661177022846834,
      "step": 36600
    },
    {
      "epoch": 0.35691882771129446,
      "grad_norm": 0.28118839859962463,
      "learning_rate": 9.730303030303031e-05,
      "loss": 0.46581272292665105,
      "step": 36700
    },
    {
      "epoch": 0.35789135857699284,
      "grad_norm": 0.27011847496032715,
      "learning_rate": 9.72929292929293e-05,
      "loss": 0.4654082631779669,
      "step": 36800
    },
    {
      "epoch": 0.3588638894426912,
      "grad_norm": 0.28235313296318054,
      "learning_rate": 9.728282828282829e-05,
      "loss": 0.4645063778574172,
      "step": 36900
    },
    {
      "epoch": 0.3598364203083895,
      "grad_norm": 0.28526008129119873,
      "learning_rate": 9.727272727272728e-05,
      "loss": 0.46479834892045596,
      "step": 37000
    },
    {
      "epoch": 0.3608089511740879,
      "grad_norm": 0.2922307252883911,
      "learning_rate": 9.726262626262626e-05,
      "loss": 0.46688103370305717,
      "step": 37100
    },
    {
      "epoch": 0.3617814820397862,
      "grad_norm": 0.29750582575798035,
      "learning_rate": 9.725252525252527e-05,
      "loss": 0.4652827828692856,
      "step": 37200
    },
    {
      "epoch": 0.3627540129054846,
      "grad_norm": 0.25953537225723267,
      "learning_rate": 9.724242424242425e-05,
      "loss": 0.4648783578538466,
      "step": 37300
    },
    {
      "epoch": 0.36372654377118296,
      "grad_norm": 0.2790977656841278,
      "learning_rate": 9.723232323232324e-05,
      "loss": 0.4647723851250015,
      "step": 37400
    },
    {
      "epoch": 0.3646990746368813,
      "grad_norm": 0.265736848115921,
      "learning_rate": 9.722222222222223e-05,
      "loss": 0.46446745717217197,
      "step": 37500
    },
    {
      "epoch": 0.36567160550257966,
      "grad_norm": 0.2942790985107422,
      "learning_rate": 9.721212121212122e-05,
      "loss": 0.4636651494953607,
      "step": 37600
    },
    {
      "epoch": 0.366644136368278,
      "grad_norm": 0.2751169800758362,
      "learning_rate": 9.72020202020202e-05,
      "loss": 0.4642555311650989,
      "step": 37700
    },
    {
      "epoch": 0.36761666723397635,
      "grad_norm": 0.2565620243549347,
      "learning_rate": 9.71919191919192e-05,
      "loss": 0.46474642132954697,
      "step": 37800
    },
    {
      "epoch": 0.36858919809967466,
      "grad_norm": 0.24992318451404572,
      "learning_rate": 9.718181818181818e-05,
      "loss": 0.464839403824686,
      "step": 37900
    },
    {
      "epoch": 0.36956172896537304,
      "grad_norm": 0.2752377986907959,
      "learning_rate": 9.717171717171718e-05,
      "loss": 0.46264452628368863,
      "step": 38000
    },
    {
      "epoch": 0.3705342598310714,
      "grad_norm": 0.26732516288757324,
      "learning_rate": 9.716161616161617e-05,
      "loss": 0.4637322418295251,
      "step": 38100
    },
    {
      "epoch": 0.37150679069676973,
      "grad_norm": 0.2594308853149414,
      "learning_rate": 9.715151515151516e-05,
      "loss": 0.46322841839389306,
      "step": 38200
    },
    {
      "epoch": 0.3724793215624681,
      "grad_norm": 0.28561079502105713,
      "learning_rate": 9.714141414141414e-05,
      "loss": 0.4630230125116096,
      "step": 38300
    },
    {
      "epoch": 0.3734518524281664,
      "grad_norm": 0.2656315267086029,
      "learning_rate": 9.713131313131314e-05,
      "loss": 0.46361334416547473,
      "step": 38400
    },
    {
      "epoch": 0.3744243832938648,
      "grad_norm": 0.302337110042572,
      "learning_rate": 9.712121212121212e-05,
      "loss": 0.4628111476350481,
      "step": 38500
    },
    {
      "epoch": 0.37539691415956317,
      "grad_norm": 0.28532323241233826,
      "learning_rate": 9.711111111111111e-05,
      "loss": 0.4611137998656518,
      "step": 38600
    },
    {
      "epoch": 0.3763694450252615,
      "grad_norm": 0.2970796823501587,
      "learning_rate": 9.710101010101011e-05,
      "loss": 0.46299714319058205,
      "step": 38700
    },
    {
      "epoch": 0.37734197589095986,
      "grad_norm": 0.27406829595565796,
      "learning_rate": 9.70909090909091e-05,
      "loss": 0.46239391022011844,
      "step": 38800
    },
    {
      "epoch": 0.3783145067566582,
      "grad_norm": 0.27089056372642517,
      "learning_rate": 9.708080808080808e-05,
      "loss": 0.46208907257372656,
      "step": 38900
    },
    {
      "epoch": 0.37928703762235655,
      "grad_norm": 0.267106294631958,
      "learning_rate": 9.707070707070708e-05,
      "loss": 0.46377340649080995,
      "step": 39000
    },
    {
      "epoch": 0.38025956848805487,
      "grad_norm": 0.27273640036582947,
      "learning_rate": 9.706060606060606e-05,
      "loss": 0.4620761629211493,
      "step": 39100
    },
    {
      "epoch": 0.38123209935375324,
      "grad_norm": 0.25787490606307983,
      "learning_rate": 9.705050505050505e-05,
      "loss": 0.46187079732943026,
      "step": 39200
    },
    {
      "epoch": 0.3822046302194516,
      "grad_norm": 0.26815691590309143,
      "learning_rate": 9.704040404040405e-05,
      "loss": 0.4635550631693531,
      "step": 39300
    },
    {
      "epoch": 0.38317716108514993,
      "grad_norm": 0.2622944116592407,
      "learning_rate": 9.703030303030304e-05,
      "loss": 0.4609628197976719,
      "step": 39400
    },
    {
      "epoch": 0.3841496919508483,
      "grad_norm": 0.2750851511955261,
      "learning_rate": 9.702020202020202e-05,
      "loss": 0.4618514411865432,
      "step": 39500
    },
    {
      "epoch": 0.3851222228165466,
      "grad_norm": 0.26309382915496826,
      "learning_rate": 9.701010101010102e-05,
      "loss": 0.4636350859960418,
      "step": 39600
    },
    {
      "epoch": 0.386094753682245,
      "grad_norm": 0.282751202583313,
      "learning_rate": 9.7e-05,
      "loss": 0.4616396402191529,
      "step": 39700
    },
    {
      "epoch": 0.3870672845479433,
      "grad_norm": 0.27355432510375977,
      "learning_rate": 9.698989898989899e-05,
      "loss": 0.4617326379969198,
      "step": 39800
    },
    {
      "epoch": 0.3880398154136417,
      "grad_norm": 0.2670155465602875,
      "learning_rate": 9.697979797979798e-05,
      "loss": 0.4613284047089944,
      "step": 39900
    },
    {
      "epoch": 0.38901234627934006,
      "grad_norm": 0.28064486384391785,
      "learning_rate": 9.696969696969698e-05,
      "loss": 0.4610236268037843,
      "step": 40000
    },
    {
      "epoch": 0.38901234627934006,
      "eval_accuracy": 0.661308432155366,
      "eval_loss": 0.4528373275044261,
      "eval_runtime": 3722.5595,
      "eval_samples_per_second": 613.758,
      "eval_steps_per_second": 6.138,
      "step": 40000
    },
    {
      "epoch": 0.3899848771450384,
      "grad_norm": 0.29129892587661743,
      "learning_rate": 9.695959595959597e-05,
      "loss": 0.46260827193074466,
      "step": 40100
    },
    {
      "epoch": 0.39095740801073675,
      "grad_norm": 0.278202086687088,
      "learning_rate": 9.694949494949496e-05,
      "loss": 0.46071242046955174,
      "step": 40200
    },
    {
      "epoch": 0.39192993887643507,
      "grad_norm": 0.25817975401878357,
      "learning_rate": 9.693939393939394e-05,
      "loss": 0.4617003847054239,
      "step": 40300
    },
    {
      "epoch": 0.39290246974213344,
      "grad_norm": 0.2588147222995758,
      "learning_rate": 9.692929292929293e-05,
      "loss": 0.4613956179148522,
      "step": 40400
    },
    {
      "epoch": 0.3938750006078318,
      "grad_norm": 0.27433717250823975,
      "learning_rate": 9.691919191919192e-05,
      "loss": 0.46049423553169305,
      "step": 40500
    },
    {
      "epoch": 0.39484753147353013,
      "grad_norm": 0.27679091691970825,
      "learning_rate": 9.690909090909091e-05,
      "loss": 0.4595928781564705,
      "step": 40600
    },
    {
      "epoch": 0.3958200623392285,
      "grad_norm": 0.2837901711463928,
      "learning_rate": 9.68989898989899e-05,
      "loss": 0.46058080210177815,
      "step": 40700
    },
    {
      "epoch": 0.3967925932049268,
      "grad_norm": 0.2745140790939331,
      "learning_rate": 9.68888888888889e-05,
      "loss": 0.46027606726579195,
      "step": 40800
    },
    {
      "epoch": 0.3977651240706252,
      "grad_norm": 0.26056593656539917,
      "learning_rate": 9.687878787878788e-05,
      "loss": 0.46026963605811005,
      "step": 40900
    },
    {
      "epoch": 0.3987376549363235,
      "grad_norm": 0.27730607986450195,
      "learning_rate": 9.686868686868688e-05,
      "loss": 0.46046206559998554,
      "step": 41000
    },
    {
      "epoch": 0.3997101858020219,
      "grad_norm": 0.2562673091888428,
      "learning_rate": 9.685858585858586e-05,
      "loss": 0.4582681939337692,
      "step": 41100
    },
    {
      "epoch": 0.40068271666772026,
      "grad_norm": 0.27662450075149536,
      "learning_rate": 9.684848484848485e-05,
      "loss": 0.46084690801177874,
      "step": 41200
    },
    {
      "epoch": 0.4016552475334186,
      "grad_norm": 0.2713431417942047,
      "learning_rate": 9.683838383838384e-05,
      "loss": 0.4600450588138041,
      "step": 41300
    },
    {
      "epoch": 0.40262777839911695,
      "grad_norm": 0.26094064116477966,
      "learning_rate": 9.682828282828284e-05,
      "loss": 0.4600386303847819,
      "step": 41400
    },
    {
      "epoch": 0.40360030926481527,
      "grad_norm": 0.26482510566711426,
      "learning_rate": 9.681818181818181e-05,
      "loss": 0.4596345082432411,
      "step": 41500
    },
    {
      "epoch": 0.40457284013051364,
      "grad_norm": 0.2805701196193695,
      "learning_rate": 9.680808080808082e-05,
      "loss": 0.4592303972163388,
      "step": 41600
    },
    {
      "epoch": 0.405545370996212,
      "grad_norm": 0.26058024168014526,
      "learning_rate": 9.67979797979798e-05,
      "loss": 0.459522241850365,
      "step": 41700
    },
    {
      "epoch": 0.40651790186191034,
      "grad_norm": 0.26954397559165955,
      "learning_rate": 9.678787878787879e-05,
      "loss": 0.4594164011078516,
      "step": 41800
    },
    {
      "epoch": 0.4074904327276087,
      "grad_norm": 0.2896418869495392,
      "learning_rate": 9.677777777777778e-05,
      "loss": 0.45960881675642895,
      "step": 41900
    },
    {
      "epoch": 0.408462963593307,
      "grad_norm": 0.25006547570228577,
      "learning_rate": 9.676767676767677e-05,
      "loss": 0.45940356092176476,
      "step": 42000
    },
    {
      "epoch": 0.4094354944590054,
      "grad_norm": 0.26205864548683167,
      "learning_rate": 9.675757575757575e-05,
      "loss": 0.460391291750229,
      "step": 42100
    },
    {
      "epoch": 0.4104080253247037,
      "grad_norm": 0.2677450478076935,
      "learning_rate": 9.674747474747476e-05,
      "loss": 0.4601860303582456,
      "step": 42200
    },
    {
      "epoch": 0.4113805561904021,
      "grad_norm": 0.2655952572822571,
      "learning_rate": 9.673737373737374e-05,
      "loss": 0.45968253758310795,
      "step": 42300
    },
    {
      "epoch": 0.41235308705610046,
      "grad_norm": 0.2663410007953644,
      "learning_rate": 9.672727272727273e-05,
      "loss": 0.45977552424623647,
      "step": 42400
    },
    {
      "epoch": 0.4133256179217988,
      "grad_norm": 0.2778872549533844,
      "learning_rate": 9.671717171717172e-05,
      "loss": 0.4592720509217162,
      "step": 42500
    },
    {
      "epoch": 0.41429814878749716,
      "grad_norm": 0.28322315216064453,
      "learning_rate": 9.670707070707071e-05,
      "loss": 0.4595638566545077,
      "step": 42600
    },
    {
      "epoch": 0.4152706796531955,
      "grad_norm": 0.26517730951309204,
      "learning_rate": 9.669696969696969e-05,
      "loss": 0.4583645527087416,
      "step": 42700
    },
    {
      "epoch": 0.41624321051889385,
      "grad_norm": 0.2510454058647156,
      "learning_rate": 9.66868686868687e-05,
      "loss": 0.459053984076891,
      "step": 42800
    },
    {
      "epoch": 0.4172157413845922,
      "grad_norm": 0.27597925066947937,
      "learning_rate": 9.667676767676768e-05,
      "loss": 0.45904756815183706,
      "step": 42900
    },
    {
      "epoch": 0.41818827225029054,
      "grad_norm": 0.23621106147766113,
      "learning_rate": 9.666666666666667e-05,
      "loss": 0.4587429444622356,
      "step": 43000
    },
    {
      "epoch": 0.4191608031159889,
      "grad_norm": 0.25887933373451233,
      "learning_rate": 9.665656565656566e-05,
      "loss": 0.4585377303074656,
      "step": 43100
    },
    {
      "epoch": 0.42013333398168723,
      "grad_norm": 0.25094282627105713,
      "learning_rate": 9.664646464646465e-05,
      "loss": 0.4572391238052632,
      "step": 43200
    },
    {
      "epoch": 0.4211058648473856,
      "grad_norm": 0.2605741024017334,
      "learning_rate": 9.663636363636363e-05,
      "loss": 0.457530928148725,
      "step": 43300
    },
    {
      "epoch": 0.4220783957130839,
      "grad_norm": 0.27656540274620056,
      "learning_rate": 9.662626262626264e-05,
      "loss": 0.45752453306361807,
      "step": 43400
    },
    {
      "epoch": 0.4230509265787823,
      "grad_norm": 0.2486497461795807,
      "learning_rate": 9.661616161616161e-05,
      "loss": 0.45642478592164315,
      "step": 43500
    },
    {
      "epoch": 0.42402345744448067,
      "grad_norm": 0.3232065737247467,
      "learning_rate": 9.66060606060606e-05,
      "loss": 0.45810810840662625,
      "step": 43600
    },
    {
      "epoch": 0.424995988310179,
      "grad_norm": 0.26534947752952576,
      "learning_rate": 9.65959595959596e-05,
      "loss": 0.45740595494542363,
      "step": 43700
    },
    {
      "epoch": 0.42596851917587736,
      "grad_norm": 0.25753286480903625,
      "learning_rate": 9.658585858585859e-05,
      "loss": 0.45670382093483836,
      "step": 43800
    },
    {
      "epoch": 0.4269410500415757,
      "grad_norm": 0.2556716501712799,
      "learning_rate": 9.657575757575758e-05,
      "loss": 0.4578901179749406,
      "step": 43900
    },
    {
      "epoch": 0.42791358090727405,
      "grad_norm": 0.27227604389190674,
      "learning_rate": 9.656565656565657e-05,
      "loss": 0.4569892190785548,
      "step": 44000
    },
    {
      "epoch": 0.42888611177297237,
      "grad_norm": 0.2650161683559418,
      "learning_rate": 9.655555555555555e-05,
      "loss": 0.45747976746787,
      "step": 44100
    },
    {
      "epoch": 0.42985864263867074,
      "grad_norm": 0.2831735908985138,
      "learning_rate": 9.654545454545454e-05,
      "loss": 0.45816907379633676,
      "step": 44200
    },
    {
      "epoch": 0.4308311735043691,
      "grad_norm": 0.24034318327903748,
      "learning_rate": 9.653535353535355e-05,
      "loss": 0.4570694391900763,
      "step": 44300
    },
    {
      "epoch": 0.43180370437006743,
      "grad_norm": 0.24262060225009918,
      "learning_rate": 9.652525252525253e-05,
      "loss": 0.45646675083689753,
      "step": 44400
    },
    {
      "epoch": 0.4327762352357658,
      "grad_norm": 0.26751473546028137,
      "learning_rate": 9.651515151515152e-05,
      "loss": 0.4561622244003826,
      "step": 44500
    },
    {
      "epoch": 0.4337487661014641,
      "grad_norm": 0.24827095866203308,
      "learning_rate": 9.650505050505051e-05,
      "loss": 0.45625522773546884,
      "step": 44600
    },
    {
      "epoch": 0.4347212969671625,
      "grad_norm": 0.24722404778003693,
      "learning_rate": 9.64949494949495e-05,
      "loss": 0.45654698623104695,
      "step": 44700
    },
    {
      "epoch": 0.43569382783286087,
      "grad_norm": 0.2751869857311249,
      "learning_rate": 9.64848484848485e-05,
      "loss": 0.45644122606966236,
      "step": 44800
    },
    {
      "epoch": 0.4366663586985592,
      "grad_norm": 0.2818026542663574,
      "learning_rate": 9.647474747474749e-05,
      "loss": 0.4562360924960213,
      "step": 44900
    },
    {
      "epoch": 0.43763888956425756,
      "grad_norm": 0.24021205306053162,
      "learning_rate": 9.646464646464647e-05,
      "loss": 0.4565278384876311,
      "step": 45000
    },
    {
      "epoch": 0.4386114204299559,
      "grad_norm": 0.2639315128326416,
      "learning_rate": 9.645454545454546e-05,
      "loss": 0.456223335669723,
      "step": 45100
    },
    {
      "epoch": 0.43958395129565425,
      "grad_norm": 0.2615940570831299,
      "learning_rate": 9.644444444444445e-05,
      "loss": 0.4563163292795005,
      "step": 45200
    },
    {
      "epoch": 0.44055648216135257,
      "grad_norm": 0.26776254177093506,
      "learning_rate": 9.643434343434344e-05,
      "loss": 0.45601183757623076,
      "step": 45300
    },
    {
      "epoch": 0.44152901302705094,
      "grad_norm": 0.2528079152107239,
      "learning_rate": 9.642424242424243e-05,
      "loss": 0.4555086157204057,
      "step": 45400
    },
    {
      "epoch": 0.4425015438927493,
      "grad_norm": 0.25596991181373596,
      "learning_rate": 9.641414141414143e-05,
      "loss": 0.45649592558193813,
      "step": 45500
    },
    {
      "epoch": 0.44347407475844763,
      "grad_norm": 0.2627776861190796,
      "learning_rate": 9.64040404040404e-05,
      "loss": 0.45629081006958455,
      "step": 45600
    },
    {
      "epoch": 0.444446605624146,
      "grad_norm": 0.2504732012748718,
      "learning_rate": 9.63939393939394e-05,
      "loss": 0.45658252549593864,
      "step": 45700
    },
    {
      "epoch": 0.4454191364898443,
      "grad_norm": 0.26752930879592896,
      "learning_rate": 9.638383838383839e-05,
      "loss": 0.4563774141515745,
      "step": 45800
    },
    {
      "epoch": 0.4463916673555427,
      "grad_norm": 0.2659963369369507,
      "learning_rate": 9.637373737373738e-05,
      "loss": 0.45647039525738375,
      "step": 45900
    },
    {
      "epoch": 0.44736419822124107,
      "grad_norm": 0.27213895320892334,
      "learning_rate": 9.636363636363637e-05,
      "loss": 0.45646401267624515,
      "step": 46000
    },
    {
      "epoch": 0.4483367290869394,
      "grad_norm": 0.28735870122909546,
      "learning_rate": 9.635353535353536e-05,
      "loss": 0.4544704397159376,
      "step": 46100
    },
    {
      "epoch": 0.44930925995263776,
      "grad_norm": 0.273592084646225,
      "learning_rate": 9.634343434343434e-05,
      "loss": 0.45476215931028113,
      "step": 46200
    },
    {
      "epoch": 0.4502817908183361,
      "grad_norm": 0.2705141305923462,
      "learning_rate": 9.633333333333335e-05,
      "loss": 0.4551532273089446,
      "step": 46300
    },
    {
      "epoch": 0.45125432168403445,
      "grad_norm": 0.25047025084495544,
      "learning_rate": 9.632323232323233e-05,
      "loss": 0.4553455734910315,
      "step": 46400
    },
    {
      "epoch": 0.45222685254973277,
      "grad_norm": 0.2495398372411728,
      "learning_rate": 9.631313131313132e-05,
      "loss": 0.45484243638432476,
      "step": 46500
    },
    {
      "epoch": 0.45319938341543115,
      "grad_norm": 0.26603543758392334,
      "learning_rate": 9.630303030303031e-05,
      "loss": 0.4547367234601537,
      "step": 46600
    },
    {
      "epoch": 0.4541719142811295,
      "grad_norm": 0.23370735347270966,
      "learning_rate": 9.62929292929293e-05,
      "loss": 0.4555251739614586,
      "step": 46700
    },
    {
      "epoch": 0.45514444514682784,
      "grad_norm": 0.2616327106952667,
      "learning_rate": 9.628282828282828e-05,
      "loss": 0.45303505904304353,
      "step": 46800
    },
    {
      "epoch": 0.4561169760125262,
      "grad_norm": 0.2684006989002228,
      "learning_rate": 9.627272727272729e-05,
      "loss": 0.4561085242330379,
      "step": 46900
    },
    {
      "epoch": 0.4570895068782245,
      "grad_norm": 0.26295989751815796,
      "learning_rate": 9.626262626262627e-05,
      "loss": 0.45490998164000657,
      "step": 47000
    },
    {
      "epoch": 0.4580620377439229,
      "grad_norm": 0.2375536859035492,
      "learning_rate": 9.625252525252526e-05,
      "loss": 0.4541088548935322,
      "step": 47100
    },
    {
      "epoch": 0.4590345686096213,
      "grad_norm": 0.24975351989269257,
      "learning_rate": 9.624242424242425e-05,
      "loss": 0.4548972581576236,
      "step": 47200
    },
    {
      "epoch": 0.4600070994753196,
      "grad_norm": 0.2554408311843872,
      "learning_rate": 9.623232323232324e-05,
      "loss": 0.4535000965584217,
      "step": 47300
    },
    {
      "epoch": 0.46097963034101797,
      "grad_norm": 0.23986046016216278,
      "learning_rate": 9.622222222222222e-05,
      "loss": 0.45299704697949283,
      "step": 47400
    },
    {
      "epoch": 0.4619521612067163,
      "grad_norm": 0.26977017521858215,
      "learning_rate": 9.621212121212123e-05,
      "loss": 0.45497751300239037,
      "step": 47500
    },
    {
      "epoch": 0.46292469207241466,
      "grad_norm": 0.27476322650909424,
      "learning_rate": 9.62020202020202e-05,
      "loss": 0.4542757791187898,
      "step": 47600
    },
    {
      "epoch": 0.463897222938113,
      "grad_norm": 0.24997808039188385,
      "learning_rate": 9.61919191919192e-05,
      "loss": 0.4556601477691184,
      "step": 47700
    },
    {
      "epoch": 0.46486975380381135,
      "grad_norm": 0.2724290192127228,
      "learning_rate": 9.618181818181819e-05,
      "loss": 0.4542630723083645,
      "step": 47800
    },
    {
      "epoch": 0.4658422846695097,
      "grad_norm": 0.27122238278388977,
      "learning_rate": 9.617171717171718e-05,
      "loss": 0.45326337379293286,
      "step": 47900
    },
    {
      "epoch": 0.46681481553520804,
      "grad_norm": 0.24445538222789764,
      "learning_rate": 9.616161616161616e-05,
      "loss": 0.4529590349159421,
      "step": 48000
    },
    {
      "epoch": 0.4677873464009064,
      "grad_norm": 0.23492714762687683,
      "learning_rate": 9.615151515151516e-05,
      "loss": 0.45295269957201695,
      "step": 48100
    },
    {
      "epoch": 0.46875987726660473,
      "grad_norm": 0.2607640027999878,
      "learning_rate": 9.614141414141414e-05,
      "loss": 0.4539396676584166,
      "step": 48200
    },
    {
      "epoch": 0.4697324081323031,
      "grad_norm": 0.25223591923713684,
      "learning_rate": 9.613131313131313e-05,
      "loss": 0.454330634236004,
      "step": 48300
    },
    {
      "epoch": 0.4707049389980015,
      "grad_norm": 0.25953659415245056,
      "learning_rate": 9.612121212121213e-05,
      "loss": 0.4541256243127159,
      "step": 48400
    },
    {
      "epoch": 0.4716774698636998,
      "grad_norm": 0.24321730434894562,
      "learning_rate": 9.611111111111112e-05,
      "loss": 0.45292735819631647,
      "step": 48500
    },
    {
      "epoch": 0.47265000072939817,
      "grad_norm": 0.27006277441978455,
      "learning_rate": 9.61010101010101e-05,
      "loss": 0.4535169715666708,
      "step": 48600
    },
    {
      "epoch": 0.4736225315950965,
      "grad_norm": 0.26141592860221863,
      "learning_rate": 9.60909090909091e-05,
      "loss": 0.45281536411208284,
      "step": 48700
    },
    {
      "epoch": 0.47459506246079486,
      "grad_norm": 0.25547558069229126,
      "learning_rate": 9.608080808080808e-05,
      "loss": 0.4527097081504338,
      "step": 48800
    },
    {
      "epoch": 0.4755675933264932,
      "grad_norm": 0.2617071568965912,
      "learning_rate": 9.607070707070707e-05,
      "loss": 0.45329929929151125,
      "step": 48900
    },
    {
      "epoch": 0.47654012419219155,
      "grad_norm": 0.24676010012626648,
      "learning_rate": 9.606060606060606e-05,
      "loss": 0.4531936391618729,
      "step": 49000
    },
    {
      "epoch": 0.4775126550578899,
      "grad_norm": 0.26195600628852844,
      "learning_rate": 9.605050505050506e-05,
      "loss": 0.453485253167151,
      "step": 49100
    },
    {
      "epoch": 0.47848518592358824,
      "grad_norm": 0.2533164620399475,
      "learning_rate": 9.604040404040405e-05,
      "loss": 0.45208847919096545,
      "step": 49200
    },
    {
      "epoch": 0.4794577167892866,
      "grad_norm": 0.2578575611114502,
      "learning_rate": 9.603030303030304e-05,
      "loss": 0.4512876344784421,
      "step": 49300
    },
    {
      "epoch": 0.48043024765498493,
      "grad_norm": 0.26008784770965576,
      "learning_rate": 9.602020202020202e-05,
      "loss": 0.4518772033902426,
      "step": 49400
    },
    {
      "epoch": 0.4814027785206833,
      "grad_norm": 0.24432924389839172,
      "learning_rate": 9.601010101010101e-05,
      "loss": 0.4507784468404209,
      "step": 49500
    },
    {
      "epoch": 0.4823753093863816,
      "grad_norm": 0.27697181701660156,
      "learning_rate": 9.6e-05,
      "loss": 0.4520631822738188,
      "step": 49600
    },
    {
      "epoch": 0.48334784025208,
      "grad_norm": 0.23796455562114716,
      "learning_rate": 9.5989898989899e-05,
      "loss": 0.4522554770507029,
      "step": 49700
    },
    {
      "epoch": 0.48432037111777837,
      "grad_norm": 0.2671639919281006,
      "learning_rate": 9.597979797979799e-05,
      "loss": 0.4527456906115344,
      "step": 49800
    },
    {
      "epoch": 0.4852929019834767,
      "grad_norm": 0.25347474217414856,
      "learning_rate": 9.596969696969698e-05,
      "loss": 0.45035599527072134,
      "step": 49900
    },
    {
      "epoch": 0.48626543284917506,
      "grad_norm": 0.2578395903110504,
      "learning_rate": 9.595959595959596e-05,
      "loss": 0.4510448320057129,
      "step": 50000
    },
    {
      "epoch": 0.48626543284917506,
      "eval_accuracy": 0.6625884509476977,
      "eval_loss": 0.44342632166812285,
      "eval_runtime": 3731.1908,
      "eval_samples_per_second": 612.338,
      "eval_steps_per_second": 6.124,
      "step": 50000
    },
    {
      "epoch": 0.4872379637148734,
      "grad_norm": 0.2789456248283386,
      "learning_rate": 9.594949494949496e-05,
      "loss": 0.4502440901035954,
      "step": 50100
    },
    {
      "epoch": 0.48821049458057175,
      "grad_norm": 0.28423038125038147,
      "learning_rate": 9.593939393939394e-05,
      "loss": 0.4522238420109716,
      "step": 50200
    },
    {
      "epoch": 0.4891830254462701,
      "grad_norm": 0.2537689507007599,
      "learning_rate": 9.592929292929293e-05,
      "loss": 0.4526147196714513,
      "step": 50300
    },
    {
      "epoch": 0.49015555631196844,
      "grad_norm": 0.2606041431427002,
      "learning_rate": 9.591919191919192e-05,
      "loss": 0.45032449221732174,
      "step": 50400
    },
    {
      "epoch": 0.4911280871776668,
      "grad_norm": 0.2643934488296509,
      "learning_rate": 9.590909090909092e-05,
      "loss": 0.45131117549111077,
      "step": 50500
    },
    {
      "epoch": 0.49210061804336513,
      "grad_norm": 0.26895177364349365,
      "learning_rate": 9.58989898989899e-05,
      "loss": 0.45110626698889855,
      "step": 50600
    },
    {
      "epoch": 0.4930731489090635,
      "grad_norm": 0.25636330246925354,
      "learning_rate": 9.58888888888889e-05,
      "loss": 0.4511992508733675,
      "step": 50700
    },
    {
      "epoch": 0.4940456797747618,
      "grad_norm": 0.24398261308670044,
      "learning_rate": 9.587878787878788e-05,
      "loss": 0.4496042302313998,
      "step": 50800
    },
    {
      "epoch": 0.4950182106404602,
      "grad_norm": 0.256700336933136,
      "learning_rate": 9.586868686868687e-05,
      "loss": 0.4510873318129434,
      "step": 50900
    },
    {
      "epoch": 0.4959907415061586,
      "grad_norm": 0.25087857246398926,
      "learning_rate": 9.585858585858586e-05,
      "loss": 0.45167676873841217,
      "step": 51000
    },
    {
      "epoch": 0.4969632723718569,
      "grad_norm": 0.22713227570056915,
      "learning_rate": 9.584848484848486e-05,
      "loss": 0.4511739984147747,
      "step": 51100
    },
    {
      "epoch": 0.49793580323755526,
      "grad_norm": 0.24907200038433075,
      "learning_rate": 9.583838383838383e-05,
      "loss": 0.4521605719315088,
      "step": 51200
    },
    {
      "epoch": 0.4989083341032536,
      "grad_norm": 0.23383015394210815,
      "learning_rate": 9.582828282828284e-05,
      "loss": 0.45116137218547836,
      "step": 51300
    },
    {
      "epoch": 0.49988086496895195,
      "grad_norm": 0.2679339349269867,
      "learning_rate": 9.581818181818182e-05,
      "loss": 0.4498643425726081,
      "step": 51400
    },
    {
      "epoch": 0.5008533958346503,
      "grad_norm": 0.2329007238149643,
      "learning_rate": 9.580808080808081e-05,
      "loss": 0.4507516079755867,
      "step": 51500
    },
    {
      "epoch": 0.5018259267003486,
      "grad_norm": 0.29375821352005005,
      "learning_rate": 9.57979797979798e-05,
      "loss": 0.4505467342066197,
      "step": 51600
    },
    {
      "epoch": 0.502798457566047,
      "grad_norm": 0.24894484877586365,
      "learning_rate": 9.57878787878788e-05,
      "loss": 0.4504411477114612,
      "step": 51700
    },
    {
      "epoch": 0.5037709884317454,
      "grad_norm": 0.26253482699394226,
      "learning_rate": 9.577777777777777e-05,
      "loss": 0.4513283672665561,
      "step": 51800
    },
    {
      "epoch": 0.5047435192974437,
      "grad_norm": 0.2629137635231018,
      "learning_rate": 9.576767676767678e-05,
      "loss": 0.4498328673058047,
      "step": 51900
    },
    {
      "epoch": 0.505716050163142,
      "grad_norm": 0.2590993046760559,
      "learning_rate": 9.575757575757576e-05,
      "loss": 0.4502236824464431,
      "step": 52000
    },
    {
      "epoch": 0.5066885810288404,
      "grad_norm": 0.295228511095047,
      "learning_rate": 9.574747474747475e-05,
      "loss": 0.4504159341541031,
      "step": 52100
    },
    {
      "epoch": 0.5076611118945388,
      "grad_norm": 0.2466283142566681,
      "learning_rate": 9.573737373737374e-05,
      "loss": 0.44892050921716126,
      "step": 52200
    },
    {
      "epoch": 0.5086336427602371,
      "grad_norm": 0.27825629711151123,
      "learning_rate": 9.572727272727273e-05,
      "loss": 0.44990696047287226,
      "step": 52300
    },
    {
      "epoch": 0.5096061736259354,
      "grad_norm": 0.2653212249279022,
      "learning_rate": 9.571717171717171e-05,
      "loss": 0.44910648810073756,
      "step": 52400
    },
    {
      "epoch": 0.5105787044916338,
      "grad_norm": 0.26705583930015564,
      "learning_rate": 9.570707070707072e-05,
      "loss": 0.44890166156898403,
      "step": 52500
    },
    {
      "epoch": 0.5115512353573322,
      "grad_norm": 0.24407342076301575,
      "learning_rate": 9.56969696969697e-05,
      "loss": 0.4498880711448008,
      "step": 52600
    },
    {
      "epoch": 0.5125237662230305,
      "grad_norm": 0.26243284344673157,
      "learning_rate": 9.568686868686869e-05,
      "loss": 0.4501795781716837,
      "step": 52700
    },
    {
      "epoch": 0.5134962970887289,
      "grad_norm": 0.24341705441474915,
      "learning_rate": 9.567676767676769e-05,
      "loss": 0.4486842810530842,
      "step": 52800
    },
    {
      "epoch": 0.5144688279544272,
      "grad_norm": 0.2614164352416992,
      "learning_rate": 9.566666666666667e-05,
      "loss": 0.4499684468612608,
      "step": 52900
    },
    {
      "epoch": 0.5154413588201255,
      "grad_norm": 0.2653907537460327,
      "learning_rate": 9.565656565656566e-05,
      "loss": 0.4492673034428287,
      "step": 53000
    },
    {
      "epoch": 0.5164138896858239,
      "grad_norm": 0.2481352686882019,
      "learning_rate": 9.564646464646465e-05,
      "loss": 0.4488639662726296,
      "step": 53100
    },
    {
      "epoch": 0.5173864205515223,
      "grad_norm": 0.2323462814092636,
      "learning_rate": 9.563636363636365e-05,
      "loss": 0.45024733599482614,
      "step": 53200
    },
    {
      "epoch": 0.5183589514172207,
      "grad_norm": 0.2807479202747345,
      "learning_rate": 9.562626262626262e-05,
      "loss": 0.4497447365587551,
      "step": 53300
    },
    {
      "epoch": 0.5193314822829189,
      "grad_norm": 0.27780377864837646,
      "learning_rate": 9.561616161616163e-05,
      "loss": 0.4501354738969242,
      "step": 53400
    },
    {
      "epoch": 0.5203040131486173,
      "grad_norm": 0.2629110515117645,
      "learning_rate": 9.560606060606061e-05,
      "loss": 0.4494343763263758,
      "step": 53500
    },
    {
      "epoch": 0.5212765440143157,
      "grad_norm": 0.273952841758728,
      "learning_rate": 9.55959595959596e-05,
      "loss": 0.4483362769295533,
      "step": 53600
    },
    {
      "epoch": 0.522249074880014,
      "grad_norm": 0.25064167380332947,
      "learning_rate": 9.558585858585859e-05,
      "loss": 0.4478337316773448,
      "step": 53700
    },
    {
      "epoch": 0.5232216057457123,
      "grad_norm": 0.2316107302904129,
      "learning_rate": 9.557575757575758e-05,
      "loss": 0.448025968102377,
      "step": 53800
    },
    {
      "epoch": 0.5241941366114107,
      "grad_norm": 0.2552761137485504,
      "learning_rate": 9.556565656565656e-05,
      "loss": 0.4488137058774903,
      "step": 53900
    },
    {
      "epoch": 0.525166667477109,
      "grad_norm": 0.2321092188358307,
      "learning_rate": 9.555555555555557e-05,
      "loss": 0.4490059228519051,
      "step": 54000
    },
    {
      "epoch": 0.5261391983428074,
      "grad_norm": 0.2527371942996979,
      "learning_rate": 9.554545454545455e-05,
      "loss": 0.44830489891583647,
      "step": 54100
    },
    {
      "epoch": 0.5271117292085058,
      "grad_norm": 0.2501784861087799,
      "learning_rate": 9.553535353535354e-05,
      "loss": 0.44909259917904504,
      "step": 54200
    },
    {
      "epoch": 0.5280842600742041,
      "grad_norm": 0.256835401058197,
      "learning_rate": 9.552525252525253e-05,
      "loss": 0.4497810316190624,
      "step": 54300
    },
    {
      "epoch": 0.5290567909399024,
      "grad_norm": 0.2417600154876709,
      "learning_rate": 9.551515151515152e-05,
      "loss": 0.4483853163121907,
      "step": 54400
    },
    {
      "epoch": 0.5300293218056008,
      "grad_norm": 0.25425806641578674,
      "learning_rate": 9.550505050505051e-05,
      "loss": 0.4480813108743539,
      "step": 54500
    },
    {
      "epoch": 0.5310018526712992,
      "grad_norm": 0.2559042274951935,
      "learning_rate": 9.54949494949495e-05,
      "loss": 0.44817427947619487,
      "step": 54600
    },
    {
      "epoch": 0.5319743835369976,
      "grad_norm": 0.2518794536590576,
      "learning_rate": 9.548484848484849e-05,
      "loss": 0.4488626855189459,
      "step": 54700
    },
    {
      "epoch": 0.5329469144026958,
      "grad_norm": 0.2504938244819641,
      "learning_rate": 9.547474747474748e-05,
      "loss": 0.4491541175220191,
      "step": 54800
    },
    {
      "epoch": 0.5339194452683942,
      "grad_norm": 0.23712573945522308,
      "learning_rate": 9.546464646464647e-05,
      "loss": 0.4471630842566009,
      "step": 54900
    },
    {
      "epoch": 0.5348919761340926,
      "grad_norm": 0.24276192486286163,
      "learning_rate": 9.545454545454546e-05,
      "loss": 0.4472560598050909,
      "step": 55000
    },
    {
      "epoch": 0.5358645069997909,
      "grad_norm": 0.255618155002594,
      "learning_rate": 9.544444444444445e-05,
      "loss": 0.4487383152847813,
      "step": 55100
    },
    {
      "epoch": 0.5368370378654893,
      "grad_norm": 0.2743552029132843,
      "learning_rate": 9.543434343434344e-05,
      "loss": 0.44922819818511844,
      "step": 55200
    },
    {
      "epoch": 0.5378095687311876,
      "grad_norm": 0.255155086517334,
      "learning_rate": 9.542424242424242e-05,
      "loss": 0.44664188447305847,
      "step": 55300
    },
    {
      "epoch": 0.5387820995968859,
      "grad_norm": 0.22947363555431366,
      "learning_rate": 9.541414141414143e-05,
      "loss": 0.44792562514631384,
      "step": 55400
    },
    {
      "epoch": 0.5397546304625843,
      "grad_norm": 0.2690829932689667,
      "learning_rate": 9.540404040404041e-05,
      "loss": 0.44841549832134225,
      "step": 55500
    },
    {
      "epoch": 0.5407271613282827,
      "grad_norm": 0.26587963104248047,
      "learning_rate": 9.53939393939394e-05,
      "loss": 0.4492030402009527,
      "step": 55600
    },
    {
      "epoch": 0.541699692193981,
      "grad_norm": 0.24292093515396118,
      "learning_rate": 9.538383838383839e-05,
      "loss": 0.44731145398227073,
      "step": 55700
    },
    {
      "epoch": 0.5426722230596793,
      "grad_norm": 0.25771716237068176,
      "learning_rate": 9.537373737373738e-05,
      "loss": 0.44730519088349563,
      "step": 55800
    },
    {
      "epoch": 0.5436447539253777,
      "grad_norm": 0.28353264927864075,
      "learning_rate": 9.536363636363636e-05,
      "loss": 0.4464059175029853,
      "step": 55900
    },
    {
      "epoch": 0.5446172847910761,
      "grad_norm": 0.265007883310318,
      "learning_rate": 9.535353535353537e-05,
      "loss": 0.44778877456248267,
      "step": 56000
    },
    {
      "epoch": 0.5455898156567744,
      "grad_norm": 0.25788652896881104,
      "learning_rate": 9.534343434343435e-05,
      "loss": 0.44778250451705853,
      "step": 56100
    },
    {
      "epoch": 0.5465623465224727,
      "grad_norm": 0.25383785367012024,
      "learning_rate": 9.533333333333334e-05,
      "loss": 0.44579185053867787,
      "step": 56200
    },
    {
      "epoch": 0.5475348773881711,
      "grad_norm": 0.26480481028556824,
      "learning_rate": 9.532323232323233e-05,
      "loss": 0.44667856854571203,
      "step": 56300
    },
    {
      "epoch": 0.5485074082538695,
      "grad_norm": 0.2774824798107147,
      "learning_rate": 9.531313131313132e-05,
      "loss": 0.4478629107987744,
      "step": 56400
    },
    {
      "epoch": 0.5494799391195678,
      "grad_norm": 0.24285756051540375,
      "learning_rate": 9.53030303030303e-05,
      "loss": 0.4463684139341444,
      "step": 56500
    },
    {
      "epoch": 0.5504524699852662,
      "grad_norm": 0.27325108647346497,
      "learning_rate": 9.52929292929293e-05,
      "loss": 0.44725508609329484,
      "step": 56600
    },
    {
      "epoch": 0.5514250008509645,
      "grad_norm": 0.26550227403640747,
      "learning_rate": 9.528282828282828e-05,
      "loss": 0.4453637902445429,
      "step": 56700
    },
    {
      "epoch": 0.5523975317166628,
      "grad_norm": 0.244295135140419,
      "learning_rate": 9.527272727272728e-05,
      "loss": 0.446052029567717,
      "step": 56800
    },
    {
      "epoch": 0.5533700625823612,
      "grad_norm": 0.2573947310447693,
      "learning_rate": 9.526262626262627e-05,
      "loss": 0.4475339252109871,
      "step": 56900
    },
    {
      "epoch": 0.5543425934480596,
      "grad_norm": 0.24309861660003662,
      "learning_rate": 9.525252525252526e-05,
      "loss": 0.44703161753417575,
      "step": 57000
    },
    {
      "epoch": 0.555315124313758,
      "grad_norm": 0.2766493558883667,
      "learning_rate": 9.524242424242424e-05,
      "loss": 0.4474221839847785,
      "step": 57100
    },
    {
      "epoch": 0.5562876551794562,
      "grad_norm": 0.2741601765155792,
      "learning_rate": 9.523232323232324e-05,
      "loss": 0.4463246597704966,
      "step": 57200
    },
    {
      "epoch": 0.5572601860451546,
      "grad_norm": 0.26000213623046875,
      "learning_rate": 9.522222222222222e-05,
      "loss": 0.44592159351961,
      "step": 57300
    },
    {
      "epoch": 0.558232716910853,
      "grad_norm": 0.2429603785276413,
      "learning_rate": 9.521212121212121e-05,
      "loss": 0.4466097661549533,
      "step": 57400
    },
    {
      "epoch": 0.5592052477765513,
      "grad_norm": 0.2374659925699234,
      "learning_rate": 9.52020202020202e-05,
      "loss": 0.4462067068507158,
      "step": 57500
    },
    {
      "epoch": 0.5601777786422497,
      "grad_norm": 0.2394726425409317,
      "learning_rate": 9.51919191919192e-05,
      "loss": 0.4462996573912693,
      "step": 57600
    },
    {
      "epoch": 0.561150309507948,
      "grad_norm": 0.25124794244766235,
      "learning_rate": 9.518181818181818e-05,
      "loss": 0.4475829854335716,
      "step": 57700
    },
    {
      "epoch": 0.5621228403736463,
      "grad_norm": 0.2479628622531891,
      "learning_rate": 9.517171717171718e-05,
      "loss": 0.4463863531690266,
      "step": 57800
    },
    {
      "epoch": 0.5630953712393447,
      "grad_norm": 0.24569739401340485,
      "learning_rate": 9.516161616161616e-05,
      "loss": 0.44399940731190385,
      "step": 57900
    },
    {
      "epoch": 0.5640679021050431,
      "grad_norm": 0.2752109467983246,
      "learning_rate": 9.515151515151515e-05,
      "loss": 0.44607626663461947,
      "step": 58000
    },
    {
      "epoch": 0.5650404329707414,
      "grad_norm": 0.269525945186615,
      "learning_rate": 9.514141414141414e-05,
      "loss": 0.4460700188184722,
      "step": 58100
    },
    {
      "epoch": 0.5660129638364397,
      "grad_norm": 0.27042973041534424,
      "learning_rate": 9.513131313131314e-05,
      "loss": 0.446063771002325,
      "step": 58200
    },
    {
      "epoch": 0.5669854947021381,
      "grad_norm": 0.2591388523578644,
      "learning_rate": 9.512121212121213e-05,
      "loss": 0.4454623830618467,
      "step": 58300
    },
    {
      "epoch": 0.5679580255678365,
      "grad_norm": 0.2445707768201828,
      "learning_rate": 9.511111111111112e-05,
      "loss": 0.444761823161934,
      "step": 58400
    },
    {
      "epoch": 0.5689305564335349,
      "grad_norm": 0.26164567470550537,
      "learning_rate": 9.51010101010101e-05,
      "loss": 0.4455490913435722,
      "step": 58500
    },
    {
      "epoch": 0.5699030872992331,
      "grad_norm": 0.2733512222766876,
      "learning_rate": 9.509090909090909e-05,
      "loss": 0.44693145241232807,
      "step": 58600
    },
    {
      "epoch": 0.5708756181649315,
      "grad_norm": 0.243785560131073,
      "learning_rate": 9.508080808080808e-05,
      "loss": 0.4447431338973549,
      "step": 58700
    },
    {
      "epoch": 0.5718481490306299,
      "grad_norm": 0.24342942237854004,
      "learning_rate": 9.507070707070707e-05,
      "loss": 0.44622465025358743,
      "step": 58800
    },
    {
      "epoch": 0.5728206798963282,
      "grad_norm": 0.2614254057407379,
      "learning_rate": 9.506060606060607e-05,
      "loss": 0.4445323110181495,
      "step": 58900
    },
    {
      "epoch": 0.5737932107620266,
      "grad_norm": 0.2542012333869934,
      "learning_rate": 9.505050505050506e-05,
      "loss": 0.44522034610984185,
      "step": 59000
    },
    {
      "epoch": 0.5747657416277249,
      "grad_norm": 0.2245129942893982,
      "learning_rate": 9.504040404040404e-05,
      "loss": 0.44521410940833306,
      "step": 59100
    },
    {
      "epoch": 0.5757382724934232,
      "grad_norm": 0.24387025833129883,
      "learning_rate": 9.503030303030304e-05,
      "loss": 0.44500951767331715,
      "step": 59200
    },
    {
      "epoch": 0.5767108033591216,
      "grad_norm": 0.2974686920642853,
      "learning_rate": 9.502020202020202e-05,
      "loss": 0.44599504502470577,
      "step": 59300
    },
    {
      "epoch": 0.57768333422482,
      "grad_norm": 0.2565464675426483,
      "learning_rate": 9.501010101010101e-05,
      "loss": 0.4449970498276189,
      "step": 59400
    },
    {
      "epoch": 0.5786558650905184,
      "grad_norm": 0.2425154596567154,
      "learning_rate": 9.5e-05,
      "loss": 0.44469329585847733,
      "step": 59500
    },
    {
      "epoch": 0.5796283959562166,
      "grad_norm": 0.2960730493068695,
      "learning_rate": 9.4989898989899e-05,
      "loss": 0.4456787856979611,
      "step": 59600
    },
    {
      "epoch": 0.580600926821915,
      "grad_norm": 0.2772237956523895,
      "learning_rate": 9.497979797979798e-05,
      "loss": 0.4458708831900124,
      "step": 59700
    },
    {
      "epoch": 0.5815734576876134,
      "grad_norm": 0.250827819108963,
      "learning_rate": 9.496969696969698e-05,
      "loss": 0.4451704524977719,
      "step": 59800
    },
    {
      "epoch": 0.5825459885533117,
      "grad_norm": 0.24191945791244507,
      "learning_rate": 9.495959595959596e-05,
      "loss": 0.4443708734647037,
      "step": 59900
    },
    {
      "epoch": 0.58351851941901,
      "grad_norm": 0.25221192836761475,
      "learning_rate": 9.494949494949495e-05,
      "loss": 0.4452571454968695,
      "step": 60000
    },
    {
      "epoch": 0.58351851941901,
      "eval_accuracy": 0.6633466050829643,
      "eval_loss": 0.43690128621846236,
      "eval_runtime": 3732.5792,
      "eval_samples_per_second": 612.11,
      "eval_steps_per_second": 6.121,
      "step": 60000
    },
    {
      "epoch": 0.5844910502847084,
      "grad_norm": 0.26314476132392883,
      "learning_rate": 9.493939393939394e-05,
      "loss": 0.4439617622398219,
      "step": 60100
    },
    {
      "epoch": 0.5854635811504068,
      "grad_norm": 0.23815038800239563,
      "learning_rate": 9.492929292929294e-05,
      "loss": 0.4453438329386479,
      "step": 60200
    },
    {
      "epoch": 0.5864361120161051,
      "grad_norm": 0.25233784317970276,
      "learning_rate": 9.491919191919191e-05,
      "loss": 0.44325518654184,
      "step": 60300
    },
    {
      "epoch": 0.5874086428818035,
      "grad_norm": 0.2584894597530365,
      "learning_rate": 9.490909090909092e-05,
      "loss": 0.4441414238407606,
      "step": 60400
    },
    {
      "epoch": 0.5883811737475018,
      "grad_norm": 0.27630648016929626,
      "learning_rate": 9.48989898989899e-05,
      "loss": 0.4435402442997534,
      "step": 60500
    },
    {
      "epoch": 0.5893537046132001,
      "grad_norm": 0.24476274847984314,
      "learning_rate": 9.488888888888889e-05,
      "loss": 0.44422813629047553,
      "step": 60600
    },
    {
      "epoch": 0.5903262354788985,
      "grad_norm": 0.23526346683502197,
      "learning_rate": 9.487878787878788e-05,
      "loss": 0.4451143221841932,
      "step": 60700
    },
    {
      "epoch": 0.5912987663445969,
      "grad_norm": 0.2731322646141052,
      "learning_rate": 9.486868686868687e-05,
      "loss": 0.443819066745488,
      "step": 60800
    },
    {
      "epoch": 0.5922712972102953,
      "grad_norm": 0.2403007298707962,
      "learning_rate": 9.485858585858585e-05,
      "loss": 0.444903540984882,
      "step": 60900
    },
    {
      "epoch": 0.5932438280759935,
      "grad_norm": 0.23990817368030548,
      "learning_rate": 9.484848484848486e-05,
      "loss": 0.44380662946504545,
      "step": 61000
    },
    {
      "epoch": 0.5942163589416919,
      "grad_norm": 0.2673702538013458,
      "learning_rate": 9.483838383838384e-05,
      "loss": 0.4427097485104648,
      "step": 61100
    },
    {
      "epoch": 0.5951888898073903,
      "grad_norm": 0.2596803903579712,
      "learning_rate": 9.482828282828283e-05,
      "loss": 0.44359589272428823,
      "step": 61200
    },
    {
      "epoch": 0.5961614206730886,
      "grad_norm": 0.2478666454553604,
      "learning_rate": 9.481818181818183e-05,
      "loss": 0.4424990451136228,
      "step": 61300
    },
    {
      "epoch": 0.597133951538787,
      "grad_norm": 0.24092809855937958,
      "learning_rate": 9.480808080808081e-05,
      "loss": 0.44457493051614294,
      "step": 61400
    },
    {
      "epoch": 0.5981064824044853,
      "grad_norm": 0.2789953649044037,
      "learning_rate": 9.47979797979798e-05,
      "loss": 0.44367639070177134,
      "step": 61500
    },
    {
      "epoch": 0.5990790132701836,
      "grad_norm": 0.2501171827316284,
      "learning_rate": 9.47878787878788e-05,
      "loss": 0.4430753084138508,
      "step": 61600
    },
    {
      "epoch": 0.600051544135882,
      "grad_norm": 0.2406519502401352,
      "learning_rate": 9.477777777777779e-05,
      "loss": 0.44267252836490467,
      "step": 61700
    },
    {
      "epoch": 0.6010240750015804,
      "grad_norm": 0.2601340413093567,
      "learning_rate": 9.476767676767677e-05,
      "loss": 0.44405430452581135,
      "step": 61800
    },
    {
      "epoch": 0.6019966058672788,
      "grad_norm": 0.2564750015735626,
      "learning_rate": 9.475757575757577e-05,
      "loss": 0.4433541016836593,
      "step": 61900
    },
    {
      "epoch": 0.602969136732977,
      "grad_norm": 0.23967796564102173,
      "learning_rate": 9.474747474747475e-05,
      "loss": 0.4428521955231625,
      "step": 62000
    },
    {
      "epoch": 0.6039416675986754,
      "grad_norm": 0.24163438379764557,
      "learning_rate": 9.473737373737374e-05,
      "loss": 0.4427468521607204,
      "step": 62100
    },
    {
      "epoch": 0.6049141984643738,
      "grad_norm": 0.24624153971672058,
      "learning_rate": 9.472727272727273e-05,
      "loss": 0.44422768496668735,
      "step": 62200
    },
    {
      "epoch": 0.6058867293300721,
      "grad_norm": 0.2696393132209778,
      "learning_rate": 9.471717171717173e-05,
      "loss": 0.4427344426668741,
      "step": 62300
    },
    {
      "epoch": 0.6068592601957704,
      "grad_norm": 0.24984371662139893,
      "learning_rate": 9.47070707070707e-05,
      "loss": 0.442728237919951,
      "step": 62400
    },
    {
      "epoch": 0.6078317910614688,
      "grad_norm": 0.2579498291015625,
      "learning_rate": 9.469696969696971e-05,
      "loss": 0.4429202965107676,
      "step": 62500
    },
    {
      "epoch": 0.6088043219271672,
      "grad_norm": 0.24918492138385773,
      "learning_rate": 9.468686868686869e-05,
      "loss": 0.44182365591024375,
      "step": 62600
    },
    {
      "epoch": 0.6097768527928655,
      "grad_norm": 0.2645260989665985,
      "learning_rate": 9.467676767676768e-05,
      "loss": 0.4424122370085506,
      "step": 62700
    },
    {
      "epoch": 0.6107493836585639,
      "grad_norm": 0.26013144850730896,
      "learning_rate": 9.466666666666667e-05,
      "loss": 0.4429016739340193,
      "step": 62800
    },
    {
      "epoch": 0.6117219145242622,
      "grad_norm": 0.2541009783744812,
      "learning_rate": 9.465656565656566e-05,
      "loss": 0.4445806103047982,
      "step": 62900
    },
    {
      "epoch": 0.6126944453899605,
      "grad_norm": 0.24179232120513916,
      "learning_rate": 9.464646464646464e-05,
      "loss": 0.4421953858273077,
      "step": 63000
    },
    {
      "epoch": 0.6136669762556589,
      "grad_norm": 0.24162861704826355,
      "learning_rate": 9.463636363636365e-05,
      "loss": 0.4435769146875085,
      "step": 63100
    },
    {
      "epoch": 0.6146395071213573,
      "grad_norm": 0.24071727693080902,
      "learning_rate": 9.462626262626263e-05,
      "loss": 0.44357069743661703,
      "step": 63200
    },
    {
      "epoch": 0.6156120379870557,
      "grad_norm": 0.2670397460460663,
      "learning_rate": 9.461616161616162e-05,
      "loss": 0.4421767924264854,
      "step": 63300
    },
    {
      "epoch": 0.6165845688527539,
      "grad_norm": 0.2596077024936676,
      "learning_rate": 9.460606060606061e-05,
      "loss": 0.44395473959444076,
      "step": 63400
    },
    {
      "epoch": 0.6175570997184523,
      "grad_norm": 0.245505228638649,
      "learning_rate": 9.45959595959596e-05,
      "loss": 0.44156969017250614,
      "step": 63500
    },
    {
      "epoch": 0.6185296305841507,
      "grad_norm": 0.24047540128231049,
      "learning_rate": 9.45858585858586e-05,
      "loss": 0.4435458284330513,
      "step": 63600
    },
    {
      "epoch": 0.619502161449849,
      "grad_norm": 0.23707328736782074,
      "learning_rate": 9.457575757575759e-05,
      "loss": 0.4420528862284767,
      "step": 63700
    },
    {
      "epoch": 0.6204746923155474,
      "grad_norm": 0.29128336906433105,
      "learning_rate": 9.456565656565657e-05,
      "loss": 0.44204668981753237,
      "step": 63800
    },
    {
      "epoch": 0.6214472231812457,
      "grad_norm": 0.23437052965164185,
      "learning_rate": 9.455555555555556e-05,
      "loss": 0.44402273777164003,
      "step": 63900
    },
    {
      "epoch": 0.622419754046944,
      "grad_norm": 0.261710524559021,
      "learning_rate": 9.454545454545455e-05,
      "loss": 0.44223251865348934,
      "step": 64000
    },
    {
      "epoch": 0.6233922849126424,
      "grad_norm": 0.24320146441459656,
      "learning_rate": 9.453535353535354e-05,
      "loss": 0.4424245383430714,
      "step": 64100
    },
    {
      "epoch": 0.6243648157783408,
      "grad_norm": 0.24751539528369904,
      "learning_rate": 9.452525252525253e-05,
      "loss": 0.44152636392243927,
      "step": 64200
    },
    {
      "epoch": 0.6253373466440391,
      "grad_norm": 0.2487853616476059,
      "learning_rate": 9.451515151515153e-05,
      "loss": 0.441520174458144,
      "step": 64300
    },
    {
      "epoch": 0.6263098775097374,
      "grad_norm": 0.24347567558288574,
      "learning_rate": 9.45050505050505e-05,
      "loss": 0.4439916167839377,
      "step": 64400
    },
    {
      "epoch": 0.6272824083754358,
      "grad_norm": 0.24249331653118134,
      "learning_rate": 9.449494949494951e-05,
      "loss": 0.440913172235911,
      "step": 64500
    },
    {
      "epoch": 0.6282549392411342,
      "grad_norm": 0.23754490911960602,
      "learning_rate": 9.448484848484849e-05,
      "loss": 0.441898016037034,
      "step": 64600
    },
    {
      "epoch": 0.6292274701068326,
      "grad_norm": 0.24455733597278595,
      "learning_rate": 9.447474747474748e-05,
      "loss": 0.44199092211903357,
      "step": 64700
    },
    {
      "epoch": 0.6302000009725308,
      "grad_norm": 0.2660384774208069,
      "learning_rate": 9.446464646464647e-05,
      "loss": 0.441588326850952,
      "step": 64800
    },
    {
      "epoch": 0.6311725318382292,
      "grad_norm": 0.25828036665916443,
      "learning_rate": 9.445454545454546e-05,
      "loss": 0.442176725947054,
      "step": 64900
    },
    {
      "epoch": 0.6321450627039276,
      "grad_norm": 0.23908258974552155,
      "learning_rate": 9.444444444444444e-05,
      "loss": 0.44068407272307947,
      "step": 65000
    },
    {
      "epoch": 0.6331175935696259,
      "grad_norm": 0.24127033352851868,
      "learning_rate": 9.443434343434345e-05,
      "loss": 0.4418670409289615,
      "step": 65100
    },
    {
      "epoch": 0.6340901244353243,
      "grad_norm": 0.2475101798772812,
      "learning_rate": 9.442424242424243e-05,
      "loss": 0.4412662809655564,
      "step": 65200
    },
    {
      "epoch": 0.6350626553010226,
      "grad_norm": 0.2537989318370819,
      "learning_rate": 9.441414141414142e-05,
      "loss": 0.4411610015122855,
      "step": 65300
    },
    {
      "epoch": 0.636035186166721,
      "grad_norm": 0.2560092806816101,
      "learning_rate": 9.440404040404041e-05,
      "loss": 0.44155118172920155,
      "step": 65400
    },
    {
      "epoch": 0.6370077170324193,
      "grad_norm": 0.2488737255334854,
      "learning_rate": 9.43939393939394e-05,
      "loss": 0.4403559110078685,
      "step": 65500
    },
    {
      "epoch": 0.6379802478981177,
      "grad_norm": 0.27255943417549133,
      "learning_rate": 9.438383838383838e-05,
      "loss": 0.4406470026251387,
      "step": 65600
    },
    {
      "epoch": 0.6389527787638161,
      "grad_norm": 0.26122212409973145,
      "learning_rate": 9.437373737373739e-05,
      "loss": 0.44064082427548185,
      "step": 65700
    },
    {
      "epoch": 0.6399253096295143,
      "grad_norm": 0.24356389045715332,
      "learning_rate": 9.436363636363636e-05,
      "loss": 0.4409319033887838,
      "step": 65800
    },
    {
      "epoch": 0.6408978404952127,
      "grad_norm": 0.28776416182518005,
      "learning_rate": 9.435353535353536e-05,
      "loss": 0.44112388973445044,
      "step": 65900
    },
    {
      "epoch": 0.6418703713609111,
      "grad_norm": 0.2899966835975647,
      "learning_rate": 9.434343434343435e-05,
      "loss": 0.43943329271859194,
      "step": 66000
    },
    {
      "epoch": 0.6428429022266094,
      "grad_norm": 0.2827445864677429,
      "learning_rate": 9.433333333333334e-05,
      "loss": 0.44031886591786423,
      "step": 66100
    },
    {
      "epoch": 0.6438154330923078,
      "grad_norm": 0.2529623508453369,
      "learning_rate": 9.432323232323232e-05,
      "loss": 0.441600735163868,
      "step": 66200
    },
    {
      "epoch": 0.6447879639580061,
      "grad_norm": 0.23454512655735016,
      "learning_rate": 9.431313131313132e-05,
      "loss": 0.44020743868019224,
      "step": 66300
    },
    {
      "epoch": 0.6457604948237045,
      "grad_norm": 0.24655228853225708,
      "learning_rate": 9.43030303030303e-05,
      "loss": 0.4409938857679138,
      "step": 66400
    },
    {
      "epoch": 0.6467330256894028,
      "grad_norm": 0.25707170367240906,
      "learning_rate": 9.42929292929293e-05,
      "loss": 0.44128493014797054,
      "step": 66500
    },
    {
      "epoch": 0.6477055565551012,
      "grad_norm": 0.2554102838039398,
      "learning_rate": 9.428282828282829e-05,
      "loss": 0.44078336854126626,
      "step": 66600
    },
    {
      "epoch": 0.6486780874207995,
      "grad_norm": 0.2563209533691406,
      "learning_rate": 9.427272727272728e-05,
      "loss": 0.4416688472661115,
      "step": 66700
    },
    {
      "epoch": 0.6496506182864978,
      "grad_norm": 0.2519184648990631,
      "learning_rate": 9.426262626262626e-05,
      "loss": 0.4403747185738807,
      "step": 66800
    },
    {
      "epoch": 0.6506231491521962,
      "grad_norm": 0.26856064796447754,
      "learning_rate": 9.425252525252526e-05,
      "loss": 0.44135924838646706,
      "step": 66900
    },
    {
      "epoch": 0.6515956800178946,
      "grad_norm": 0.24429988861083984,
      "learning_rate": 9.424242424242424e-05,
      "loss": 0.4403623674318864,
      "step": 67000
    },
    {
      "epoch": 0.652568210883593,
      "grad_norm": 0.25700849294662476,
      "learning_rate": 9.423232323232323e-05,
      "loss": 0.44025712410119044,
      "step": 67100
    },
    {
      "epoch": 0.6535407417492912,
      "grad_norm": 0.27565640211105347,
      "learning_rate": 9.422222222222223e-05,
      "loss": 0.4384677552528823,
      "step": 67200
    },
    {
      "epoch": 0.6545132726149896,
      "grad_norm": 0.23889434337615967,
      "learning_rate": 9.421212121212122e-05,
      "loss": 0.4408391656240904,
      "step": 67300
    },
    {
      "epoch": 0.655485803480688,
      "grad_norm": 0.263499915599823,
      "learning_rate": 9.420202020202021e-05,
      "loss": 0.44142736465670007,
      "step": 67400
    },
    {
      "epoch": 0.6564583343463863,
      "grad_norm": 0.2564028203487396,
      "learning_rate": 9.41919191919192e-05,
      "loss": 0.44033148957690044,
      "step": 67500
    },
    {
      "epoch": 0.6574308652120847,
      "grad_norm": 0.23922373354434967,
      "learning_rate": 9.418181818181818e-05,
      "loss": 0.4402262531928536,
      "step": 67600
    },
    {
      "epoch": 0.658403396077783,
      "grad_norm": 0.24354363977909088,
      "learning_rate": 9.417171717171717e-05,
      "loss": 0.44140879209582484,
      "step": 67700
    },
    {
      "epoch": 0.6593759269434813,
      "grad_norm": 0.23798301815986633,
      "learning_rate": 9.416161616161616e-05,
      "loss": 0.44041202089829895,
      "step": 67800
    },
    {
      "epoch": 0.6603484578091797,
      "grad_norm": 0.2545859217643738,
      "learning_rate": 9.415151515151516e-05,
      "loss": 0.4405049005830322,
      "step": 67900
    },
    {
      "epoch": 0.6613209886748781,
      "grad_norm": 0.25448086857795715,
      "learning_rate": 9.414141414141415e-05,
      "loss": 0.4406968327448364,
      "step": 68000
    },
    {
      "epoch": 0.6622935195405765,
      "grad_norm": 0.24144992232322693,
      "learning_rate": 9.413131313131314e-05,
      "loss": 0.44138402868132454,
      "step": 68100
    },
    {
      "epoch": 0.6632660504062747,
      "grad_norm": 0.2555105984210968,
      "learning_rate": 9.412121212121212e-05,
      "loss": 0.4409816279194161,
      "step": 68200
    },
    {
      "epoch": 0.6642385812719731,
      "grad_norm": 0.2624731659889221,
      "learning_rate": 9.411111111111111e-05,
      "loss": 0.43899442086828944,
      "step": 68300
    },
    {
      "epoch": 0.6652111121376715,
      "grad_norm": 0.24713554978370667,
      "learning_rate": 9.41010101010101e-05,
      "loss": 0.4402759094379258,
      "step": 68400
    },
    {
      "epoch": 0.6661836430033699,
      "grad_norm": 0.2411111295223236,
      "learning_rate": 9.40909090909091e-05,
      "loss": 0.43927925077611435,
      "step": 68500
    },
    {
      "epoch": 0.6671561738690681,
      "grad_norm": 0.25166255235671997,
      "learning_rate": 9.408080808080809e-05,
      "loss": 0.440362605215683,
      "step": 68600
    },
    {
      "epoch": 0.6681287047347665,
      "grad_norm": 0.2565003037452698,
      "learning_rate": 9.407070707070708e-05,
      "loss": 0.4388707452990289,
      "step": 68700
    },
    {
      "epoch": 0.6691012356004649,
      "grad_norm": 0.2487567514181137,
      "learning_rate": 9.406060606060606e-05,
      "loss": 0.43916172160192507,
      "step": 68800
    },
    {
      "epoch": 0.6700737664661632,
      "grad_norm": 0.24694384634494781,
      "learning_rate": 9.405050505050506e-05,
      "loss": 0.4386603475547448,
      "step": 68900
    },
    {
      "epoch": 0.6710462973318616,
      "grad_norm": 0.2490338385105133,
      "learning_rate": 9.404040404040404e-05,
      "loss": 0.43914940102518646,
      "step": 69000
    },
    {
      "epoch": 0.6720188281975599,
      "grad_norm": 0.26981282234191895,
      "learning_rate": 9.403030303030303e-05,
      "loss": 0.43904420076371464,
      "step": 69100
    },
    {
      "epoch": 0.6729913590632582,
      "grad_norm": 0.25089946389198303,
      "learning_rate": 9.402020202020202e-05,
      "loss": 0.43963227336731175,
      "step": 69200
    },
    {
      "epoch": 0.6739638899289566,
      "grad_norm": 0.24669092893600464,
      "learning_rate": 9.401010101010102e-05,
      "loss": 0.43942803174340744,
      "step": 69300
    },
    {
      "epoch": 0.674936420794655,
      "grad_norm": 0.2282775193452835,
      "learning_rate": 9.4e-05,
      "loss": 0.4398180105075013,
      "step": 69400
    },
    {
      "epoch": 0.6759089516603534,
      "grad_norm": 0.24747374653816223,
      "learning_rate": 9.3989898989899e-05,
      "loss": 0.43872246192020664,
      "step": 69500
    },
    {
      "epoch": 0.6768814825260516,
      "grad_norm": 0.2537521719932556,
      "learning_rate": 9.397979797979798e-05,
      "loss": 0.44069696771822725,
      "step": 69600
    },
    {
      "epoch": 0.67785401339175,
      "grad_norm": 0.2499084174633026,
      "learning_rate": 9.396969696969697e-05,
      "loss": 0.44049272192633354,
      "step": 69700
    },
    {
      "epoch": 0.6788265442574484,
      "grad_norm": 0.2512696087360382,
      "learning_rate": 9.395959595959598e-05,
      "loss": 0.4393972094616138,
      "step": 69800
    },
    {
      "epoch": 0.6797990751231467,
      "grad_norm": 0.27145037055015564,
      "learning_rate": 9.394949494949495e-05,
      "loss": 0.4393910450052551,
      "step": 69900
    },
    {
      "epoch": 0.6807716059888451,
      "grad_norm": 0.23982985317707062,
      "learning_rate": 9.393939393939395e-05,
      "loss": 0.4392858530797621,
      "step": 70000
    },
    {
      "epoch": 0.6807716059888451,
      "eval_accuracy": 0.6641338672550647,
      "eval_loss": 0.43120928815567855,
      "eval_runtime": 3735.5985,
      "eval_samples_per_second": 611.616,
      "eval_steps_per_second": 6.116,
      "step": 70000
    },
    {
      "epoch": 0.6817441368545434,
      "grad_norm": 0.25385481119155884,
      "learning_rate": 9.392929292929294e-05,
      "loss": 0.43858650745410177,
      "step": 70100
    },
    {
      "epoch": 0.6827166677202418,
      "grad_norm": 0.23727446794509888,
      "learning_rate": 9.391919191919193e-05,
      "loss": 0.43887742818380554,
      "step": 70200
    },
    {
      "epoch": 0.6836891985859401,
      "grad_norm": 0.24540530145168304,
      "learning_rate": 9.390909090909091e-05,
      "loss": 0.43976248038439975,
      "step": 70300
    },
    {
      "epoch": 0.6846617294516385,
      "grad_norm": 0.2575775980949402,
      "learning_rate": 9.389898989898991e-05,
      "loss": 0.4396572884589067,
      "step": 70400
    },
    {
      "epoch": 0.6856342603173369,
      "grad_norm": 0.2532587945461273,
      "learning_rate": 9.388888888888889e-05,
      "loss": 0.43806679147479444,
      "step": 70500
    },
    {
      "epoch": 0.6866067911830351,
      "grad_norm": 0.2411896288394928,
      "learning_rate": 9.387878787878788e-05,
      "loss": 0.43806064507972325,
      "step": 70600
    },
    {
      "epoch": 0.6875793220487335,
      "grad_norm": 0.2575770914554596,
      "learning_rate": 9.386868686868688e-05,
      "loss": 0.4391436938667341,
      "step": 70700
    },
    {
      "epoch": 0.6885518529144319,
      "grad_norm": 0.2304987907409668,
      "learning_rate": 9.385858585858587e-05,
      "loss": 0.439137532189035,
      "step": 70800
    },
    {
      "epoch": 0.6895243837801303,
      "grad_norm": 0.24519899487495422,
      "learning_rate": 9.384848484848485e-05,
      "loss": 0.4382402358248416,
      "step": 70900
    },
    {
      "epoch": 0.6904969146458285,
      "grad_norm": 0.24911443889141083,
      "learning_rate": 9.383838383838385e-05,
      "loss": 0.4391252088336368,
      "step": 71000
    },
    {
      "epoch": 0.6914694455115269,
      "grad_norm": 0.2537868320941925,
      "learning_rate": 9.382828282828283e-05,
      "loss": 0.438722998409912,
      "step": 71100
    },
    {
      "epoch": 0.6924419763772253,
      "grad_norm": 0.2309868335723877,
      "learning_rate": 9.381818181818182e-05,
      "loss": 0.4367366263460001,
      "step": 71200
    },
    {
      "epoch": 0.6934145072429236,
      "grad_norm": 0.26061660051345825,
      "learning_rate": 9.380808080808082e-05,
      "loss": 0.4388096955769991,
      "step": 71300
    },
    {
      "epoch": 0.694387038108622,
      "grad_norm": 0.24025490880012512,
      "learning_rate": 9.379797979797981e-05,
      "loss": 0.4387045300487724,
      "step": 71400
    },
    {
      "epoch": 0.6953595689743203,
      "grad_norm": 0.2599264979362488,
      "learning_rate": 9.378787878787879e-05,
      "loss": 0.43889638718676693,
      "step": 71500
    },
    {
      "epoch": 0.6963320998400186,
      "grad_norm": 0.24272705614566803,
      "learning_rate": 9.377777777777779e-05,
      "loss": 0.43809818636886844,
      "step": 71600
    },
    {
      "epoch": 0.697304630705717,
      "grad_norm": 0.24138358235359192,
      "learning_rate": 9.376767676767677e-05,
      "loss": 0.4383890501360502,
      "step": 71700
    },
    {
      "epoch": 0.6982771615714154,
      "grad_norm": 0.27248623967170715,
      "learning_rate": 9.375757575757576e-05,
      "loss": 0.4370958661880889,
      "step": 71800
    },
    {
      "epoch": 0.6992496924371138,
      "grad_norm": 0.24249114096164703,
      "learning_rate": 9.374747474747475e-05,
      "loss": 0.4372877344407218,
      "step": 71900
    },
    {
      "epoch": 0.700222223302812,
      "grad_norm": 0.2537432909011841,
      "learning_rate": 9.373737373737375e-05,
      "loss": 0.43728159777095926,
      "step": 72000
    },
    {
      "epoch": 0.7011947541685104,
      "grad_norm": 0.2502550482749939,
      "learning_rate": 9.372727272727272e-05,
      "loss": 0.4365824730487384,
      "step": 72100
    },
    {
      "epoch": 0.7021672850342088,
      "grad_norm": 0.24820221960544586,
      "learning_rate": 9.371717171717173e-05,
      "loss": 0.4378633058547053,
      "step": 72200
    },
    {
      "epoch": 0.7031398158999072,
      "grad_norm": 0.24941307306289673,
      "learning_rate": 9.370707070707071e-05,
      "loss": 0.4379561563635126,
      "step": 72300
    },
    {
      "epoch": 0.7041123467656055,
      "grad_norm": 0.2770954668521881,
      "learning_rate": 9.36969696969697e-05,
      "loss": 0.43715805696669013,
      "step": 72400
    },
    {
      "epoch": 0.7050848776313038,
      "grad_norm": 0.23552602529525757,
      "learning_rate": 9.368686868686869e-05,
      "loss": 0.43754789262981375,
      "step": 72500
    },
    {
      "epoch": 0.7060574084970022,
      "grad_norm": 0.2342364490032196,
      "learning_rate": 9.367676767676768e-05,
      "loss": 0.43694780371270603,
      "step": 72600
    },
    {
      "epoch": 0.7070299393627005,
      "grad_norm": 0.25919613242149353,
      "learning_rate": 9.366666666666668e-05,
      "loss": 0.4364467214247855,
      "step": 72700
    },
    {
      "epoch": 0.7080024702283989,
      "grad_norm": 0.2519848942756653,
      "learning_rate": 9.365656565656567e-05,
      "loss": 0.4377274472523572,
      "step": 72800
    },
    {
      "epoch": 0.7089750010940973,
      "grad_norm": 0.27783629298210144,
      "learning_rate": 9.364646464646465e-05,
      "loss": 0.437424342100236,
      "step": 72900
    },
    {
      "epoch": 0.7099475319597955,
      "grad_norm": 0.24289007484912872,
      "learning_rate": 9.363636363636364e-05,
      "loss": 0.43712124528409363,
      "step": 73000
    },
    {
      "epoch": 0.7109200628254939,
      "grad_norm": 0.25216391682624817,
      "learning_rate": 9.362626262626263e-05,
      "loss": 0.4379069852029429,
      "step": 73100
    },
    {
      "epoch": 0.7118925936911923,
      "grad_norm": 0.2479313611984253,
      "learning_rate": 9.361616161616162e-05,
      "loss": 0.4365150766597455,
      "step": 73200
    },
    {
      "epoch": 0.7128651245568907,
      "grad_norm": 0.2620636224746704,
      "learning_rate": 9.360606060606061e-05,
      "loss": 0.4373997843065473,
      "step": 73300
    },
    {
      "epoch": 0.7138376554225889,
      "grad_norm": 0.2503966987133026,
      "learning_rate": 9.35959595959596e-05,
      "loss": 0.43818548671349167,
      "step": 73400
    },
    {
      "epoch": 0.7148101862882873,
      "grad_norm": 0.2740076184272766,
      "learning_rate": 9.358585858585858e-05,
      "loss": 0.437486484252294,
      "step": 73500
    },
    {
      "epoch": 0.7157827171539857,
      "grad_norm": 0.241762176156044,
      "learning_rate": 9.357575757575759e-05,
      "loss": 0.43807420813410924,
      "step": 73600
    },
    {
      "epoch": 0.716755248019684,
      "grad_norm": 0.2573162913322449,
      "learning_rate": 9.356565656565657e-05,
      "loss": 0.4373752265128586,
      "step": 73700
    },
    {
      "epoch": 0.7177277788853824,
      "grad_norm": 0.274971604347229,
      "learning_rate": 9.355555555555556e-05,
      "loss": 0.4373690870644364,
      "step": 73800
    },
    {
      "epoch": 0.7187003097510807,
      "grad_norm": 0.23596692085266113,
      "learning_rate": 9.354545454545455e-05,
      "loss": 0.43706602776019887,
      "step": 73900
    },
    {
      "epoch": 0.719672840616779,
      "grad_norm": 0.2470138967037201,
      "learning_rate": 9.353535353535354e-05,
      "loss": 0.4368619486878822,
      "step": 74000
    },
    {
      "epoch": 0.7206453714824774,
      "grad_norm": 0.22075654566287994,
      "learning_rate": 9.352525252525252e-05,
      "loss": 0.4364599341596604,
      "step": 74100
    },
    {
      "epoch": 0.7216179023481758,
      "grad_norm": 0.2474374771118164,
      "learning_rate": 9.351515151515153e-05,
      "loss": 0.43724556015346544,
      "step": 74200
    },
    {
      "epoch": 0.7225904332138742,
      "grad_norm": 0.25666114687919617,
      "learning_rate": 9.35050505050505e-05,
      "loss": 0.43783322846208833,
      "step": 74300
    },
    {
      "epoch": 0.7235629640795724,
      "grad_norm": 0.2351183146238327,
      "learning_rate": 9.34949494949495e-05,
      "loss": 0.4357487889559395,
      "step": 74400
    },
    {
      "epoch": 0.7245354949452708,
      "grad_norm": 0.26575812697410583,
      "learning_rate": 9.348484848484849e-05,
      "loss": 0.4365343913311377,
      "step": 74500
    },
    {
      "epoch": 0.7255080258109692,
      "grad_norm": 0.25432196259498596,
      "learning_rate": 9.347474747474748e-05,
      "loss": 0.4356375909576859,
      "step": 74600
    },
    {
      "epoch": 0.7264805566766676,
      "grad_norm": 0.24590136110782623,
      "learning_rate": 9.346464646464646e-05,
      "loss": 0.4369179833461036,
      "step": 74700
    },
    {
      "epoch": 0.7274530875423659,
      "grad_norm": 0.2519955337047577,
      "learning_rate": 9.345454545454547e-05,
      "loss": 0.4365160063297866,
      "step": 74800
    },
    {
      "epoch": 0.7284256184080642,
      "grad_norm": 0.25051161646842957,
      "learning_rate": 9.344444444444444e-05,
      "loss": 0.436806756171924,
      "step": 74900
    },
    {
      "epoch": 0.7293981492737626,
      "grad_norm": 0.2502342760562897,
      "learning_rate": 9.343434343434344e-05,
      "loss": 0.43650374966221916,
      "step": 75000
    },
    {
      "epoch": 0.7303706801394609,
      "grad_norm": 0.24699275195598602,
      "learning_rate": 9.342424242424243e-05,
      "loss": 0.43610179487517914,
      "step": 75100
    },
    {
      "epoch": 0.7313432110051593,
      "grad_norm": 0.2527647018432617,
      "learning_rate": 9.341414141414142e-05,
      "loss": 0.43718417956254174,
      "step": 75200
    },
    {
      "epoch": 0.7323157418708576,
      "grad_norm": 0.23619362711906433,
      "learning_rate": 9.34040404040404e-05,
      "loss": 0.43589164165294125,
      "step": 75300
    },
    {
      "epoch": 0.733288272736556,
      "grad_norm": 0.22132135927677155,
      "learning_rate": 9.33939393939394e-05,
      "loss": 0.43647923632708435,
      "step": 75400
    },
    {
      "epoch": 0.7342608036022543,
      "grad_norm": 0.23934373259544373,
      "learning_rate": 9.338383838383838e-05,
      "loss": 0.43716576538526464,
      "step": 75500
    },
    {
      "epoch": 0.7352333344679527,
      "grad_norm": 0.2527174651622772,
      "learning_rate": 9.337373737373738e-05,
      "loss": 0.4353785333262014,
      "step": 75600
    },
    {
      "epoch": 0.7362058653336511,
      "grad_norm": 0.24809275567531586,
      "learning_rate": 9.336363636363637e-05,
      "loss": 0.436361903048398,
      "step": 75700
    },
    {
      "epoch": 0.7371783961993493,
      "grad_norm": 0.24066080152988434,
      "learning_rate": 9.335353535353536e-05,
      "loss": 0.43625682921593867,
      "step": 75800
    },
    {
      "epoch": 0.7381509270650477,
      "grad_norm": 0.23064406216144562,
      "learning_rate": 9.334343434343434e-05,
      "loss": 0.4359538671647877,
      "step": 75900
    },
    {
      "epoch": 0.7391234579307461,
      "grad_norm": 0.2551531195640564,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.43555196934026974,
      "step": 76000
    },
    {
      "epoch": 0.7400959887964444,
      "grad_norm": 0.2509545683860779,
      "learning_rate": 9.332323232323232e-05,
      "loss": 0.43613950983055044,
      "step": 76100
    },
    {
      "epoch": 0.7410685196621428,
      "grad_norm": 0.25219348073005676,
      "learning_rate": 9.331313131313131e-05,
      "loss": 0.4363312683261285,
      "step": 76200
    },
    {
      "epoch": 0.7420410505278411,
      "grad_norm": 0.2622454762458801,
      "learning_rate": 9.33030303030303e-05,
      "loss": 0.4362262014403182,
      "step": 76300
    },
    {
      "epoch": 0.7430135813935395,
      "grad_norm": 0.23021720349788666,
      "learning_rate": 9.32929292929293e-05,
      "loss": 0.43602219878114096,
      "step": 76400
    },
    {
      "epoch": 0.7439861122592378,
      "grad_norm": 0.23836448788642883,
      "learning_rate": 9.328282828282829e-05,
      "loss": 0.43542245302849586,
      "step": 76500
    },
    {
      "epoch": 0.7449586431249362,
      "grad_norm": 0.2443879395723343,
      "learning_rate": 9.327272727272728e-05,
      "loss": 0.43502059549454236,
      "step": 76600
    },
    {
      "epoch": 0.7459311739906346,
      "grad_norm": 0.24220314621925354,
      "learning_rate": 9.326262626262626e-05,
      "loss": 0.4359048960677103,
      "step": 76700
    },
    {
      "epoch": 0.7469037048563328,
      "grad_norm": 0.26188674569129944,
      "learning_rate": 9.325252525252525e-05,
      "loss": 0.43431584676525814,
      "step": 76800
    },
    {
      "epoch": 0.7478762357220312,
      "grad_norm": 0.26500818133354187,
      "learning_rate": 9.324242424242424e-05,
      "loss": 0.43688196934721646,
      "step": 76900
    },
    {
      "epoch": 0.7488487665877296,
      "grad_norm": 0.24420472979545593,
      "learning_rate": 9.323232323232324e-05,
      "loss": 0.43529295061002,
      "step": 77000
    },
    {
      "epoch": 0.749821297453428,
      "grad_norm": 0.23113767802715302,
      "learning_rate": 9.322222222222223e-05,
      "loss": 0.43627612582604336,
      "step": 77100
    },
    {
      "epoch": 0.7507938283191263,
      "grad_norm": 0.26670825481414795,
      "learning_rate": 9.321212121212122e-05,
      "loss": 0.434093595259051,
      "step": 77200
    },
    {
      "epoch": 0.7517663591848246,
      "grad_norm": 0.2536223530769348,
      "learning_rate": 9.32020202020202e-05,
      "loss": 0.43448320307208604,
      "step": 77300
    },
    {
      "epoch": 0.752738890050523,
      "grad_norm": 0.22423473000526428,
      "learning_rate": 9.319191919191919e-05,
      "loss": 0.43408140250065474,
      "step": 77400
    },
    {
      "epoch": 0.7537114209162213,
      "grad_norm": 0.2550415098667145,
      "learning_rate": 9.318181818181818e-05,
      "loss": 0.4354602318930383,
      "step": 77500
    },
    {
      "epoch": 0.7546839517819197,
      "grad_norm": 0.2389991730451584,
      "learning_rate": 9.317171717171717e-05,
      "loss": 0.43614656922370504,
      "step": 77600
    },
    {
      "epoch": 0.755656482647618,
      "grad_norm": 0.2405882179737091,
      "learning_rate": 9.316161616161617e-05,
      "loss": 0.43544800023340735,
      "step": 77700
    },
    {
      "epoch": 0.7566290135133164,
      "grad_norm": 0.24779731035232544,
      "learning_rate": 9.315151515151516e-05,
      "loss": 0.43623323721486607,
      "step": 77800
    },
    {
      "epoch": 0.7576015443790147,
      "grad_norm": 0.27142930030822754,
      "learning_rate": 9.314141414141414e-05,
      "loss": 0.4353368508616971,
      "step": 77900
    },
    {
      "epoch": 0.7585740752447131,
      "grad_norm": 0.2662670910358429,
      "learning_rate": 9.313131313131314e-05,
      "loss": 0.43533073642121145,
      "step": 78000
    },
    {
      "epoch": 0.7595466061104115,
      "grad_norm": 0.2530241906642914,
      "learning_rate": 9.312121212121212e-05,
      "loss": 0.43542353691414565,
      "step": 78100
    },
    {
      "epoch": 0.7605191369761097,
      "grad_norm": 0.2524949610233307,
      "learning_rate": 9.311111111111111e-05,
      "loss": 0.4357141617166004,
      "step": 78200
    },
    {
      "epoch": 0.7614916678418081,
      "grad_norm": 0.2509996294975281,
      "learning_rate": 9.31010101010101e-05,
      "loss": 0.43432327155215233,
      "step": 78300
    },
    {
      "epoch": 0.7624641987075065,
      "grad_norm": 0.2809649407863617,
      "learning_rate": 9.30909090909091e-05,
      "loss": 0.43580083248642104,
      "step": 78400
    },
    {
      "epoch": 0.7634367295732049,
      "grad_norm": 0.23989048600196838,
      "learning_rate": 9.308080808080809e-05,
      "loss": 0.4340143423294754,
      "step": 78500
    },
    {
      "epoch": 0.7644092604389032,
      "grad_norm": 0.23996150493621826,
      "learning_rate": 9.307070707070708e-05,
      "loss": 0.43667876159308894,
      "step": 78600
    },
    {
      "epoch": 0.7653817913046015,
      "grad_norm": 0.25078707933425903,
      "learning_rate": 9.306060606060607e-05,
      "loss": 0.43449668255828416,
      "step": 78700
    },
    {
      "epoch": 0.7663543221702999,
      "grad_norm": 0.252613365650177,
      "learning_rate": 9.305050505050505e-05,
      "loss": 0.43557853652166,
      "step": 78800
    },
    {
      "epoch": 0.7673268530359982,
      "grad_norm": 0.24536728858947754,
      "learning_rate": 9.304040404040406e-05,
      "loss": 0.43507789881927805,
      "step": 78900
    },
    {
      "epoch": 0.7682993839016966,
      "grad_norm": 0.23626874387264252,
      "learning_rate": 9.303030303030303e-05,
      "loss": 0.43566520173416146,
      "step": 79000
    },
    {
      "epoch": 0.769271914767395,
      "grad_norm": 0.24494200944900513,
      "learning_rate": 9.302020202020203e-05,
      "loss": 0.4351645765357478,
      "step": 79100
    },
    {
      "epoch": 0.7702444456330932,
      "grad_norm": 0.2382095903158188,
      "learning_rate": 9.301010101010102e-05,
      "loss": 0.4349606641830081,
      "step": 79200
    },
    {
      "epoch": 0.7712169764987916,
      "grad_norm": 0.24041928350925446,
      "learning_rate": 9.300000000000001e-05,
      "loss": 0.43544904521782263,
      "step": 79300
    },
    {
      "epoch": 0.77218950736449,
      "grad_norm": 0.23630522191524506,
      "learning_rate": 9.298989898989899e-05,
      "loss": 0.43484954676588333,
      "step": 79400
    },
    {
      "epoch": 0.7731620382301884,
      "grad_norm": 0.24072875082492828,
      "learning_rate": 9.2979797979798e-05,
      "loss": 0.4355357062623347,
      "step": 79500
    },
    {
      "epoch": 0.7741345690958866,
      "grad_norm": 0.2545780539512634,
      "learning_rate": 9.296969696969697e-05,
      "loss": 0.4338483880648233,
      "step": 79600
    },
    {
      "epoch": 0.775107099961585,
      "grad_norm": 0.22786912322044373,
      "learning_rate": 9.295959595959597e-05,
      "loss": 0.4337434003708121,
      "step": 79700
    },
    {
      "epoch": 0.7760796308272834,
      "grad_norm": 0.26465603709220886,
      "learning_rate": 9.294949494949496e-05,
      "loss": 0.4344295459739654,
      "step": 79800
    },
    {
      "epoch": 0.7770521616929817,
      "grad_norm": 0.2789650857448578,
      "learning_rate": 9.293939393939395e-05,
      "loss": 0.43541234190295114,
      "step": 79900
    },
    {
      "epoch": 0.7780246925586801,
      "grad_norm": 0.25909486413002014,
      "learning_rate": 9.292929292929293e-05,
      "loss": 0.43431845078611747,
      "step": 80000
    },
    {
      "epoch": 0.7780246925586801,
      "eval_accuracy": 0.6646072452871101,
      "eval_loss": 0.42746632044240374,
      "eval_runtime": 3733.0085,
      "eval_samples_per_second": 612.04,
      "eval_steps_per_second": 6.121,
      "step": 80000
    },
    {
      "epoch": 0.7789972234243784,
      "grad_norm": 0.23943500220775604,
      "learning_rate": 9.291919191919193e-05,
      "loss": 0.4344112359964239,
      "step": 80100
    },
    {
      "epoch": 0.7799697542900768,
      "grad_norm": 0.26351797580718994,
      "learning_rate": 9.290909090909091e-05,
      "loss": 0.43450401842807057,
      "step": 80200
    },
    {
      "epoch": 0.7809422851557751,
      "grad_norm": 0.25842300057411194,
      "learning_rate": 9.28989898989899e-05,
      "loss": 0.43400349187207327,
      "step": 80300
    },
    {
      "epoch": 0.7819148160214735,
      "grad_norm": 0.25852277874946594,
      "learning_rate": 9.28888888888889e-05,
      "loss": 0.4338985111247112,
      "step": 80400
    },
    {
      "epoch": 0.7828873468871719,
      "grad_norm": 0.2791961431503296,
      "learning_rate": 9.287878787878789e-05,
      "loss": 0.43438682269303525,
      "step": 80500
    },
    {
      "epoch": 0.7838598777528701,
      "grad_norm": 0.24854587018489838,
      "learning_rate": 9.286868686868687e-05,
      "loss": 0.4344795995673627,
      "step": 80600
    },
    {
      "epoch": 0.7848324086185685,
      "grad_norm": 0.2394045740365982,
      "learning_rate": 9.285858585858587e-05,
      "loss": 0.4340779796088064,
      "step": 80700
    },
    {
      "epoch": 0.7858049394842669,
      "grad_norm": 0.24925942718982697,
      "learning_rate": 9.284848484848485e-05,
      "loss": 0.4344673901370088,
      "step": 80800
    },
    {
      "epoch": 0.7867774703499653,
      "grad_norm": 0.235203355550766,
      "learning_rate": 9.283838383838384e-05,
      "loss": 0.4343624093896466,
      "step": 80900
    },
    {
      "epoch": 0.7877500012156636,
      "grad_norm": 0.25405412912368774,
      "learning_rate": 9.282828282828283e-05,
      "loss": 0.4346529299923656,
      "step": 81000
    },
    {
      "epoch": 0.7887225320813619,
      "grad_norm": 0.2491247057914734,
      "learning_rate": 9.281818181818183e-05,
      "loss": 0.4348445690055802,
      "step": 81100
    },
    {
      "epoch": 0.7896950629470603,
      "grad_norm": 0.2565079927444458,
      "learning_rate": 9.28080808080808e-05,
      "loss": 0.4331576370417556,
      "step": 81200
    },
    {
      "epoch": 0.7906675938127586,
      "grad_norm": 0.2311435490846634,
      "learning_rate": 9.279797979797981e-05,
      "loss": 0.433942514186794,
      "step": 81300
    },
    {
      "epoch": 0.791640124678457,
      "grad_norm": 0.24820411205291748,
      "learning_rate": 9.278787878787879e-05,
      "loss": 0.4343318927604108,
      "step": 81400
    },
    {
      "epoch": 0.7926126555441554,
      "grad_norm": 0.25348901748657227,
      "learning_rate": 9.277777777777778e-05,
      "loss": 0.4334359801687063,
      "step": 81500
    },
    {
      "epoch": 0.7935851864098536,
      "grad_norm": 0.2574830949306488,
      "learning_rate": 9.276767676767677e-05,
      "loss": 0.4323423599711853,
      "step": 81600
    },
    {
      "epoch": 0.794557717275552,
      "grad_norm": 0.25667884945869446,
      "learning_rate": 9.275757575757576e-05,
      "loss": 0.43342379852494856,
      "step": 81700
    },
    {
      "epoch": 0.7955302481412504,
      "grad_norm": 0.23560549318790436,
      "learning_rate": 9.274747474747476e-05,
      "loss": 0.4346040700416729,
      "step": 81800
    },
    {
      "epoch": 0.7965027790069488,
      "grad_norm": 0.23966772854328156,
      "learning_rate": 9.273737373737375e-05,
      "loss": 0.4332138926034165,
      "step": 81900
    },
    {
      "epoch": 0.797475309872647,
      "grad_norm": 0.24817994236946106,
      "learning_rate": 9.272727272727273e-05,
      "loss": 0.4336032475584265,
      "step": 82000
    },
    {
      "epoch": 0.7984478407383454,
      "grad_norm": 0.23213647305965424,
      "learning_rate": 9.271717171717172e-05,
      "loss": 0.4333005758772055,
      "step": 82100
    },
    {
      "epoch": 0.7994203716040438,
      "grad_norm": 0.24317395687103271,
      "learning_rate": 9.270707070707071e-05,
      "loss": 0.43319562847375864,
      "step": 82200
    },
    {
      "epoch": 0.8003929024697422,
      "grad_norm": 0.2576545178890228,
      "learning_rate": 9.26969696969697e-05,
      "loss": 0.43309068384897154,
      "step": 82300
    },
    {
      "epoch": 0.8013654333354405,
      "grad_norm": 0.26106590032577515,
      "learning_rate": 9.26868686868687e-05,
      "loss": 0.43328230757955827,
      "step": 82400
    },
    {
      "epoch": 0.8023379642011388,
      "grad_norm": 0.25667446851730347,
      "learning_rate": 9.267676767676769e-05,
      "loss": 0.43386934096445884,
      "step": 82500
    },
    {
      "epoch": 0.8033104950668372,
      "grad_norm": 0.23003433644771576,
      "learning_rate": 9.266666666666666e-05,
      "loss": 0.4347529149181373,
      "step": 82600
    },
    {
      "epoch": 0.8042830259325355,
      "grad_norm": 0.24034740030765533,
      "learning_rate": 9.265656565656567e-05,
      "loss": 0.432374380063673,
      "step": 82700
    },
    {
      "epoch": 0.8052555567982339,
      "grad_norm": 0.2764512002468109,
      "learning_rate": 9.264646464646465e-05,
      "loss": 0.43385104765887506,
      "step": 82800
    },
    {
      "epoch": 0.8062280876639323,
      "grad_norm": 0.2515829801559448,
      "learning_rate": 9.263636363636364e-05,
      "loss": 0.4344380393638814,
      "step": 82900
    },
    {
      "epoch": 0.8072006185296305,
      "grad_norm": 0.24456782639026642,
      "learning_rate": 9.262626262626263e-05,
      "loss": 0.4324549961341898,
      "step": 83000
    },
    {
      "epoch": 0.8081731493953289,
      "grad_norm": 0.24828700721263885,
      "learning_rate": 9.261616161616162e-05,
      "loss": 0.4324489178162792,
      "step": 83100
    },
    {
      "epoch": 0.8091456802610273,
      "grad_norm": 0.24581342935562134,
      "learning_rate": 9.26060606060606e-05,
      "loss": 0.43372781250716375,
      "step": 83200
    },
    {
      "epoch": 0.8101182111267257,
      "grad_norm": 0.2503318190574646,
      "learning_rate": 9.259595959595961e-05,
      "loss": 0.43273328924526744,
      "step": 83300
    },
    {
      "epoch": 0.811090741992424,
      "grad_norm": 0.25475990772247314,
      "learning_rate": 9.258585858585859e-05,
      "loss": 0.4324306828625474,
      "step": 83400
    },
    {
      "epoch": 0.8120632728581223,
      "grad_norm": 0.2570047080516815,
      "learning_rate": 9.257575757575758e-05,
      "loss": 0.4347967623752816,
      "step": 83500
    },
    {
      "epoch": 0.8130358037238207,
      "grad_norm": 0.24485154449939728,
      "learning_rate": 9.256565656565657e-05,
      "loss": 0.4336045884700908,
      "step": 83600
    },
    {
      "epoch": 0.814008334589519,
      "grad_norm": 0.25667375326156616,
      "learning_rate": 9.255555555555556e-05,
      "loss": 0.4333019820873708,
      "step": 83700
    },
    {
      "epoch": 0.8149808654552174,
      "grad_norm": 0.2400846779346466,
      "learning_rate": 9.254545454545454e-05,
      "loss": 0.43299938404062954,
      "step": 83800
    },
    {
      "epoch": 0.8159533963209157,
      "grad_norm": 0.2789924442768097,
      "learning_rate": 9.253535353535355e-05,
      "loss": 0.4324991256252852,
      "step": 83900
    },
    {
      "epoch": 0.816925927186614,
      "grad_norm": 0.25212815403938293,
      "learning_rate": 9.252525252525253e-05,
      "loss": 0.43249304591804477,
      "step": 84000
    },
    {
      "epoch": 0.8178984580523124,
      "grad_norm": 0.24700109660625458,
      "learning_rate": 9.251515151515152e-05,
      "loss": 0.4314986504744909,
      "step": 84100
    },
    {
      "epoch": 0.8188709889180108,
      "grad_norm": 0.26689815521240234,
      "learning_rate": 9.250505050505051e-05,
      "loss": 0.432480886503564,
      "step": 84200
    },
    {
      "epoch": 0.8198435197837092,
      "grad_norm": 0.25380414724349976,
      "learning_rate": 9.24949494949495e-05,
      "loss": 0.4338584099259279,
      "step": 84300
    },
    {
      "epoch": 0.8208160506494074,
      "grad_norm": 0.28109511733055115,
      "learning_rate": 9.248484848484848e-05,
      "loss": 0.4321722448721574,
      "step": 84400
    },
    {
      "epoch": 0.8217885815151058,
      "grad_norm": 0.24417048692703247,
      "learning_rate": 9.247474747474749e-05,
      "loss": 0.4313755612024094,
      "step": 84500
    },
    {
      "epoch": 0.8227611123808042,
      "grad_norm": 0.2661144733428955,
      "learning_rate": 9.246464646464646e-05,
      "loss": 0.4330495154364961,
      "step": 84600
    },
    {
      "epoch": 0.8237336432465026,
      "grad_norm": 0.25411561131477356,
      "learning_rate": 9.245454545454546e-05,
      "loss": 0.4311657858778793,
      "step": 84700
    },
    {
      "epoch": 0.8247061741122009,
      "grad_norm": 0.2452753782272339,
      "learning_rate": 9.244444444444445e-05,
      "loss": 0.4312585460802491,
      "step": 84800
    },
    {
      "epoch": 0.8256787049778992,
      "grad_norm": 0.25672629475593567,
      "learning_rate": 9.243434343434344e-05,
      "loss": 0.43293243084784544,
      "step": 84900
    },
    {
      "epoch": 0.8266512358435976,
      "grad_norm": 0.245237335562706,
      "learning_rate": 9.242424242424242e-05,
      "loss": 0.4322346107063144,
      "step": 85000
    },
    {
      "epoch": 0.8276237667092959,
      "grad_norm": 0.25684890151023865,
      "learning_rate": 9.241414141414142e-05,
      "loss": 0.43341434594173284,
      "step": 85100
    },
    {
      "epoch": 0.8285962975749943,
      "grad_norm": 0.22617767751216888,
      "learning_rate": 9.24040404040404e-05,
      "loss": 0.4310366613571112,
      "step": 85200
    },
    {
      "epoch": 0.8295688284406927,
      "grad_norm": 0.2538162171840668,
      "learning_rate": 9.23939393939394e-05,
      "loss": 0.43191993521555105,
      "step": 85300
    },
    {
      "epoch": 0.830541359306391,
      "grad_norm": 0.2386772334575653,
      "learning_rate": 9.238383838383839e-05,
      "loss": 0.4318150489426158,
      "step": 85400
    },
    {
      "epoch": 0.8315138901720893,
      "grad_norm": 0.24351410567760468,
      "learning_rate": 9.237373737373738e-05,
      "loss": 0.4332911594165649,
      "step": 85500
    },
    {
      "epoch": 0.8324864210377877,
      "grad_norm": 0.2443864941596985,
      "learning_rate": 9.236363636363636e-05,
      "loss": 0.4320993384011454,
      "step": 85600
    },
    {
      "epoch": 0.8334589519034861,
      "grad_norm": 0.26442256569862366,
      "learning_rate": 9.235353535353536e-05,
      "loss": 0.4327849282723753,
      "step": 85700
    },
    {
      "epoch": 0.8344314827691844,
      "grad_norm": 0.2536507248878479,
      "learning_rate": 9.234343434343434e-05,
      "loss": 0.43317407482791415,
      "step": 85800
    },
    {
      "epoch": 0.8354040136348827,
      "grad_norm": 0.26951465010643005,
      "learning_rate": 9.233333333333333e-05,
      "loss": 0.43198230521769754,
      "step": 85900
    },
    {
      "epoch": 0.8363765445005811,
      "grad_norm": 0.30466827750205994,
      "learning_rate": 9.232323232323232e-05,
      "loss": 0.4311857896568564,
      "step": 86000
    },
    {
      "epoch": 0.8373490753662794,
      "grad_norm": 0.22773832082748413,
      "learning_rate": 9.231313131313132e-05,
      "loss": 0.43216776449192557,
      "step": 86100
    },
    {
      "epoch": 0.8383216062319778,
      "grad_norm": 0.2632632851600647,
      "learning_rate": 9.230303030303031e-05,
      "loss": 0.4318652803702286,
      "step": 86200
    },
    {
      "epoch": 0.8392941370976761,
      "grad_norm": 0.2565859854221344,
      "learning_rate": 9.22929292929293e-05,
      "loss": 0.43225441164313955,
      "step": 86300
    },
    {
      "epoch": 0.8402666679633745,
      "grad_norm": 0.2368122786283493,
      "learning_rate": 9.228282828282828e-05,
      "loss": 0.43175433523000006,
      "step": 86400
    },
    {
      "epoch": 0.8412391988290728,
      "grad_norm": 0.23980489373207092,
      "learning_rate": 9.227272727272727e-05,
      "loss": 0.4319458603181704,
      "step": 86500
    },
    {
      "epoch": 0.8422117296947712,
      "grad_norm": 0.247519388794899,
      "learning_rate": 9.226262626262626e-05,
      "loss": 0.4303590367220708,
      "step": 86600
    },
    {
      "epoch": 0.8431842605604696,
      "grad_norm": 0.26171109080314636,
      "learning_rate": 9.225252525252525e-05,
      "loss": 0.4318349165672717,
      "step": 86700
    },
    {
      "epoch": 0.8441567914261678,
      "grad_norm": 0.2496572881937027,
      "learning_rate": 9.224242424242425e-05,
      "loss": 0.43103849131286803,
      "step": 86800
    },
    {
      "epoch": 0.8451293222918662,
      "grad_norm": 0.24664905667304993,
      "learning_rate": 9.223232323232324e-05,
      "loss": 0.43083484432212166,
      "step": 86900
    },
    {
      "epoch": 0.8461018531575646,
      "grad_norm": 0.2465517818927765,
      "learning_rate": 9.222222222222223e-05,
      "loss": 0.43181669828549757,
      "step": 87000
    },
    {
      "epoch": 0.847074384023263,
      "grad_norm": 0.22824783623218536,
      "learning_rate": 9.221212121212122e-05,
      "loss": 0.43052635690632374,
      "step": 87100
    },
    {
      "epoch": 0.8480469148889613,
      "grad_norm": 0.24770285189151764,
      "learning_rate": 9.220202020202021e-05,
      "loss": 0.4320021297731294,
      "step": 87200
    },
    {
      "epoch": 0.8490194457546596,
      "grad_norm": 0.21242745220661163,
      "learning_rate": 9.219191919191919e-05,
      "loss": 0.43140333154341337,
      "step": 87300
    },
    {
      "epoch": 0.849991976620358,
      "grad_norm": 0.24310770630836487,
      "learning_rate": 9.218181818181819e-05,
      "loss": 0.43139726434014125,
      "step": 87400
    },
    {
      "epoch": 0.8509645074860563,
      "grad_norm": 0.27073588967323303,
      "learning_rate": 9.217171717171718e-05,
      "loss": 0.43050213810910826,
      "step": 87500
    },
    {
      "epoch": 0.8519370383517547,
      "grad_norm": 0.25337091088294983,
      "learning_rate": 9.216161616161617e-05,
      "loss": 0.4310887810923328,
      "step": 87600
    },
    {
      "epoch": 0.8529095692174531,
      "grad_norm": 0.23938381671905518,
      "learning_rate": 9.215151515151516e-05,
      "loss": 0.4322680967501492,
      "step": 87700
    },
    {
      "epoch": 0.8538821000831514,
      "grad_norm": 0.2717060446739197,
      "learning_rate": 9.214141414141415e-05,
      "loss": 0.4312742153586242,
      "step": 87800
    },
    {
      "epoch": 0.8548546309488497,
      "grad_norm": 0.24862182140350342,
      "learning_rate": 9.213131313131313e-05,
      "loss": 0.430971813207386,
      "step": 87900
    },
    {
      "epoch": 0.8558271618145481,
      "grad_norm": 0.24135755002498627,
      "learning_rate": 9.212121212121214e-05,
      "loss": 0.43047186461258885,
      "step": 88000
    },
    {
      "epoch": 0.8567996926802465,
      "grad_norm": 0.276845246553421,
      "learning_rate": 9.211111111111112e-05,
      "loss": 0.4301694819119679,
      "step": 88100
    },
    {
      "epoch": 0.8577722235459447,
      "grad_norm": 0.23891374468803406,
      "learning_rate": 9.210101010101011e-05,
      "loss": 0.43134872671396407,
      "step": 88200
    },
    {
      "epoch": 0.8587447544116431,
      "grad_norm": 0.2353176772594452,
      "learning_rate": 9.20909090909091e-05,
      "loss": 0.43183652561958896,
      "step": 88300
    },
    {
      "epoch": 0.8597172852773415,
      "grad_norm": 0.2272002249956131,
      "learning_rate": 9.208080808080809e-05,
      "loss": 0.4305464176478229,
      "step": 88400
    },
    {
      "epoch": 0.8606898161430399,
      "grad_norm": 0.24298742413520813,
      "learning_rate": 9.207070707070707e-05,
      "loss": 0.4297501980142309,
      "step": 88500
    },
    {
      "epoch": 0.8616623470087382,
      "grad_norm": 0.2415378987789154,
      "learning_rate": 9.206060606060608e-05,
      "loss": 0.4308306126319255,
      "step": 88600
    },
    {
      "epoch": 0.8626348778744365,
      "grad_norm": 0.23882260918617249,
      "learning_rate": 9.205050505050505e-05,
      "loss": 0.4318122290199042,
      "step": 88700
    },
    {
      "epoch": 0.8636074087401349,
      "grad_norm": 0.25734198093414307,
      "learning_rate": 9.204040404040405e-05,
      "loss": 0.4306209595684186,
      "step": 88800
    },
    {
      "epoch": 0.8645799396058332,
      "grad_norm": 0.2366855889558792,
      "learning_rate": 9.203030303030304e-05,
      "loss": 0.4295284883466489,
      "step": 88900
    },
    {
      "epoch": 0.8655524704715316,
      "grad_norm": 0.24507713317871094,
      "learning_rate": 9.202020202020203e-05,
      "loss": 0.43011502713013783,
      "step": 89000
    },
    {
      "epoch": 0.86652500133723,
      "grad_norm": 0.24324971437454224,
      "learning_rate": 9.201010101010101e-05,
      "loss": 0.4304052629202462,
      "step": 89100
    },
    {
      "epoch": 0.8674975322029282,
      "grad_norm": 0.2596302926540375,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.4309917725278092,
      "step": 89200
    },
    {
      "epoch": 0.8684700630686266,
      "grad_norm": 0.24754159152507782,
      "learning_rate": 9.198989898989899e-05,
      "loss": 0.43147950613493313,
      "step": 89300
    },
    {
      "epoch": 0.869442593934325,
      "grad_norm": 0.22861744463443756,
      "learning_rate": 9.197979797979798e-05,
      "loss": 0.4304858567614861,
      "step": 89400
    },
    {
      "epoch": 0.8704151248000234,
      "grad_norm": 0.2358877807855606,
      "learning_rate": 9.196969696969698e-05,
      "loss": 0.43146736617106957,
      "step": 89500
    },
    {
      "epoch": 0.8713876556657217,
      "grad_norm": 0.25347068905830383,
      "learning_rate": 9.195959595959597e-05,
      "loss": 0.42889366201634826,
      "step": 89600
    },
    {
      "epoch": 0.87236018653142,
      "grad_norm": 0.21547311544418335,
      "learning_rate": 9.194949494949495e-05,
      "loss": 0.43125771866488183,
      "step": 89700
    },
    {
      "epoch": 0.8733327173971184,
      "grad_norm": 0.25683799386024475,
      "learning_rate": 9.193939393939395e-05,
      "loss": 0.4300666228796222,
      "step": 89800
    },
    {
      "epoch": 0.8743052482628167,
      "grad_norm": 0.23520003259181976,
      "learning_rate": 9.192929292929293e-05,
      "loss": 0.4292705644082879,
      "step": 89900
    },
    {
      "epoch": 0.8752777791285151,
      "grad_norm": 0.25619783997535706,
      "learning_rate": 9.191919191919192e-05,
      "loss": 0.429857022610648,
      "step": 90000
    },
    {
      "epoch": 0.8752777791285151,
      "eval_accuracy": 0.6649768237947862,
      "eval_loss": 0.4237337593462243,
      "eval_runtime": 3747.3258,
      "eval_samples_per_second": 609.701,
      "eval_steps_per_second": 6.097,
      "step": 90000
    },
    {
      "epoch": 0.8762503099942135,
      "grad_norm": 0.24110735952854156,
      "learning_rate": 9.190909090909091e-05,
      "loss": 0.4307397087825789,
      "step": 90100
    },
    {
      "epoch": 0.8772228408599118,
      "grad_norm": 0.23644500970840454,
      "learning_rate": 9.18989898989899e-05,
      "loss": 0.4297461802808252,
      "step": 90200
    },
    {
      "epoch": 0.8781953717256101,
      "grad_norm": 0.22371059656143188,
      "learning_rate": 9.188888888888888e-05,
      "loss": 0.43082633370451595,
      "step": 90300
    },
    {
      "epoch": 0.8791679025913085,
      "grad_norm": 0.25141793489456177,
      "learning_rate": 9.187878787878789e-05,
      "loss": 0.43042529587514927,
      "step": 90400
    },
    {
      "epoch": 0.8801404334570069,
      "grad_norm": 0.24198691546916962,
      "learning_rate": 9.186868686868687e-05,
      "loss": 0.4298267838473738,
      "step": 90500
    },
    {
      "epoch": 0.8811129643227051,
      "grad_norm": 0.25367501378059387,
      "learning_rate": 9.185858585858586e-05,
      "loss": 0.4309068900338511,
      "step": 90600
    },
    {
      "epoch": 0.8820854951884035,
      "grad_norm": 0.24126362800598145,
      "learning_rate": 9.184848484848485e-05,
      "loss": 0.43011090797565615,
      "step": 90700
    },
    {
      "epoch": 0.8830580260541019,
      "grad_norm": 0.23230402171611786,
      "learning_rate": 9.183838383838384e-05,
      "loss": 0.4303023330320803,
      "step": 90800
    },
    {
      "epoch": 0.8840305569198003,
      "grad_norm": 0.259857714176178,
      "learning_rate": 9.182828282828284e-05,
      "loss": 0.4305924896303896,
      "step": 90900
    },
    {
      "epoch": 0.8850030877854986,
      "grad_norm": 0.2582778334617615,
      "learning_rate": 9.181818181818183e-05,
      "loss": 0.43048769505322176,
      "step": 91000
    },
    {
      "epoch": 0.8859756186511969,
      "grad_norm": 0.2496909350156784,
      "learning_rate": 9.180808080808081e-05,
      "loss": 0.4294942943698104,
      "step": 91100
    },
    {
      "epoch": 0.8869481495168953,
      "grad_norm": 0.2550885081291199,
      "learning_rate": 9.17979797979798e-05,
      "loss": 0.4300806483724348,
      "step": 91200
    },
    {
      "epoch": 0.8879206803825936,
      "grad_norm": 0.24641630053520203,
      "learning_rate": 9.178787878787879e-05,
      "loss": 0.4294822072004796,
      "step": 91300
    },
    {
      "epoch": 0.888893211248292,
      "grad_norm": 0.22810880839824677,
      "learning_rate": 9.177777777777778e-05,
      "loss": 0.429969814378591,
      "step": 91400
    },
    {
      "epoch": 0.8898657421139904,
      "grad_norm": 0.27759164571762085,
      "learning_rate": 9.176767676767677e-05,
      "loss": 0.4311485090059831,
      "step": 91500
    },
    {
      "epoch": 0.8908382729796887,
      "grad_norm": 0.2910732328891754,
      "learning_rate": 9.175757575757577e-05,
      "loss": 0.4304513501854408,
      "step": 91600
    },
    {
      "epoch": 0.891810803845387,
      "grad_norm": 0.24445466697216034,
      "learning_rate": 9.174747474747475e-05,
      "loss": 0.43074147066117513,
      "step": 91700
    },
    {
      "epoch": 0.8927833347110854,
      "grad_norm": 0.2769329845905304,
      "learning_rate": 9.173737373737374e-05,
      "loss": 0.4292545400866803,
      "step": 91800
    },
    {
      "epoch": 0.8937558655767838,
      "grad_norm": 0.23113258183002472,
      "learning_rate": 9.172727272727273e-05,
      "loss": 0.430334454545644,
      "step": 91900
    },
    {
      "epoch": 0.8947283964424821,
      "grad_norm": 0.2490711361169815,
      "learning_rate": 9.171717171717172e-05,
      "loss": 0.4293411802912452,
      "step": 92000
    },
    {
      "epoch": 0.8957009273081804,
      "grad_norm": 0.2487202286720276,
      "learning_rate": 9.170707070707071e-05,
      "loss": 0.43121082621359696,
      "step": 92100
    },
    {
      "epoch": 0.8966734581738788,
      "grad_norm": 0.24491535127162933,
      "learning_rate": 9.16969696969697e-05,
      "loss": 0.4297239720522416,
      "step": 92200
    },
    {
      "epoch": 0.8976459890395772,
      "grad_norm": 0.24736540019512177,
      "learning_rate": 9.168686868686868e-05,
      "loss": 0.42922433605665133,
      "step": 92300
    },
    {
      "epoch": 0.8986185199052755,
      "grad_norm": 0.2521122395992279,
      "learning_rate": 9.167676767676769e-05,
      "loss": 0.42941572776916004,
      "step": 92400
    },
    {
      "epoch": 0.8995910507709739,
      "grad_norm": 0.2799170911312103,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.42921225444463973,
      "step": 92500
    },
    {
      "epoch": 0.9005635816366722,
      "grad_norm": 0.2356080859899521,
      "learning_rate": 9.165656565656566e-05,
      "loss": 0.4284165057938526,
      "step": 92600
    },
    {
      "epoch": 0.9015361125023705,
      "grad_norm": 0.24787592887878418,
      "learning_rate": 9.164646464646465e-05,
      "loss": 0.4285091881937531,
      "step": 92700
    },
    {
      "epoch": 0.9025086433680689,
      "grad_norm": 0.3018956184387207,
      "learning_rate": 9.163636363636364e-05,
      "loss": 0.4296876855363126,
      "step": 92800
    },
    {
      "epoch": 0.9034811742337673,
      "grad_norm": 0.24246956408023834,
      "learning_rate": 9.162626262626262e-05,
      "loss": 0.429385509845833,
      "step": 92900
    },
    {
      "epoch": 0.9044537050994655,
      "grad_norm": 0.2471102923154831,
      "learning_rate": 9.161616161616163e-05,
      "loss": 0.42918205041461077,
      "step": 93000
    },
    {
      "epoch": 0.9054262359651639,
      "grad_norm": 0.24206949770450592,
      "learning_rate": 9.16060606060606e-05,
      "loss": 0.4288798900067591,
      "step": 93100
    },
    {
      "epoch": 0.9063987668308623,
      "grad_norm": 0.253490149974823,
      "learning_rate": 9.15959595959596e-05,
      "loss": 0.42867644307950514,
      "step": 93200
    },
    {
      "epoch": 0.9073712976965607,
      "grad_norm": 0.2701416313648224,
      "learning_rate": 9.158585858585859e-05,
      "loss": 0.42896652048601536,
      "step": 93300
    },
    {
      "epoch": 0.908343828562259,
      "grad_norm": 0.2626318037509918,
      "learning_rate": 9.157575757575758e-05,
      "loss": 0.43063842267997554,
      "step": 93400
    },
    {
      "epoch": 0.9093163594279573,
      "grad_norm": 0.2447054237127304,
      "learning_rate": 9.156565656565656e-05,
      "loss": 0.4295466502910993,
      "step": 93500
    },
    {
      "epoch": 0.9102888902936557,
      "grad_norm": 0.25632444024086,
      "learning_rate": 9.155555555555557e-05,
      "loss": 0.429145805578576,
      "step": 93600
    },
    {
      "epoch": 0.911261421159354,
      "grad_norm": 0.23611663281917572,
      "learning_rate": 9.154545454545454e-05,
      "loss": 0.4288436701786609,
      "step": 93700
    },
    {
      "epoch": 0.9122339520250524,
      "grad_norm": 0.25758057832717896,
      "learning_rate": 9.153535353535354e-05,
      "loss": 0.4282454526888047,
      "step": 93800
    },
    {
      "epoch": 0.9132064828907508,
      "grad_norm": 0.2465498149394989,
      "learning_rate": 9.152525252525253e-05,
      "loss": 0.4288315969026281,
      "step": 93900
    },
    {
      "epoch": 0.914179013756449,
      "grad_norm": 0.24888795614242554,
      "learning_rate": 9.151515151515152e-05,
      "loss": 0.42882556026461177,
      "step": 94000
    },
    {
      "epoch": 0.9151515446221474,
      "grad_norm": 0.24202239513397217,
      "learning_rate": 9.15050505050505e-05,
      "loss": 0.42832606042334265,
      "step": 94100
    },
    {
      "epoch": 0.9161240754878458,
      "grad_norm": 0.2553674578666687,
      "learning_rate": 9.14949494949495e-05,
      "loss": 0.42861610448593757,
      "step": 94200
    },
    {
      "epoch": 0.9170966063535442,
      "grad_norm": 0.2748863101005554,
      "learning_rate": 9.148484848484848e-05,
      "loss": 0.4276231720066716,
      "step": 94300
    },
    {
      "epoch": 0.9180691372192425,
      "grad_norm": 0.2663232386112213,
      "learning_rate": 9.147474747474747e-05,
      "loss": 0.4289987906578685,
      "step": 94400
    },
    {
      "epoch": 0.9190416680849408,
      "grad_norm": 0.24146686494350433,
      "learning_rate": 9.146464646464647e-05,
      "loss": 0.4303743704078308,
      "step": 94500
    },
    {
      "epoch": 0.9200141989506392,
      "grad_norm": 0.2436094433069229,
      "learning_rate": 9.145454545454546e-05,
      "loss": 0.42849328335450904,
      "step": 94600
    },
    {
      "epoch": 0.9209867298163376,
      "grad_norm": 0.21982993185520172,
      "learning_rate": 9.144444444444444e-05,
      "loss": 0.4293754096265273,
      "step": 94700
    },
    {
      "epoch": 0.9219592606820359,
      "grad_norm": 0.24103263020515442,
      "learning_rate": 9.143434343434344e-05,
      "loss": 0.42936936465253206,
      "step": 94800
    },
    {
      "epoch": 0.9229317915477342,
      "grad_norm": 0.25102585554122925,
      "learning_rate": 9.142424242424242e-05,
      "loss": 0.4280804598403799,
      "step": 94900
    },
    {
      "epoch": 0.9239043224134326,
      "grad_norm": 0.25566011667251587,
      "learning_rate": 9.141414141414141e-05,
      "loss": 0.42876519388444806,
      "step": 95000
    },
    {
      "epoch": 0.9248768532791309,
      "grad_norm": 0.23447419703006744,
      "learning_rate": 9.14040404040404e-05,
      "loss": 0.42856179975172676,
      "step": 95100
    },
    {
      "epoch": 0.9258493841448293,
      "grad_norm": 0.2525424659252167,
      "learning_rate": 9.13939393939394e-05,
      "loss": 0.4299372489046871,
      "step": 95200
    },
    {
      "epoch": 0.9268219150105277,
      "grad_norm": 0.2443542182445526,
      "learning_rate": 9.138383838383839e-05,
      "loss": 0.42864840800170606,
      "step": 95300
    },
    {
      "epoch": 0.927794445876226,
      "grad_norm": 0.2254420667886734,
      "learning_rate": 9.137373737373738e-05,
      "loss": 0.4285436981736565,
      "step": 95400
    },
    {
      "epoch": 0.9287669767419243,
      "grad_norm": 0.2464253306388855,
      "learning_rate": 9.136363636363637e-05,
      "loss": 0.42784695198406714,
      "step": 95500
    },
    {
      "epoch": 0.9297395076076227,
      "grad_norm": 0.2509317696094513,
      "learning_rate": 9.135353535353535e-05,
      "loss": 0.42853163045494297,
      "step": 95600
    },
    {
      "epoch": 0.9307120384733211,
      "grad_norm": 0.24608242511749268,
      "learning_rate": 9.134343434343436e-05,
      "loss": 0.4290189486524543,
      "step": 95700
    },
    {
      "epoch": 0.9316845693390194,
      "grad_norm": 0.24031029641628265,
      "learning_rate": 9.133333333333334e-05,
      "loss": 0.4280262176260105,
      "step": 95800
    },
    {
      "epoch": 0.9326571002047177,
      "grad_norm": 0.2509390413761139,
      "learning_rate": 9.132323232323233e-05,
      "loss": 0.4289081994077287,
      "step": 95900
    },
    {
      "epoch": 0.9336296310704161,
      "grad_norm": 0.26816943287849426,
      "learning_rate": 9.131313131313132e-05,
      "loss": 0.4282114962873634,
      "step": 96000
    },
    {
      "epoch": 0.9346021619361145,
      "grad_norm": 0.2539677023887634,
      "learning_rate": 9.130303030303031e-05,
      "loss": 0.4284027963041048,
      "step": 96100
    },
    {
      "epoch": 0.9355746928018128,
      "grad_norm": 0.25208044052124023,
      "learning_rate": 9.12929292929293e-05,
      "loss": 0.42859409076352706,
      "step": 96200
    },
    {
      "epoch": 0.9365472236675112,
      "grad_norm": 0.26171597838401794,
      "learning_rate": 9.12828282828283e-05,
      "loss": 0.42829206928865615,
      "step": 96300
    },
    {
      "epoch": 0.9375197545332095,
      "grad_norm": 0.24185536801815033,
      "learning_rate": 9.127272727272727e-05,
      "loss": 0.42868068095221884,
      "step": 96400
    },
    {
      "epoch": 0.9384922853989078,
      "grad_norm": 0.24525098502635956,
      "learning_rate": 9.126262626262627e-05,
      "loss": 0.42768805134685106,
      "step": 96500
    },
    {
      "epoch": 0.9394648162646062,
      "grad_norm": 0.24563173949718475,
      "learning_rate": 9.125252525252526e-05,
      "loss": 0.42827397604656475,
      "step": 96600
    },
    {
      "epoch": 0.9404373471303046,
      "grad_norm": 0.2602192461490631,
      "learning_rate": 9.124242424242425e-05,
      "loss": 0.42797197541164106,
      "step": 96700
    },
    {
      "epoch": 0.941409877996003,
      "grad_norm": 0.25764036178588867,
      "learning_rate": 9.123232323232324e-05,
      "loss": 0.4277686382414419,
      "step": 96800
    },
    {
      "epoch": 0.9423824088617012,
      "grad_norm": 0.23065832257270813,
      "learning_rate": 9.122222222222223e-05,
      "loss": 0.42894645898038475,
      "step": 96900
    },
    {
      "epoch": 0.9433549397273996,
      "grad_norm": 0.23487865924835205,
      "learning_rate": 9.121212121212121e-05,
      "loss": 0.4281511993736901,
      "step": 97000
    },
    {
      "epoch": 0.944327470593098,
      "grad_norm": 0.26926398277282715,
      "learning_rate": 9.120202020202022e-05,
      "loss": 0.42706000911400355,
      "step": 97100
    },
    {
      "epoch": 0.9453000014587963,
      "grad_norm": 0.24611248075962067,
      "learning_rate": 9.11919191919192e-05,
      "loss": 0.4274485929909701,
      "step": 97200
    },
    {
      "epoch": 0.9462725323244946,
      "grad_norm": 0.24447467923164368,
      "learning_rate": 9.118181818181819e-05,
      "loss": 0.42783716575329805,
      "step": 97300
    },
    {
      "epoch": 0.947245063190193,
      "grad_norm": 0.23606398701667786,
      "learning_rate": 9.117171717171718e-05,
      "loss": 0.4279297870226871,
      "step": 97400
    },
    {
      "epoch": 0.9482175940558913,
      "grad_norm": 0.2701232135295868,
      "learning_rate": 9.116161616161617e-05,
      "loss": 0.42792376010997935,
      "step": 97500
    },
    {
      "epoch": 0.9491901249215897,
      "grad_norm": 0.26654720306396484,
      "learning_rate": 9.115151515151515e-05,
      "loss": 0.42702993707030634,
      "step": 97600
    },
    {
      "epoch": 0.9501626557872881,
      "grad_norm": 0.25113779306411743,
      "learning_rate": 9.114141414141416e-05,
      "loss": 0.4266293521624571,
      "step": 97700
    },
    {
      "epoch": 0.9511351866529864,
      "grad_norm": 0.2620149850845337,
      "learning_rate": 9.113131313131313e-05,
      "loss": 0.4283988855490944,
      "step": 97800
    },
    {
      "epoch": 0.9521077175186847,
      "grad_norm": 0.25317272543907166,
      "learning_rate": 9.112121212121213e-05,
      "loss": 0.42740645322855936,
      "step": 97900
    },
    {
      "epoch": 0.9530802483843831,
      "grad_norm": 0.23561877012252808,
      "learning_rate": 9.111111111111112e-05,
      "loss": 0.4280909024600168,
      "step": 98000
    },
    {
      "epoch": 0.9540527792500815,
      "grad_norm": 0.25770294666290283,
      "learning_rate": 9.110101010101011e-05,
      "loss": 0.42749305036390023,
      "step": 98100
    },
    {
      "epoch": 0.9550253101157798,
      "grad_norm": 0.23863078653812408,
      "learning_rate": 9.109090909090909e-05,
      "loss": 0.4275856646866401,
      "step": 98200
    },
    {
      "epoch": 0.9559978409814781,
      "grad_norm": 0.24784144759178162,
      "learning_rate": 9.10808080808081e-05,
      "loss": 0.42807281338591485,
      "step": 98300
    },
    {
      "epoch": 0.9569703718471765,
      "grad_norm": 0.24081310629844666,
      "learning_rate": 9.107070707070707e-05,
      "loss": 0.4271790875993284,
      "step": 98400
    },
    {
      "epoch": 0.9579429027128749,
      "grad_norm": 0.24798540771007538,
      "learning_rate": 9.106060606060606e-05,
      "loss": 0.4283566485335971,
      "step": 98500
    },
    {
      "epoch": 0.9589154335785732,
      "grad_norm": 0.23505716025829315,
      "learning_rate": 9.105050505050506e-05,
      "loss": 0.4273643134661486,
      "step": 98600
    },
    {
      "epoch": 0.9598879644442716,
      "grad_norm": 0.21776342391967773,
      "learning_rate": 9.104040404040405e-05,
      "loss": 0.42597749125937884,
      "step": 98700
    },
    {
      "epoch": 0.9608604953099699,
      "grad_norm": 0.2234065681695938,
      "learning_rate": 9.103030303030303e-05,
      "loss": 0.42754952821833037,
      "step": 98800
    },
    {
      "epoch": 0.9618330261756682,
      "grad_norm": 0.2314610779285431,
      "learning_rate": 9.102020202020203e-05,
      "loss": 0.4276421314264318,
      "step": 98900
    },
    {
      "epoch": 0.9628055570413666,
      "grad_norm": 0.22449715435504913,
      "learning_rate": 9.101010101010101e-05,
      "loss": 0.4260581142765447,
      "step": 99000
    },
    {
      "epoch": 0.963778087907065,
      "grad_norm": 0.2371160238981247,
      "learning_rate": 9.1e-05,
      "loss": 0.42772870633249577,
      "step": 99100
    },
    {
      "epoch": 0.9647506187727632,
      "grad_norm": 0.2515937089920044,
      "learning_rate": 9.0989898989899e-05,
      "loss": 0.42752543723945724,
      "step": 99200
    },
    {
      "epoch": 0.9657231496384616,
      "grad_norm": 0.2520221769809723,
      "learning_rate": 9.097979797979799e-05,
      "loss": 0.4278152756812405,
      "step": 99300
    },
    {
      "epoch": 0.96669568050416,
      "grad_norm": 0.243520125746727,
      "learning_rate": 9.096969696969697e-05,
      "loss": 0.4264285826821429,
      "step": 99400
    },
    {
      "epoch": 0.9676682113698584,
      "grad_norm": 0.246808260679245,
      "learning_rate": 9.095959595959597e-05,
      "loss": 0.42770460423898415,
      "step": 99500
    },
    {
      "epoch": 0.9686407422355567,
      "grad_norm": 0.22491547465324402,
      "learning_rate": 9.094949494949495e-05,
      "loss": 0.42720549757805076,
      "step": 99600
    },
    {
      "epoch": 0.969613273101255,
      "grad_norm": 0.22938960790634155,
      "learning_rate": 9.093939393939394e-05,
      "loss": 0.4273967086776845,
      "step": 99700
    },
    {
      "epoch": 0.9705858039669534,
      "grad_norm": 0.25958213210105896,
      "learning_rate": 9.092929292929293e-05,
      "loss": 0.427587914219999,
      "step": 99800
    },
    {
      "epoch": 0.9715583348326517,
      "grad_norm": 0.2982853055000305,
      "learning_rate": 9.091919191919193e-05,
      "loss": 0.4280749503835592,
      "step": 99900
    },
    {
      "epoch": 0.9725308656983501,
      "grad_norm": 0.24076083302497864,
      "learning_rate": 9.090909090909092e-05,
      "loss": 0.4275758659519028,
      "step": 100000
    },
    {
      "epoch": 0.9725308656983501,
      "eval_accuracy": 0.6653609623440623,
      "eval_loss": 0.4201596268746692,
      "eval_runtime": 3741.2483,
      "eval_samples_per_second": 610.692,
      "eval_steps_per_second": 6.107,
      "step": 100000
    },
    {
      "epoch": 0.9735033965640485,
      "grad_norm": 0.23897993564605713,
      "learning_rate": 9.089898989898991e-05,
      "loss": 0.4267809675709583,
      "step": 100100
    },
    {
      "epoch": 0.9744759274297468,
      "grad_norm": 0.2722252905368805,
      "learning_rate": 9.088888888888889e-05,
      "loss": 0.4267749545515487,
      "step": 100200
    },
    {
      "epoch": 0.9754484582954451,
      "grad_norm": 0.24528872966766357,
      "learning_rate": 9.087878787878788e-05,
      "loss": 0.42696615453654385,
      "step": 100300
    },
    {
      "epoch": 0.9764209891611435,
      "grad_norm": 0.23073944449424744,
      "learning_rate": 9.086868686868687e-05,
      "loss": 0.4269601387384746,
      "step": 100400
    },
    {
      "epoch": 0.9773935200268419,
      "grad_norm": 0.2904273271560669,
      "learning_rate": 9.085858585858586e-05,
      "loss": 0.4279401601758336,
      "step": 100500
    },
    {
      "epoch": 0.9783660508925403,
      "grad_norm": 0.23039603233337402,
      "learning_rate": 9.084848484848486e-05,
      "loss": 0.4272439141449752,
      "step": 100600
    },
    {
      "epoch": 0.9793385817582385,
      "grad_norm": 0.23265276849269867,
      "learning_rate": 9.083838383838385e-05,
      "loss": 0.4267448894545005,
      "step": 100700
    },
    {
      "epoch": 0.9803111126239369,
      "grad_norm": 0.2453530877828598,
      "learning_rate": 9.082828282828283e-05,
      "loss": 0.4269360755461976,
      "step": 100800
    },
    {
      "epoch": 0.9812836434896353,
      "grad_norm": 0.24874351918697357,
      "learning_rate": 9.081818181818182e-05,
      "loss": 0.4263384707507869,
      "step": 100900
    },
    {
      "epoch": 0.9822561743553336,
      "grad_norm": 0.2927621006965637,
      "learning_rate": 9.080808080808081e-05,
      "loss": 0.4273184310576343,
      "step": 101000
    },
    {
      "epoch": 0.983228705221032,
      "grad_norm": 0.24354423582553864,
      "learning_rate": 9.07979797979798e-05,
      "loss": 0.4261292650514781,
      "step": 101100
    },
    {
      "epoch": 0.9842012360867303,
      "grad_norm": 0.262521356344223,
      "learning_rate": 9.07878787878788e-05,
      "loss": 0.427700764339794,
      "step": 101200
    },
    {
      "epoch": 0.9851737669524286,
      "grad_norm": 0.23221543431282043,
      "learning_rate": 9.077777777777779e-05,
      "loss": 0.42582147785790353,
      "step": 101300
    },
    {
      "epoch": 0.986146297818127,
      "grad_norm": 0.2431449443101883,
      "learning_rate": 9.076767676767676e-05,
      "loss": 0.4268013895382076,
      "step": 101400
    },
    {
      "epoch": 0.9871188286838254,
      "grad_norm": 0.24304929375648499,
      "learning_rate": 9.075757575757577e-05,
      "loss": 0.4268939649597129,
      "step": 101500
    },
    {
      "epoch": 0.9880913595495237,
      "grad_norm": 0.25482288002967834,
      "learning_rate": 9.074747474747475e-05,
      "loss": 0.4262964185161541,
      "step": 101600
    },
    {
      "epoch": 0.989063890415222,
      "grad_norm": 0.22953897714614868,
      "learning_rate": 9.073737373737374e-05,
      "loss": 0.4268819333635744,
      "step": 101700
    },
    {
      "epoch": 0.9900364212809204,
      "grad_norm": 0.26085716485977173,
      "learning_rate": 9.072727272727273e-05,
      "loss": 0.42687591756550514,
      "step": 101800
    },
    {
      "epoch": 0.9910089521466188,
      "grad_norm": 0.2692917585372925,
      "learning_rate": 9.071717171717172e-05,
      "loss": 0.4253911376735533,
      "step": 101900
    },
    {
      "epoch": 0.9919814830123171,
      "grad_norm": 0.2564065158367157,
      "learning_rate": 9.07070707070707e-05,
      "loss": 0.4254837255990269,
      "step": 102000
    },
    {
      "epoch": 0.9929540138780154,
      "grad_norm": 0.23790977895259857,
      "learning_rate": 9.069696969696971e-05,
      "loss": 0.426364962699968,
      "step": 102100
    },
    {
      "epoch": 0.9939265447437138,
      "grad_norm": 0.2395394742488861,
      "learning_rate": 9.068686868686869e-05,
      "loss": 0.42488025227450654,
      "step": 102200
    },
    {
      "epoch": 0.9948990756094122,
      "grad_norm": 0.25953832268714905,
      "learning_rate": 9.067676767676768e-05,
      "loss": 0.4255643152722774,
      "step": 102300
    },
    {
      "epoch": 0.9958716064751105,
      "grad_norm": 0.23041988909244537,
      "learning_rate": 9.066666666666667e-05,
      "loss": 0.42585404951432504,
      "step": 102400
    },
    {
      "epoch": 0.9968441373408089,
      "grad_norm": 0.2476668506860733,
      "learning_rate": 9.065656565656566e-05,
      "loss": 0.42614377542039383,
      "step": 102500
    },
    {
      "epoch": 0.9978166682065072,
      "grad_norm": 0.23039890825748444,
      "learning_rate": 9.064646464646464e-05,
      "loss": 0.4265320675381006,
      "step": 102600
    },
    {
      "epoch": 0.9987891990722055,
      "grad_norm": 0.2584172785282135,
      "learning_rate": 9.063636363636365e-05,
      "loss": 0.4255403243251504,
      "step": 102700
    },
    {
      "epoch": 0.9997617299379039,
      "grad_norm": 0.2601638734340668,
      "learning_rate": 9.062626262626262e-05,
      "loss": 0.4272100466606416,
      "step": 102800
    },
    {
      "epoch": 1.0007342608036023,
      "grad_norm": 0.25202301144599915,
      "learning_rate": 9.061616161616162e-05,
      "loss": 0.426021180749724,
      "step": 102900
    },
    {
      "epoch": 1.0017067916693005,
      "grad_norm": 0.2522163391113281,
      "learning_rate": 9.060606060606061e-05,
      "loss": 0.422762399386472,
      "step": 103000
    },
    {
      "epoch": 1.002679322534999,
      "grad_norm": 0.23887556791305542,
      "learning_rate": 9.05959595959596e-05,
      "loss": 0.4238406841615707,
      "step": 103100
    },
    {
      "epoch": 1.0036518534006973,
      "grad_norm": 0.24783913791179657,
      "learning_rate": 9.058585858585858e-05,
      "loss": 0.4233418789852057,
      "step": 103200
    },
    {
      "epoch": 1.0046243842663956,
      "grad_norm": 0.24257448315620422,
      "learning_rate": 9.057575757575758e-05,
      "loss": 0.42313878216906337,
      "step": 103300
    },
    {
      "epoch": 1.005596915132094,
      "grad_norm": 0.2495829313993454,
      "learning_rate": 9.056565656565656e-05,
      "loss": 0.4253997767346998,
      "step": 103400
    },
    {
      "epoch": 1.0065694459977923,
      "grad_norm": 0.24304696917533875,
      "learning_rate": 9.055555555555556e-05,
      "loss": 0.424211035863465,
      "step": 103500
    },
    {
      "epoch": 1.0075419768634908,
      "grad_norm": 0.26999229192733765,
      "learning_rate": 9.054545454545455e-05,
      "loss": 0.42479642011388336,
      "step": 103600
    },
    {
      "epoch": 1.008514507729189,
      "grad_norm": 0.2392805516719818,
      "learning_rate": 9.053535353535354e-05,
      "loss": 0.4238048394525207,
      "step": 103700
    },
    {
      "epoch": 1.0094870385948873,
      "grad_norm": 0.2452821582555771,
      "learning_rate": 9.052525252525252e-05,
      "loss": 0.4230104023290725,
      "step": 103800
    },
    {
      "epoch": 1.0104595694605858,
      "grad_norm": 0.24773570895195007,
      "learning_rate": 9.051515151515152e-05,
      "loss": 0.4246798995931348,
      "step": 103900
    },
    {
      "epoch": 1.011432100326284,
      "grad_norm": 0.23922359943389893,
      "learning_rate": 9.050505050505052e-05,
      "loss": 0.42388547219499517,
      "step": 104000
    },
    {
      "epoch": 1.0124046311919825,
      "grad_norm": 0.26405033469200134,
      "learning_rate": 9.04949494949495e-05,
      "loss": 0.4241751578104996,
      "step": 104100
    },
    {
      "epoch": 1.0133771620576808,
      "grad_norm": 0.24639616906642914,
      "learning_rate": 9.04848484848485e-05,
      "loss": 0.42407062581666544,
      "step": 104200
    },
    {
      "epoch": 1.014349692923379,
      "grad_norm": 0.24886897206306458,
      "learning_rate": 9.047474747474748e-05,
      "loss": 0.4242617493885212,
      "step": 104300
    },
    {
      "epoch": 1.0153222237890775,
      "grad_norm": 0.26334646344184875,
      "learning_rate": 9.046464646464647e-05,
      "loss": 0.4232702729268941,
      "step": 104400
    },
    {
      "epoch": 1.0162947546547758,
      "grad_norm": 0.247116357088089,
      "learning_rate": 9.045454545454546e-05,
      "loss": 0.42365849835677016,
      "step": 104500
    },
    {
      "epoch": 1.0172672855204743,
      "grad_norm": 0.2644595205783844,
      "learning_rate": 9.044444444444445e-05,
      "loss": 0.4243423529550697,
      "step": 104600
    },
    {
      "epoch": 1.0182398163861726,
      "grad_norm": 0.25899359583854675,
      "learning_rate": 9.043434343434343e-05,
      "loss": 0.4231538260406254,
      "step": 104700
    },
    {
      "epoch": 1.0192123472518708,
      "grad_norm": 0.24711570143699646,
      "learning_rate": 9.042424242424244e-05,
      "loss": 0.42492165194092846,
      "step": 104800
    },
    {
      "epoch": 1.0201848781175693,
      "grad_norm": 0.26928451657295227,
      "learning_rate": 9.041414141414142e-05,
      "loss": 0.42304335188320186,
      "step": 104900
    },
    {
      "epoch": 1.0211574089832676,
      "grad_norm": 0.23694008588790894,
      "learning_rate": 9.040404040404041e-05,
      "loss": 0.42353009350884274,
      "step": 105000
    },
    {
      "epoch": 1.022129939848966,
      "grad_norm": 0.2382529377937317,
      "learning_rate": 9.03939393939394e-05,
      "loss": 0.423327042540584,
      "step": 105100
    },
    {
      "epoch": 1.0231024707146643,
      "grad_norm": 0.2527582347393036,
      "learning_rate": 9.038383838383839e-05,
      "loss": 0.42391230452997936,
      "step": 105200
    },
    {
      "epoch": 1.0240750015803626,
      "grad_norm": 0.25695541501045227,
      "learning_rate": 9.037373737373738e-05,
      "loss": 0.42341364245458457,
      "step": 105300
    },
    {
      "epoch": 1.025047532446061,
      "grad_norm": 0.24145640432834625,
      "learning_rate": 9.036363636363638e-05,
      "loss": 0.42311206556525227,
      "step": 105400
    },
    {
      "epoch": 1.0260200633117593,
      "grad_norm": 0.22858615219593048,
      "learning_rate": 9.035353535353535e-05,
      "loss": 0.42497825066705197,
      "step": 105500
    },
    {
      "epoch": 1.0269925941774578,
      "grad_norm": 0.24713705480098724,
      "learning_rate": 9.034343434343435e-05,
      "loss": 0.42339573260402785,
      "step": 105600
    },
    {
      "epoch": 1.027965125043156,
      "grad_norm": 0.23891638219356537,
      "learning_rate": 9.033333333333334e-05,
      "loss": 0.4237838885674134,
      "step": 105700
    },
    {
      "epoch": 1.0289376559088543,
      "grad_norm": 0.25345638394355774,
      "learning_rate": 9.032323232323233e-05,
      "loss": 0.42328526261459365,
      "step": 105800
    },
    {
      "epoch": 1.0299101867745528,
      "grad_norm": 0.278022438287735,
      "learning_rate": 9.031313131313132e-05,
      "loss": 0.4245601671502692,
      "step": 105900
    },
    {
      "epoch": 1.030882717640251,
      "grad_norm": 0.2356889247894287,
      "learning_rate": 9.030303030303031e-05,
      "loss": 0.4228792162512687,
      "step": 106000
    },
    {
      "epoch": 1.0318552485059496,
      "grad_norm": 0.2613368034362793,
      "learning_rate": 9.029292929292929e-05,
      "loss": 0.4228732532477321,
      "step": 106100
    },
    {
      "epoch": 1.0328277793716478,
      "grad_norm": 0.23598246276378632,
      "learning_rate": 9.02828282828283e-05,
      "loss": 0.422276143053733,
      "step": 106200
    },
    {
      "epoch": 1.033800310237346,
      "grad_norm": 0.2479698657989502,
      "learning_rate": 9.027272727272728e-05,
      "loss": 0.42335394295272877,
      "step": 106300
    },
    {
      "epoch": 1.0347728411030446,
      "grad_norm": 0.2484702467918396,
      "learning_rate": 9.026262626262627e-05,
      "loss": 0.4233479730025432,
      "step": 106400
    },
    {
      "epoch": 1.0357453719687428,
      "grad_norm": 0.26319533586502075,
      "learning_rate": 9.025252525252526e-05,
      "loss": 0.4240316455986379,
      "step": 106500
    },
    {
      "epoch": 1.0367179028344413,
      "grad_norm": 0.2641465663909912,
      "learning_rate": 9.024242424242425e-05,
      "loss": 0.42373010899987007,
      "step": 106600
    },
    {
      "epoch": 1.0376904337001396,
      "grad_norm": 0.23912478983402252,
      "learning_rate": 9.023232323232323e-05,
      "loss": 0.4235270983221759,
      "step": 106700
    },
    {
      "epoch": 1.0386629645658378,
      "grad_norm": 0.26739662885665894,
      "learning_rate": 9.022222222222224e-05,
      "loss": 0.42430925515944995,
      "step": 106800
    },
    {
      "epoch": 1.0396354954315363,
      "grad_norm": 0.2545514702796936,
      "learning_rate": 9.021212121212121e-05,
      "loss": 0.42312109363874506,
      "step": 106900
    },
    {
      "epoch": 1.0406080262972346,
      "grad_norm": 0.24521248042583466,
      "learning_rate": 9.02020202020202e-05,
      "loss": 0.4236076935527456,
      "step": 107000
    },
    {
      "epoch": 1.0415805571629329,
      "grad_norm": 0.2256375253200531,
      "learning_rate": 9.01919191919192e-05,
      "loss": 0.42310915929569315,
      "step": 107100
    },
    {
      "epoch": 1.0425530880286313,
      "grad_norm": 0.2610507011413574,
      "learning_rate": 9.018181818181819e-05,
      "loss": 0.4236942559548412,
      "step": 107200
    },
    {
      "epoch": 1.0435256188943296,
      "grad_norm": 0.2629798948764801,
      "learning_rate": 9.017171717171717e-05,
      "loss": 0.4248703914367268,
      "step": 107300
    },
    {
      "epoch": 1.044498149760028,
      "grad_norm": 0.24470366537570953,
      "learning_rate": 9.016161616161617e-05,
      "loss": 0.42387932065940376,
      "step": 107400
    },
    {
      "epoch": 1.0454706806257263,
      "grad_norm": 0.24510110914707184,
      "learning_rate": 9.015151515151515e-05,
      "loss": 0.4244643811959768,
      "step": 107500
    },
    {
      "epoch": 1.0464432114914246,
      "grad_norm": 0.2690991461277008,
      "learning_rate": 9.014141414141415e-05,
      "loss": 0.42347334376256907,
      "step": 107600
    },
    {
      "epoch": 1.047415742357123,
      "grad_norm": 0.2303532063961029,
      "learning_rate": 9.013131313131314e-05,
      "loss": 0.42307335626653736,
      "step": 107700
    },
    {
      "epoch": 1.0483882732228214,
      "grad_norm": 0.24378307163715363,
      "learning_rate": 9.012121212121213e-05,
      "loss": 0.42277188218761097,
      "step": 107800
    },
    {
      "epoch": 1.0493608040885198,
      "grad_norm": 0.25034311413764954,
      "learning_rate": 9.011111111111111e-05,
      "loss": 0.42266741827093735,
      "step": 107900
    },
    {
      "epoch": 1.050333334954218,
      "grad_norm": 0.24061216413974762,
      "learning_rate": 9.010101010101011e-05,
      "loss": 0.42344945284718827,
      "step": 108000
    },
    {
      "epoch": 1.0513058658199164,
      "grad_norm": 0.2607676386833191,
      "learning_rate": 9.009090909090909e-05,
      "loss": 0.42236000063909174,
      "step": 108100
    },
    {
      "epoch": 1.0522783966856148,
      "grad_norm": 0.2489752471446991,
      "learning_rate": 9.008080808080808e-05,
      "loss": 0.42294502366375997,
      "step": 108200
    },
    {
      "epoch": 1.053250927551313,
      "grad_norm": 0.23275035619735718,
      "learning_rate": 9.007070707070708e-05,
      "loss": 0.42333303930483496,
      "step": 108300
    },
    {
      "epoch": 1.0542234584170116,
      "grad_norm": 0.25538432598114014,
      "learning_rate": 9.006060606060607e-05,
      "loss": 0.4227361041663917,
      "step": 108400
    },
    {
      "epoch": 1.0551959892827099,
      "grad_norm": 0.24761749804019928,
      "learning_rate": 9.005050505050505e-05,
      "loss": 0.42282863374001334,
      "step": 108500
    },
    {
      "epoch": 1.0561685201484081,
      "grad_norm": 0.23482534289360046,
      "learning_rate": 9.004040404040405e-05,
      "loss": 0.4230196517228037,
      "step": 108600
    },
    {
      "epoch": 1.0571410510141066,
      "grad_norm": 0.2543018162250519,
      "learning_rate": 9.003030303030303e-05,
      "loss": 0.4239000927377647,
      "step": 108700
    },
    {
      "epoch": 1.0581135818798049,
      "grad_norm": 0.2605457007884979,
      "learning_rate": 9.002020202020202e-05,
      "loss": 0.42300771737975174,
      "step": 108800
    },
    {
      "epoch": 1.0590861127455033,
      "grad_norm": 0.24465645849704742,
      "learning_rate": 9.001010101010101e-05,
      "loss": 0.4230017502082258,
      "step": 108900
    },
    {
      "epoch": 1.0600586436112016,
      "grad_norm": 0.2490639090538025,
      "learning_rate": 9e-05,
      "loss": 0.4229957830366998,
      "step": 109000
    },
    {
      "epoch": 1.0610311744768999,
      "grad_norm": 0.239023357629776,
      "learning_rate": 8.998989898989898e-05,
      "loss": 0.42328526858871185,
      "step": 109100
    },
    {
      "epoch": 1.0620037053425984,
      "grad_norm": 0.2614831030368805,
      "learning_rate": 8.997979797979799e-05,
      "loss": 0.42406716006399253,
      "step": 109200
    },
    {
      "epoch": 1.0629762362082966,
      "grad_norm": 0.24002119898796082,
      "learning_rate": 8.996969696969697e-05,
      "loss": 0.42356877029724016,
      "step": 109300
    },
    {
      "epoch": 1.063948767073995,
      "grad_norm": 0.2502364218235016,
      "learning_rate": 8.995959595959596e-05,
      "loss": 0.42385823500930503,
      "step": 109400
    },
    {
      "epoch": 1.0649212979396934,
      "grad_norm": 0.251475065946579,
      "learning_rate": 8.994949494949495e-05,
      "loss": 0.42444312743697127,
      "step": 109500
    },
    {
      "epoch": 1.0658938288053916,
      "grad_norm": 0.2568974792957306,
      "learning_rate": 8.993939393939394e-05,
      "loss": 0.4228615027130138,
      "step": 109600
    },
    {
      "epoch": 1.0668663596710901,
      "grad_norm": 0.24295733869075775,
      "learning_rate": 8.992929292929294e-05,
      "loss": 0.42187077787881294,
      "step": 109700
    },
    {
      "epoch": 1.0678388905367884,
      "grad_norm": 0.25956377387046814,
      "learning_rate": 8.991919191919193e-05,
      "loss": 0.42344041824384543,
      "step": 109800
    },
    {
      "epoch": 1.0688114214024869,
      "grad_norm": 0.23933404684066772,
      "learning_rate": 8.99090909090909e-05,
      "loss": 0.4226466591133436,
      "step": 109900
    },
    {
      "epoch": 1.0697839522681851,
      "grad_norm": 0.22838790714740753,
      "learning_rate": 8.98989898989899e-05,
      "loss": 0.42283763958422915,
      "step": 110000
    },
    {
      "epoch": 1.0697839522681851,
      "eval_accuracy": 0.6657834725859835,
      "eval_loss": 0.41714915778248346,
      "eval_runtime": 3740.7037,
      "eval_samples_per_second": 610.781,
      "eval_steps_per_second": 6.108,
      "step": 110000
    },
    {
      "epoch": 1.0707564831338834,
      "grad_norm": 0.24499541521072388,
      "learning_rate": 8.988888888888889e-05,
      "loss": 0.42194544067110185,
      "step": 110100
    },
    {
      "epoch": 1.0717290139995819,
      "grad_norm": 0.23588988184928894,
      "learning_rate": 8.987878787878788e-05,
      "loss": 0.423318052812594,
      "step": 110200
    },
    {
      "epoch": 1.0727015448652801,
      "grad_norm": 0.25580015778541565,
      "learning_rate": 8.986868686868687e-05,
      "loss": 0.42400135306830017,
      "step": 110300
    },
    {
      "epoch": 1.0736740757309784,
      "grad_norm": 0.2717757225036621,
      "learning_rate": 8.985858585858587e-05,
      "loss": 0.42369997207447085,
      "step": 110400
    },
    {
      "epoch": 1.0746466065966769,
      "grad_norm": 0.24712364375591278,
      "learning_rate": 8.984848484848484e-05,
      "loss": 0.42211855713931434,
      "step": 110500
    },
    {
      "epoch": 1.0756191374623751,
      "grad_norm": 0.2627618908882141,
      "learning_rate": 8.983838383838385e-05,
      "loss": 0.4213248938725693,
      "step": 110600
    },
    {
      "epoch": 1.0765916683280736,
      "grad_norm": 0.2345079928636551,
      "learning_rate": 8.982828282828283e-05,
      "loss": 0.42171279697792985,
      "step": 110700
    },
    {
      "epoch": 1.0775641991937719,
      "grad_norm": 0.2517472803592682,
      "learning_rate": 8.981818181818182e-05,
      "loss": 0.4229868345718051,
      "step": 110800
    },
    {
      "epoch": 1.0785367300594704,
      "grad_norm": 0.22638598084449768,
      "learning_rate": 8.980808080808081e-05,
      "loss": 0.42209473291176436,
      "step": 110900
    },
    {
      "epoch": 1.0795092609251686,
      "grad_norm": 0.2614813446998596,
      "learning_rate": 8.97979797979798e-05,
      "loss": 0.4229748974500935,
      "step": 111000
    },
    {
      "epoch": 1.080481791790867,
      "grad_norm": 0.25315558910369873,
      "learning_rate": 8.978787878787878e-05,
      "loss": 0.4234612111621535,
      "step": 111100
    },
    {
      "epoch": 1.0814543226565654,
      "grad_norm": 0.23774808645248413,
      "learning_rate": 8.977777777777779e-05,
      "loss": 0.42335678058939535,
      "step": 111200
    },
    {
      "epoch": 1.0824268535222636,
      "grad_norm": 0.24143578112125397,
      "learning_rate": 8.976767676767677e-05,
      "loss": 0.42335080647122036,
      "step": 111300
    },
    {
      "epoch": 1.083399384387962,
      "grad_norm": 0.24820077419281006,
      "learning_rate": 8.975757575757576e-05,
      "loss": 0.4237386414994203,
      "step": 111400
    },
    {
      "epoch": 1.0843719152536604,
      "grad_norm": 0.2794422507286072,
      "learning_rate": 8.974747474747475e-05,
      "loss": 0.4219605456731754,
      "step": 111500
    },
    {
      "epoch": 1.0853444461193587,
      "grad_norm": 0.2359258532524109,
      "learning_rate": 8.973737373737374e-05,
      "loss": 0.42274218706909067,
      "step": 111600
    },
    {
      "epoch": 1.0863169769850571,
      "grad_norm": 0.24654291570186615,
      "learning_rate": 8.972727272727272e-05,
      "loss": 0.42244087693108157,
      "step": 111700
    },
    {
      "epoch": 1.0872895078507554,
      "grad_norm": 0.2853366732597351,
      "learning_rate": 8.971717171717173e-05,
      "loss": 0.42213957512905137,
      "step": 111800
    },
    {
      "epoch": 1.0882620387164537,
      "grad_norm": 0.2635321021080017,
      "learning_rate": 8.97070707070707e-05,
      "loss": 0.42252739904261283,
      "step": 111900
    },
    {
      "epoch": 1.0892345695821521,
      "grad_norm": 0.26170599460601807,
      "learning_rate": 8.96969696969697e-05,
      "loss": 0.4230136557921507,
      "step": 112000
    },
    {
      "epoch": 1.0902071004478504,
      "grad_norm": 0.2886684834957123,
      "learning_rate": 8.968686868686869e-05,
      "loss": 0.42359834120967554,
      "step": 112100
    },
    {
      "epoch": 1.091179631313549,
      "grad_norm": 0.22058893740177155,
      "learning_rate": 8.967676767676768e-05,
      "loss": 0.42152509831245055,
      "step": 112200
    },
    {
      "epoch": 1.0921521621792472,
      "grad_norm": 0.26000985503196716,
      "learning_rate": 8.966666666666666e-05,
      "loss": 0.422995745941594,
      "step": 112300
    },
    {
      "epoch": 1.0931246930449454,
      "grad_norm": 0.2542818784713745,
      "learning_rate": 8.965656565656567e-05,
      "loss": 0.42279289920481705,
      "step": 112400
    },
    {
      "epoch": 1.094097223910644,
      "grad_norm": 0.2804430425167084,
      "learning_rate": 8.964646464646466e-05,
      "loss": 0.4231806800491545,
      "step": 112500
    },
    {
      "epoch": 1.0950697547763422,
      "grad_norm": 0.2433260977268219,
      "learning_rate": 8.963636363636364e-05,
      "loss": 0.422288786788585,
      "step": 112600
    },
    {
      "epoch": 1.0960422856420406,
      "grad_norm": 0.267741858959198,
      "learning_rate": 8.962626262626264e-05,
      "loss": 0.42159378698656447,
      "step": 112700
    },
    {
      "epoch": 1.097014816507739,
      "grad_norm": 0.2361019253730774,
      "learning_rate": 8.961616161616162e-05,
      "loss": 0.4215878364869962,
      "step": 112800
    },
    {
      "epoch": 1.0979873473734372,
      "grad_norm": 0.24491611123085022,
      "learning_rate": 8.960606060606061e-05,
      "loss": 0.42305835768712713,
      "step": 112900
    },
    {
      "epoch": 1.0989598782391357,
      "grad_norm": 0.25762200355529785,
      "learning_rate": 8.95959595959596e-05,
      "loss": 0.42226494588907726,
      "step": 113000
    },
    {
      "epoch": 1.099932409104834,
      "grad_norm": 0.24063535034656525,
      "learning_rate": 8.95858585858586e-05,
      "loss": 0.4216684136562784,
      "step": 113100
    },
    {
      "epoch": 1.1009049399705324,
      "grad_norm": 0.2575596570968628,
      "learning_rate": 8.957575757575757e-05,
      "loss": 0.42077661625946566,
      "step": 113200
    },
    {
      "epoch": 1.1018774708362307,
      "grad_norm": 0.25475433468818665,
      "learning_rate": 8.956565656565658e-05,
      "loss": 0.4228376205504108,
      "step": 113300
    },
    {
      "epoch": 1.102850001701929,
      "grad_norm": 0.2411043792963028,
      "learning_rate": 8.955555555555556e-05,
      "loss": 0.42224110498956957,
      "step": 113400
    },
    {
      "epoch": 1.1038225325676274,
      "grad_norm": 0.2859330475330353,
      "learning_rate": 8.954545454545455e-05,
      "loss": 0.42174302921135387,
      "step": 113500
    },
    {
      "epoch": 1.1047950634333257,
      "grad_norm": 0.2370835542678833,
      "learning_rate": 8.953535353535354e-05,
      "loss": 0.421737075933126,
      "step": 113600
    },
    {
      "epoch": 1.1057675942990242,
      "grad_norm": 0.2481769621372223,
      "learning_rate": 8.952525252525253e-05,
      "loss": 0.42271532597497957,
      "step": 113700
    },
    {
      "epoch": 1.1067401251647224,
      "grad_norm": 0.24896664917469025,
      "learning_rate": 8.951515151515151e-05,
      "loss": 0.42251252091809693,
      "step": 113800
    },
    {
      "epoch": 1.1077126560304207,
      "grad_norm": 0.25257885456085205,
      "learning_rate": 8.950505050505052e-05,
      "loss": 0.4218176336517908,
      "step": 113900
    },
    {
      "epoch": 1.1086851868961192,
      "grad_norm": 0.2647121548652649,
      "learning_rate": 8.94949494949495e-05,
      "loss": 0.42200851131227063,
      "step": 114000
    },
    {
      "epoch": 1.1096577177618174,
      "grad_norm": 0.25539344549179077,
      "learning_rate": 8.948484848484849e-05,
      "loss": 0.42141206521791985,
      "step": 114100
    },
    {
      "epoch": 1.110630248627516,
      "grad_norm": 0.2518918216228485,
      "learning_rate": 8.947474747474748e-05,
      "loss": 0.42288231688806793,
      "step": 114200
    },
    {
      "epoch": 1.1116027794932142,
      "grad_norm": 0.23777274787425995,
      "learning_rate": 8.946464646464647e-05,
      "loss": 0.42189222697758944,
      "step": 114300
    },
    {
      "epoch": 1.1125753103589124,
      "grad_norm": 0.25368672609329224,
      "learning_rate": 8.945454545454546e-05,
      "loss": 0.42227991334749987,
      "step": 114400
    },
    {
      "epoch": 1.113547841224611,
      "grad_norm": 0.22223100066184998,
      "learning_rate": 8.944444444444446e-05,
      "loss": 0.4224707701680325,
      "step": 114500
    },
    {
      "epoch": 1.1145203720903092,
      "grad_norm": 0.24882256984710693,
      "learning_rate": 8.943434343434343e-05,
      "loss": 0.4209886883545681,
      "step": 114600
    },
    {
      "epoch": 1.1154929029560077,
      "grad_norm": 0.25790223479270935,
      "learning_rate": 8.942424242424243e-05,
      "loss": 0.4216715898726193,
      "step": 114700
    },
    {
      "epoch": 1.116465433821706,
      "grad_norm": 0.24288469552993774,
      "learning_rate": 8.941414141414142e-05,
      "loss": 0.4223544719400532,
      "step": 114800
    },
    {
      "epoch": 1.1174379646874042,
      "grad_norm": 0.2548713684082031,
      "learning_rate": 8.940404040404041e-05,
      "loss": 0.4227421235767184,
      "step": 114900
    },
    {
      "epoch": 1.1184104955531027,
      "grad_norm": 0.25214436650276184,
      "learning_rate": 8.93939393939394e-05,
      "loss": 0.4214569254964944,
      "step": 115000
    },
    {
      "epoch": 1.119383026418801,
      "grad_norm": 0.2513048052787781,
      "learning_rate": 8.93838383838384e-05,
      "loss": 0.4231237899805702,
      "step": 115100
    },
    {
      "epoch": 1.1203555572844994,
      "grad_norm": 0.2666292190551758,
      "learning_rate": 8.937373737373737e-05,
      "loss": 0.42292101687827316,
      "step": 115200
    },
    {
      "epoch": 1.1213280881501977,
      "grad_norm": 0.2627492845058441,
      "learning_rate": 8.936363636363636e-05,
      "loss": 0.4208486853814023,
      "step": 115300
    },
    {
      "epoch": 1.122300619015896,
      "grad_norm": 0.2297593653202057,
      "learning_rate": 8.935353535353536e-05,
      "loss": 0.42202350377862974,
      "step": 115400
    },
    {
      "epoch": 1.1232731498815944,
      "grad_norm": 0.23783504962921143,
      "learning_rate": 8.934343434343435e-05,
      "loss": 0.4213287776745815,
      "step": 115500
    },
    {
      "epoch": 1.1242456807472927,
      "grad_norm": 0.2564486265182495,
      "learning_rate": 8.933333333333334e-05,
      "loss": 0.42142122249908476,
      "step": 115600
    },
    {
      "epoch": 1.125218211612991,
      "grad_norm": 0.24431030452251434,
      "learning_rate": 8.932323232323233e-05,
      "loss": 0.4212184869086925,
      "step": 115700
    },
    {
      "epoch": 1.1261907424786894,
      "grad_norm": 0.2839449644088745,
      "learning_rate": 8.931313131313131e-05,
      "loss": 0.42288518884118087,
      "step": 115800
    },
    {
      "epoch": 1.1271632733443877,
      "grad_norm": 0.24269507825374603,
      "learning_rate": 8.930303030303032e-05,
      "loss": 0.4222888789011513,
      "step": 115900
    },
    {
      "epoch": 1.1281358042100862,
      "grad_norm": 0.22350910305976868,
      "learning_rate": 8.92929292929293e-05,
      "loss": 0.42100386699112147,
      "step": 116000
    },
    {
      "epoch": 1.1291083350757845,
      "grad_norm": 0.2574050724506378,
      "learning_rate": 8.928282828282829e-05,
      "loss": 0.4223753398821709,
      "step": 116100
    },
    {
      "epoch": 1.130080865941483,
      "grad_norm": 0.27595916390419006,
      "learning_rate": 8.927272727272728e-05,
      "loss": 0.4198113499214682,
      "step": 116200
    },
    {
      "epoch": 1.1310533968071812,
      "grad_norm": 0.25613006949424744,
      "learning_rate": 8.926262626262627e-05,
      "loss": 0.420690879536075,
      "step": 116300
    },
    {
      "epoch": 1.1320259276728795,
      "grad_norm": 0.26484256982803345,
      "learning_rate": 8.925252525252525e-05,
      "loss": 0.421176852862332,
      "step": 116400
    },
    {
      "epoch": 1.132998458538578,
      "grad_norm": 0.23975305259227753,
      "learning_rate": 8.924242424242426e-05,
      "loss": 0.42067899798755576,
      "step": 116500
    },
    {
      "epoch": 1.1339709894042762,
      "grad_norm": 0.24964556097984314,
      "learning_rate": 8.923232323232323e-05,
      "loss": 0.4214600975448458,
      "step": 116600
    },
    {
      "epoch": 1.1349435202699745,
      "grad_norm": 0.23813806474208832,
      "learning_rate": 8.922222222222223e-05,
      "loss": 0.42204441756863115,
      "step": 116700
    },
    {
      "epoch": 1.135916051135673,
      "grad_norm": 0.26195409893989563,
      "learning_rate": 8.921212121212122e-05,
      "loss": 0.4215465710298337,
      "step": 116800
    },
    {
      "epoch": 1.1368885820013712,
      "grad_norm": 0.24093937873840332,
      "learning_rate": 8.920202020202021e-05,
      "loss": 0.4216389936250602,
      "step": 116900
    },
    {
      "epoch": 1.1378611128670697,
      "grad_norm": 0.27393072843551636,
      "learning_rate": 8.919191919191919e-05,
      "loss": 0.42064929411625773,
      "step": 117000
    },
    {
      "epoch": 1.138833643732768,
      "grad_norm": 0.2397228330373764,
      "learning_rate": 8.91818181818182e-05,
      "loss": 0.42270918833268617,
      "step": 117100
    },
    {
      "epoch": 1.1398061745984662,
      "grad_norm": 0.2368953973054886,
      "learning_rate": 8.917171717171717e-05,
      "loss": 0.42201461644424654,
      "step": 117200
    },
    {
      "epoch": 1.1407787054641647,
      "grad_norm": 0.23732709884643555,
      "learning_rate": 8.916161616161616e-05,
      "loss": 0.4218119155870995,
      "step": 117300
    },
    {
      "epoch": 1.141751236329863,
      "grad_norm": 0.23262295126914978,
      "learning_rate": 8.915151515151516e-05,
      "loss": 0.42170758921407697,
      "step": 117400
    },
    {
      "epoch": 1.1427237671955615,
      "grad_norm": 0.23767942190170288,
      "learning_rate": 8.914141414141415e-05,
      "loss": 0.4205212227074842,
      "step": 117500
    },
    {
      "epoch": 1.1436962980612597,
      "grad_norm": 0.24153460562229156,
      "learning_rate": 8.913131313131313e-05,
      "loss": 0.4213022125077195,
      "step": 117600
    },
    {
      "epoch": 1.144668828926958,
      "grad_norm": 0.23551751673221588,
      "learning_rate": 8.912121212121213e-05,
      "loss": 0.42129626200815123,
      "step": 117700
    },
    {
      "epoch": 1.1456413597926565,
      "grad_norm": 0.26129528880119324,
      "learning_rate": 8.911111111111111e-05,
      "loss": 0.42158540161704094,
      "step": 117800
    },
    {
      "epoch": 1.1466138906583547,
      "grad_norm": 0.23761385679244995,
      "learning_rate": 8.91010101010101e-05,
      "loss": 0.42138272298917084,
      "step": 117900
    },
    {
      "epoch": 1.1475864215240532,
      "grad_norm": 0.24891705811023712,
      "learning_rate": 8.90909090909091e-05,
      "loss": 0.4199997228287033,
      "step": 118000
    },
    {
      "epoch": 1.1485589523897515,
      "grad_norm": 0.2478291094303131,
      "learning_rate": 8.908080808080809e-05,
      "loss": 0.4216658968158645,
      "step": 118100
    },
    {
      "epoch": 1.1495314832554497,
      "grad_norm": 0.28099164366722107,
      "learning_rate": 8.907070707070706e-05,
      "loss": 0.4215615829468102,
      "step": 118200
    },
    {
      "epoch": 1.1505040141211482,
      "grad_norm": 0.24944311380386353,
      "learning_rate": 8.906060606060607e-05,
      "loss": 0.4203753512052089,
      "step": 118300
    },
    {
      "epoch": 1.1514765449868465,
      "grad_norm": 0.24405637383460999,
      "learning_rate": 8.905050505050505e-05,
      "loss": 0.4219430937457235,
      "step": 118400
    },
    {
      "epoch": 1.152449075852545,
      "grad_norm": 0.2714049816131592,
      "learning_rate": 8.904040404040404e-05,
      "loss": 0.4201667679256542,
      "step": 118500
    },
    {
      "epoch": 1.1534216067182432,
      "grad_norm": 0.24155880510807037,
      "learning_rate": 8.903030303030303e-05,
      "loss": 0.4210460030023417,
      "step": 118600
    },
    {
      "epoch": 1.1543941375839415,
      "grad_norm": 0.24982434511184692,
      "learning_rate": 8.902020202020202e-05,
      "loss": 0.42163016047453955,
      "step": 118700
    },
    {
      "epoch": 1.15536666844964,
      "grad_norm": 0.2727270722389221,
      "learning_rate": 8.901010101010102e-05,
      "loss": 0.4210341075605244,
      "step": 118800
    },
    {
      "epoch": 1.1563391993153382,
      "grad_norm": 0.22905366122722626,
      "learning_rate": 8.900000000000001e-05,
      "loss": 0.4191595461893115,
      "step": 118900
    },
    {
      "epoch": 1.1573117301810365,
      "grad_norm": 0.2508644461631775,
      "learning_rate": 8.898989898989899e-05,
      "loss": 0.42072717202612214,
      "step": 119000
    },
    {
      "epoch": 1.158284261046735,
      "grad_norm": 0.23745141923427582,
      "learning_rate": 8.897979797979798e-05,
      "loss": 0.42101626439779843,
      "step": 119100
    },
    {
      "epoch": 1.1592567919124332,
      "grad_norm": 0.24728132784366608,
      "learning_rate": 8.896969696969697e-05,
      "loss": 0.4205185970825463,
      "step": 119200
    },
    {
      "epoch": 1.1602293227781317,
      "grad_norm": 0.22904562950134277,
      "learning_rate": 8.895959595959596e-05,
      "loss": 0.4213977390741367,
      "step": 119300
    },
    {
      "epoch": 1.16120185364383,
      "grad_norm": 0.23952363431453705,
      "learning_rate": 8.894949494949495e-05,
      "loss": 0.4208017389546544,
      "step": 119400
    },
    {
      "epoch": 1.1621743845095285,
      "grad_norm": 0.258884996175766,
      "learning_rate": 8.893939393939395e-05,
      "loss": 0.42000907600537113,
      "step": 119500
    },
    {
      "epoch": 1.1631469153752267,
      "grad_norm": 0.2300906628370285,
      "learning_rate": 8.892929292929293e-05,
      "loss": 0.4213798792394531,
      "step": 119600
    },
    {
      "epoch": 1.164119446240925,
      "grad_norm": 0.2657243013381958,
      "learning_rate": 8.891919191919193e-05,
      "loss": 0.4206855671556876,
      "step": 119700
    },
    {
      "epoch": 1.1650919771066235,
      "grad_norm": 0.22421351075172424,
      "learning_rate": 8.890909090909091e-05,
      "loss": 0.4199912745225394,
      "step": 119800
    },
    {
      "epoch": 1.1660645079723218,
      "grad_norm": 0.24170243740081787,
      "learning_rate": 8.88989898989899e-05,
      "loss": 0.42047701166272905,
      "step": 119900
    },
    {
      "epoch": 1.16703703883802,
      "grad_norm": 0.2521513104438782,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.4207660693011602,
      "step": 120000
    },
    {
      "epoch": 1.16703703883802,
      "eval_accuracy": 0.6660121407296685,
      "eval_loss": 0.4151216487320199,
      "eval_runtime": 3738.8108,
      "eval_samples_per_second": 611.09,
      "eval_steps_per_second": 6.111,
      "step": 120000
    },
    {
      "epoch": 1.1680095697037185,
      "grad_norm": 0.26748141646385193,
      "learning_rate": 8.887878787878789e-05,
      "loss": 0.42046513011420983,
      "step": 120100
    },
    {
      "epoch": 1.1689821005694168,
      "grad_norm": 0.2501380145549774,
      "learning_rate": 8.886868686868686e-05,
      "loss": 0.4192792290331029,
      "step": 120200
    },
    {
      "epoch": 1.1699546314351152,
      "grad_norm": 0.23318342864513397,
      "learning_rate": 8.885858585858587e-05,
      "loss": 0.4208465631106538,
      "step": 120300
    },
    {
      "epoch": 1.1709271623008135,
      "grad_norm": 0.24985840916633606,
      "learning_rate": 8.884848484848485e-05,
      "loss": 0.420250653297609,
      "step": 120400
    },
    {
      "epoch": 1.171899693166512,
      "grad_norm": 0.26390835642814636,
      "learning_rate": 8.883838383838384e-05,
      "loss": 0.42152295145056456,
      "step": 120500
    },
    {
      "epoch": 1.1728722240322103,
      "grad_norm": 0.2718590199947357,
      "learning_rate": 8.882828282828283e-05,
      "loss": 0.41994380390165487,
      "step": 120600
    },
    {
      "epoch": 1.1738447548979085,
      "grad_norm": 0.23962587118148804,
      "learning_rate": 8.881818181818182e-05,
      "loss": 0.4203311623897306,
      "step": 120700
    },
    {
      "epoch": 1.174817285763607,
      "grad_norm": 0.2399970442056656,
      "learning_rate": 8.88080808080808e-05,
      "loss": 0.4210134748319432,
      "step": 120800
    },
    {
      "epoch": 1.1757898166293053,
      "grad_norm": 0.2543540894985199,
      "learning_rate": 8.879797979797981e-05,
      "loss": 0.4195327212177754,
      "step": 120900
    },
    {
      "epoch": 1.1767623474950035,
      "grad_norm": 0.27266985177993774,
      "learning_rate": 8.87878787878788e-05,
      "loss": 0.42050998205680556,
      "step": 121000
    },
    {
      "epoch": 1.177734878360702,
      "grad_norm": 0.2372269481420517,
      "learning_rate": 8.877777777777778e-05,
      "loss": 0.42070067493642077,
      "step": 121100
    },
    {
      "epoch": 1.1787074092264003,
      "grad_norm": 0.2473779320716858,
      "learning_rate": 8.876767676767678e-05,
      "loss": 0.42069472999417173,
      "step": 121200
    },
    {
      "epoch": 1.1796799400920988,
      "grad_norm": 0.24254989624023438,
      "learning_rate": 8.875757575757576e-05,
      "loss": 0.4212786735095791,
      "step": 121300
    },
    {
      "epoch": 1.180652470957797,
      "grad_norm": 0.26577645540237427,
      "learning_rate": 8.874747474747475e-05,
      "loss": 0.41920813980547994,
      "step": 121400
    },
    {
      "epoch": 1.1816250018234953,
      "grad_norm": 0.24179252982139587,
      "learning_rate": 8.873737373737375e-05,
      "loss": 0.420087023381726,
      "step": 121500
    },
    {
      "epoch": 1.1825975326891938,
      "grad_norm": 0.2661088705062866,
      "learning_rate": 8.872727272727274e-05,
      "loss": 0.4198844656255492,
      "step": 121600
    },
    {
      "epoch": 1.183570063554892,
      "grad_norm": 0.2318393588066101,
      "learning_rate": 8.871717171717172e-05,
      "loss": 0.41987853179793866,
      "step": 121700
    },
    {
      "epoch": 1.1845425944205905,
      "grad_norm": 0.27645978331565857,
      "learning_rate": 8.870707070707072e-05,
      "loss": 0.41997090576662177,
      "step": 121800
    },
    {
      "epoch": 1.1855151252862888,
      "grad_norm": 0.24899405241012573,
      "learning_rate": 8.86969696969697e-05,
      "loss": 0.41957174492182586,
      "step": 121900
    },
    {
      "epoch": 1.186487656151987,
      "grad_norm": 0.2822572886943817,
      "learning_rate": 8.868686868686869e-05,
      "loss": 0.41966412027983885,
      "step": 122000
    },
    {
      "epoch": 1.1874601870176855,
      "grad_norm": 0.24141967296600342,
      "learning_rate": 8.867676767676768e-05,
      "loss": 0.4205429218856262,
      "step": 122100
    },
    {
      "epoch": 1.1884327178833838,
      "grad_norm": 0.24012762308120728,
      "learning_rate": 8.866666666666668e-05,
      "loss": 0.42043867609373253,
      "step": 122200
    },
    {
      "epoch": 1.1894052487490823,
      "grad_norm": 0.23265109956264496,
      "learning_rate": 8.865656565656565e-05,
      "loss": 0.4203344330804984,
      "step": 122300
    },
    {
      "epoch": 1.1903777796147805,
      "grad_norm": 0.2614690661430359,
      "learning_rate": 8.864646464646466e-05,
      "loss": 0.42111488798875757,
      "step": 122400
    },
    {
      "epoch": 1.1913503104804788,
      "grad_norm": 0.24946682155132294,
      "learning_rate": 8.863636363636364e-05,
      "loss": 0.4203225515319792,
      "step": 122500
    },
    {
      "epoch": 1.1923228413461773,
      "grad_norm": 0.2514941096305847,
      "learning_rate": 8.862626262626263e-05,
      "loss": 0.4196285339861331,
      "step": 122600
    },
    {
      "epoch": 1.1932953722118755,
      "grad_norm": 0.24415242671966553,
      "learning_rate": 8.861616161616162e-05,
      "loss": 0.41922942176788053,
      "step": 122700
    },
    {
      "epoch": 1.194267903077574,
      "grad_norm": 0.25279635190963745,
      "learning_rate": 8.860606060606061e-05,
      "loss": 0.4195183779852356,
      "step": 122800
    },
    {
      "epoch": 1.1952404339432723,
      "grad_norm": 0.23434151709079742,
      "learning_rate": 8.859595959595959e-05,
      "loss": 0.41961074083928024,
      "step": 122900
    },
    {
      "epoch": 1.1962129648089705,
      "grad_norm": 0.23536288738250732,
      "learning_rate": 8.85858585858586e-05,
      "loss": 0.4192116452929853,
      "step": 123000
    },
    {
      "epoch": 1.197185495674669,
      "grad_norm": 0.23085138201713562,
      "learning_rate": 8.857575757575758e-05,
      "loss": 0.4197954582113907,
      "step": 123100
    },
    {
      "epoch": 1.1981580265403673,
      "grad_norm": 0.23226596415042877,
      "learning_rate": 8.856565656565657e-05,
      "loss": 0.4201826777664856,
      "step": 123200
    },
    {
      "epoch": 1.1991305574060656,
      "grad_norm": 0.2507060170173645,
      "learning_rate": 8.855555555555556e-05,
      "loss": 0.41998016446886266,
      "step": 123300
    },
    {
      "epoch": 1.200103088271764,
      "grad_norm": 0.24582630395889282,
      "learning_rate": 8.854545454545455e-05,
      "loss": 0.42066222683170973,
      "step": 123400
    },
    {
      "epoch": 1.2010756191374623,
      "grad_norm": 0.2663002610206604,
      "learning_rate": 8.853535353535354e-05,
      "loss": 0.4190837336571398,
      "step": 123500
    },
    {
      "epoch": 1.2020481500031608,
      "grad_norm": 0.24393965303897858,
      "learning_rate": 8.852525252525254e-05,
      "loss": 0.41976578907333784,
      "step": 123600
    },
    {
      "epoch": 1.203020680868859,
      "grad_norm": 0.23260320723056793,
      "learning_rate": 8.851515151515152e-05,
      "loss": 0.4203495436398913,
      "step": 123700
    },
    {
      "epoch": 1.2039932117345575,
      "grad_norm": 0.25687533617019653,
      "learning_rate": 8.850505050505051e-05,
      "loss": 0.4201470414569068,
      "step": 123800
    },
    {
      "epoch": 1.2049657426002558,
      "grad_norm": 0.23874777555465698,
      "learning_rate": 8.84949494949495e-05,
      "loss": 0.41906003724793217,
      "step": 123900
    },
    {
      "epoch": 1.205938273465954,
      "grad_norm": 0.26757314801216125,
      "learning_rate": 8.848484848484849e-05,
      "loss": 0.4201351626870472,
      "step": 124000
    },
    {
      "epoch": 1.2069108043316525,
      "grad_norm": 0.2550731301307678,
      "learning_rate": 8.847474747474748e-05,
      "loss": 0.42022749914382546,
      "step": 124100
    },
    {
      "epoch": 1.2078833351973508,
      "grad_norm": 0.22751781344413757,
      "learning_rate": 8.846464646464648e-05,
      "loss": 0.41953363720291786,
      "step": 124200
    },
    {
      "epoch": 1.208855866063049,
      "grad_norm": 0.2437601387500763,
      "learning_rate": 8.845454545454545e-05,
      "loss": 0.4185449755234821,
      "step": 124300
    },
    {
      "epoch": 1.2098283969287476,
      "grad_norm": 0.2564448118209839,
      "learning_rate": 8.844444444444445e-05,
      "loss": 0.41883387338898526,
      "step": 124400
    },
    {
      "epoch": 1.2108009277944458,
      "grad_norm": 0.2790752053260803,
      "learning_rate": 8.843434343434344e-05,
      "loss": 0.4193193034872873,
      "step": 124500
    },
    {
      "epoch": 1.2117734586601443,
      "grad_norm": 0.2437463104724884,
      "learning_rate": 8.842424242424243e-05,
      "loss": 0.42000125748240935,
      "step": 124600
    },
    {
      "epoch": 1.2127459895258426,
      "grad_norm": 0.255805104970932,
      "learning_rate": 8.841414141414142e-05,
      "loss": 0.41930744694670463,
      "step": 124700
    },
    {
      "epoch": 1.213718520391541,
      "grad_norm": 0.24843184649944305,
      "learning_rate": 8.840404040404041e-05,
      "loss": 0.4189084542108155,
      "step": 124800
    },
    {
      "epoch": 1.2146910512572393,
      "grad_norm": 0.23231390118598938,
      "learning_rate": 8.839393939393939e-05,
      "loss": 0.4191973256790523,
      "step": 124900
    },
    {
      "epoch": 1.2156635821229376,
      "grad_norm": 0.23879553377628326,
      "learning_rate": 8.83838383838384e-05,
      "loss": 0.4198792421622696,
      "step": 125000
    },
    {
      "epoch": 1.216636112988636,
      "grad_norm": 0.2698473334312439,
      "learning_rate": 8.837373737373738e-05,
      "loss": 0.4197750436075894,
      "step": 125100
    },
    {
      "epoch": 1.2176086438543343,
      "grad_norm": 0.24225680530071259,
      "learning_rate": 8.836363636363637e-05,
      "loss": 0.4192778055952479,
      "step": 125200
    },
    {
      "epoch": 1.2185811747200326,
      "grad_norm": 0.2507578730583191,
      "learning_rate": 8.835353535353536e-05,
      "loss": 0.4209422832107134,
      "step": 125300
    },
    {
      "epoch": 1.219553705585731,
      "grad_norm": 0.23231031000614166,
      "learning_rate": 8.834343434343435e-05,
      "loss": 0.4192659490546653,
      "step": 125400
    },
    {
      "epoch": 1.2205262364514293,
      "grad_norm": 0.25472015142440796,
      "learning_rate": 8.833333333333333e-05,
      "loss": 0.4192600207843739,
      "step": 125500
    },
    {
      "epoch": 1.2214987673171278,
      "grad_norm": 0.25546199083328247,
      "learning_rate": 8.832323232323234e-05,
      "loss": 0.41895932750879966,
      "step": 125600
    },
    {
      "epoch": 1.222471298182826,
      "grad_norm": 0.24598655104637146,
      "learning_rate": 8.831313131313131e-05,
      "loss": 0.4189534034064978,
      "step": 125700
    },
    {
      "epoch": 1.2234438290485243,
      "grad_norm": 0.23510923981666565,
      "learning_rate": 8.83030303030303e-05,
      "loss": 0.4204212626507162,
      "step": 125800
    },
    {
      "epoch": 1.2244163599142228,
      "grad_norm": 0.26853856444358826,
      "learning_rate": 8.82929292929293e-05,
      "loss": 0.41972756187206633,
      "step": 125900
    },
    {
      "epoch": 1.225388890779921,
      "grad_norm": 0.2627321183681488,
      "learning_rate": 8.828282828282829e-05,
      "loss": 0.4188373816551503,
      "step": 126000
    },
    {
      "epoch": 1.2263614216456196,
      "grad_norm": 0.26075077056884766,
      "learning_rate": 8.827272727272727e-05,
      "loss": 0.42001043560352136,
      "step": 126100
    },
    {
      "epoch": 1.2273339525113178,
      "grad_norm": 0.23625972867012024,
      "learning_rate": 8.826262626262627e-05,
      "loss": 0.4188255362292061,
      "step": 126200
    },
    {
      "epoch": 1.228306483377016,
      "grad_norm": 0.22705650329589844,
      "learning_rate": 8.825252525252525e-05,
      "loss": 0.41852487768687696,
      "step": 126300
    },
    {
      "epoch": 1.2292790142427146,
      "grad_norm": 0.2690603733062744,
      "learning_rate": 8.824242424242424e-05,
      "loss": 0.4189119346903844,
      "step": 126400
    },
    {
      "epoch": 1.2302515451084128,
      "grad_norm": 0.2371109127998352,
      "learning_rate": 8.823232323232324e-05,
      "loss": 0.41939722307704597,
      "step": 126500
    },
    {
      "epoch": 1.2312240759741113,
      "grad_norm": 0.2751857042312622,
      "learning_rate": 8.822222222222223e-05,
      "loss": 0.4196860153534837,
      "step": 126600
    },
    {
      "epoch": 1.2321966068398096,
      "grad_norm": 0.25377029180526733,
      "learning_rate": 8.82121212121212e-05,
      "loss": 0.4185994432260794,
      "step": 126700
    },
    {
      "epoch": 1.2331691377055078,
      "grad_norm": 0.25691697001457214,
      "learning_rate": 8.820202020202021e-05,
      "loss": 0.4188882382811767,
      "step": 126800
    },
    {
      "epoch": 1.2341416685712063,
      "grad_norm": 0.23806646466255188,
      "learning_rate": 8.819191919191919e-05,
      "loss": 0.41888231417887484,
      "step": 126900
    },
    {
      "epoch": 1.2351141994369046,
      "grad_norm": 0.24772335588932037,
      "learning_rate": 8.818181818181818e-05,
      "loss": 0.41897462562771653,
      "step": 127000
    },
    {
      "epoch": 1.236086730302603,
      "grad_norm": 0.23606909811496735,
      "learning_rate": 8.817171717171717e-05,
      "loss": 0.41936163678334026,
      "step": 127100
    },
    {
      "epoch": 1.2370592611683013,
      "grad_norm": 0.22477468848228455,
      "learning_rate": 8.816161616161617e-05,
      "loss": 0.4182751452370648,
      "step": 127200
    },
    {
      "epoch": 1.2380317920339996,
      "grad_norm": 0.255505234003067,
      "learning_rate": 8.815151515151515e-05,
      "loss": 0.4199391629843638,
      "step": 127300
    },
    {
      "epoch": 1.239004322899698,
      "grad_norm": 0.24521680176258087,
      "learning_rate": 8.814141414141415e-05,
      "loss": 0.4187544636735408,
      "step": 127400
    },
    {
      "epoch": 1.2399768537653963,
      "grad_norm": 0.2350122332572937,
      "learning_rate": 8.813131313131313e-05,
      "loss": 0.4180609407291064,
      "step": 127500
    },
    {
      "epoch": 1.2409493846310946,
      "grad_norm": 0.258335679769516,
      "learning_rate": 8.812121212121212e-05,
      "loss": 0.41864439103243184,
      "step": 127600
    },
    {
      "epoch": 1.241921915496793,
      "grad_norm": 0.23993149399757385,
      "learning_rate": 8.811111111111111e-05,
      "loss": 0.4181473405796145,
      "step": 127700
    },
    {
      "epoch": 1.2428944463624914,
      "grad_norm": 0.24669121205806732,
      "learning_rate": 8.81010101010101e-05,
      "loss": 0.41863254838514724,
      "step": 127800
    },
    {
      "epoch": 1.2438669772281898,
      "grad_norm": 0.26970168948173523,
      "learning_rate": 8.80909090909091e-05,
      "loss": 0.4189212962030311,
      "step": 127900
    },
    {
      "epoch": 1.244839508093888,
      "grad_norm": 0.2492653876543045,
      "learning_rate": 8.808080808080809e-05,
      "loss": 0.41812959744863476,
      "step": 128000
    },
    {
      "epoch": 1.2458120389595866,
      "grad_norm": 0.26220840215682983,
      "learning_rate": 8.807070707070707e-05,
      "loss": 0.41930232629383074,
      "step": 128100
    },
    {
      "epoch": 1.2467845698252848,
      "grad_norm": 0.22498668730258942,
      "learning_rate": 8.806060606060606e-05,
      "loss": 0.41851064421139206,
      "step": 128200
    },
    {
      "epoch": 1.2477571006909831,
      "grad_norm": 0.24368789792060852,
      "learning_rate": 8.805050505050505e-05,
      "loss": 0.4188975942365042,
      "step": 128300
    },
    {
      "epoch": 1.2487296315566816,
      "grad_norm": 0.23956041038036346,
      "learning_rate": 8.804040404040404e-05,
      "loss": 0.4169273467343455,
      "step": 128400
    },
    {
      "epoch": 1.2497021624223799,
      "grad_norm": 0.23709595203399658,
      "learning_rate": 8.803030303030304e-05,
      "loss": 0.41898395796443727,
      "step": 128500
    },
    {
      "epoch": 1.2506746932880781,
      "grad_norm": 0.2728523910045624,
      "learning_rate": 8.802020202020203e-05,
      "loss": 0.4183887511522754,
      "step": 128600
    },
    {
      "epoch": 1.2516472241537766,
      "grad_norm": 0.2708335518836975,
      "learning_rate": 8.8010101010101e-05,
      "loss": 0.4202488593254943,
      "step": 128700
    },
    {
      "epoch": 1.2526197550194749,
      "grad_norm": 0.2344921976327896,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.41798407188948156,
      "step": 128800
    },
    {
      "epoch": 1.2535922858851734,
      "grad_norm": 0.23163998126983643,
      "learning_rate": 8.798989898989899e-05,
      "loss": 0.41876383213283663,
      "step": 128900
    },
    {
      "epoch": 1.2545648167508716,
      "grad_norm": 0.24579210579395294,
      "learning_rate": 8.797979797979798e-05,
      "loss": 0.4185614925014397,
      "step": 129000
    },
    {
      "epoch": 1.25553734761657,
      "grad_norm": 0.2593584656715393,
      "learning_rate": 8.796969696969697e-05,
      "loss": 0.41875198392823276,
      "step": 129100
    },
    {
      "epoch": 1.2565098784822684,
      "grad_norm": 0.24720129370689392,
      "learning_rate": 8.795959595959597e-05,
      "loss": 0.4188442648118187,
      "step": 129200
    },
    {
      "epoch": 1.2574824093479666,
      "grad_norm": 0.26930853724479675,
      "learning_rate": 8.794949494949494e-05,
      "loss": 0.41834732133739666,
      "step": 129300
    },
    {
      "epoch": 1.258454940213665,
      "grad_norm": 0.2453739047050476,
      "learning_rate": 8.793939393939395e-05,
      "loss": 0.41794859396350087,
      "step": 129400
    },
    {
      "epoch": 1.2594274710793634,
      "grad_norm": 0.2404949814081192,
      "learning_rate": 8.792929292929294e-05,
      "loss": 0.41784448015793896,
      "step": 129500
    },
    {
      "epoch": 1.2604000019450616,
      "grad_norm": 0.23587262630462646,
      "learning_rate": 8.791919191919192e-05,
      "loss": 0.41872236341672325,
      "step": 129600
    },
    {
      "epoch": 1.2613725328107601,
      "grad_norm": 0.2384701669216156,
      "learning_rate": 8.790909090909091e-05,
      "loss": 0.4191092314713767,
      "step": 129700
    },
    {
      "epoch": 1.2623450636764584,
      "grad_norm": 0.2489946335554123,
      "learning_rate": 8.78989898989899e-05,
      "loss": 0.41831772861248323,
      "step": 129800
    },
    {
      "epoch": 1.2633175945421569,
      "grad_norm": 0.2475949078798294,
      "learning_rate": 8.78888888888889e-05,
      "loss": 0.41860639584923826,
      "step": 129900
    },
    {
      "epoch": 1.2642901254078551,
      "grad_norm": 0.2515203058719635,
      "learning_rate": 8.787878787878789e-05,
      "loss": 0.4197787995912592,
      "step": 130000
    },
    {
      "epoch": 1.2642901254078551,
      "eval_accuracy": 0.6662925520327742,
      "eval_loss": 0.41255699642714555,
      "eval_runtime": 3728.0061,
      "eval_samples_per_second": 612.861,
      "eval_steps_per_second": 6.129,
      "step": 130000
    },
    {
      "epoch": 1.2652626562735536,
      "grad_norm": 0.23794887959957123,
      "learning_rate": 8.786868686868688e-05,
      "loss": 0.4185945504232941,
      "step": 130100
    },
    {
      "epoch": 1.2662351871392519,
      "grad_norm": 0.24089452624320984,
      "learning_rate": 8.785858585858586e-05,
      "loss": 0.4174103345992443,
      "step": 130200
    },
    {
      "epoch": 1.2672077180049501,
      "grad_norm": 0.23934617638587952,
      "learning_rate": 8.784848484848486e-05,
      "loss": 0.41858270499734984,
      "step": 130300
    },
    {
      "epoch": 1.2681802488706486,
      "grad_norm": 0.23701928555965424,
      "learning_rate": 8.783838383838384e-05,
      "loss": 0.4176930874590061,
      "step": 130400
    },
    {
      "epoch": 1.2691527797363469,
      "grad_norm": 0.24409641325473785,
      "learning_rate": 8.782828282828283e-05,
      "loss": 0.4169016818531992,
      "step": 130500
    },
    {
      "epoch": 1.2701253106020451,
      "grad_norm": 0.24944692850112915,
      "learning_rate": 8.781818181818183e-05,
      "loss": 0.41797582364681013,
      "step": 130600
    },
    {
      "epoch": 1.2710978414677436,
      "grad_norm": 0.22757229208946228,
      "learning_rate": 8.780808080808082e-05,
      "loss": 0.4184608299995207,
      "step": 130700
    },
    {
      "epoch": 1.272070372333442,
      "grad_norm": 0.23687420785427094,
      "learning_rate": 8.77979797979798e-05,
      "loss": 0.4177676293796017,
      "step": 130800
    },
    {
      "epoch": 1.2730429031991402,
      "grad_norm": 0.23544512689113617,
      "learning_rate": 8.77878787878788e-05,
      "loss": 0.41736899231214347,
      "step": 130900
    },
    {
      "epoch": 1.2740154340648386,
      "grad_norm": 0.24581606686115265,
      "learning_rate": 8.777777777777778e-05,
      "loss": 0.4181485260947398,
      "step": 131000
    },
    {
      "epoch": 1.2749879649305371,
      "grad_norm": 0.24710188806056976,
      "learning_rate": 8.776767676767677e-05,
      "loss": 0.4190262162366807,
      "step": 131100
    },
    {
      "epoch": 1.2759604957962354,
      "grad_norm": 0.24079763889312744,
      "learning_rate": 8.775757575757576e-05,
      "loss": 0.41833304618201744,
      "step": 131200
    },
    {
      "epoch": 1.2769330266619336,
      "grad_norm": 0.24113424122333527,
      "learning_rate": 8.774747474747476e-05,
      "loss": 0.41852347786762883,
      "step": 131300
    },
    {
      "epoch": 1.2779055575276321,
      "grad_norm": 0.26778656244277954,
      "learning_rate": 8.773737373737373e-05,
      "loss": 0.4176339853689678,
      "step": 131400
    },
    {
      "epoch": 1.2788780883933304,
      "grad_norm": 0.25116583704948425,
      "learning_rate": 8.772727272727274e-05,
      "loss": 0.41851163244168466,
      "step": 131500
    },
    {
      "epoch": 1.2798506192590287,
      "grad_norm": 0.23847591876983643,
      "learning_rate": 8.771717171717172e-05,
      "loss": 0.41693496345715275,
      "step": 131600
    },
    {
      "epoch": 1.2808231501247271,
      "grad_norm": 0.2351926565170288,
      "learning_rate": 8.770707070707071e-05,
      "loss": 0.4181071060051697,
      "step": 131700
    },
    {
      "epoch": 1.2817956809904254,
      "grad_norm": 0.2784827649593353,
      "learning_rate": 8.76969696969697e-05,
      "loss": 0.41780668225957823,
      "step": 131800
    },
    {
      "epoch": 1.2827682118561237,
      "grad_norm": 0.23475618660449982,
      "learning_rate": 8.76868686868687e-05,
      "loss": 0.4185861090637793,
      "step": 131900
    },
    {
      "epoch": 1.2837407427218221,
      "grad_norm": 0.245948925614357,
      "learning_rate": 8.767676767676767e-05,
      "loss": 0.4186783510461306,
      "step": 132000
    },
    {
      "epoch": 1.2847132735875204,
      "grad_norm": 0.2371583878993988,
      "learning_rate": 8.766666666666668e-05,
      "loss": 0.41759261390594093,
      "step": 132100
    },
    {
      "epoch": 1.285685804453219,
      "grad_norm": 0.2647175192832947,
      "learning_rate": 8.765656565656566e-05,
      "loss": 0.4178811936149181,
      "step": 132200
    },
    {
      "epoch": 1.2866583353189172,
      "grad_norm": 0.24367721378803253,
      "learning_rate": 8.764646464646465e-05,
      "loss": 0.4185624126545716,
      "step": 132300
    },
    {
      "epoch": 1.2876308661846156,
      "grad_norm": 0.26711317896842957,
      "learning_rate": 8.763636363636364e-05,
      "loss": 0.41767304380626347,
      "step": 132400
    },
    {
      "epoch": 1.288603397050314,
      "grad_norm": 0.2597106993198395,
      "learning_rate": 8.762626262626263e-05,
      "loss": 0.4178634504839383,
      "step": 132500
    },
    {
      "epoch": 1.2895759279160122,
      "grad_norm": 0.24192535877227783,
      "learning_rate": 8.761616161616161e-05,
      "loss": 0.4175630628609218,
      "step": 132600
    },
    {
      "epoch": 1.2905484587817107,
      "grad_norm": 0.25797632336616516,
      "learning_rate": 8.760606060606062e-05,
      "loss": 0.4183424035266748,
      "step": 132700
    },
    {
      "epoch": 1.291520989647409,
      "grad_norm": 0.26463496685028076,
      "learning_rate": 8.75959595959596e-05,
      "loss": 0.4164715377727515,
      "step": 132800
    },
    {
      "epoch": 1.2924935205131072,
      "grad_norm": 0.24365980923175812,
      "learning_rate": 8.758585858585859e-05,
      "loss": 0.4181342537180202,
      "step": 132900
    },
    {
      "epoch": 1.2934660513788057,
      "grad_norm": 0.2613142728805542,
      "learning_rate": 8.757575757575758e-05,
      "loss": 0.41744126983355134,
      "step": 133000
    },
    {
      "epoch": 1.294438582244504,
      "grad_norm": 0.24577181041240692,
      "learning_rate": 8.756565656565657e-05,
      "loss": 0.41782796422197876,
      "step": 133100
    },
    {
      "epoch": 1.2954111131102024,
      "grad_norm": 0.2385687679052353,
      "learning_rate": 8.755555555555556e-05,
      "loss": 0.4167424063053344,
      "step": 133200
    },
    {
      "epoch": 1.2963836439759007,
      "grad_norm": 0.24979306757450104,
      "learning_rate": 8.754545454545456e-05,
      "loss": 0.4187976157016498,
      "step": 133300
    },
    {
      "epoch": 1.2973561748415992,
      "grad_norm": 0.24616321921348572,
      "learning_rate": 8.753535353535353e-05,
      "loss": 0.4169269013846756,
      "step": 133400
    },
    {
      "epoch": 1.2983287057072974,
      "grad_norm": 0.24396687746047974,
      "learning_rate": 8.752525252525253e-05,
      "loss": 0.4170191447563567,
      "step": 133500
    },
    {
      "epoch": 1.2993012365729957,
      "grad_norm": 0.25547781586647034,
      "learning_rate": 8.751515151515152e-05,
      "loss": 0.4181909677585178,
      "step": 133600
    },
    {
      "epoch": 1.3002737674386942,
      "grad_norm": 0.28016000986099243,
      "learning_rate": 8.750505050505051e-05,
      "loss": 0.41857761768839136,
      "step": 133700
    },
    {
      "epoch": 1.3012462983043924,
      "grad_norm": 0.24590668082237244,
      "learning_rate": 8.74949494949495e-05,
      "loss": 0.417786563583026,
      "step": 133800
    },
    {
      "epoch": 1.3022188291700907,
      "grad_norm": 0.2981511056423187,
      "learning_rate": 8.74848484848485e-05,
      "loss": 0.41699553170693754,
      "step": 133900
    },
    {
      "epoch": 1.3031913600357892,
      "grad_norm": 0.2886793613433838,
      "learning_rate": 8.747474747474747e-05,
      "loss": 0.4174803199348682,
      "step": 134000
    },
    {
      "epoch": 1.3041638909014874,
      "grad_norm": 0.2799435257911682,
      "learning_rate": 8.746464646464648e-05,
      "loss": 0.4179650942695008,
      "step": 134100
    },
    {
      "epoch": 1.305136421767186,
      "grad_norm": 0.24298341572284698,
      "learning_rate": 8.745454545454546e-05,
      "loss": 0.4176647705556555,
      "step": 134200
    },
    {
      "epoch": 1.3061089526328842,
      "grad_norm": 0.22779357433319092,
      "learning_rate": 8.744444444444445e-05,
      "loss": 0.417070052787586,
      "step": 134300
    },
    {
      "epoch": 1.3070814834985827,
      "grad_norm": 0.2505844831466675,
      "learning_rate": 8.743434343434344e-05,
      "loss": 0.4169660153951636,
      "step": 134400
    },
    {
      "epoch": 1.308054014364281,
      "grad_norm": 0.23265431821346283,
      "learning_rate": 8.742424242424243e-05,
      "loss": 0.4181376883497054,
      "step": 134500
    },
    {
      "epoch": 1.3090265452299792,
      "grad_norm": 0.2483486533164978,
      "learning_rate": 8.741414141414141e-05,
      "loss": 0.41803363845331465,
      "step": 134600
    },
    {
      "epoch": 1.3099990760956777,
      "grad_norm": 0.2431032657623291,
      "learning_rate": 8.740404040404042e-05,
      "loss": 0.4169483056080992,
      "step": 134700
    },
    {
      "epoch": 1.310971606961376,
      "grad_norm": 0.26067879796028137,
      "learning_rate": 8.73939393939394e-05,
      "loss": 0.41674614797890724,
      "step": 134800
    },
    {
      "epoch": 1.3119441378270742,
      "grad_norm": 0.24530939757823944,
      "learning_rate": 8.738383838383839e-05,
      "loss": 0.41713275067156724,
      "step": 134900
    },
    {
      "epoch": 1.3129166686927727,
      "grad_norm": 0.25277385115623474,
      "learning_rate": 8.737373737373738e-05,
      "loss": 0.41702872022579385,
      "step": 135000
    },
    {
      "epoch": 1.313889199558471,
      "grad_norm": 0.2504452168941498,
      "learning_rate": 8.736363636363637e-05,
      "loss": 0.4164340774815341,
      "step": 135100
    },
    {
      "epoch": 1.3148617304241692,
      "grad_norm": 0.2324703186750412,
      "learning_rate": 8.735353535353535e-05,
      "loss": 0.41701691092242466,
      "step": 135200
    },
    {
      "epoch": 1.3158342612898677,
      "grad_norm": 0.23607131838798523,
      "learning_rate": 8.734343434343435e-05,
      "loss": 0.41710912650750964,
      "step": 135300
    },
    {
      "epoch": 1.3168067921555662,
      "grad_norm": 0.27943679690361023,
      "learning_rate": 8.733333333333333e-05,
      "loss": 0.4164163885344168,
      "step": 135400
    },
    {
      "epoch": 1.3177793230212644,
      "grad_norm": 0.2595076858997345,
      "learning_rate": 8.732323232323232e-05,
      "loss": 0.416508609676821,
      "step": 135500
    },
    {
      "epoch": 1.3187518538869627,
      "grad_norm": 0.25770723819732666,
      "learning_rate": 8.731313131313132e-05,
      "loss": 0.4186612654849491,
      "step": 135600
    },
    {
      "epoch": 1.3197243847526612,
      "grad_norm": 0.22984394431114197,
      "learning_rate": 8.730303030303031e-05,
      "loss": 0.4177723050996045,
      "step": 135700
    },
    {
      "epoch": 1.3206969156183594,
      "grad_norm": 0.24660588800907135,
      "learning_rate": 8.729292929292929e-05,
      "loss": 0.41786450262340197,
      "step": 135800
    },
    {
      "epoch": 1.3216694464840577,
      "grad_norm": 0.23704418540000916,
      "learning_rate": 8.728282828282829e-05,
      "loss": 0.4164850188566788,
      "step": 135900
    },
    {
      "epoch": 1.3226419773497562,
      "grad_norm": 0.24651409685611725,
      "learning_rate": 8.727272727272727e-05,
      "loss": 0.41677345268602606,
      "step": 136000
    },
    {
      "epoch": 1.3236145082154545,
      "grad_norm": 0.25024592876434326,
      "learning_rate": 8.726262626262626e-05,
      "loss": 0.41666944169086995,
      "step": 136100
    },
    {
      "epoch": 1.3245870390811527,
      "grad_norm": 0.26755163073539734,
      "learning_rate": 8.725252525252526e-05,
      "loss": 0.4166635412071748,
      "step": 136200
    },
    {
      "epoch": 1.3255595699468512,
      "grad_norm": 0.2328168749809265,
      "learning_rate": 8.724242424242425e-05,
      "loss": 0.4170500660973656,
      "step": 136300
    },
    {
      "epoch": 1.3265321008125495,
      "grad_norm": 0.2416210025548935,
      "learning_rate": 8.723232323232323e-05,
      "loss": 0.4171422650104929,
      "step": 136400
    },
    {
      "epoch": 1.327504631678248,
      "grad_norm": 0.23770709335803986,
      "learning_rate": 8.722222222222223e-05,
      "loss": 0.4162534254968417,
      "step": 136500
    },
    {
      "epoch": 1.3284771625439462,
      "grad_norm": 0.250127375125885,
      "learning_rate": 8.721212121212121e-05,
      "loss": 0.4166399392723941,
      "step": 136600
    },
    {
      "epoch": 1.3294496934096447,
      "grad_norm": 0.24843621253967285,
      "learning_rate": 8.72020202020202e-05,
      "loss": 0.41702644193330796,
      "step": 136700
    },
    {
      "epoch": 1.330422224275343,
      "grad_norm": 0.23386433720588684,
      "learning_rate": 8.71919191919192e-05,
      "loss": 0.417314834082761,
      "step": 136800
    },
    {
      "epoch": 1.3313947551410412,
      "grad_norm": 0.2620561420917511,
      "learning_rate": 8.718181818181819e-05,
      "loss": 0.41642604180632325,
      "step": 136900
    },
    {
      "epoch": 1.3323672860067397,
      "grad_norm": 0.23726600408554077,
      "learning_rate": 8.717171717171718e-05,
      "loss": 0.4164201441012877,
      "step": 137000
    },
    {
      "epoch": 1.333339816872438,
      "grad_norm": 0.2530756890773773,
      "learning_rate": 8.716161616161617e-05,
      "loss": 0.41700281776925036,
      "step": 137100
    },
    {
      "epoch": 1.3343123477381362,
      "grad_norm": 0.2429032176733017,
      "learning_rate": 8.715151515151515e-05,
      "loss": 0.41640834869121657,
      "step": 137200
    },
    {
      "epoch": 1.3352848786038347,
      "grad_norm": 0.22568367421627045,
      "learning_rate": 8.714141414141414e-05,
      "loss": 0.41767765283843555,
      "step": 137300
    },
    {
      "epoch": 1.336257409469533,
      "grad_norm": 0.26393479108810425,
      "learning_rate": 8.713131313131313e-05,
      "loss": 0.4169850996462071,
      "step": 137400
    },
    {
      "epoch": 1.3372299403352315,
      "grad_norm": 0.24354751408100128,
      "learning_rate": 8.712121212121212e-05,
      "loss": 0.4166849245906513,
      "step": 137500
    },
    {
      "epoch": 1.3382024712009297,
      "grad_norm": 0.2267182618379593,
      "learning_rate": 8.711111111111112e-05,
      "loss": 0.41618858130670633,
      "step": 137600
    },
    {
      "epoch": 1.3391750020666282,
      "grad_norm": 0.2646947205066681,
      "learning_rate": 8.710101010101011e-05,
      "loss": 0.4167712077374556,
      "step": 137700
    },
    {
      "epoch": 1.3401475329323265,
      "grad_norm": 0.2922463119029999,
      "learning_rate": 8.709090909090909e-05,
      "loss": 0.41833467253149087,
      "step": 137800
    },
    {
      "epoch": 1.3411200637980247,
      "grad_norm": 0.2487250566482544,
      "learning_rate": 8.708080808080809e-05,
      "loss": 0.4170536535553297,
      "step": 137900
    },
    {
      "epoch": 1.3420925946637232,
      "grad_norm": 0.24545185267925262,
      "learning_rate": 8.707070707070707e-05,
      "loss": 0.417243911574715,
      "step": 138000
    },
    {
      "epoch": 1.3430651255294215,
      "grad_norm": 0.26709577441215515,
      "learning_rate": 8.706060606060606e-05,
      "loss": 0.4156686999971519,
      "step": 138100
    },
    {
      "epoch": 1.3440376563951197,
      "grad_norm": 0.2558070421218872,
      "learning_rate": 8.705050505050505e-05,
      "loss": 0.4173301711029125,
      "step": 138200
    },
    {
      "epoch": 1.3450101872608182,
      "grad_norm": 0.24686399102210999,
      "learning_rate": 8.704040404040405e-05,
      "loss": 0.4159511597083241,
      "step": 138300
    },
    {
      "epoch": 1.3459827181265165,
      "grad_norm": 0.24565774202346802,
      "learning_rate": 8.703030303030304e-05,
      "loss": 0.4159452675606078,
      "step": 138400
    },
    {
      "epoch": 1.346955248992215,
      "grad_norm": 0.23526452481746674,
      "learning_rate": 8.702020202020203e-05,
      "loss": 0.4158412996346757,
      "step": 138500
    },
    {
      "epoch": 1.3479277798579132,
      "grad_norm": 0.24280115962028503,
      "learning_rate": 8.701010101010102e-05,
      "loss": 0.4171103759318061,
      "step": 138600
    },
    {
      "epoch": 1.3489003107236117,
      "grad_norm": 0.26422256231307983,
      "learning_rate": 8.7e-05,
      "loss": 0.41661410211435157,
      "step": 138700
    },
    {
      "epoch": 1.34987284158931,
      "grad_norm": 0.24859081208705902,
      "learning_rate": 8.698989898989899e-05,
      "loss": 0.4169024150720055,
      "step": 138800
    },
    {
      "epoch": 1.3508453724550082,
      "grad_norm": 0.26365283131599426,
      "learning_rate": 8.697979797979798e-05,
      "loss": 0.4156215961593367,
      "step": 138900
    },
    {
      "epoch": 1.3518179033207067,
      "grad_norm": 0.25329160690307617,
      "learning_rate": 8.696969696969698e-05,
      "loss": 0.4158118458427431,
      "step": 139000
    },
    {
      "epoch": 1.352790434186405,
      "grad_norm": 0.2766052186489105,
      "learning_rate": 8.695959595959597e-05,
      "loss": 0.416296292295541,
      "step": 139100
    },
    {
      "epoch": 1.3537629650521033,
      "grad_norm": 0.2538958191871643,
      "learning_rate": 8.694949494949496e-05,
      "loss": 0.4173691211724832,
      "step": 139200
    },
    {
      "epoch": 1.3547354959178017,
      "grad_norm": 0.2602337598800659,
      "learning_rate": 8.693939393939394e-05,
      "loss": 0.417069014194088,
      "step": 139300
    },
    {
      "epoch": 1.3557080267835,
      "grad_norm": 0.25346848368644714,
      "learning_rate": 8.692929292929294e-05,
      "loss": 0.4166708522774241,
      "step": 139400
    },
    {
      "epoch": 1.3566805576491983,
      "grad_norm": 0.2601834833621979,
      "learning_rate": 8.691919191919192e-05,
      "loss": 0.4169591346698223,
      "step": 139500
    },
    {
      "epoch": 1.3576530885148967,
      "grad_norm": 0.2409214824438095,
      "learning_rate": 8.690909090909091e-05,
      "loss": 0.4167571062483024,
      "step": 139600
    },
    {
      "epoch": 1.3586256193805952,
      "grad_norm": 0.23625053465366364,
      "learning_rate": 8.68989898989899e-05,
      "loss": 0.4166531424903598,
      "step": 139700
    },
    {
      "epoch": 1.3595981502462935,
      "grad_norm": 0.2576104402542114,
      "learning_rate": 8.68888888888889e-05,
      "loss": 0.4167452969449332,
      "step": 139800
    },
    {
      "epoch": 1.3605706811119918,
      "grad_norm": 0.25119972229003906,
      "learning_rate": 8.687878787878788e-05,
      "loss": 0.41605299800005974,
      "step": 139900
    },
    {
      "epoch": 1.3615432119776902,
      "grad_norm": 0.23471714556217194,
      "learning_rate": 8.686868686868688e-05,
      "loss": 0.41683154257983257,
      "step": 140000
    },
    {
      "epoch": 1.3615432119776902,
      "eval_accuracy": 0.6665392013861352,
      "eval_loss": 0.4103160303138883,
      "eval_runtime": 3728.4552,
      "eval_samples_per_second": 612.787,
      "eval_steps_per_second": 6.128,
      "step": 140000
    },
    {
      "epoch": 1.3625157428433885,
      "grad_norm": 0.23153600096702576,
      "learning_rate": 4.579166666666667e-05,
      "loss": 0.41319765522808277,
      "step": 140100
    },
    {
      "epoch": 1.3634882737090868,
      "grad_norm": 0.2539452016353607,
      "learning_rate": 4.575e-05,
      "loss": 0.4094658185271298,
      "step": 140200
    },
    {
      "epoch": 1.3644608045747852,
      "grad_norm": 0.23266758024692535,
      "learning_rate": 4.570833333333334e-05,
      "loss": 0.40965611822640935,
      "step": 140300
    },
    {
      "epoch": 1.3654333354404835,
      "grad_norm": 0.23487012088298798,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.4085717704160279,
      "step": 140400
    },
    {
      "epoch": 1.3664058663061818,
      "grad_norm": 0.2561752498149872,
      "learning_rate": 4.5625e-05,
      "loss": 0.4080757411206195,
      "step": 140500
    },
    {
      "epoch": 1.3673783971718803,
      "grad_norm": 0.23119448125362396,
      "learning_rate": 4.5583333333333335e-05,
      "loss": 0.4083640985368273,
      "step": 140600
    },
    {
      "epoch": 1.3683509280375785,
      "grad_norm": 0.22752967476844788,
      "learning_rate": 4.554166666666667e-05,
      "loss": 0.4085544024040963,
      "step": 140700
    },
    {
      "epoch": 1.369323458903277,
      "grad_norm": 0.2488718032836914,
      "learning_rate": 4.55e-05,
      "loss": 0.4076662186541145,
      "step": 140800
    },
    {
      "epoch": 1.3702959897689753,
      "grad_norm": 0.2787248194217682,
      "learning_rate": 4.545833333333334e-05,
      "loss": 0.40766044182077227,
      "step": 140900
    },
    {
      "epoch": 1.3712685206346737,
      "grad_norm": 0.2245897352695465,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.4059879672229312,
      "step": 141000
    },
    {
      "epoch": 1.372241051500372,
      "grad_norm": 0.23050557076931,
      "learning_rate": 4.5375e-05,
      "loss": 0.40833516574357276,
      "step": 141100
    },
    {
      "epoch": 1.3732135823660703,
      "grad_norm": 0.24145068228244781,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.40685880519025824,
      "step": 141200
    },
    {
      "epoch": 1.3741861132317688,
      "grad_norm": 0.215300515294075,
      "learning_rate": 4.529166666666667e-05,
      "loss": 0.40616678133268685,
      "step": 141300
    },
    {
      "epoch": 1.375158644097467,
      "grad_norm": 0.2184024602174759,
      "learning_rate": 4.525e-05,
      "loss": 0.4078276286293636,
      "step": 141400
    },
    {
      "epoch": 1.3761311749631653,
      "grad_norm": 0.23565755784511566,
      "learning_rate": 4.520833333333334e-05,
      "loss": 0.4063513375425395,
      "step": 141500
    },
    {
      "epoch": 1.3771037058288638,
      "grad_norm": 0.23823508620262146,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.4055613170985514,
      "step": 141600
    },
    {
      "epoch": 1.378076236694562,
      "grad_norm": 0.26207736134529114,
      "learning_rate": 4.5125e-05,
      "loss": 0.4065358826377538,
      "step": 141700
    },
    {
      "epoch": 1.3790487675602605,
      "grad_norm": 0.23977982997894287,
      "learning_rate": 4.5083333333333336e-05,
      "loss": 0.40623603129604324,
      "step": 141800
    },
    {
      "epoch": 1.3800212984259588,
      "grad_norm": 0.25180715322494507,
      "learning_rate": 4.504166666666667e-05,
      "loss": 0.40613224537231607,
      "step": 141900
    },
    {
      "epoch": 1.3809938292916573,
      "grad_norm": 0.2250404804944992,
      "learning_rate": 4.5e-05,
      "loss": 0.40583240792390357,
      "step": 142000
    },
    {
      "epoch": 1.3819663601573555,
      "grad_norm": 0.21244962513446808,
      "learning_rate": 4.495833333333333e-05,
      "loss": 0.40504244999975686,
      "step": 142100
    },
    {
      "epoch": 1.3829388910230538,
      "grad_norm": 0.25966086983680725,
      "learning_rate": 4.491666666666667e-05,
      "loss": 0.4061149773921307,
      "step": 142200
    },
    {
      "epoch": 1.3839114218887523,
      "grad_norm": 0.23048460483551025,
      "learning_rate": 4.4875e-05,
      "loss": 0.4044428306761243,
      "step": 142300
    },
    {
      "epoch": 1.3848839527544505,
      "grad_norm": 0.2587435841560364,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.4058094006222808,
      "step": 142400
    },
    {
      "epoch": 1.3858564836201488,
      "grad_norm": 0.24655243754386902,
      "learning_rate": 4.4791666666666673e-05,
      "loss": 0.4054115679767815,
      "step": 142500
    },
    {
      "epoch": 1.3868290144858473,
      "grad_norm": 0.23790571093559265,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 0.40520978407730784,
      "step": 142600
    },
    {
      "epoch": 1.3878015453515455,
      "grad_norm": 0.25874748826026917,
      "learning_rate": 4.4708333333333334e-05,
      "loss": 0.4058901625724275,
      "step": 142700
    },
    {
      "epoch": 1.388774076217244,
      "grad_norm": 0.2507341206073761,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.4051982970984542,
      "step": 142800
    },
    {
      "epoch": 1.3897466070829423,
      "grad_norm": 0.23045805096626282,
      "learning_rate": 4.4625e-05,
      "loss": 0.40548659755213984,
      "step": 142900
    },
    {
      "epoch": 1.3907191379486408,
      "grad_norm": 0.23577071726322174,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.40469674382772863,
      "step": 143000
    },
    {
      "epoch": 1.391691668814339,
      "grad_norm": 0.21859069168567657,
      "learning_rate": 4.454166666666667e-05,
      "loss": 0.40537709036826275,
      "step": 143100
    },
    {
      "epoch": 1.3926641996800373,
      "grad_norm": 0.2543262839317322,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.40380317642474034,
      "step": 143200
    },
    {
      "epoch": 1.3936367305457358,
      "grad_norm": 0.2376600056886673,
      "learning_rate": 4.4458333333333334e-05,
      "loss": 0.4039934705667007,
      "step": 143300
    },
    {
      "epoch": 1.394609261411434,
      "grad_norm": 0.24529772996902466,
      "learning_rate": 4.4416666666666664e-05,
      "loss": 0.40379172834712124,
      "step": 143400
    },
    {
      "epoch": 1.3955817922771323,
      "grad_norm": 0.23080411553382874,
      "learning_rate": 4.4375e-05,
      "loss": 0.40466806111383957,
      "step": 143500
    },
    {
      "epoch": 1.3965543231428308,
      "grad_norm": 0.25880852341651917,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.4037802802695022,
      "step": 143600
    },
    {
      "epoch": 1.397526854008529,
      "grad_norm": 0.268735408782959,
      "learning_rate": 4.429166666666667e-05,
      "loss": 0.40563662335894096,
      "step": 143700
    },
    {
      "epoch": 1.3984993848742273,
      "grad_norm": 0.2713814973831177,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 0.4042588429105625,
      "step": 143800
    },
    {
      "epoch": 1.3994719157399258,
      "grad_norm": 0.2538584768772125,
      "learning_rate": 4.4208333333333335e-05,
      "loss": 0.4036651073986675,
      "step": 143900
    },
    {
      "epoch": 1.4004444466056243,
      "grad_norm": 0.2314295470714569,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.4133,
      "step": 144000
    },
    {
      "epoch": 1.4014169774713225,
      "grad_norm": 0.23877279460430145,
      "learning_rate": 4.4125e-05,
      "loss": 0.4119,
      "step": 144100
    },
    {
      "epoch": 1.4023895083370208,
      "grad_norm": 0.2507564425468445,
      "learning_rate": 4.408333333333334e-05,
      "loss": 0.4131,
      "step": 144200
    },
    {
      "epoch": 1.4033620392027193,
      "grad_norm": 0.24433216452598572,
      "learning_rate": 4.404166666666667e-05,
      "loss": 0.4123,
      "step": 144300
    },
    {
      "epoch": 1.4043345700684176,
      "grad_norm": 0.2314912974834442,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.413,
      "step": 144400
    },
    {
      "epoch": 1.4053071009341158,
      "grad_norm": 0.2501506805419922,
      "learning_rate": 4.3958333333333336e-05,
      "loss": 0.4132,
      "step": 144500
    },
    {
      "epoch": 1.4062796317998143,
      "grad_norm": 0.2357538491487503,
      "learning_rate": 4.3916666666666666e-05,
      "loss": 0.4127,
      "step": 144600
    },
    {
      "epoch": 1.4072521626655126,
      "grad_norm": 0.2402064949274063,
      "learning_rate": 4.3875e-05,
      "loss": 0.4131,
      "step": 144700
    },
    {
      "epoch": 1.4082246935312108,
      "grad_norm": 0.2307245284318924,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.4125,
      "step": 144800
    },
    {
      "epoch": 1.4091972243969093,
      "grad_norm": 0.24457789957523346,
      "learning_rate": 4.379166666666667e-05,
      "loss": 0.4118,
      "step": 144900
    },
    {
      "epoch": 1.4101697552626076,
      "grad_norm": 0.24228546023368835,
      "learning_rate": 4.375e-05,
      "loss": 0.4121,
      "step": 145000
    },
    {
      "epoch": 1.411142286128306,
      "grad_norm": 0.2529112696647644,
      "learning_rate": 4.3708333333333336e-05,
      "loss": 0.4114,
      "step": 145100
    },
    {
      "epoch": 1.4121148169940043,
      "grad_norm": 0.26909151673316956,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.412,
      "step": 145200
    },
    {
      "epoch": 1.4130873478597028,
      "grad_norm": 0.24358581006526947,
      "learning_rate": 4.3625e-05,
      "loss": 0.4118,
      "step": 145300
    },
    {
      "epoch": 1.414059878725401,
      "grad_norm": 0.24126991629600525,
      "learning_rate": 4.358333333333334e-05,
      "loss": 0.4113,
      "step": 145400
    },
    {
      "epoch": 1.4150324095910993,
      "grad_norm": 0.23768350481987,
      "learning_rate": 4.354166666666667e-05,
      "loss": 0.4118,
      "step": 145500
    },
    {
      "epoch": 1.4160049404567978,
      "grad_norm": 0.23742301762104034,
      "learning_rate": 4.35e-05,
      "loss": 0.4113,
      "step": 145600
    },
    {
      "epoch": 1.416977471322496,
      "grad_norm": 0.22382231056690216,
      "learning_rate": 4.345833333333334e-05,
      "loss": 0.4111,
      "step": 145700
    },
    {
      "epoch": 1.4179500021881943,
      "grad_norm": 0.2508736550807953,
      "learning_rate": 4.341666666666667e-05,
      "loss": 0.4118,
      "step": 145800
    },
    {
      "epoch": 1.4189225330538928,
      "grad_norm": 0.23899540305137634,
      "learning_rate": 4.3375000000000004e-05,
      "loss": 0.4118,
      "step": 145900
    },
    {
      "epoch": 1.419895063919591,
      "grad_norm": 0.22697408497333527,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.4113,
      "step": 146000
    },
    {
      "epoch": 1.4208675947852896,
      "grad_norm": 0.2613721191883087,
      "learning_rate": 4.329166666666667e-05,
      "loss": 0.4115,
      "step": 146100
    },
    {
      "epoch": 1.4218401256509878,
      "grad_norm": 0.2381870597600937,
      "learning_rate": 4.325e-05,
      "loss": 0.4118,
      "step": 146200
    },
    {
      "epoch": 1.4228126565166863,
      "grad_norm": 0.2448185831308365,
      "learning_rate": 4.320833333333333e-05,
      "loss": 0.4114,
      "step": 146300
    },
    {
      "epoch": 1.4237851873823846,
      "grad_norm": 0.2387389838695526,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.4119,
      "step": 146400
    },
    {
      "epoch": 1.4247577182480828,
      "grad_norm": 0.23515869677066803,
      "learning_rate": 4.3125000000000005e-05,
      "loss": 0.4129,
      "step": 146500
    },
    {
      "epoch": 1.4257302491137813,
      "grad_norm": 0.2645079493522644,
      "learning_rate": 4.3083333333333335e-05,
      "loss": 0.4118,
      "step": 146600
    },
    {
      "epoch": 1.4267027799794796,
      "grad_norm": 0.2295677661895752,
      "learning_rate": 4.304166666666667e-05,
      "loss": 0.4116,
      "step": 146700
    },
    {
      "epoch": 1.4276753108451778,
      "grad_norm": 0.23421888053417206,
      "learning_rate": 4.3e-05,
      "loss": 0.4113,
      "step": 146800
    },
    {
      "epoch": 1.4286478417108763,
      "grad_norm": 0.2293311208486557,
      "learning_rate": 4.295833333333333e-05,
      "loss": 0.4099,
      "step": 146900
    },
    {
      "epoch": 1.4296203725765746,
      "grad_norm": 0.24050822854042053,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.4117,
      "step": 147000
    },
    {
      "epoch": 1.430592903442273,
      "grad_norm": 0.24289841949939728,
      "learning_rate": 4.2875000000000005e-05,
      "loss": 0.4112,
      "step": 147100
    },
    {
      "epoch": 1.4315654343079713,
      "grad_norm": 0.24560755491256714,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.4102,
      "step": 147200
    },
    {
      "epoch": 1.4325379651736698,
      "grad_norm": 0.24414469301700592,
      "learning_rate": 4.2791666666666666e-05,
      "loss": 0.4103,
      "step": 147300
    },
    {
      "epoch": 1.433510496039368,
      "grad_norm": 0.25680291652679443,
      "learning_rate": 4.275e-05,
      "loss": 0.4111,
      "step": 147400
    },
    {
      "epoch": 1.4344830269050663,
      "grad_norm": 0.243479385972023,
      "learning_rate": 4.270833333333333e-05,
      "loss": 0.4117,
      "step": 147500
    },
    {
      "epoch": 1.4354555577707648,
      "grad_norm": 0.25978490710258484,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.4115,
      "step": 147600
    },
    {
      "epoch": 1.436428088636463,
      "grad_norm": 0.2517019212245941,
      "learning_rate": 4.2625000000000006e-05,
      "loss": 0.4101,
      "step": 147700
    },
    {
      "epoch": 1.4374006195021614,
      "grad_norm": 0.2327570766210556,
      "learning_rate": 4.2583333333333336e-05,
      "loss": 0.4094,
      "step": 147800
    },
    {
      "epoch": 1.4383731503678598,
      "grad_norm": 0.24426531791687012,
      "learning_rate": 4.2541666666666666e-05,
      "loss": 0.4099,
      "step": 147900
    },
    {
      "epoch": 1.439345681233558,
      "grad_norm": 0.265001505613327,
      "learning_rate": 4.25e-05,
      "loss": 0.4111,
      "step": 148000
    },
    {
      "epoch": 1.4403182120992564,
      "grad_norm": 0.2303103357553482,
      "learning_rate": 4.245833333333333e-05,
      "loss": 0.4095,
      "step": 148100
    },
    {
      "epoch": 1.4412907429649549,
      "grad_norm": 0.2646205723285675,
      "learning_rate": 4.241666666666667e-05,
      "loss": 0.4116,
      "step": 148200
    },
    {
      "epoch": 1.4422632738306533,
      "grad_norm": 0.2557111084461212,
      "learning_rate": 4.237500000000001e-05,
      "loss": 0.4113,
      "step": 148300
    },
    {
      "epoch": 1.4432358046963516,
      "grad_norm": 0.2508905231952667,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.4107,
      "step": 148400
    },
    {
      "epoch": 1.4442083355620499,
      "grad_norm": 0.2549825608730316,
      "learning_rate": 4.229166666666667e-05,
      "loss": 0.4114,
      "step": 148500
    },
    {
      "epoch": 1.4451808664277483,
      "grad_norm": 0.23487000167369843,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 0.411,
      "step": 148600
    },
    {
      "epoch": 1.4461533972934466,
      "grad_norm": 0.2704876959323883,
      "learning_rate": 4.2208333333333334e-05,
      "loss": 0.4098,
      "step": 148700
    },
    {
      "epoch": 1.4471259281591449,
      "grad_norm": 0.2434140145778656,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.4106,
      "step": 148800
    },
    {
      "epoch": 1.4480984590248434,
      "grad_norm": 0.265533983707428,
      "learning_rate": 4.2125e-05,
      "loss": 0.4102,
      "step": 148900
    },
    {
      "epoch": 1.4490709898905416,
      "grad_norm": 0.26195386052131653,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.4103,
      "step": 149000
    },
    {
      "epoch": 1.4500435207562399,
      "grad_norm": 0.24838706851005554,
      "learning_rate": 4.204166666666667e-05,
      "loss": 0.4087,
      "step": 149100
    },
    {
      "epoch": 1.4510160516219384,
      "grad_norm": 0.2605881094932556,
      "learning_rate": 4.2e-05,
      "loss": 0.4113,
      "step": 149200
    },
    {
      "epoch": 1.4519885824876366,
      "grad_norm": 0.26664483547210693,
      "learning_rate": 4.1958333333333335e-05,
      "loss": 0.411,
      "step": 149300
    },
    {
      "epoch": 1.452961113353335,
      "grad_norm": 0.24923749268054962,
      "learning_rate": 4.191666666666667e-05,
      "loss": 0.4098,
      "step": 149400
    },
    {
      "epoch": 1.4539336442190334,
      "grad_norm": 0.27286288142204285,
      "learning_rate": 4.1875e-05,
      "loss": 0.4105,
      "step": 149500
    },
    {
      "epoch": 1.4549061750847319,
      "grad_norm": 0.2533448040485382,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.4098,
      "step": 149600
    },
    {
      "epoch": 1.4558787059504301,
      "grad_norm": 0.25405314564704895,
      "learning_rate": 4.179166666666667e-05,
      "loss": 0.4114,
      "step": 149700
    },
    {
      "epoch": 1.4568512368161284,
      "grad_norm": 0.24852852523326874,
      "learning_rate": 4.175e-05,
      "loss": 0.41,
      "step": 149800
    },
    {
      "epoch": 1.4578237676818269,
      "grad_norm": 0.2719057500362396,
      "learning_rate": 4.1708333333333335e-05,
      "loss": 0.4091,
      "step": 149900
    },
    {
      "epoch": 1.4587962985475251,
      "grad_norm": 0.2809092700481415,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.41,
      "step": 150000
    },
    {
      "epoch": 1.4587962985475251,
      "eval_accuracy": 0.6681004787426805,
      "eval_loss": 0.4074991047382355,
      "eval_runtime": 3887.7685,
      "eval_samples_per_second": 587.676,
      "eval_steps_per_second": 5.877,
      "step": 150000
    },
    {
      "epoch": 1.4597688294132234,
      "grad_norm": 0.254545122385025,
      "learning_rate": 4.1625e-05,
      "loss": 0.4105,
      "step": 150100
    },
    {
      "epoch": 1.4607413602789219,
      "grad_norm": 0.2549356520175934,
      "learning_rate": 4.158333333333333e-05,
      "loss": 0.4094,
      "step": 150200
    },
    {
      "epoch": 1.4617138911446201,
      "grad_norm": 0.24007104337215424,
      "learning_rate": 4.154166666666667e-05,
      "loss": 0.4087,
      "step": 150300
    },
    {
      "epoch": 1.4626864220103186,
      "grad_norm": 0.2478698194026947,
      "learning_rate": 4.15e-05,
      "loss": 0.4096,
      "step": 150400
    },
    {
      "epoch": 1.4636589528760169,
      "grad_norm": 0.28382059931755066,
      "learning_rate": 4.1458333333333336e-05,
      "loss": 0.4097,
      "step": 150500
    },
    {
      "epoch": 1.4646314837417154,
      "grad_norm": 0.2401227504014969,
      "learning_rate": 4.141666666666667e-05,
      "loss": 0.4093,
      "step": 150600
    },
    {
      "epoch": 1.4656040146074136,
      "grad_norm": 0.24763205647468567,
      "learning_rate": 4.1375e-05,
      "loss": 0.4094,
      "step": 150700
    },
    {
      "epoch": 1.466576545473112,
      "grad_norm": 0.23248878121376038,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.4092,
      "step": 150800
    },
    {
      "epoch": 1.4675490763388104,
      "grad_norm": 0.25472012162208557,
      "learning_rate": 4.129166666666667e-05,
      "loss": 0.4101,
      "step": 150900
    },
    {
      "epoch": 1.4685216072045086,
      "grad_norm": 0.2529371976852417,
      "learning_rate": 4.125e-05,
      "loss": 0.4095,
      "step": 151000
    },
    {
      "epoch": 1.469494138070207,
      "grad_norm": 0.24109385907649994,
      "learning_rate": 4.120833333333334e-05,
      "loss": 0.409,
      "step": 151100
    },
    {
      "epoch": 1.4704666689359054,
      "grad_norm": 0.27787095308303833,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.4095,
      "step": 151200
    },
    {
      "epoch": 1.4714391998016036,
      "grad_norm": 0.27105218172073364,
      "learning_rate": 4.1125000000000004e-05,
      "loss": 0.4092,
      "step": 151300
    },
    {
      "epoch": 1.4724117306673021,
      "grad_norm": 0.2552568316459656,
      "learning_rate": 4.1083333333333334e-05,
      "loss": 0.4103,
      "step": 151400
    },
    {
      "epoch": 1.4733842615330004,
      "grad_norm": 0.2423887550830841,
      "learning_rate": 4.104166666666667e-05,
      "loss": 0.4093,
      "step": 151500
    },
    {
      "epoch": 1.4743567923986989,
      "grad_norm": 0.22588951885700226,
      "learning_rate": 4.1e-05,
      "loss": 0.4093,
      "step": 151600
    },
    {
      "epoch": 1.4753293232643971,
      "grad_norm": 0.2403227686882019,
      "learning_rate": 4.095833333333334e-05,
      "loss": 0.4101,
      "step": 151700
    },
    {
      "epoch": 1.4763018541300954,
      "grad_norm": 0.24059829115867615,
      "learning_rate": 4.091666666666667e-05,
      "loss": 0.4091,
      "step": 151800
    },
    {
      "epoch": 1.4772743849957939,
      "grad_norm": 0.26829102635383606,
      "learning_rate": 4.0875000000000004e-05,
      "loss": 0.4092,
      "step": 151900
    },
    {
      "epoch": 1.4782469158614922,
      "grad_norm": 0.24920876324176788,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.4092,
      "step": 152000
    },
    {
      "epoch": 1.4792194467271904,
      "grad_norm": 0.24023424088954926,
      "learning_rate": 4.0791666666666664e-05,
      "loss": 0.41,
      "step": 152100
    },
    {
      "epoch": 1.480191977592889,
      "grad_norm": 0.2829729914665222,
      "learning_rate": 4.075e-05,
      "loss": 0.4092,
      "step": 152200
    },
    {
      "epoch": 1.4811645084585872,
      "grad_norm": 0.2596418559551239,
      "learning_rate": 4.070833333333334e-05,
      "loss": 0.4099,
      "step": 152300
    },
    {
      "epoch": 1.4821370393242854,
      "grad_norm": 0.2570141851902008,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.4095,
      "step": 152400
    },
    {
      "epoch": 1.483109570189984,
      "grad_norm": 0.2602840065956116,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 0.4086,
      "step": 152500
    },
    {
      "epoch": 1.4840821010556824,
      "grad_norm": 0.25017932057380676,
      "learning_rate": 4.0583333333333335e-05,
      "loss": 0.4082,
      "step": 152600
    },
    {
      "epoch": 1.4850546319213807,
      "grad_norm": 0.23239624500274658,
      "learning_rate": 4.0541666666666665e-05,
      "loss": 0.4091,
      "step": 152700
    },
    {
      "epoch": 1.486027162787079,
      "grad_norm": 0.2783851623535156,
      "learning_rate": 4.05e-05,
      "loss": 0.4083,
      "step": 152800
    },
    {
      "epoch": 1.4869996936527774,
      "grad_norm": 0.24764211475849152,
      "learning_rate": 4.045833333333334e-05,
      "loss": 0.408,
      "step": 152900
    },
    {
      "epoch": 1.4879722245184757,
      "grad_norm": 0.2607201039791107,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.4099,
      "step": 153000
    },
    {
      "epoch": 1.488944755384174,
      "grad_norm": 0.26832908391952515,
      "learning_rate": 4.0375e-05,
      "loss": 0.4091,
      "step": 153100
    },
    {
      "epoch": 1.4899172862498724,
      "grad_norm": 0.2622240483760834,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.4087,
      "step": 153200
    },
    {
      "epoch": 1.4908898171155707,
      "grad_norm": 0.24445687234401703,
      "learning_rate": 4.0291666666666666e-05,
      "loss": 0.4085,
      "step": 153300
    },
    {
      "epoch": 1.491862347981269,
      "grad_norm": 0.24933494627475739,
      "learning_rate": 4.025e-05,
      "loss": 0.4077,
      "step": 153400
    },
    {
      "epoch": 1.4928348788469674,
      "grad_norm": 0.2434130758047104,
      "learning_rate": 4.020833333333334e-05,
      "loss": 0.4081,
      "step": 153500
    },
    {
      "epoch": 1.4938074097126657,
      "grad_norm": 0.24472865462303162,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.4088,
      "step": 153600
    },
    {
      "epoch": 1.4947799405783642,
      "grad_norm": 0.249252051115036,
      "learning_rate": 4.0125e-05,
      "loss": 0.4079,
      "step": 153700
    },
    {
      "epoch": 1.4957524714440624,
      "grad_norm": 0.26479363441467285,
      "learning_rate": 4.0083333333333336e-05,
      "loss": 0.4085,
      "step": 153800
    },
    {
      "epoch": 1.496725002309761,
      "grad_norm": 0.26357942819595337,
      "learning_rate": 4.0041666666666666e-05,
      "loss": 0.4088,
      "step": 153900
    },
    {
      "epoch": 1.4976975331754592,
      "grad_norm": 0.255023330450058,
      "learning_rate": 4e-05,
      "loss": 0.4094,
      "step": 154000
    },
    {
      "epoch": 1.4986700640411574,
      "grad_norm": 0.2661164700984955,
      "learning_rate": 3.995833333333333e-05,
      "loss": 0.4075,
      "step": 154100
    },
    {
      "epoch": 1.499642594906856,
      "grad_norm": 0.23474830389022827,
      "learning_rate": 3.991666666666667e-05,
      "loss": 0.4083,
      "step": 154200
    },
    {
      "epoch": 1.5006151257725542,
      "grad_norm": 0.2500605285167694,
      "learning_rate": 3.9875e-05,
      "loss": 0.4093,
      "step": 154300
    },
    {
      "epoch": 1.5015876566382524,
      "grad_norm": 0.24197958409786224,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.408,
      "step": 154400
    },
    {
      "epoch": 1.502560187503951,
      "grad_norm": 0.2632761597633362,
      "learning_rate": 3.979166666666667e-05,
      "loss": 0.409,
      "step": 154500
    },
    {
      "epoch": 1.5035327183696494,
      "grad_norm": 0.23567716777324677,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 0.4086,
      "step": 154600
    },
    {
      "epoch": 1.5045052492353475,
      "grad_norm": 0.26106369495391846,
      "learning_rate": 3.9708333333333334e-05,
      "loss": 0.4077,
      "step": 154700
    },
    {
      "epoch": 1.505477780101046,
      "grad_norm": 0.25344118475914,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.4081,
      "step": 154800
    },
    {
      "epoch": 1.5064503109667444,
      "grad_norm": 0.2690846025943756,
      "learning_rate": 3.9625e-05,
      "loss": 0.4079,
      "step": 154900
    },
    {
      "epoch": 1.5074228418324427,
      "grad_norm": 0.26207235455513,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.4083,
      "step": 155000
    },
    {
      "epoch": 1.508395372698141,
      "grad_norm": 0.26644471287727356,
      "learning_rate": 3.9541666666666675e-05,
      "loss": 0.4071,
      "step": 155100
    },
    {
      "epoch": 1.5093679035638394,
      "grad_norm": 0.2722599506378174,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.4079,
      "step": 155200
    },
    {
      "epoch": 1.5103404344295377,
      "grad_norm": 0.2727232873439789,
      "learning_rate": 3.9458333333333335e-05,
      "loss": 0.4086,
      "step": 155300
    },
    {
      "epoch": 1.511312965295236,
      "grad_norm": 0.27087175846099854,
      "learning_rate": 3.941666666666667e-05,
      "loss": 0.4075,
      "step": 155400
    },
    {
      "epoch": 1.5122854961609344,
      "grad_norm": 0.24191156029701233,
      "learning_rate": 3.9375e-05,
      "loss": 0.4078,
      "step": 155500
    },
    {
      "epoch": 1.513258027026633,
      "grad_norm": 0.2535794675350189,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.4083,
      "step": 155600
    },
    {
      "epoch": 1.514230557892331,
      "grad_norm": 0.25962191820144653,
      "learning_rate": 3.929166666666667e-05,
      "loss": 0.4078,
      "step": 155700
    },
    {
      "epoch": 1.5152030887580294,
      "grad_norm": 0.25999823212623596,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 0.408,
      "step": 155800
    },
    {
      "epoch": 1.516175619623728,
      "grad_norm": 0.24033720791339874,
      "learning_rate": 3.9208333333333335e-05,
      "loss": 0.4083,
      "step": 155900
    },
    {
      "epoch": 1.5171481504894262,
      "grad_norm": 0.26168906688690186,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.4075,
      "step": 156000
    },
    {
      "epoch": 1.5181206813551245,
      "grad_norm": 0.2529641389846802,
      "learning_rate": 3.9125e-05,
      "loss": 0.4068,
      "step": 156100
    },
    {
      "epoch": 1.519093212220823,
      "grad_norm": 0.2419569194316864,
      "learning_rate": 3.908333333333333e-05,
      "loss": 0.408,
      "step": 156200
    },
    {
      "epoch": 1.5200657430865212,
      "grad_norm": 0.27186599373817444,
      "learning_rate": 3.904166666666667e-05,
      "loss": 0.4072,
      "step": 156300
    },
    {
      "epoch": 1.5210382739522195,
      "grad_norm": 0.2567269206047058,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.407,
      "step": 156400
    },
    {
      "epoch": 1.522010804817918,
      "grad_norm": 0.25975123047828674,
      "learning_rate": 3.8958333333333336e-05,
      "loss": 0.408,
      "step": 156500
    },
    {
      "epoch": 1.5229833356836162,
      "grad_norm": 0.2752474546432495,
      "learning_rate": 3.8916666666666666e-05,
      "loss": 0.4074,
      "step": 156600
    },
    {
      "epoch": 1.5239558665493145,
      "grad_norm": 0.24488459527492523,
      "learning_rate": 3.8875e-05,
      "loss": 0.4067,
      "step": 156700
    },
    {
      "epoch": 1.524928397415013,
      "grad_norm": 0.23994244635105133,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.4078,
      "step": 156800
    },
    {
      "epoch": 1.5259009282807114,
      "grad_norm": 0.24190916121006012,
      "learning_rate": 3.879166666666667e-05,
      "loss": 0.4074,
      "step": 156900
    },
    {
      "epoch": 1.5268734591464097,
      "grad_norm": 0.2383890002965927,
      "learning_rate": 3.875e-05,
      "loss": 0.407,
      "step": 157000
    },
    {
      "epoch": 1.527845990012108,
      "grad_norm": 0.2547897696495056,
      "learning_rate": 3.870833333333334e-05,
      "loss": 0.4073,
      "step": 157100
    },
    {
      "epoch": 1.5288185208778065,
      "grad_norm": 0.26117363572120667,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.4078,
      "step": 157200
    },
    {
      "epoch": 1.5297910517435047,
      "grad_norm": 0.2706320285797119,
      "learning_rate": 3.8625e-05,
      "loss": 0.406,
      "step": 157300
    },
    {
      "epoch": 1.530763582609203,
      "grad_norm": 0.2583014965057373,
      "learning_rate": 3.8583333333333334e-05,
      "loss": 0.4073,
      "step": 157400
    },
    {
      "epoch": 1.5317361134749015,
      "grad_norm": 0.2707655727863312,
      "learning_rate": 3.854166666666667e-05,
      "loss": 0.407,
      "step": 157500
    },
    {
      "epoch": 1.5327086443405997,
      "grad_norm": 0.2669283449649811,
      "learning_rate": 3.85e-05,
      "loss": 0.4072,
      "step": 157600
    },
    {
      "epoch": 1.533681175206298,
      "grad_norm": 0.2663826048374176,
      "learning_rate": 3.845833333333334e-05,
      "loss": 0.4077,
      "step": 157700
    },
    {
      "epoch": 1.5346537060719965,
      "grad_norm": 0.2705162763595581,
      "learning_rate": 3.841666666666667e-05,
      "loss": 0.4074,
      "step": 157800
    },
    {
      "epoch": 1.535626236937695,
      "grad_norm": 0.2975132465362549,
      "learning_rate": 3.8375e-05,
      "loss": 0.4072,
      "step": 157900
    },
    {
      "epoch": 1.536598767803393,
      "grad_norm": 0.29179292917251587,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.4069,
      "step": 158000
    },
    {
      "epoch": 1.5375712986690915,
      "grad_norm": 0.27062416076660156,
      "learning_rate": 3.829166666666667e-05,
      "loss": 0.407,
      "step": 158100
    },
    {
      "epoch": 1.53854382953479,
      "grad_norm": 0.2503772974014282,
      "learning_rate": 3.825e-05,
      "loss": 0.407,
      "step": 158200
    },
    {
      "epoch": 1.5395163604004882,
      "grad_norm": 0.26351025700569153,
      "learning_rate": 3.820833333333334e-05,
      "loss": 0.4069,
      "step": 158300
    },
    {
      "epoch": 1.5404888912661865,
      "grad_norm": 0.25518491864204407,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.407,
      "step": 158400
    },
    {
      "epoch": 1.541461422131885,
      "grad_norm": 0.2619391083717346,
      "learning_rate": 3.8125e-05,
      "loss": 0.4071,
      "step": 158500
    },
    {
      "epoch": 1.5424339529975832,
      "grad_norm": 0.2765694558620453,
      "learning_rate": 3.8083333333333335e-05,
      "loss": 0.4065,
      "step": 158600
    },
    {
      "epoch": 1.5434064838632815,
      "grad_norm": 0.256077378988266,
      "learning_rate": 3.804166666666667e-05,
      "loss": 0.4058,
      "step": 158700
    },
    {
      "epoch": 1.54437901472898,
      "grad_norm": 0.2357557862997055,
      "learning_rate": 3.8e-05,
      "loss": 0.4066,
      "step": 158800
    },
    {
      "epoch": 1.5453515455946785,
      "grad_norm": 0.2597561478614807,
      "learning_rate": 3.795833333333333e-05,
      "loss": 0.4064,
      "step": 158900
    },
    {
      "epoch": 1.5463240764603765,
      "grad_norm": 0.25415292382240295,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.4066,
      "step": 159000
    },
    {
      "epoch": 1.547296607326075,
      "grad_norm": 0.26685988903045654,
      "learning_rate": 3.7875e-05,
      "loss": 0.4058,
      "step": 159100
    },
    {
      "epoch": 1.5482691381917735,
      "grad_norm": 0.23925641179084778,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.4063,
      "step": 159200
    },
    {
      "epoch": 1.5492416690574717,
      "grad_norm": 0.24835655093193054,
      "learning_rate": 3.779166666666667e-05,
      "loss": 0.4071,
      "step": 159300
    },
    {
      "epoch": 1.55021419992317,
      "grad_norm": 0.2689920663833618,
      "learning_rate": 3.775e-05,
      "loss": 0.4073,
      "step": 159400
    },
    {
      "epoch": 1.5511867307888685,
      "grad_norm": 0.2649604082107544,
      "learning_rate": 3.770833333333333e-05,
      "loss": 0.4067,
      "step": 159500
    },
    {
      "epoch": 1.5521592616545667,
      "grad_norm": 0.2531604468822479,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.4076,
      "step": 159600
    },
    {
      "epoch": 1.553131792520265,
      "grad_norm": 0.24852558970451355,
      "learning_rate": 3.7625e-05,
      "loss": 0.4068,
      "step": 159700
    },
    {
      "epoch": 1.5541043233859635,
      "grad_norm": 0.27483370900154114,
      "learning_rate": 3.7583333333333337e-05,
      "loss": 0.4057,
      "step": 159800
    },
    {
      "epoch": 1.555076854251662,
      "grad_norm": 0.26131170988082886,
      "learning_rate": 3.754166666666667e-05,
      "loss": 0.4056,
      "step": 159900
    },
    {
      "epoch": 1.55604938511736,
      "grad_norm": 0.24998612701892853,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.4056,
      "step": 160000
    },
    {
      "epoch": 1.55604938511736,
      "eval_accuracy": 0.6684099258044213,
      "eval_loss": 0.40480276942253113,
      "eval_runtime": 3897.1453,
      "eval_samples_per_second": 586.262,
      "eval_steps_per_second": 5.863,
      "step": 160000
    },
    {
      "epoch": 1.5570219159830585,
      "grad_norm": 0.25413572788238525,
      "learning_rate": 3.7458333333333334e-05,
      "loss": 0.4063,
      "step": 160100
    },
    {
      "epoch": 1.557994446848757,
      "grad_norm": 0.24785716831684113,
      "learning_rate": 3.7416666666666664e-05,
      "loss": 0.4053,
      "step": 160200
    },
    {
      "epoch": 1.5589669777144552,
      "grad_norm": 0.25867006182670593,
      "learning_rate": 3.737500000000001e-05,
      "loss": 0.406,
      "step": 160300
    },
    {
      "epoch": 1.5599395085801535,
      "grad_norm": 0.2613013684749603,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.4078,
      "step": 160400
    },
    {
      "epoch": 1.560912039445852,
      "grad_norm": 0.2518720328807831,
      "learning_rate": 3.729166666666667e-05,
      "loss": 0.4061,
      "step": 160500
    },
    {
      "epoch": 1.5618845703115503,
      "grad_norm": 0.24997074902057648,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 0.4065,
      "step": 160600
    },
    {
      "epoch": 1.5628571011772485,
      "grad_norm": 0.2536137104034424,
      "learning_rate": 3.7208333333333334e-05,
      "loss": 0.4058,
      "step": 160700
    },
    {
      "epoch": 1.563829632042947,
      "grad_norm": 0.2610147297382355,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.4055,
      "step": 160800
    },
    {
      "epoch": 1.5648021629086453,
      "grad_norm": 0.25373515486717224,
      "learning_rate": 3.7125e-05,
      "loss": 0.407,
      "step": 160900
    },
    {
      "epoch": 1.5657746937743435,
      "grad_norm": 0.27137571573257446,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.4061,
      "step": 161000
    },
    {
      "epoch": 1.566747224640042,
      "grad_norm": 0.25521335005760193,
      "learning_rate": 3.704166666666667e-05,
      "loss": 0.405,
      "step": 161100
    },
    {
      "epoch": 1.5677197555057405,
      "grad_norm": 0.2626108229160309,
      "learning_rate": 3.7e-05,
      "loss": 0.4064,
      "step": 161200
    },
    {
      "epoch": 1.5686922863714388,
      "grad_norm": 0.24555225670337677,
      "learning_rate": 3.6958333333333335e-05,
      "loss": 0.4066,
      "step": 161300
    },
    {
      "epoch": 1.569664817237137,
      "grad_norm": 0.2618723213672638,
      "learning_rate": 3.6916666666666665e-05,
      "loss": 0.4051,
      "step": 161400
    },
    {
      "epoch": 1.5706373481028355,
      "grad_norm": 0.2507570683956146,
      "learning_rate": 3.6875e-05,
      "loss": 0.405,
      "step": 161500
    },
    {
      "epoch": 1.5716098789685338,
      "grad_norm": 0.2625335454940796,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.4058,
      "step": 161600
    },
    {
      "epoch": 1.572582409834232,
      "grad_norm": 0.2617324888706207,
      "learning_rate": 3.679166666666667e-05,
      "loss": 0.4057,
      "step": 161700
    },
    {
      "epoch": 1.5735549406999305,
      "grad_norm": 0.26110538840293884,
      "learning_rate": 3.675e-05,
      "loss": 0.4057,
      "step": 161800
    },
    {
      "epoch": 1.5745274715656288,
      "grad_norm": 0.26049837470054626,
      "learning_rate": 3.6708333333333336e-05,
      "loss": 0.4059,
      "step": 161900
    },
    {
      "epoch": 1.575500002431327,
      "grad_norm": 0.2618466913700104,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.4059,
      "step": 162000
    },
    {
      "epoch": 1.5764725332970255,
      "grad_norm": 0.25240641832351685,
      "learning_rate": 3.6625e-05,
      "loss": 0.4058,
      "step": 162100
    },
    {
      "epoch": 1.577445064162724,
      "grad_norm": 0.2533635199069977,
      "learning_rate": 3.658333333333334e-05,
      "loss": 0.4055,
      "step": 162200
    },
    {
      "epoch": 1.578417595028422,
      "grad_norm": 0.2659037709236145,
      "learning_rate": 3.654166666666667e-05,
      "loss": 0.4056,
      "step": 162300
    },
    {
      "epoch": 1.5793901258941205,
      "grad_norm": 0.2662041187286377,
      "learning_rate": 3.65e-05,
      "loss": 0.4062,
      "step": 162400
    },
    {
      "epoch": 1.580362656759819,
      "grad_norm": 0.2688125967979431,
      "learning_rate": 3.6458333333333336e-05,
      "loss": 0.4072,
      "step": 162500
    },
    {
      "epoch": 1.5813351876255173,
      "grad_norm": 0.25657063722610474,
      "learning_rate": 3.641666666666667e-05,
      "loss": 0.4054,
      "step": 162600
    },
    {
      "epoch": 1.5823077184912155,
      "grad_norm": 0.26112836599349976,
      "learning_rate": 3.6375e-05,
      "loss": 0.4046,
      "step": 162700
    },
    {
      "epoch": 1.583280249356914,
      "grad_norm": 0.27322918176651,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.4051,
      "step": 162800
    },
    {
      "epoch": 1.5842527802226123,
      "grad_norm": 0.2630179822444916,
      "learning_rate": 3.629166666666667e-05,
      "loss": 0.4055,
      "step": 162900
    },
    {
      "epoch": 1.5852253110883106,
      "grad_norm": 0.24995416402816772,
      "learning_rate": 3.625e-05,
      "loss": 0.4055,
      "step": 163000
    },
    {
      "epoch": 1.586197841954009,
      "grad_norm": 0.26225537061691284,
      "learning_rate": 3.620833333333333e-05,
      "loss": 0.4059,
      "step": 163100
    },
    {
      "epoch": 1.5871703728197075,
      "grad_norm": 0.2516181170940399,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.4053,
      "step": 163200
    },
    {
      "epoch": 1.5881429036854056,
      "grad_norm": 0.2720840573310852,
      "learning_rate": 3.6125000000000004e-05,
      "loss": 0.4052,
      "step": 163300
    },
    {
      "epoch": 1.589115434551104,
      "grad_norm": 0.2686188519001007,
      "learning_rate": 3.6083333333333334e-05,
      "loss": 0.4061,
      "step": 163400
    },
    {
      "epoch": 1.5900879654168025,
      "grad_norm": 0.2829788625240326,
      "learning_rate": 3.604166666666667e-05,
      "loss": 0.4037,
      "step": 163500
    },
    {
      "epoch": 1.5910604962825008,
      "grad_norm": 0.2565120458602905,
      "learning_rate": 3.6e-05,
      "loss": 0.4058,
      "step": 163600
    },
    {
      "epoch": 1.592033027148199,
      "grad_norm": 0.2515724301338196,
      "learning_rate": 3.595833333333333e-05,
      "loss": 0.4059,
      "step": 163700
    },
    {
      "epoch": 1.5930055580138975,
      "grad_norm": 0.24616029858589172,
      "learning_rate": 3.591666666666667e-05,
      "loss": 0.4049,
      "step": 163800
    },
    {
      "epoch": 1.5939780888795958,
      "grad_norm": 0.25374579429626465,
      "learning_rate": 3.5875000000000005e-05,
      "loss": 0.4047,
      "step": 163900
    },
    {
      "epoch": 1.594950619745294,
      "grad_norm": 0.2566252648830414,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.4049,
      "step": 164000
    },
    {
      "epoch": 1.5959231506109925,
      "grad_norm": 0.2944873571395874,
      "learning_rate": 3.5791666666666665e-05,
      "loss": 0.405,
      "step": 164100
    },
    {
      "epoch": 1.596895681476691,
      "grad_norm": 0.25372767448425293,
      "learning_rate": 3.575e-05,
      "loss": 0.4049,
      "step": 164200
    },
    {
      "epoch": 1.597868212342389,
      "grad_norm": 0.26635631918907166,
      "learning_rate": 3.570833333333333e-05,
      "loss": 0.405,
      "step": 164300
    },
    {
      "epoch": 1.5988407432080876,
      "grad_norm": 0.2441154569387436,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.404,
      "step": 164400
    },
    {
      "epoch": 1.599813274073786,
      "grad_norm": 0.2575565278530121,
      "learning_rate": 3.5625000000000005e-05,
      "loss": 0.4063,
      "step": 164500
    },
    {
      "epoch": 1.6007858049394843,
      "grad_norm": 0.2607060372829437,
      "learning_rate": 3.5583333333333335e-05,
      "loss": 0.4048,
      "step": 164600
    },
    {
      "epoch": 1.6017583358051826,
      "grad_norm": 0.2602505385875702,
      "learning_rate": 3.5541666666666665e-05,
      "loss": 0.4031,
      "step": 164700
    },
    {
      "epoch": 1.602730866670881,
      "grad_norm": 0.24214527010917664,
      "learning_rate": 3.55e-05,
      "loss": 0.405,
      "step": 164800
    },
    {
      "epoch": 1.6037033975365793,
      "grad_norm": 0.2750842273235321,
      "learning_rate": 3.545833333333333e-05,
      "loss": 0.404,
      "step": 164900
    },
    {
      "epoch": 1.6046759284022776,
      "grad_norm": 0.24738000333309174,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.4054,
      "step": 165000
    },
    {
      "epoch": 1.605648459267976,
      "grad_norm": 0.2772357165813446,
      "learning_rate": 3.5375e-05,
      "loss": 0.4044,
      "step": 165100
    },
    {
      "epoch": 1.6066209901336743,
      "grad_norm": 0.27560994029045105,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.4044,
      "step": 165200
    },
    {
      "epoch": 1.6075935209993726,
      "grad_norm": 0.2519012987613678,
      "learning_rate": 3.5291666666666666e-05,
      "loss": 0.4048,
      "step": 165300
    },
    {
      "epoch": 1.608566051865071,
      "grad_norm": 0.2408108413219452,
      "learning_rate": 3.525e-05,
      "loss": 0.4043,
      "step": 165400
    },
    {
      "epoch": 1.6095385827307696,
      "grad_norm": 0.27094826102256775,
      "learning_rate": 3.520833333333334e-05,
      "loss": 0.405,
      "step": 165500
    },
    {
      "epoch": 1.6105111135964678,
      "grad_norm": 0.255878746509552,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.4033,
      "step": 165600
    },
    {
      "epoch": 1.611483644462166,
      "grad_norm": 0.24232760071754456,
      "learning_rate": 3.5125e-05,
      "loss": 0.4047,
      "step": 165700
    },
    {
      "epoch": 1.6124561753278646,
      "grad_norm": 0.26683804392814636,
      "learning_rate": 3.508333333333334e-05,
      "loss": 0.4046,
      "step": 165800
    },
    {
      "epoch": 1.6134287061935628,
      "grad_norm": 0.2570292055606842,
      "learning_rate": 3.504166666666667e-05,
      "loss": 0.4052,
      "step": 165900
    },
    {
      "epoch": 1.614401237059261,
      "grad_norm": 0.2675350606441498,
      "learning_rate": 3.5e-05,
      "loss": 0.4042,
      "step": 166000
    },
    {
      "epoch": 1.6153737679249596,
      "grad_norm": 0.27007153630256653,
      "learning_rate": 3.495833333333334e-05,
      "loss": 0.4044,
      "step": 166100
    },
    {
      "epoch": 1.6163462987906578,
      "grad_norm": 0.2549265921115875,
      "learning_rate": 3.491666666666667e-05,
      "loss": 0.4042,
      "step": 166200
    },
    {
      "epoch": 1.617318829656356,
      "grad_norm": 0.24856260418891907,
      "learning_rate": 3.4875e-05,
      "loss": 0.4046,
      "step": 166300
    },
    {
      "epoch": 1.6182913605220546,
      "grad_norm": 0.26000744104385376,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.404,
      "step": 166400
    },
    {
      "epoch": 1.619263891387753,
      "grad_norm": 0.26412129402160645,
      "learning_rate": 3.479166666666667e-05,
      "loss": 0.4045,
      "step": 166500
    },
    {
      "epoch": 1.620236422253451,
      "grad_norm": 0.2912655472755432,
      "learning_rate": 3.475e-05,
      "loss": 0.4034,
      "step": 166600
    },
    {
      "epoch": 1.6212089531191496,
      "grad_norm": 0.26143965125083923,
      "learning_rate": 3.4708333333333334e-05,
      "loss": 0.4037,
      "step": 166700
    },
    {
      "epoch": 1.622181483984848,
      "grad_norm": 0.24627089500427246,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.4046,
      "step": 166800
    },
    {
      "epoch": 1.6231540148505463,
      "grad_norm": 0.24513718485832214,
      "learning_rate": 3.4625e-05,
      "loss": 0.4039,
      "step": 166900
    },
    {
      "epoch": 1.6241265457162446,
      "grad_norm": 0.25662463903427124,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.4037,
      "step": 167000
    },
    {
      "epoch": 1.625099076581943,
      "grad_norm": 0.25938645005226135,
      "learning_rate": 3.454166666666667e-05,
      "loss": 0.4049,
      "step": 167100
    },
    {
      "epoch": 1.6260716074476413,
      "grad_norm": 0.24541758000850677,
      "learning_rate": 3.45e-05,
      "loss": 0.4051,
      "step": 167200
    },
    {
      "epoch": 1.6270441383133396,
      "grad_norm": 0.25340041518211365,
      "learning_rate": 3.4458333333333335e-05,
      "loss": 0.4036,
      "step": 167300
    },
    {
      "epoch": 1.628016669179038,
      "grad_norm": 0.26809999346733093,
      "learning_rate": 3.441666666666667e-05,
      "loss": 0.4035,
      "step": 167400
    },
    {
      "epoch": 1.6289892000447366,
      "grad_norm": 0.27476003766059875,
      "learning_rate": 3.4375e-05,
      "loss": 0.4038,
      "step": 167500
    },
    {
      "epoch": 1.6299617309104346,
      "grad_norm": 0.2673843502998352,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.4037,
      "step": 167600
    },
    {
      "epoch": 1.630934261776133,
      "grad_norm": 0.25746020674705505,
      "learning_rate": 3.429166666666667e-05,
      "loss": 0.4022,
      "step": 167700
    },
    {
      "epoch": 1.6319067926418316,
      "grad_norm": 0.28313136100769043,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.4042,
      "step": 167800
    },
    {
      "epoch": 1.6328793235075298,
      "grad_norm": 0.25285714864730835,
      "learning_rate": 3.4208333333333336e-05,
      "loss": 0.4032,
      "step": 167900
    },
    {
      "epoch": 1.633851854373228,
      "grad_norm": 0.24350811541080475,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.4036,
      "step": 168000
    },
    {
      "epoch": 1.6348243852389266,
      "grad_norm": 0.2781265377998352,
      "learning_rate": 3.4125e-05,
      "loss": 0.4028,
      "step": 168100
    },
    {
      "epoch": 1.6357969161046249,
      "grad_norm": 0.275734543800354,
      "learning_rate": 3.408333333333333e-05,
      "loss": 0.4046,
      "step": 168200
    },
    {
      "epoch": 1.6367694469703231,
      "grad_norm": 0.24703837931156158,
      "learning_rate": 3.404166666666666e-05,
      "loss": 0.404,
      "step": 168300
    },
    {
      "epoch": 1.6377419778360216,
      "grad_norm": 0.25624847412109375,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.403,
      "step": 168400
    },
    {
      "epoch": 1.63871450870172,
      "grad_norm": 0.2893727421760559,
      "learning_rate": 3.3958333333333337e-05,
      "loss": 0.4042,
      "step": 168500
    },
    {
      "epoch": 1.6396870395674181,
      "grad_norm": 0.2718683183193207,
      "learning_rate": 3.391666666666667e-05,
      "loss": 0.403,
      "step": 168600
    },
    {
      "epoch": 1.6406595704331166,
      "grad_norm": 0.26761025190353394,
      "learning_rate": 3.3875000000000003e-05,
      "loss": 0.4037,
      "step": 168700
    },
    {
      "epoch": 1.641632101298815,
      "grad_norm": 0.26960840821266174,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.4034,
      "step": 168800
    },
    {
      "epoch": 1.6426046321645134,
      "grad_norm": 0.2710494101047516,
      "learning_rate": 3.3791666666666664e-05,
      "loss": 0.4035,
      "step": 168900
    },
    {
      "epoch": 1.6435771630302116,
      "grad_norm": 0.27103057503700256,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.4029,
      "step": 169000
    },
    {
      "epoch": 1.64454969389591,
      "grad_norm": 0.2712656259536743,
      "learning_rate": 3.370833333333334e-05,
      "loss": 0.4044,
      "step": 169100
    },
    {
      "epoch": 1.6455222247616084,
      "grad_norm": 0.2739052474498749,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.4031,
      "step": 169200
    },
    {
      "epoch": 1.6464947556273066,
      "grad_norm": 0.2725251019001007,
      "learning_rate": 3.3625000000000004e-05,
      "loss": 0.4031,
      "step": 169300
    },
    {
      "epoch": 1.6474672864930051,
      "grad_norm": 0.2647170126438141,
      "learning_rate": 3.3583333333333334e-05,
      "loss": 0.4032,
      "step": 169400
    },
    {
      "epoch": 1.6484398173587034,
      "grad_norm": 0.24014964699745178,
      "learning_rate": 3.3541666666666664e-05,
      "loss": 0.4036,
      "step": 169500
    },
    {
      "epoch": 1.6494123482244016,
      "grad_norm": 0.2610302269458771,
      "learning_rate": 3.35e-05,
      "loss": 0.4032,
      "step": 169600
    },
    {
      "epoch": 1.6503848790901001,
      "grad_norm": 0.25893014669418335,
      "learning_rate": 3.345833333333334e-05,
      "loss": 0.4033,
      "step": 169700
    },
    {
      "epoch": 1.6513574099557986,
      "grad_norm": 0.2500803470611572,
      "learning_rate": 3.341666666666667e-05,
      "loss": 0.4024,
      "step": 169800
    },
    {
      "epoch": 1.6523299408214969,
      "grad_norm": 0.24101057648658752,
      "learning_rate": 3.3375e-05,
      "loss": 0.403,
      "step": 169900
    },
    {
      "epoch": 1.6533024716871951,
      "grad_norm": 0.25490570068359375,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.4028,
      "step": 170000
    },
    {
      "epoch": 1.6533024716871951,
      "eval_accuracy": 0.6687783197527283,
      "eval_loss": 0.40272605419158936,
      "eval_runtime": 4215.075,
      "eval_samples_per_second": 542.043,
      "eval_steps_per_second": 5.421,
      "step": 170000
    },
    {
      "epoch": 1.6542750025528936,
      "grad_norm": 0.27317115664482117,
      "learning_rate": 3.329166666666667e-05,
      "loss": 0.403,
      "step": 170100
    },
    {
      "epoch": 1.6552475334185919,
      "grad_norm": 0.2526010572910309,
      "learning_rate": 3.325e-05,
      "loss": 0.4026,
      "step": 170200
    },
    {
      "epoch": 1.6562200642842901,
      "grad_norm": 0.24939197301864624,
      "learning_rate": 3.320833333333334e-05,
      "loss": 0.4029,
      "step": 170300
    },
    {
      "epoch": 1.6571925951499886,
      "grad_norm": 0.2746925950050354,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.4029,
      "step": 170400
    },
    {
      "epoch": 1.6581651260156869,
      "grad_norm": 0.26782384514808655,
      "learning_rate": 3.3125e-05,
      "loss": 0.4038,
      "step": 170500
    },
    {
      "epoch": 1.6591376568813851,
      "grad_norm": 0.26247209310531616,
      "learning_rate": 3.3083333333333336e-05,
      "loss": 0.4017,
      "step": 170600
    },
    {
      "epoch": 1.6601101877470836,
      "grad_norm": 0.26293209195137024,
      "learning_rate": 3.304166666666667e-05,
      "loss": 0.4025,
      "step": 170700
    },
    {
      "epoch": 1.6610827186127821,
      "grad_norm": 0.23962396383285522,
      "learning_rate": 3.3e-05,
      "loss": 0.4038,
      "step": 170800
    },
    {
      "epoch": 1.6620552494784804,
      "grad_norm": 0.2619897425174713,
      "learning_rate": 3.295833333333333e-05,
      "loss": 0.4026,
      "step": 170900
    },
    {
      "epoch": 1.6630277803441786,
      "grad_norm": 0.28432124853134155,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.4026,
      "step": 171000
    },
    {
      "epoch": 1.6640003112098771,
      "grad_norm": 0.2729610800743103,
      "learning_rate": 3.2875e-05,
      "loss": 0.4024,
      "step": 171100
    },
    {
      "epoch": 1.6649728420755754,
      "grad_norm": 0.2694344222545624,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.4032,
      "step": 171200
    },
    {
      "epoch": 1.6659453729412737,
      "grad_norm": 0.2662404477596283,
      "learning_rate": 3.279166666666667e-05,
      "loss": 0.4036,
      "step": 171300
    },
    {
      "epoch": 1.6669179038069721,
      "grad_norm": 0.2660764753818512,
      "learning_rate": 3.275e-05,
      "loss": 0.4016,
      "step": 171400
    },
    {
      "epoch": 1.6678904346726704,
      "grad_norm": 0.2627689838409424,
      "learning_rate": 3.270833333333333e-05,
      "loss": 0.4021,
      "step": 171500
    },
    {
      "epoch": 1.6688629655383687,
      "grad_norm": 0.27135828137397766,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.4026,
      "step": 171600
    },
    {
      "epoch": 1.6698354964040671,
      "grad_norm": 0.26615121960639954,
      "learning_rate": 3.2625e-05,
      "loss": 0.4036,
      "step": 171700
    },
    {
      "epoch": 1.6708080272697656,
      "grad_norm": 0.2502598166465759,
      "learning_rate": 3.258333333333333e-05,
      "loss": 0.4028,
      "step": 171800
    },
    {
      "epoch": 1.6717805581354637,
      "grad_norm": 0.2727484405040741,
      "learning_rate": 3.254166666666667e-05,
      "loss": 0.4014,
      "step": 171900
    },
    {
      "epoch": 1.6727530890011622,
      "grad_norm": 0.2535943388938904,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.4022,
      "step": 172000
    },
    {
      "epoch": 1.6737256198668606,
      "grad_norm": 0.31023794412612915,
      "learning_rate": 3.2458333333333334e-05,
      "loss": 0.4024,
      "step": 172100
    },
    {
      "epoch": 1.674698150732559,
      "grad_norm": 0.2751990258693695,
      "learning_rate": 3.2416666666666664e-05,
      "loss": 0.4013,
      "step": 172200
    },
    {
      "epoch": 1.6756706815982572,
      "grad_norm": 0.279072642326355,
      "learning_rate": 3.2375e-05,
      "loss": 0.4023,
      "step": 172300
    },
    {
      "epoch": 1.6766432124639556,
      "grad_norm": 0.2580992579460144,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.4018,
      "step": 172400
    },
    {
      "epoch": 1.677615743329654,
      "grad_norm": 0.2704072594642639,
      "learning_rate": 3.229166666666667e-05,
      "loss": 0.4015,
      "step": 172500
    },
    {
      "epoch": 1.6785882741953522,
      "grad_norm": 0.2938663065433502,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.4019,
      "step": 172600
    },
    {
      "epoch": 1.6795608050610507,
      "grad_norm": 0.2713489532470703,
      "learning_rate": 3.2208333333333335e-05,
      "loss": 0.401,
      "step": 172700
    },
    {
      "epoch": 1.6805333359267491,
      "grad_norm": 0.27565932273864746,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.4038,
      "step": 172800
    },
    {
      "epoch": 1.6815058667924472,
      "grad_norm": 0.2791111469268799,
      "learning_rate": 3.2125e-05,
      "loss": 0.4023,
      "step": 172900
    },
    {
      "epoch": 1.6824783976581457,
      "grad_norm": 0.27038004994392395,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.4024,
      "step": 173000
    },
    {
      "epoch": 1.6834509285238441,
      "grad_norm": 0.29162198305130005,
      "learning_rate": 3.204166666666667e-05,
      "loss": 0.4014,
      "step": 173100
    },
    {
      "epoch": 1.6844234593895424,
      "grad_norm": 0.26009902358055115,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.4023,
      "step": 173200
    },
    {
      "epoch": 1.6853959902552407,
      "grad_norm": 0.24683918058872223,
      "learning_rate": 3.1958333333333335e-05,
      "loss": 0.4011,
      "step": 173300
    },
    {
      "epoch": 1.6863685211209392,
      "grad_norm": 0.27240800857543945,
      "learning_rate": 3.1916666666666665e-05,
      "loss": 0.4016,
      "step": 173400
    },
    {
      "epoch": 1.6873410519866374,
      "grad_norm": 0.28134971857070923,
      "learning_rate": 3.1875e-05,
      "loss": 0.4019,
      "step": 173500
    },
    {
      "epoch": 1.6883135828523357,
      "grad_norm": 0.2632810175418854,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.4016,
      "step": 173600
    },
    {
      "epoch": 1.6892861137180342,
      "grad_norm": 0.25760218501091003,
      "learning_rate": 3.179166666666667e-05,
      "loss": 0.4022,
      "step": 173700
    },
    {
      "epoch": 1.6902586445837324,
      "grad_norm": 0.27003541588783264,
      "learning_rate": 3.175e-05,
      "loss": 0.4012,
      "step": 173800
    },
    {
      "epoch": 1.6912311754494307,
      "grad_norm": 0.2810216546058655,
      "learning_rate": 3.1708333333333336e-05,
      "loss": 0.4016,
      "step": 173900
    },
    {
      "epoch": 1.6922037063151292,
      "grad_norm": 0.2727639675140381,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.4017,
      "step": 174000
    },
    {
      "epoch": 1.6931762371808277,
      "grad_norm": 0.27607443928718567,
      "learning_rate": 3.1624999999999996e-05,
      "loss": 0.4009,
      "step": 174100
    },
    {
      "epoch": 1.694148768046526,
      "grad_norm": 0.2714710831642151,
      "learning_rate": 3.158333333333334e-05,
      "loss": 0.4012,
      "step": 174200
    },
    {
      "epoch": 1.6951212989122242,
      "grad_norm": 0.26288339495658875,
      "learning_rate": 3.154166666666667e-05,
      "loss": 0.4021,
      "step": 174300
    },
    {
      "epoch": 1.6960938297779227,
      "grad_norm": 0.2686326801776886,
      "learning_rate": 3.15e-05,
      "loss": 0.4007,
      "step": 174400
    },
    {
      "epoch": 1.697066360643621,
      "grad_norm": 0.2743627429008484,
      "learning_rate": 3.145833333333334e-05,
      "loss": 0.4023,
      "step": 174500
    },
    {
      "epoch": 1.6980388915093192,
      "grad_norm": 0.25062376260757446,
      "learning_rate": 3.141666666666667e-05,
      "loss": 0.4014,
      "step": 174600
    },
    {
      "epoch": 1.6990114223750177,
      "grad_norm": 0.27869969606399536,
      "learning_rate": 3.1375e-05,
      "loss": 0.4011,
      "step": 174700
    },
    {
      "epoch": 1.699983953240716,
      "grad_norm": 0.26495859026908875,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.4013,
      "step": 174800
    },
    {
      "epoch": 1.7009564841064142,
      "grad_norm": 0.2748548984527588,
      "learning_rate": 3.129166666666667e-05,
      "loss": 0.4013,
      "step": 174900
    },
    {
      "epoch": 1.7019290149721127,
      "grad_norm": 0.2856152057647705,
      "learning_rate": 3.125e-05,
      "loss": 0.4029,
      "step": 175000
    },
    {
      "epoch": 1.7029015458378112,
      "grad_norm": 0.27891048789024353,
      "learning_rate": 3.120833333333333e-05,
      "loss": 0.4014,
      "step": 175100
    },
    {
      "epoch": 1.7038740767035094,
      "grad_norm": 0.26425859332084656,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.4011,
      "step": 175200
    },
    {
      "epoch": 1.7048466075692077,
      "grad_norm": 0.2582915127277374,
      "learning_rate": 3.1125000000000004e-05,
      "loss": 0.401,
      "step": 175300
    },
    {
      "epoch": 1.7058191384349062,
      "grad_norm": 0.2604307234287262,
      "learning_rate": 3.1083333333333334e-05,
      "loss": 0.401,
      "step": 175400
    },
    {
      "epoch": 1.7067916693006044,
      "grad_norm": 0.2802254855632782,
      "learning_rate": 3.104166666666667e-05,
      "loss": 0.4003,
      "step": 175500
    },
    {
      "epoch": 1.7077642001663027,
      "grad_norm": 0.28250542283058167,
      "learning_rate": 3.1e-05,
      "loss": 0.4003,
      "step": 175600
    },
    {
      "epoch": 1.7087367310320012,
      "grad_norm": 0.28869232535362244,
      "learning_rate": 3.095833333333333e-05,
      "loss": 0.4012,
      "step": 175700
    },
    {
      "epoch": 1.7097092618976995,
      "grad_norm": 0.2706592381000519,
      "learning_rate": 3.091666666666667e-05,
      "loss": 0.4009,
      "step": 175800
    },
    {
      "epoch": 1.7106817927633977,
      "grad_norm": 0.2723744511604309,
      "learning_rate": 3.0875000000000005e-05,
      "loss": 0.4015,
      "step": 175900
    },
    {
      "epoch": 1.7116543236290962,
      "grad_norm": 0.269098699092865,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.4007,
      "step": 176000
    },
    {
      "epoch": 1.7126268544947947,
      "grad_norm": 0.2590506374835968,
      "learning_rate": 3.079166666666667e-05,
      "loss": 0.4003,
      "step": 176100
    },
    {
      "epoch": 1.7135993853604927,
      "grad_norm": 0.24864259362220764,
      "learning_rate": 3.075e-05,
      "loss": 0.3999,
      "step": 176200
    },
    {
      "epoch": 1.7145719162261912,
      "grad_norm": 0.2528771460056305,
      "learning_rate": 3.070833333333333e-05,
      "loss": 0.4013,
      "step": 176300
    },
    {
      "epoch": 1.7155444470918897,
      "grad_norm": 0.27021321654319763,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.4012,
      "step": 176400
    },
    {
      "epoch": 1.716516977957588,
      "grad_norm": 0.281851202249527,
      "learning_rate": 3.0625000000000006e-05,
      "loss": 0.4009,
      "step": 176500
    },
    {
      "epoch": 1.7174895088232862,
      "grad_norm": 0.2655010521411896,
      "learning_rate": 3.0583333333333336e-05,
      "loss": 0.4004,
      "step": 176600
    },
    {
      "epoch": 1.7184620396889847,
      "grad_norm": 0.2709862291812897,
      "learning_rate": 3.0541666666666666e-05,
      "loss": 0.4019,
      "step": 176700
    },
    {
      "epoch": 1.719434570554683,
      "grad_norm": 0.25920170545578003,
      "learning_rate": 3.05e-05,
      "loss": 0.4014,
      "step": 176800
    },
    {
      "epoch": 1.7204071014203812,
      "grad_norm": 0.2655830383300781,
      "learning_rate": 3.0458333333333333e-05,
      "loss": 0.3997,
      "step": 176900
    },
    {
      "epoch": 1.7213796322860797,
      "grad_norm": 0.27070000767707825,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.4015,
      "step": 177000
    },
    {
      "epoch": 1.7223521631517782,
      "grad_norm": 0.2805749177932739,
      "learning_rate": 3.0375000000000003e-05,
      "loss": 0.4008,
      "step": 177100
    },
    {
      "epoch": 1.7233246940174762,
      "grad_norm": 0.2770942449569702,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.4028,
      "step": 177200
    },
    {
      "epoch": 1.7242972248831747,
      "grad_norm": 0.2852897644042969,
      "learning_rate": 3.0291666666666667e-05,
      "loss": 0.4075,
      "step": 177300
    },
    {
      "epoch": 1.7252697557488732,
      "grad_norm": 0.2892535328865051,
      "learning_rate": 3.025e-05,
      "loss": 0.4063,
      "step": 177400
    },
    {
      "epoch": 1.7262422866145715,
      "grad_norm": 0.2679862082004547,
      "learning_rate": 3.0208333333333334e-05,
      "loss": 0.4077,
      "step": 177500
    },
    {
      "epoch": 1.7272148174802697,
      "grad_norm": 0.28533780574798584,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.4073,
      "step": 177600
    },
    {
      "epoch": 1.7281873483459682,
      "grad_norm": 0.28116297721862793,
      "learning_rate": 3.0125000000000004e-05,
      "loss": 0.4078,
      "step": 177700
    },
    {
      "epoch": 1.7291598792116665,
      "grad_norm": 0.26025229692459106,
      "learning_rate": 3.0083333333333337e-05,
      "loss": 0.4065,
      "step": 177800
    },
    {
      "epoch": 1.7301324100773647,
      "grad_norm": 0.270214706659317,
      "learning_rate": 3.0041666666666667e-05,
      "loss": 0.4069,
      "step": 177900
    },
    {
      "epoch": 1.7311049409430632,
      "grad_norm": 0.2747398614883423,
      "learning_rate": 3e-05,
      "loss": 0.407,
      "step": 178000
    },
    {
      "epoch": 1.7320774718087615,
      "grad_norm": 0.2523138225078583,
      "learning_rate": 2.9958333333333334e-05,
      "loss": 0.4064,
      "step": 178100
    },
    {
      "epoch": 1.7330500026744597,
      "grad_norm": 0.2660115659236908,
      "learning_rate": 2.991666666666667e-05,
      "loss": 0.4066,
      "step": 178200
    },
    {
      "epoch": 1.7340225335401582,
      "grad_norm": 0.28704652190208435,
      "learning_rate": 2.9875000000000004e-05,
      "loss": 0.4082,
      "step": 178300
    },
    {
      "epoch": 1.7349950644058567,
      "grad_norm": 0.2542400062084198,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.4065,
      "step": 178400
    },
    {
      "epoch": 1.735967595271555,
      "grad_norm": 0.2558864951133728,
      "learning_rate": 2.9791666666666668e-05,
      "loss": 0.4074,
      "step": 178500
    },
    {
      "epoch": 1.7369401261372532,
      "grad_norm": 0.27275019884109497,
      "learning_rate": 2.975e-05,
      "loss": 0.4072,
      "step": 178600
    },
    {
      "epoch": 1.7379126570029517,
      "grad_norm": 0.2765890061855316,
      "learning_rate": 2.970833333333333e-05,
      "loss": 0.4068,
      "step": 178700
    },
    {
      "epoch": 1.73888518786865,
      "grad_norm": 0.2669787108898163,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.4072,
      "step": 178800
    },
    {
      "epoch": 1.7398577187343482,
      "grad_norm": 0.26945674419403076,
      "learning_rate": 2.9625000000000002e-05,
      "loss": 0.4063,
      "step": 178900
    },
    {
      "epoch": 1.7408302496000467,
      "grad_norm": 0.28219500184059143,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.4063,
      "step": 179000
    },
    {
      "epoch": 1.741802780465745,
      "grad_norm": 0.26826944947242737,
      "learning_rate": 2.954166666666667e-05,
      "loss": 0.4059,
      "step": 179100
    },
    {
      "epoch": 1.7427753113314433,
      "grad_norm": 0.2931509017944336,
      "learning_rate": 2.95e-05,
      "loss": 0.4065,
      "step": 179200
    },
    {
      "epoch": 1.7437478421971417,
      "grad_norm": 0.2623171806335449,
      "learning_rate": 2.9458333333333332e-05,
      "loss": 0.4066,
      "step": 179300
    },
    {
      "epoch": 1.7447203730628402,
      "grad_norm": 0.2721937894821167,
      "learning_rate": 2.941666666666667e-05,
      "loss": 0.4064,
      "step": 179400
    },
    {
      "epoch": 1.7456929039285385,
      "grad_norm": 0.24698933959007263,
      "learning_rate": 2.9375000000000003e-05,
      "loss": 0.406,
      "step": 179500
    },
    {
      "epoch": 1.7466654347942367,
      "grad_norm": 0.2642591893672943,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.4062,
      "step": 179600
    },
    {
      "epoch": 1.7476379656599352,
      "grad_norm": 0.2801448106765747,
      "learning_rate": 2.9291666666666666e-05,
      "loss": 0.4063,
      "step": 179700
    },
    {
      "epoch": 1.7486104965256335,
      "grad_norm": 0.26731762290000916,
      "learning_rate": 2.925e-05,
      "loss": 0.4065,
      "step": 179800
    },
    {
      "epoch": 1.7495830273913318,
      "grad_norm": 0.2800140976905823,
      "learning_rate": 2.9208333333333333e-05,
      "loss": 0.4054,
      "step": 179900
    },
    {
      "epoch": 1.7505555582570302,
      "grad_norm": 0.2594730854034424,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.4061,
      "step": 180000
    },
    {
      "epoch": 1.7505555582570302,
      "eval_accuracy": 0.6690384931601735,
      "eval_loss": 0.40077826380729675,
      "eval_runtime": 3912.1166,
      "eval_samples_per_second": 584.019,
      "eval_steps_per_second": 5.84,
      "step": 180000
    },
    {
      "epoch": 1.751532951777057,
      "grad_norm": 0.2876311242580414,
      "learning_rate": 2.9125000000000003e-05,
      "loss": 0.4059,
      "step": 180100
    },
    {
      "epoch": 1.7525054826427553,
      "grad_norm": 0.28033527731895447,
      "learning_rate": 2.9083333333333333e-05,
      "loss": 0.4062,
      "step": 180200
    },
    {
      "epoch": 1.7534780135084538,
      "grad_norm": 0.2756671607494354,
      "learning_rate": 2.9041666666666667e-05,
      "loss": 0.4051,
      "step": 180300
    },
    {
      "epoch": 1.754450544374152,
      "grad_norm": 0.24284274876117706,
      "learning_rate": 2.9e-05,
      "loss": 0.4071,
      "step": 180400
    },
    {
      "epoch": 1.7554230752398503,
      "grad_norm": 0.25285324454307556,
      "learning_rate": 2.8958333333333337e-05,
      "loss": 0.4054,
      "step": 180500
    },
    {
      "epoch": 1.7563956061055488,
      "grad_norm": 0.2765139639377594,
      "learning_rate": 2.891666666666667e-05,
      "loss": 0.4058,
      "step": 180600
    },
    {
      "epoch": 1.7573681369712473,
      "grad_norm": 0.2681090533733368,
      "learning_rate": 2.8875e-05,
      "loss": 0.4054,
      "step": 180700
    },
    {
      "epoch": 1.7583406678369453,
      "grad_norm": 0.2525911033153534,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.4067,
      "step": 180800
    },
    {
      "epoch": 1.7593131987026438,
      "grad_norm": 0.26088660955429077,
      "learning_rate": 2.8791666666666667e-05,
      "loss": 0.4062,
      "step": 180900
    },
    {
      "epoch": 1.7602857295683423,
      "grad_norm": 0.2812153995037079,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.4069,
      "step": 181000
    },
    {
      "epoch": 1.7612582604340405,
      "grad_norm": 0.26314160227775574,
      "learning_rate": 2.8708333333333338e-05,
      "loss": 0.4052,
      "step": 181100
    },
    {
      "epoch": 1.7622307912997388,
      "grad_norm": 0.25943395495414734,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.4058,
      "step": 181200
    },
    {
      "epoch": 1.7632033221654373,
      "grad_norm": 0.277410089969635,
      "learning_rate": 2.8625e-05,
      "loss": 0.407,
      "step": 181300
    },
    {
      "epoch": 1.7641758530311356,
      "grad_norm": 0.27614688873291016,
      "learning_rate": 2.8583333333333335e-05,
      "loss": 0.4056,
      "step": 181400
    },
    {
      "epoch": 1.7651483838968338,
      "grad_norm": 0.2753792405128479,
      "learning_rate": 2.8541666666666668e-05,
      "loss": 0.4064,
      "step": 181500
    },
    {
      "epoch": 1.7661209147625323,
      "grad_norm": 0.2727391719818115,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.4055,
      "step": 181600
    },
    {
      "epoch": 1.7670934456282308,
      "grad_norm": 0.3093445599079132,
      "learning_rate": 2.845833333333334e-05,
      "loss": 0.4061,
      "step": 181700
    },
    {
      "epoch": 1.7680659764939288,
      "grad_norm": 0.24150539934635162,
      "learning_rate": 2.841666666666667e-05,
      "loss": 0.4071,
      "step": 181800
    },
    {
      "epoch": 1.7690385073596273,
      "grad_norm": 0.2747388184070587,
      "learning_rate": 2.8375000000000002e-05,
      "loss": 0.4055,
      "step": 181900
    },
    {
      "epoch": 1.7700110382253258,
      "grad_norm": 0.2622022032737732,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.4058,
      "step": 182000
    },
    {
      "epoch": 1.770983569091024,
      "grad_norm": 0.2707992196083069,
      "learning_rate": 2.8291666666666665e-05,
      "loss": 0.4062,
      "step": 182100
    },
    {
      "epoch": 1.7719560999567223,
      "grad_norm": 0.26148825883865356,
      "learning_rate": 2.825e-05,
      "loss": 0.4062,
      "step": 182200
    },
    {
      "epoch": 1.7729286308224208,
      "grad_norm": 0.25241440534591675,
      "learning_rate": 2.8208333333333336e-05,
      "loss": 0.4061,
      "step": 182300
    },
    {
      "epoch": 1.773901161688119,
      "grad_norm": 0.2881399989128113,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.405,
      "step": 182400
    },
    {
      "epoch": 1.7748736925538173,
      "grad_norm": 0.2762109637260437,
      "learning_rate": 2.8125000000000003e-05,
      "loss": 0.4051,
      "step": 182500
    },
    {
      "epoch": 1.7758462234195158,
      "grad_norm": 0.2568463385105133,
      "learning_rate": 2.8083333333333333e-05,
      "loss": 0.407,
      "step": 182600
    },
    {
      "epoch": 1.776818754285214,
      "grad_norm": 0.29086002707481384,
      "learning_rate": 2.8041666666666666e-05,
      "loss": 0.406,
      "step": 182700
    },
    {
      "epoch": 1.7777912851509123,
      "grad_norm": 0.26238319277763367,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.4061,
      "step": 182800
    },
    {
      "epoch": 1.7787638160166108,
      "grad_norm": 0.27115944027900696,
      "learning_rate": 2.7958333333333336e-05,
      "loss": 0.4058,
      "step": 182900
    },
    {
      "epoch": 1.7797363468823093,
      "grad_norm": 0.27818694710731506,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.4056,
      "step": 183000
    },
    {
      "epoch": 1.7807088777480076,
      "grad_norm": 0.2870015501976013,
      "learning_rate": 2.7875e-05,
      "loss": 0.4061,
      "step": 183100
    },
    {
      "epoch": 1.7816814086137058,
      "grad_norm": 0.2696670889854431,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.4064,
      "step": 183200
    },
    {
      "epoch": 1.7826539394794043,
      "grad_norm": 0.2879292368888855,
      "learning_rate": 2.7791666666666667e-05,
      "loss": 0.4058,
      "step": 183300
    },
    {
      "epoch": 1.7836264703451026,
      "grad_norm": 0.25725144147872925,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.4066,
      "step": 183400
    },
    {
      "epoch": 1.7845990012108008,
      "grad_norm": 0.26976650953292847,
      "learning_rate": 2.7708333333333337e-05,
      "loss": 0.4066,
      "step": 183500
    },
    {
      "epoch": 1.7855715320764993,
      "grad_norm": 0.25895145535469055,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.405,
      "step": 183600
    },
    {
      "epoch": 1.7865440629421976,
      "grad_norm": 0.27469655871391296,
      "learning_rate": 2.7625e-05,
      "loss": 0.4047,
      "step": 183700
    },
    {
      "epoch": 1.7875165938078958,
      "grad_norm": 0.26718851923942566,
      "learning_rate": 2.7583333333333334e-05,
      "loss": 0.4054,
      "step": 183800
    },
    {
      "epoch": 1.7884891246735943,
      "grad_norm": 0.24899257719516754,
      "learning_rate": 2.7541666666666664e-05,
      "loss": 0.4059,
      "step": 183900
    },
    {
      "epoch": 1.7894616555392928,
      "grad_norm": 0.2716010510921478,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.4049,
      "step": 184000
    },
    {
      "epoch": 1.790434186404991,
      "grad_norm": 0.2708175778388977,
      "learning_rate": 2.7458333333333334e-05,
      "loss": 0.4042,
      "step": 184100
    },
    {
      "epoch": 1.7914067172706893,
      "grad_norm": 0.2829821705818176,
      "learning_rate": 2.7416666666666668e-05,
      "loss": 0.4049,
      "step": 184200
    },
    {
      "epoch": 1.7923792481363878,
      "grad_norm": 0.26861807703971863,
      "learning_rate": 2.7375e-05,
      "loss": 0.4051,
      "step": 184300
    },
    {
      "epoch": 1.793351779002086,
      "grad_norm": 0.28600215911865234,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.4036,
      "step": 184400
    },
    {
      "epoch": 1.7943243098677844,
      "grad_norm": 0.2551890015602112,
      "learning_rate": 2.7291666666666665e-05,
      "loss": 0.4043,
      "step": 184500
    },
    {
      "epoch": 1.7952968407334828,
      "grad_norm": 0.2808477282524109,
      "learning_rate": 2.725e-05,
      "loss": 0.4054,
      "step": 184600
    },
    {
      "epoch": 1.796269371599181,
      "grad_norm": 0.293133407831192,
      "learning_rate": 2.7208333333333335e-05,
      "loss": 0.4051,
      "step": 184700
    },
    {
      "epoch": 1.7972419024648794,
      "grad_norm": 0.26327967643737793,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.4058,
      "step": 184800
    },
    {
      "epoch": 1.7982144333305778,
      "grad_norm": 0.2734159231185913,
      "learning_rate": 2.7125000000000002e-05,
      "loss": 0.4053,
      "step": 184900
    },
    {
      "epoch": 1.7991869641962763,
      "grad_norm": 0.2815954387187958,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.4055,
      "step": 185000
    },
    {
      "epoch": 1.8001594950619744,
      "grad_norm": 0.2560538649559021,
      "learning_rate": 2.7041666666666672e-05,
      "loss": 0.4039,
      "step": 185100
    },
    {
      "epoch": 1.8011320259276729,
      "grad_norm": 0.2583199441432953,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.405,
      "step": 185200
    },
    {
      "epoch": 1.8021045567933713,
      "grad_norm": 0.24617229402065277,
      "learning_rate": 2.6958333333333336e-05,
      "loss": 0.4048,
      "step": 185300
    },
    {
      "epoch": 1.8030770876590696,
      "grad_norm": 0.3021530508995056,
      "learning_rate": 2.691666666666667e-05,
      "loss": 0.4054,
      "step": 185400
    },
    {
      "epoch": 1.8040496185247679,
      "grad_norm": 0.25739094614982605,
      "learning_rate": 2.6875e-05,
      "loss": 0.4058,
      "step": 185500
    },
    {
      "epoch": 1.8050221493904663,
      "grad_norm": 0.24951189756393433,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.4047,
      "step": 185600
    },
    {
      "epoch": 1.8059946802561646,
      "grad_norm": 0.28394410014152527,
      "learning_rate": 2.679166666666667e-05,
      "loss": 0.4053,
      "step": 185700
    },
    {
      "epoch": 1.8069672111218629,
      "grad_norm": 0.2694568634033203,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.4046,
      "step": 185800
    },
    {
      "epoch": 1.8079397419875614,
      "grad_norm": 0.24679788947105408,
      "learning_rate": 2.6708333333333337e-05,
      "loss": 0.4048,
      "step": 185900
    },
    {
      "epoch": 1.8089122728532598,
      "grad_norm": 0.2702367901802063,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.404,
      "step": 186000
    },
    {
      "epoch": 1.8098848037189579,
      "grad_norm": 0.2670100927352905,
      "learning_rate": 2.6625e-05,
      "loss": 0.4057,
      "step": 186100
    },
    {
      "epoch": 1.8108573345846564,
      "grad_norm": 0.2700839638710022,
      "learning_rate": 2.6583333333333333e-05,
      "loss": 0.4056,
      "step": 186200
    },
    {
      "epoch": 1.8118298654503548,
      "grad_norm": 0.275020569562912,
      "learning_rate": 2.654166666666667e-05,
      "loss": 0.4045,
      "step": 186300
    },
    {
      "epoch": 1.812802396316053,
      "grad_norm": 0.279360294342041,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.4058,
      "step": 186400
    },
    {
      "epoch": 1.8137749271817514,
      "grad_norm": 0.26031678915023804,
      "learning_rate": 2.6458333333333334e-05,
      "loss": 0.4048,
      "step": 186500
    },
    {
      "epoch": 1.8147474580474499,
      "grad_norm": 0.2669851779937744,
      "learning_rate": 2.6416666666666667e-05,
      "loss": 0.4039,
      "step": 186600
    },
    {
      "epoch": 1.8157199889131481,
      "grad_norm": 0.26771771907806396,
      "learning_rate": 2.6375e-05,
      "loss": 0.4049,
      "step": 186700
    },
    {
      "epoch": 1.8166925197788464,
      "grad_norm": 0.259054958820343,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.4056,
      "step": 186800
    },
    {
      "epoch": 1.8176650506445449,
      "grad_norm": 0.260739803314209,
      "learning_rate": 2.629166666666667e-05,
      "loss": 0.4038,
      "step": 186900
    },
    {
      "epoch": 1.8186375815102431,
      "grad_norm": 0.26015862822532654,
      "learning_rate": 2.625e-05,
      "loss": 0.4041,
      "step": 187000
    },
    {
      "epoch": 1.8196101123759414,
      "grad_norm": 0.2793867290019989,
      "learning_rate": 2.6208333333333335e-05,
      "loss": 0.404,
      "step": 187100
    },
    {
      "epoch": 1.8205826432416399,
      "grad_norm": 0.2778339982032776,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.4052,
      "step": 187200
    },
    {
      "epoch": 1.8215551741073384,
      "grad_norm": 0.25893065333366394,
      "learning_rate": 2.6124999999999998e-05,
      "loss": 0.4043,
      "step": 187300
    },
    {
      "epoch": 1.8225277049730366,
      "grad_norm": 0.2686000168323517,
      "learning_rate": 2.608333333333333e-05,
      "loss": 0.4045,
      "step": 187400
    },
    {
      "epoch": 1.8235002358387349,
      "grad_norm": 0.2840152084827423,
      "learning_rate": 2.604166666666667e-05,
      "loss": 0.4035,
      "step": 187500
    },
    {
      "epoch": 1.8244727667044334,
      "grad_norm": 0.27501025795936584,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4046,
      "step": 187600
    },
    {
      "epoch": 1.8254452975701316,
      "grad_norm": 0.26236024498939514,
      "learning_rate": 2.5958333333333335e-05,
      "loss": 0.4054,
      "step": 187700
    },
    {
      "epoch": 1.82641782843583,
      "grad_norm": 0.2779146730899811,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.4045,
      "step": 187800
    },
    {
      "epoch": 1.8273903593015284,
      "grad_norm": 0.2805993854999542,
      "learning_rate": 2.5875e-05,
      "loss": 0.4042,
      "step": 187900
    },
    {
      "epoch": 1.8283628901672266,
      "grad_norm": 0.29049187898635864,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.4045,
      "step": 188000
    },
    {
      "epoch": 1.829335421032925,
      "grad_norm": 0.2807929813861847,
      "learning_rate": 2.579166666666667e-05,
      "loss": 0.4056,
      "step": 188100
    },
    {
      "epoch": 1.8303079518986234,
      "grad_norm": 0.280754953622818,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.4049,
      "step": 188200
    },
    {
      "epoch": 1.8312804827643219,
      "grad_norm": 0.28647321462631226,
      "learning_rate": 2.5708333333333336e-05,
      "loss": 0.4048,
      "step": 188300
    },
    {
      "epoch": 1.8322530136300201,
      "grad_norm": 0.28533878922462463,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.404,
      "step": 188400
    },
    {
      "epoch": 1.8332255444957184,
      "grad_norm": 0.27757638692855835,
      "learning_rate": 2.5625e-05,
      "loss": 0.4051,
      "step": 188500
    },
    {
      "epoch": 1.8341980753614169,
      "grad_norm": 0.26311060786247253,
      "learning_rate": 2.5583333333333336e-05,
      "loss": 0.4053,
      "step": 188600
    },
    {
      "epoch": 1.8351706062271151,
      "grad_norm": 0.2842138409614563,
      "learning_rate": 2.554166666666667e-05,
      "loss": 0.4042,
      "step": 188700
    },
    {
      "epoch": 1.8361431370928134,
      "grad_norm": 0.2682960331439972,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.4041,
      "step": 188800
    },
    {
      "epoch": 1.8371156679585119,
      "grad_norm": 0.27300572395324707,
      "learning_rate": 2.5458333333333333e-05,
      "loss": 0.4031,
      "step": 188900
    },
    {
      "epoch": 1.8380881988242102,
      "grad_norm": 0.2674650251865387,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.4045,
      "step": 189000
    },
    {
      "epoch": 1.8390607296899084,
      "grad_norm": 0.27988380193710327,
      "learning_rate": 2.5375e-05,
      "loss": 0.4037,
      "step": 189100
    },
    {
      "epoch": 1.840033260555607,
      "grad_norm": 0.25884974002838135,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.4042,
      "step": 189200
    },
    {
      "epoch": 1.8410057914213054,
      "grad_norm": 0.2846849858760834,
      "learning_rate": 2.529166666666667e-05,
      "loss": 0.4037,
      "step": 189300
    },
    {
      "epoch": 1.8419783222870034,
      "grad_norm": 0.28026920557022095,
      "learning_rate": 2.525e-05,
      "loss": 0.4052,
      "step": 189400
    },
    {
      "epoch": 1.842950853152702,
      "grad_norm": 0.2661326825618744,
      "learning_rate": 2.5208333333333334e-05,
      "loss": 0.4041,
      "step": 189500
    },
    {
      "epoch": 1.8439233840184004,
      "grad_norm": 0.2766348719596863,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.4031,
      "step": 189600
    },
    {
      "epoch": 1.8448959148840987,
      "grad_norm": 0.26329585909843445,
      "learning_rate": 2.5124999999999997e-05,
      "loss": 0.405,
      "step": 189700
    },
    {
      "epoch": 1.845868445749797,
      "grad_norm": 0.24946317076683044,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.4037,
      "step": 189800
    },
    {
      "epoch": 1.8468409766154954,
      "grad_norm": 0.2719127833843231,
      "learning_rate": 2.5041666666666668e-05,
      "loss": 0.4044,
      "step": 189900
    },
    {
      "epoch": 1.8478135074811937,
      "grad_norm": 0.2625379264354706,
      "learning_rate": 2.5e-05,
      "loss": 0.4046,
      "step": 190000
    },
    {
      "epoch": 1.8478135074811937,
      "eval_accuracy": 0.6693465227661268,
      "eval_loss": 0.39863136410713196,
      "eval_runtime": 3963.5317,
      "eval_samples_per_second": 576.443,
      "eval_steps_per_second": 5.765,
      "step": 190000
    },
    {
      "epoch": 1.848786038346892,
      "grad_norm": 0.28903928399086,
      "learning_rate": 2.4958333333333335e-05,
      "loss": 0.404,
      "step": 190100
    },
    {
      "epoch": 1.8497585692125904,
      "grad_norm": 0.2751901149749756,
      "learning_rate": 2.4916666666666668e-05,
      "loss": 0.4049,
      "step": 190200
    },
    {
      "epoch": 1.850731100078289,
      "grad_norm": 0.28154975175857544,
      "learning_rate": 2.4875e-05,
      "loss": 0.4043,
      "step": 190300
    },
    {
      "epoch": 1.851703630943987,
      "grad_norm": 0.269827663898468,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.4045,
      "step": 190400
    },
    {
      "epoch": 1.8526761618096854,
      "grad_norm": 0.29393887519836426,
      "learning_rate": 2.479166666666667e-05,
      "loss": 0.4048,
      "step": 190500
    },
    {
      "epoch": 1.853648692675384,
      "grad_norm": 0.2626855671405792,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 0.4039,
      "step": 190600
    },
    {
      "epoch": 1.8546212235410822,
      "grad_norm": 0.2787182331085205,
      "learning_rate": 2.4708333333333332e-05,
      "loss": 0.404,
      "step": 190700
    },
    {
      "epoch": 1.8555937544067804,
      "grad_norm": 0.253605455160141,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.4039,
      "step": 190800
    },
    {
      "epoch": 1.856566285272479,
      "grad_norm": 0.27295082807540894,
      "learning_rate": 2.4625000000000002e-05,
      "loss": 0.4046,
      "step": 190900
    },
    {
      "epoch": 1.8575388161381772,
      "grad_norm": 0.26909077167510986,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.4041,
      "step": 191000
    },
    {
      "epoch": 1.8585113470038754,
      "grad_norm": 0.248287171125412,
      "learning_rate": 2.454166666666667e-05,
      "loss": 0.4033,
      "step": 191100
    },
    {
      "epoch": 1.859483877869574,
      "grad_norm": 0.26953575015068054,
      "learning_rate": 2.45e-05,
      "loss": 0.4035,
      "step": 191200
    },
    {
      "epoch": 1.8604564087352722,
      "grad_norm": 0.2704882025718689,
      "learning_rate": 2.4458333333333336e-05,
      "loss": 0.4038,
      "step": 191300
    },
    {
      "epoch": 1.8614289396009704,
      "grad_norm": 0.2736867070198059,
      "learning_rate": 2.441666666666667e-05,
      "loss": 0.4043,
      "step": 191400
    },
    {
      "epoch": 1.862401470466669,
      "grad_norm": 0.26510128378868103,
      "learning_rate": 2.4375e-05,
      "loss": 0.4044,
      "step": 191500
    },
    {
      "epoch": 1.8633740013323674,
      "grad_norm": 0.26749861240386963,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.4038,
      "step": 191600
    },
    {
      "epoch": 1.8643465321980657,
      "grad_norm": 0.25878068804740906,
      "learning_rate": 2.4291666666666666e-05,
      "loss": 0.404,
      "step": 191700
    },
    {
      "epoch": 1.865319063063764,
      "grad_norm": 0.30491122603416443,
      "learning_rate": 2.425e-05,
      "loss": 0.4054,
      "step": 191800
    },
    {
      "epoch": 1.8662915939294624,
      "grad_norm": 0.28618124127388,
      "learning_rate": 2.4208333333333337e-05,
      "loss": 0.4042,
      "step": 191900
    },
    {
      "epoch": 1.8672641247951607,
      "grad_norm": 0.26271212100982666,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.4032,
      "step": 192000
    },
    {
      "epoch": 1.868236655660859,
      "grad_norm": 0.2703900635242462,
      "learning_rate": 2.4125e-05,
      "loss": 0.4022,
      "step": 192100
    },
    {
      "epoch": 1.8692091865265574,
      "grad_norm": 0.2772201597690582,
      "learning_rate": 2.4083333333333337e-05,
      "loss": 0.4034,
      "step": 192200
    },
    {
      "epoch": 1.8701817173922557,
      "grad_norm": 0.278281033039093,
      "learning_rate": 2.4041666666666667e-05,
      "loss": 0.403,
      "step": 192300
    },
    {
      "epoch": 1.871154248257954,
      "grad_norm": 0.2584460973739624,
      "learning_rate": 2.4e-05,
      "loss": 0.4035,
      "step": 192400
    },
    {
      "epoch": 1.8721267791236524,
      "grad_norm": 0.27366364002227783,
      "learning_rate": 2.3958333333333334e-05,
      "loss": 0.4029,
      "step": 192500
    },
    {
      "epoch": 1.873099309989351,
      "grad_norm": 0.26113811135292053,
      "learning_rate": 2.3916666666666668e-05,
      "loss": 0.403,
      "step": 192600
    },
    {
      "epoch": 1.8740718408550492,
      "grad_norm": 0.2897797226905823,
      "learning_rate": 2.3875e-05,
      "loss": 0.404,
      "step": 192700
    },
    {
      "epoch": 1.8750443717207474,
      "grad_norm": 0.257186621427536,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.4035,
      "step": 192800
    },
    {
      "epoch": 1.876016902586446,
      "grad_norm": 0.25706058740615845,
      "learning_rate": 2.3791666666666668e-05,
      "loss": 0.4032,
      "step": 192900
    },
    {
      "epoch": 1.8769894334521442,
      "grad_norm": 0.27284181118011475,
      "learning_rate": 2.375e-05,
      "loss": 0.4033,
      "step": 193000
    },
    {
      "epoch": 1.8779619643178425,
      "grad_norm": 0.27403777837753296,
      "learning_rate": 2.3708333333333335e-05,
      "loss": 0.4029,
      "step": 193100
    },
    {
      "epoch": 1.878934495183541,
      "grad_norm": 0.2902626097202301,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.4031,
      "step": 193200
    },
    {
      "epoch": 1.8799070260492392,
      "grad_norm": 0.26637595891952515,
      "learning_rate": 2.3624999999999998e-05,
      "loss": 0.4026,
      "step": 193300
    },
    {
      "epoch": 1.8808795569149375,
      "grad_norm": 0.2745942175388336,
      "learning_rate": 2.3583333333333335e-05,
      "loss": 0.4039,
      "step": 193400
    },
    {
      "epoch": 1.881852087780636,
      "grad_norm": 0.27363279461860657,
      "learning_rate": 2.354166666666667e-05,
      "loss": 0.4044,
      "step": 193500
    },
    {
      "epoch": 1.8828246186463344,
      "grad_norm": 0.29120054841041565,
      "learning_rate": 2.35e-05,
      "loss": 0.4033,
      "step": 193600
    },
    {
      "epoch": 1.8837971495120325,
      "grad_norm": 0.27705618739128113,
      "learning_rate": 2.3458333333333335e-05,
      "loss": 0.4035,
      "step": 193700
    },
    {
      "epoch": 1.884769680377731,
      "grad_norm": 0.30340132117271423,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.4029,
      "step": 193800
    },
    {
      "epoch": 1.8857422112434294,
      "grad_norm": 0.2727491855621338,
      "learning_rate": 2.3375000000000002e-05,
      "loss": 0.4031,
      "step": 193900
    },
    {
      "epoch": 1.8867147421091277,
      "grad_norm": 0.2821042239665985,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.4034,
      "step": 194000
    },
    {
      "epoch": 1.887687272974826,
      "grad_norm": 0.2739940285682678,
      "learning_rate": 2.3291666666666666e-05,
      "loss": 0.4028,
      "step": 194100
    },
    {
      "epoch": 1.8886598038405245,
      "grad_norm": 0.27784284949302673,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 0.4035,
      "step": 194200
    },
    {
      "epoch": 1.8896323347062227,
      "grad_norm": 0.2790377140045166,
      "learning_rate": 2.3208333333333336e-05,
      "loss": 0.4028,
      "step": 194300
    },
    {
      "epoch": 1.890604865571921,
      "grad_norm": 0.27520090341567993,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.4025,
      "step": 194400
    },
    {
      "epoch": 1.8915773964376195,
      "grad_norm": 0.2931537628173828,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 0.4036,
      "step": 194500
    },
    {
      "epoch": 1.892549927303318,
      "grad_norm": 0.25175896286964417,
      "learning_rate": 2.3083333333333333e-05,
      "loss": 0.4029,
      "step": 194600
    },
    {
      "epoch": 1.893522458169016,
      "grad_norm": 0.24993595480918884,
      "learning_rate": 2.3041666666666667e-05,
      "loss": 0.4023,
      "step": 194700
    },
    {
      "epoch": 1.8944949890347145,
      "grad_norm": 0.26239728927612305,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.4028,
      "step": 194800
    },
    {
      "epoch": 1.895467519900413,
      "grad_norm": 0.28426095843315125,
      "learning_rate": 2.2958333333333333e-05,
      "loss": 0.4037,
      "step": 194900
    },
    {
      "epoch": 1.8964400507661112,
      "grad_norm": 0.2916000187397003,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.4026,
      "step": 195000
    },
    {
      "epoch": 1.8974125816318095,
      "grad_norm": 0.257093220949173,
      "learning_rate": 2.2875e-05,
      "loss": 0.4029,
      "step": 195100
    },
    {
      "epoch": 1.898385112497508,
      "grad_norm": 0.262909859418869,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.403,
      "step": 195200
    },
    {
      "epoch": 1.8993576433632062,
      "grad_norm": 0.28299564123153687,
      "learning_rate": 2.2791666666666667e-05,
      "loss": 0.4033,
      "step": 195300
    },
    {
      "epoch": 1.9003301742289045,
      "grad_norm": 0.2562151551246643,
      "learning_rate": 2.275e-05,
      "loss": 0.403,
      "step": 195400
    },
    {
      "epoch": 1.901302705094603,
      "grad_norm": 0.2649156451225281,
      "learning_rate": 2.2708333333333334e-05,
      "loss": 0.4022,
      "step": 195500
    },
    {
      "epoch": 1.9022752359603012,
      "grad_norm": 0.2565390169620514,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.4026,
      "step": 195600
    },
    {
      "epoch": 1.9032477668259995,
      "grad_norm": 0.28111350536346436,
      "learning_rate": 2.2625e-05,
      "loss": 0.4022,
      "step": 195700
    },
    {
      "epoch": 1.904220297691698,
      "grad_norm": 0.2766091525554657,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.4033,
      "step": 195800
    },
    {
      "epoch": 1.9051928285573965,
      "grad_norm": 0.2623799741268158,
      "learning_rate": 2.2541666666666668e-05,
      "loss": 0.4025,
      "step": 195900
    },
    {
      "epoch": 1.9061653594230947,
      "grad_norm": 0.26012077927589417,
      "learning_rate": 2.25e-05,
      "loss": 0.403,
      "step": 196000
    },
    {
      "epoch": 1.907137890288793,
      "grad_norm": 0.26537537574768066,
      "learning_rate": 2.2458333333333335e-05,
      "loss": 0.4015,
      "step": 196100
    },
    {
      "epoch": 1.9081104211544915,
      "grad_norm": 0.24486638605594635,
      "learning_rate": 2.2416666666666665e-05,
      "loss": 0.4021,
      "step": 196200
    },
    {
      "epoch": 1.9090829520201897,
      "grad_norm": 0.2749541997909546,
      "learning_rate": 2.2375000000000002e-05,
      "loss": 0.4025,
      "step": 196300
    },
    {
      "epoch": 1.910055482885888,
      "grad_norm": 0.25414276123046875,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.4023,
      "step": 196400
    },
    {
      "epoch": 1.9110280137515865,
      "grad_norm": 0.2863958179950714,
      "learning_rate": 2.229166666666667e-05,
      "loss": 0.4035,
      "step": 196500
    },
    {
      "epoch": 1.9120005446172847,
      "grad_norm": 0.2817860245704651,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.4031,
      "step": 196600
    },
    {
      "epoch": 1.912973075482983,
      "grad_norm": 0.2902780771255493,
      "learning_rate": 2.2208333333333332e-05,
      "loss": 0.4031,
      "step": 196700
    },
    {
      "epoch": 1.9139456063486815,
      "grad_norm": 0.279476523399353,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.4026,
      "step": 196800
    },
    {
      "epoch": 1.91491813721438,
      "grad_norm": 0.267268568277359,
      "learning_rate": 2.2125000000000002e-05,
      "loss": 0.4036,
      "step": 196900
    },
    {
      "epoch": 1.9158906680800782,
      "grad_norm": 0.2807284891605377,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.4018,
      "step": 197000
    },
    {
      "epoch": 1.9168631989457765,
      "grad_norm": 0.26952826976776123,
      "learning_rate": 2.204166666666667e-05,
      "loss": 0.4036,
      "step": 197100
    },
    {
      "epoch": 1.917835729811475,
      "grad_norm": 0.28412386775016785,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.4032,
      "step": 197200
    },
    {
      "epoch": 1.9188082606771732,
      "grad_norm": 0.2762128710746765,
      "learning_rate": 2.1958333333333333e-05,
      "loss": 0.4019,
      "step": 197300
    },
    {
      "epoch": 1.9197807915428715,
      "grad_norm": 0.2628931403160095,
      "learning_rate": 2.191666666666667e-05,
      "loss": 0.4037,
      "step": 197400
    },
    {
      "epoch": 1.92075332240857,
      "grad_norm": 0.26044613122940063,
      "learning_rate": 2.1875e-05,
      "loss": 0.4023,
      "step": 197500
    },
    {
      "epoch": 1.9217258532742683,
      "grad_norm": 0.2711801528930664,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.4018,
      "step": 197600
    },
    {
      "epoch": 1.9226983841399665,
      "grad_norm": 0.28250494599342346,
      "learning_rate": 2.179166666666667e-05,
      "loss": 0.4018,
      "step": 197700
    },
    {
      "epoch": 1.923670915005665,
      "grad_norm": 0.27031025290489197,
      "learning_rate": 2.175e-05,
      "loss": 0.4016,
      "step": 197800
    },
    {
      "epoch": 1.9246434458713635,
      "grad_norm": 0.25598418712615967,
      "learning_rate": 2.1708333333333334e-05,
      "loss": 0.4018,
      "step": 197900
    },
    {
      "epoch": 1.9256159767370615,
      "grad_norm": 0.25659820437431335,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.4019,
      "step": 198000
    },
    {
      "epoch": 1.92658850760276,
      "grad_norm": 0.27726927399635315,
      "learning_rate": 2.1625e-05,
      "loss": 0.4015,
      "step": 198100
    },
    {
      "epoch": 1.9275610384684585,
      "grad_norm": 0.24561606347560883,
      "learning_rate": 2.1583333333333334e-05,
      "loss": 0.4011,
      "step": 198200
    },
    {
      "epoch": 1.9285335693341568,
      "grad_norm": 0.29605546593666077,
      "learning_rate": 2.1541666666666667e-05,
      "loss": 0.4028,
      "step": 198300
    },
    {
      "epoch": 1.929506100199855,
      "grad_norm": 0.2866264879703522,
      "learning_rate": 2.15e-05,
      "loss": 0.4029,
      "step": 198400
    },
    {
      "epoch": 1.9304786310655535,
      "grad_norm": 0.3017263412475586,
      "learning_rate": 2.1458333333333334e-05,
      "loss": 0.4021,
      "step": 198500
    },
    {
      "epoch": 1.9314511619312518,
      "grad_norm": 0.28123751282691956,
      "learning_rate": 2.1416666666666668e-05,
      "loss": 0.4015,
      "step": 198600
    },
    {
      "epoch": 1.93242369279695,
      "grad_norm": 0.27751418948173523,
      "learning_rate": 2.1375e-05,
      "loss": 0.4024,
      "step": 198700
    },
    {
      "epoch": 1.9333962236626485,
      "grad_norm": 0.27474042773246765,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.4031,
      "step": 198800
    },
    {
      "epoch": 1.934368754528347,
      "grad_norm": 0.26035791635513306,
      "learning_rate": 2.1291666666666668e-05,
      "loss": 0.4033,
      "step": 198900
    },
    {
      "epoch": 1.935341285394045,
      "grad_norm": 0.2802029550075531,
      "learning_rate": 2.125e-05,
      "loss": 0.4032,
      "step": 199000
    },
    {
      "epoch": 1.9363138162597435,
      "grad_norm": 0.2922920882701874,
      "learning_rate": 2.1208333333333335e-05,
      "loss": 0.4031,
      "step": 199100
    },
    {
      "epoch": 1.937286347125442,
      "grad_norm": 0.26805782318115234,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.402,
      "step": 199200
    },
    {
      "epoch": 1.9382588779911403,
      "grad_norm": 0.2634817063808441,
      "learning_rate": 2.1125000000000002e-05,
      "loss": 0.4026,
      "step": 199300
    },
    {
      "epoch": 1.9392314088568385,
      "grad_norm": 0.2654128670692444,
      "learning_rate": 2.1083333333333335e-05,
      "loss": 0.4026,
      "step": 199400
    },
    {
      "epoch": 1.940203939722537,
      "grad_norm": 0.2731466293334961,
      "learning_rate": 2.104166666666667e-05,
      "loss": 0.4029,
      "step": 199500
    },
    {
      "epoch": 1.9411764705882353,
      "grad_norm": 0.27742236852645874,
      "learning_rate": 2.1e-05,
      "loss": 0.4024,
      "step": 199600
    },
    {
      "epoch": 1.9421490014539335,
      "grad_norm": 0.2766144573688507,
      "learning_rate": 2.0958333333333336e-05,
      "loss": 0.4025,
      "step": 199700
    },
    {
      "epoch": 1.943121532319632,
      "grad_norm": 0.27981990575790405,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.402,
      "step": 199800
    },
    {
      "epoch": 1.9440940631853303,
      "grad_norm": 0.2807961106300354,
      "learning_rate": 2.0875e-05,
      "loss": 0.4018,
      "step": 199900
    },
    {
      "epoch": 1.9450665940510286,
      "grad_norm": 0.27425485849380493,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.4019,
      "step": 200000
    },
    {
      "epoch": 1.9450665940510286,
      "eval_accuracy": 0.6696225532072949,
      "eval_loss": 0.3965396583080292,
      "eval_runtime": 3786.1332,
      "eval_samples_per_second": 603.452,
      "eval_steps_per_second": 6.035,
      "step": 200000
    },
    {
      "epoch": 1.946039124916727,
      "grad_norm": 0.26906055212020874,
      "learning_rate": 2.0791666666666666e-05,
      "loss": 0.4025,
      "step": 200100
    },
    {
      "epoch": 1.9470116557824255,
      "grad_norm": 0.2832205295562744,
      "learning_rate": 2.075e-05,
      "loss": 0.402,
      "step": 200200
    },
    {
      "epoch": 1.9479841866481238,
      "grad_norm": 0.2802020013332367,
      "learning_rate": 2.0708333333333336e-05,
      "loss": 0.4018,
      "step": 200300
    },
    {
      "epoch": 1.948956717513822,
      "grad_norm": 0.2705368399620056,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.4025,
      "step": 200400
    },
    {
      "epoch": 1.9499292483795205,
      "grad_norm": 0.25961416959762573,
      "learning_rate": 2.0625e-05,
      "loss": 0.4019,
      "step": 200500
    },
    {
      "epoch": 1.9509017792452188,
      "grad_norm": 0.27170073986053467,
      "learning_rate": 2.0583333333333333e-05,
      "loss": 0.4017,
      "step": 200600
    },
    {
      "epoch": 1.951874310110917,
      "grad_norm": 0.28614336252212524,
      "learning_rate": 2.0541666666666667e-05,
      "loss": 0.402,
      "step": 200700
    },
    {
      "epoch": 1.9528468409766155,
      "grad_norm": 0.28665485978126526,
      "learning_rate": 2.05e-05,
      "loss": 0.4017,
      "step": 200800
    },
    {
      "epoch": 1.9538193718423138,
      "grad_norm": 0.26934537291526794,
      "learning_rate": 2.0458333333333334e-05,
      "loss": 0.4022,
      "step": 200900
    },
    {
      "epoch": 1.954791902708012,
      "grad_norm": 0.2770920693874359,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.4014,
      "step": 201000
    },
    {
      "epoch": 1.9557644335737105,
      "grad_norm": 0.284466952085495,
      "learning_rate": 2.0375e-05,
      "loss": 0.4016,
      "step": 201100
    },
    {
      "epoch": 1.956736964439409,
      "grad_norm": 0.2523525357246399,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.4015,
      "step": 201200
    },
    {
      "epoch": 1.9577094953051073,
      "grad_norm": 0.2645037770271301,
      "learning_rate": 2.0291666666666667e-05,
      "loss": 0.403,
      "step": 201300
    },
    {
      "epoch": 1.9586820261708056,
      "grad_norm": 0.2849137485027313,
      "learning_rate": 2.025e-05,
      "loss": 0.4023,
      "step": 201400
    },
    {
      "epoch": 1.959654557036504,
      "grad_norm": 0.2967127859592438,
      "learning_rate": 2.0208333333333334e-05,
      "loss": 0.4019,
      "step": 201500
    },
    {
      "epoch": 1.9606270879022023,
      "grad_norm": 0.27575457096099854,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.4024,
      "step": 201600
    },
    {
      "epoch": 1.9615996187679006,
      "grad_norm": 0.2849702835083008,
      "learning_rate": 2.0125e-05,
      "loss": 0.4012,
      "step": 201700
    },
    {
      "epoch": 1.962572149633599,
      "grad_norm": 0.2862958610057831,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.4019,
      "step": 201800
    },
    {
      "epoch": 1.9635446804992973,
      "grad_norm": 0.26923370361328125,
      "learning_rate": 2.0041666666666668e-05,
      "loss": 0.4014,
      "step": 201900
    },
    {
      "epoch": 1.9645172113649956,
      "grad_norm": 0.27717921137809753,
      "learning_rate": 2e-05,
      "loss": 0.4019,
      "step": 202000
    },
    {
      "epoch": 1.965489742230694,
      "grad_norm": 0.29145002365112305,
      "learning_rate": 1.9958333333333335e-05,
      "loss": 0.401,
      "step": 202100
    },
    {
      "epoch": 1.9664622730963925,
      "grad_norm": 0.2593843340873718,
      "learning_rate": 1.9916666666666665e-05,
      "loss": 0.4014,
      "step": 202200
    },
    {
      "epoch": 1.9674348039620906,
      "grad_norm": 0.2817401587963104,
      "learning_rate": 1.9875000000000002e-05,
      "loss": 0.4019,
      "step": 202300
    },
    {
      "epoch": 1.968407334827789,
      "grad_norm": 0.2724657356739044,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.4008,
      "step": 202400
    },
    {
      "epoch": 1.9693798656934876,
      "grad_norm": 0.2670876383781433,
      "learning_rate": 1.9791666666666665e-05,
      "loss": 0.4012,
      "step": 202500
    },
    {
      "epoch": 1.9703523965591858,
      "grad_norm": 0.26930081844329834,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.4023,
      "step": 202600
    },
    {
      "epoch": 1.971324927424884,
      "grad_norm": 0.2803794741630554,
      "learning_rate": 1.9708333333333336e-05,
      "loss": 0.4027,
      "step": 202700
    },
    {
      "epoch": 1.9722974582905826,
      "grad_norm": 0.28604140877723694,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.4013,
      "step": 202800
    },
    {
      "epoch": 1.9732699891562808,
      "grad_norm": 0.27029508352279663,
      "learning_rate": 1.9625000000000003e-05,
      "loss": 0.401,
      "step": 202900
    },
    {
      "epoch": 1.974242520021979,
      "grad_norm": 0.2829933166503906,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.4011,
      "step": 203000
    },
    {
      "epoch": 1.9752150508876776,
      "grad_norm": 0.28413939476013184,
      "learning_rate": 1.9541666666666666e-05,
      "loss": 0.4028,
      "step": 203100
    },
    {
      "epoch": 1.976187581753376,
      "grad_norm": 0.2746051847934723,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.4015,
      "step": 203200
    },
    {
      "epoch": 1.977160112619074,
      "grad_norm": 0.28949519991874695,
      "learning_rate": 1.9458333333333333e-05,
      "loss": 0.4014,
      "step": 203300
    },
    {
      "epoch": 1.9781326434847726,
      "grad_norm": 0.26937419176101685,
      "learning_rate": 1.9416666666666667e-05,
      "loss": 0.4017,
      "step": 203400
    },
    {
      "epoch": 1.979105174350471,
      "grad_norm": 0.27665987610816956,
      "learning_rate": 1.9375e-05,
      "loss": 0.4015,
      "step": 203500
    },
    {
      "epoch": 1.9800777052161693,
      "grad_norm": 0.2770301103591919,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.4019,
      "step": 203600
    },
    {
      "epoch": 1.9810502360818676,
      "grad_norm": 0.27760156989097595,
      "learning_rate": 1.9291666666666667e-05,
      "loss": 0.4012,
      "step": 203700
    },
    {
      "epoch": 1.982022766947566,
      "grad_norm": 0.280579537153244,
      "learning_rate": 1.925e-05,
      "loss": 0.4014,
      "step": 203800
    },
    {
      "epoch": 1.9829952978132643,
      "grad_norm": 0.27908632159233093,
      "learning_rate": 1.9208333333333334e-05,
      "loss": 0.4014,
      "step": 203900
    },
    {
      "epoch": 1.9839678286789626,
      "grad_norm": 0.2854999601840973,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.4015,
      "step": 204000
    },
    {
      "epoch": 1.984940359544661,
      "grad_norm": 0.27065372467041016,
      "learning_rate": 1.9125e-05,
      "loss": 0.4011,
      "step": 204100
    },
    {
      "epoch": 1.9859128904103593,
      "grad_norm": 0.26239556074142456,
      "learning_rate": 1.9083333333333334e-05,
      "loss": 0.4016,
      "step": 204200
    },
    {
      "epoch": 1.9868854212760576,
      "grad_norm": 0.2913048267364502,
      "learning_rate": 1.9041666666666668e-05,
      "loss": 0.4005,
      "step": 204300
    },
    {
      "epoch": 1.987857952141756,
      "grad_norm": 0.2739632725715637,
      "learning_rate": 1.9e-05,
      "loss": 0.4023,
      "step": 204400
    },
    {
      "epoch": 1.9888304830074546,
      "grad_norm": 0.2705167829990387,
      "learning_rate": 1.8958333333333334e-05,
      "loss": 0.4008,
      "step": 204500
    },
    {
      "epoch": 1.9898030138731528,
      "grad_norm": 0.2848774492740631,
      "learning_rate": 1.8916666666666668e-05,
      "loss": 0.4008,
      "step": 204600
    },
    {
      "epoch": 1.990775544738851,
      "grad_norm": 0.28518009185791016,
      "learning_rate": 1.8875e-05,
      "loss": 0.4019,
      "step": 204700
    },
    {
      "epoch": 1.9917480756045496,
      "grad_norm": 0.25598961114883423,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.401,
      "step": 204800
    },
    {
      "epoch": 1.9927206064702478,
      "grad_norm": 0.277185320854187,
      "learning_rate": 1.8791666666666668e-05,
      "loss": 0.4003,
      "step": 204900
    },
    {
      "epoch": 1.993693137335946,
      "grad_norm": 0.29170143604278564,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.4013,
      "step": 205000
    },
    {
      "epoch": 1.9946656682016446,
      "grad_norm": 0.30227264761924744,
      "learning_rate": 1.8708333333333332e-05,
      "loss": 0.4011,
      "step": 205100
    },
    {
      "epoch": 1.9956381990673429,
      "grad_norm": 0.26984304189682007,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.4012,
      "step": 205200
    },
    {
      "epoch": 1.9966107299330411,
      "grad_norm": 0.269464910030365,
      "learning_rate": 1.8625000000000002e-05,
      "loss": 0.4016,
      "step": 205300
    },
    {
      "epoch": 1.9975832607987396,
      "grad_norm": 0.2822186350822449,
      "learning_rate": 1.8583333333333332e-05,
      "loss": 0.4014,
      "step": 205400
    },
    {
      "epoch": 1.998555791664438,
      "grad_norm": 0.28745579719543457,
      "learning_rate": 1.854166666666667e-05,
      "loss": 0.4006,
      "step": 205500
    },
    {
      "epoch": 1.9995283225301363,
      "grad_norm": 0.26854339241981506,
      "learning_rate": 1.85e-05,
      "loss": 0.4012,
      "step": 205600
    },
    {
      "epoch": 2.0005008533958346,
      "grad_norm": 0.2904132008552551,
      "learning_rate": 1.8458333333333333e-05,
      "loss": 0.4008,
      "step": 205700
    },
    {
      "epoch": 2.001473384261533,
      "grad_norm": 0.2711549401283264,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.4009,
      "step": 205800
    },
    {
      "epoch": 2.002445915127231,
      "grad_norm": 0.30339711904525757,
      "learning_rate": 1.8375e-05,
      "loss": 0.4011,
      "step": 205900
    },
    {
      "epoch": 2.0034184459929296,
      "grad_norm": 0.296446293592453,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.4013,
      "step": 206000
    },
    {
      "epoch": 2.004390976858628,
      "grad_norm": 0.2782582938671112,
      "learning_rate": 1.829166666666667e-05,
      "loss": 0.4028,
      "step": 206100
    },
    {
      "epoch": 2.0053635077243266,
      "grad_norm": 0.30363115668296814,
      "learning_rate": 1.825e-05,
      "loss": 0.4015,
      "step": 206200
    },
    {
      "epoch": 2.0063360385900246,
      "grad_norm": 0.2832475006580353,
      "learning_rate": 1.8208333333333337e-05,
      "loss": 0.4009,
      "step": 206300
    },
    {
      "epoch": 2.007308569455723,
      "grad_norm": 0.27062031626701355,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.4,
      "step": 206400
    },
    {
      "epoch": 2.0082811003214216,
      "grad_norm": 0.2863771319389343,
      "learning_rate": 1.8125e-05,
      "loss": 0.4003,
      "step": 206500
    },
    {
      "epoch": 2.0092536311871196,
      "grad_norm": 0.25921428203582764,
      "learning_rate": 1.8083333333333337e-05,
      "loss": 0.4006,
      "step": 206600
    },
    {
      "epoch": 2.010226162052818,
      "grad_norm": 0.2725997269153595,
      "learning_rate": 1.8041666666666667e-05,
      "loss": 0.4008,
      "step": 206700
    },
    {
      "epoch": 2.0111986929185166,
      "grad_norm": 0.27988892793655396,
      "learning_rate": 1.8e-05,
      "loss": 0.4011,
      "step": 206800
    },
    {
      "epoch": 2.0121712237842146,
      "grad_norm": 0.2704628109931946,
      "learning_rate": 1.7958333333333334e-05,
      "loss": 0.4011,
      "step": 206900
    },
    {
      "epoch": 2.013143754649913,
      "grad_norm": 0.2867714464664459,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.4008,
      "step": 207000
    },
    {
      "epoch": 2.0141162855156116,
      "grad_norm": 0.27649980783462524,
      "learning_rate": 1.7875e-05,
      "loss": 0.4006,
      "step": 207100
    },
    {
      "epoch": 2.01508881638131,
      "grad_norm": 0.28201401233673096,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.401,
      "step": 207200
    },
    {
      "epoch": 2.016061347247008,
      "grad_norm": 0.2799062430858612,
      "learning_rate": 1.7791666666666668e-05,
      "loss": 0.3989,
      "step": 207300
    },
    {
      "epoch": 2.0170338781127066,
      "grad_norm": 0.27894553542137146,
      "learning_rate": 1.775e-05,
      "loss": 0.4007,
      "step": 207400
    },
    {
      "epoch": 2.018006408978405,
      "grad_norm": 0.2741168439388275,
      "learning_rate": 1.7708333333333335e-05,
      "loss": 0.4009,
      "step": 207500
    },
    {
      "epoch": 2.018978939844103,
      "grad_norm": 0.27298927307128906,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.4001,
      "step": 207600
    },
    {
      "epoch": 2.0199514707098016,
      "grad_norm": 0.29734423756599426,
      "learning_rate": 1.7625e-05,
      "loss": 0.4013,
      "step": 207700
    },
    {
      "epoch": 2.0209240015755,
      "grad_norm": 0.27144330739974976,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.4009,
      "step": 207800
    },
    {
      "epoch": 2.021896532441198,
      "grad_norm": 0.2698211073875427,
      "learning_rate": 1.754166666666667e-05,
      "loss": 0.4011,
      "step": 207900
    },
    {
      "epoch": 2.0228690633068966,
      "grad_norm": 0.2899340093135834,
      "learning_rate": 1.75e-05,
      "loss": 0.4014,
      "step": 208000
    },
    {
      "epoch": 2.023841594172595,
      "grad_norm": 0.2683008313179016,
      "learning_rate": 1.7458333333333335e-05,
      "loss": 0.4003,
      "step": 208100
    },
    {
      "epoch": 2.0248141250382936,
      "grad_norm": 0.28569984436035156,
      "learning_rate": 1.741666666666667e-05,
      "loss": 0.3999,
      "step": 208200
    },
    {
      "epoch": 2.0257866559039917,
      "grad_norm": 0.288083553314209,
      "learning_rate": 1.7375e-05,
      "loss": 0.4007,
      "step": 208300
    },
    {
      "epoch": 2.02675918676969,
      "grad_norm": 0.2838444113731384,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.3996,
      "step": 208400
    },
    {
      "epoch": 2.0277317176353886,
      "grad_norm": 0.2759054899215698,
      "learning_rate": 1.7291666666666666e-05,
      "loss": 0.3994,
      "step": 208500
    },
    {
      "epoch": 2.0287042485010867,
      "grad_norm": 0.2886776328086853,
      "learning_rate": 1.725e-05,
      "loss": 0.4,
      "step": 208600
    },
    {
      "epoch": 2.029676779366785,
      "grad_norm": 0.2802843749523163,
      "learning_rate": 1.7208333333333336e-05,
      "loss": 0.4014,
      "step": 208700
    },
    {
      "epoch": 2.0306493102324836,
      "grad_norm": 0.2814495265483856,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.4001,
      "step": 208800
    },
    {
      "epoch": 2.0316218410981817,
      "grad_norm": 0.2617117464542389,
      "learning_rate": 1.7125000000000003e-05,
      "loss": 0.4005,
      "step": 208900
    },
    {
      "epoch": 2.03259437196388,
      "grad_norm": 0.27543771266937256,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.3993,
      "step": 209000
    },
    {
      "epoch": 2.0335669028295786,
      "grad_norm": 0.2931128740310669,
      "learning_rate": 1.7041666666666666e-05,
      "loss": 0.4002,
      "step": 209100
    },
    {
      "epoch": 2.034539433695277,
      "grad_norm": 0.2981145977973938,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.3993,
      "step": 209200
    },
    {
      "epoch": 2.035511964560975,
      "grad_norm": 0.2913394272327423,
      "learning_rate": 1.6958333333333333e-05,
      "loss": 0.4002,
      "step": 209300
    },
    {
      "epoch": 2.0364844954266736,
      "grad_norm": 0.2755526602268219,
      "learning_rate": 1.6916666666666667e-05,
      "loss": 0.4011,
      "step": 209400
    },
    {
      "epoch": 2.037457026292372,
      "grad_norm": 0.2889014184474945,
      "learning_rate": 1.6875000000000004e-05,
      "loss": 0.4008,
      "step": 209500
    },
    {
      "epoch": 2.03842955715807,
      "grad_norm": 0.2619468569755554,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.3996,
      "step": 209600
    },
    {
      "epoch": 2.0394020880237687,
      "grad_norm": 0.2746221423149109,
      "learning_rate": 1.6791666666666667e-05,
      "loss": 0.3994,
      "step": 209700
    },
    {
      "epoch": 2.040374618889467,
      "grad_norm": 0.2694110870361328,
      "learning_rate": 1.675e-05,
      "loss": 0.4008,
      "step": 209800
    },
    {
      "epoch": 2.041347149755165,
      "grad_norm": 0.2850872576236725,
      "learning_rate": 1.6708333333333334e-05,
      "loss": 0.3999,
      "step": 209900
    },
    {
      "epoch": 2.0423196806208637,
      "grad_norm": 0.27608874440193176,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.4015,
      "step": 210000
    },
    {
      "epoch": 2.0423196806208637,
      "eval_accuracy": 0.6698670904310692,
      "eval_loss": 0.39459410309791565,
      "eval_runtime": 3742.8224,
      "eval_samples_per_second": 610.435,
      "eval_steps_per_second": 6.104,
      "step": 210000
    },
    {
      "epoch": 2.043292211486562,
      "grad_norm": 0.27803418040275574,
      "learning_rate": 1.6625e-05,
      "loss": 0.4005,
      "step": 210100
    },
    {
      "epoch": 2.04426474235226,
      "grad_norm": 0.3148842453956604,
      "learning_rate": 1.6583333333333334e-05,
      "loss": 0.4003,
      "step": 210200
    },
    {
      "epoch": 2.0452372732179587,
      "grad_norm": 0.2536375820636749,
      "learning_rate": 1.6541666666666668e-05,
      "loss": 0.3995,
      "step": 210300
    },
    {
      "epoch": 2.046209804083657,
      "grad_norm": 0.26894596219062805,
      "learning_rate": 1.65e-05,
      "loss": 0.3991,
      "step": 210400
    },
    {
      "epoch": 2.0471823349493556,
      "grad_norm": 0.28333672881126404,
      "learning_rate": 1.6458333333333335e-05,
      "loss": 0.3989,
      "step": 210500
    },
    {
      "epoch": 2.0481548658150537,
      "grad_norm": 0.2852044105529785,
      "learning_rate": 1.6416666666666665e-05,
      "loss": 0.4001,
      "step": 210600
    },
    {
      "epoch": 2.049127396680752,
      "grad_norm": 0.2779984474182129,
      "learning_rate": 1.6375e-05,
      "loss": 0.3989,
      "step": 210700
    },
    {
      "epoch": 2.0500999275464507,
      "grad_norm": 0.2752462327480316,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.4008,
      "step": 210800
    },
    {
      "epoch": 2.0510724584121487,
      "grad_norm": 0.2823602557182312,
      "learning_rate": 1.6291666666666665e-05,
      "loss": 0.3993,
      "step": 210900
    },
    {
      "epoch": 2.052044989277847,
      "grad_norm": 0.2596426010131836,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.3995,
      "step": 211000
    },
    {
      "epoch": 2.0530175201435457,
      "grad_norm": 0.2877238690853119,
      "learning_rate": 1.6208333333333332e-05,
      "loss": 0.3996,
      "step": 211100
    },
    {
      "epoch": 2.0539900510092437,
      "grad_norm": 0.2795281708240509,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.4,
      "step": 211200
    },
    {
      "epoch": 2.054962581874942,
      "grad_norm": 0.2743079364299774,
      "learning_rate": 1.6125000000000002e-05,
      "loss": 0.4006,
      "step": 211300
    },
    {
      "epoch": 2.0559351127406407,
      "grad_norm": 0.25551077723503113,
      "learning_rate": 1.6083333333333332e-05,
      "loss": 0.4001,
      "step": 211400
    },
    {
      "epoch": 2.056907643606339,
      "grad_norm": 0.2959273159503937,
      "learning_rate": 1.604166666666667e-05,
      "loss": 0.3997,
      "step": 211500
    },
    {
      "epoch": 2.057880174472037,
      "grad_norm": 0.30464401841163635,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.3996,
      "step": 211600
    },
    {
      "epoch": 2.0588527053377357,
      "grad_norm": 0.27905455231666565,
      "learning_rate": 1.5958333333333333e-05,
      "loss": 0.4006,
      "step": 211700
    },
    {
      "epoch": 2.059825236203434,
      "grad_norm": 0.295991450548172,
      "learning_rate": 1.591666666666667e-05,
      "loss": 0.4003,
      "step": 211800
    },
    {
      "epoch": 2.060797767069132,
      "grad_norm": 0.28605926036834717,
      "learning_rate": 1.5875e-05,
      "loss": 0.3992,
      "step": 211900
    },
    {
      "epoch": 2.0617702979348307,
      "grad_norm": 0.28163591027259827,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.3981,
      "step": 212000
    },
    {
      "epoch": 2.062742828800529,
      "grad_norm": 0.2840014696121216,
      "learning_rate": 1.579166666666667e-05,
      "loss": 0.3989,
      "step": 212100
    },
    {
      "epoch": 2.063715359666227,
      "grad_norm": 0.2975727915763855,
      "learning_rate": 1.575e-05,
      "loss": 0.399,
      "step": 212200
    },
    {
      "epoch": 2.0646878905319257,
      "grad_norm": 0.29813870787620544,
      "learning_rate": 1.5708333333333333e-05,
      "loss": 0.4006,
      "step": 212300
    },
    {
      "epoch": 2.065660421397624,
      "grad_norm": 0.30549705028533936,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.399,
      "step": 212400
    },
    {
      "epoch": 2.0666329522633227,
      "grad_norm": 0.2787177562713623,
      "learning_rate": 1.5625e-05,
      "loss": 0.3988,
      "step": 212500
    },
    {
      "epoch": 2.0676054831290207,
      "grad_norm": 0.27758583426475525,
      "learning_rate": 1.5583333333333334e-05,
      "loss": 0.3994,
      "step": 212600
    },
    {
      "epoch": 2.068578013994719,
      "grad_norm": 0.3079952299594879,
      "learning_rate": 1.5541666666666667e-05,
      "loss": 0.3994,
      "step": 212700
    },
    {
      "epoch": 2.0695505448604177,
      "grad_norm": 0.27814990282058716,
      "learning_rate": 1.55e-05,
      "loss": 0.3996,
      "step": 212800
    },
    {
      "epoch": 2.0705230757261157,
      "grad_norm": 0.2810305953025818,
      "learning_rate": 1.5458333333333334e-05,
      "loss": 0.4002,
      "step": 212900
    },
    {
      "epoch": 2.071495606591814,
      "grad_norm": 0.26894253492355347,
      "learning_rate": 1.5416666666666668e-05,
      "loss": 0.4004,
      "step": 213000
    },
    {
      "epoch": 2.0724681374575127,
      "grad_norm": 0.3045996129512787,
      "learning_rate": 1.5375e-05,
      "loss": 0.4,
      "step": 213100
    },
    {
      "epoch": 2.0734406683232107,
      "grad_norm": 0.29771900177001953,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.3982,
      "step": 213200
    },
    {
      "epoch": 2.074413199188909,
      "grad_norm": 0.27804598212242126,
      "learning_rate": 1.5291666666666668e-05,
      "loss": 0.4005,
      "step": 213300
    },
    {
      "epoch": 2.0753857300546077,
      "grad_norm": 0.2796401381492615,
      "learning_rate": 1.525e-05,
      "loss": 0.3992,
      "step": 213400
    },
    {
      "epoch": 2.0763582609203057,
      "grad_norm": 0.2804580628871918,
      "learning_rate": 1.5208333333333333e-05,
      "loss": 0.3993,
      "step": 213500
    },
    {
      "epoch": 2.077330791786004,
      "grad_norm": 0.2896333634853363,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.3994,
      "step": 213600
    },
    {
      "epoch": 2.0783033226517027,
      "grad_norm": 0.29876697063446045,
      "learning_rate": 1.5125e-05,
      "loss": 0.3994,
      "step": 213700
    },
    {
      "epoch": 2.079275853517401,
      "grad_norm": 0.2738015651702881,
      "learning_rate": 1.5083333333333335e-05,
      "loss": 0.3994,
      "step": 213800
    },
    {
      "epoch": 2.0802483843830992,
      "grad_norm": 0.2837720811367035,
      "learning_rate": 1.5041666666666669e-05,
      "loss": 0.4003,
      "step": 213900
    },
    {
      "epoch": 2.0812209152487977,
      "grad_norm": 0.2812393605709076,
      "learning_rate": 1.5e-05,
      "loss": 0.3989,
      "step": 214000
    },
    {
      "epoch": 2.082193446114496,
      "grad_norm": 0.28097400069236755,
      "learning_rate": 1.4958333333333336e-05,
      "loss": 0.4,
      "step": 214100
    },
    {
      "epoch": 2.0831659769801942,
      "grad_norm": 0.29248350858688354,
      "learning_rate": 1.4916666666666667e-05,
      "loss": 0.4,
      "step": 214200
    },
    {
      "epoch": 2.0841385078458927,
      "grad_norm": 0.2958994209766388,
      "learning_rate": 1.4875e-05,
      "loss": 0.3997,
      "step": 214300
    },
    {
      "epoch": 2.085111038711591,
      "grad_norm": 0.2785075008869171,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.4,
      "step": 214400
    },
    {
      "epoch": 2.0860835695772897,
      "grad_norm": 0.29670628905296326,
      "learning_rate": 1.4791666666666668e-05,
      "loss": 0.3995,
      "step": 214500
    },
    {
      "epoch": 2.0870561004429877,
      "grad_norm": 0.2800738215446472,
      "learning_rate": 1.475e-05,
      "loss": 0.3987,
      "step": 214600
    },
    {
      "epoch": 2.088028631308686,
      "grad_norm": 0.2987944483757019,
      "learning_rate": 1.4708333333333335e-05,
      "loss": 0.3991,
      "step": 214700
    },
    {
      "epoch": 2.0890011621743847,
      "grad_norm": 0.3039252460002899,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.3988,
      "step": 214800
    },
    {
      "epoch": 2.0899736930400827,
      "grad_norm": 0.272357702255249,
      "learning_rate": 1.4625e-05,
      "loss": 0.3981,
      "step": 214900
    },
    {
      "epoch": 2.090946223905781,
      "grad_norm": 0.3063870370388031,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.3992,
      "step": 215000
    },
    {
      "epoch": 2.0919187547714797,
      "grad_norm": 0.29288768768310547,
      "learning_rate": 1.4541666666666667e-05,
      "loss": 0.3998,
      "step": 215100
    },
    {
      "epoch": 2.0928912856371777,
      "grad_norm": 0.26252615451812744,
      "learning_rate": 1.45e-05,
      "loss": 0.3992,
      "step": 215200
    },
    {
      "epoch": 2.0938638165028762,
      "grad_norm": 0.28989797830581665,
      "learning_rate": 1.4458333333333335e-05,
      "loss": 0.3978,
      "step": 215300
    },
    {
      "epoch": 2.0948363473685747,
      "grad_norm": 0.2730097472667694,
      "learning_rate": 1.4416666666666667e-05,
      "loss": 0.3988,
      "step": 215400
    },
    {
      "epoch": 2.0958088782342728,
      "grad_norm": 0.2791329026222229,
      "learning_rate": 1.4374999999999999e-05,
      "loss": 0.3988,
      "step": 215500
    },
    {
      "epoch": 2.0967814090999712,
      "grad_norm": 0.2789154648780823,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.398,
      "step": 215600
    },
    {
      "epoch": 2.0977539399656697,
      "grad_norm": 0.28331759572029114,
      "learning_rate": 1.4291666666666667e-05,
      "loss": 0.3991,
      "step": 215700
    },
    {
      "epoch": 2.098726470831368,
      "grad_norm": 0.2864353060722351,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.3996,
      "step": 215800
    },
    {
      "epoch": 2.0996990016970662,
      "grad_norm": 0.2937341034412384,
      "learning_rate": 1.4208333333333334e-05,
      "loss": 0.3982,
      "step": 215900
    },
    {
      "epoch": 2.1006715325627647,
      "grad_norm": 0.2743794322013855,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.3982,
      "step": 216000
    },
    {
      "epoch": 2.101644063428463,
      "grad_norm": 0.3136337101459503,
      "learning_rate": 1.4125e-05,
      "loss": 0.3989,
      "step": 216100
    },
    {
      "epoch": 2.1026165942941613,
      "grad_norm": 0.27768194675445557,
      "learning_rate": 1.4083333333333335e-05,
      "loss": 0.3988,
      "step": 216200
    },
    {
      "epoch": 2.1035891251598597,
      "grad_norm": 0.2689090967178345,
      "learning_rate": 1.4041666666666666e-05,
      "loss": 0.399,
      "step": 216300
    },
    {
      "epoch": 2.1045616560255582,
      "grad_norm": 0.28697749972343445,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.3995,
      "step": 216400
    },
    {
      "epoch": 2.1055341868912563,
      "grad_norm": 0.2770169675350189,
      "learning_rate": 1.3958333333333335e-05,
      "loss": 0.3993,
      "step": 216500
    },
    {
      "epoch": 2.1065067177569547,
      "grad_norm": 0.30547261238098145,
      "learning_rate": 1.3916666666666667e-05,
      "loss": 0.3981,
      "step": 216600
    },
    {
      "epoch": 2.1074792486226532,
      "grad_norm": 0.3023488223552704,
      "learning_rate": 1.3875000000000002e-05,
      "loss": 0.3994,
      "step": 216700
    },
    {
      "epoch": 2.1084517794883517,
      "grad_norm": 0.2693280279636383,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.3985,
      "step": 216800
    },
    {
      "epoch": 2.1094243103540498,
      "grad_norm": 0.2693469226360321,
      "learning_rate": 1.3791666666666667e-05,
      "loss": 0.399,
      "step": 216900
    },
    {
      "epoch": 2.1103968412197482,
      "grad_norm": 0.287112832069397,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.3997,
      "step": 217000
    },
    {
      "epoch": 2.1113693720854467,
      "grad_norm": 0.26812177896499634,
      "learning_rate": 1.3708333333333334e-05,
      "loss": 0.3988,
      "step": 217100
    },
    {
      "epoch": 2.1123419029511448,
      "grad_norm": 0.2630558907985687,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.398,
      "step": 217200
    },
    {
      "epoch": 2.1133144338168433,
      "grad_norm": 0.2804103195667267,
      "learning_rate": 1.3625e-05,
      "loss": 0.3978,
      "step": 217300
    },
    {
      "epoch": 2.1142869646825417,
      "grad_norm": 0.2748226225376129,
      "learning_rate": 1.3583333333333334e-05,
      "loss": 0.3983,
      "step": 217400
    },
    {
      "epoch": 2.1152594955482398,
      "grad_norm": 0.27802586555480957,
      "learning_rate": 1.3541666666666666e-05,
      "loss": 0.3985,
      "step": 217500
    },
    {
      "epoch": 2.1162320264139383,
      "grad_norm": 0.2671980559825897,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.3996,
      "step": 217600
    },
    {
      "epoch": 2.1172045572796367,
      "grad_norm": 0.31431466341018677,
      "learning_rate": 1.3458333333333335e-05,
      "loss": 0.3994,
      "step": 217700
    },
    {
      "epoch": 2.118177088145335,
      "grad_norm": 0.2688765823841095,
      "learning_rate": 1.3416666666666666e-05,
      "loss": 0.3998,
      "step": 217800
    },
    {
      "epoch": 2.1191496190110333,
      "grad_norm": 0.2787071764469147,
      "learning_rate": 1.3375000000000002e-05,
      "loss": 0.3983,
      "step": 217900
    },
    {
      "epoch": 2.1201221498767318,
      "grad_norm": 0.2825058698654175,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.3977,
      "step": 218000
    },
    {
      "epoch": 2.1210946807424302,
      "grad_norm": 0.3153066337108612,
      "learning_rate": 1.3291666666666667e-05,
      "loss": 0.3979,
      "step": 218100
    },
    {
      "epoch": 2.1220672116081283,
      "grad_norm": 0.2697572708129883,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.3979,
      "step": 218200
    },
    {
      "epoch": 2.1230397424738268,
      "grad_norm": 0.2846742570400238,
      "learning_rate": 1.3208333333333334e-05,
      "loss": 0.3983,
      "step": 218300
    },
    {
      "epoch": 2.1240122733395252,
      "grad_norm": 0.2751951515674591,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.3975,
      "step": 218400
    },
    {
      "epoch": 2.1249848042052233,
      "grad_norm": 0.2627638280391693,
      "learning_rate": 1.3125e-05,
      "loss": 0.3978,
      "step": 218500
    },
    {
      "epoch": 2.1259573350709218,
      "grad_norm": 0.28418394923210144,
      "learning_rate": 1.3083333333333334e-05,
      "loss": 0.3991,
      "step": 218600
    },
    {
      "epoch": 2.1269298659366203,
      "grad_norm": 0.2778332531452179,
      "learning_rate": 1.3041666666666666e-05,
      "loss": 0.3995,
      "step": 218700
    },
    {
      "epoch": 2.1279023968023187,
      "grad_norm": 0.2856643795967102,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.3974,
      "step": 218800
    },
    {
      "epoch": 2.128874927668017,
      "grad_norm": 0.2813580632209778,
      "learning_rate": 1.2958333333333333e-05,
      "loss": 0.3983,
      "step": 218900
    },
    {
      "epoch": 2.1298474585337153,
      "grad_norm": 0.2666797935962677,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.3982,
      "step": 219000
    },
    {
      "epoch": 2.1308199893994137,
      "grad_norm": 0.28925997018814087,
      "learning_rate": 1.2875000000000001e-05,
      "loss": 0.3989,
      "step": 219100
    },
    {
      "epoch": 2.131792520265112,
      "grad_norm": 0.28436073660850525,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.3993,
      "step": 219200
    },
    {
      "epoch": 2.1327650511308103,
      "grad_norm": 0.2950543165206909,
      "learning_rate": 1.2791666666666668e-05,
      "loss": 0.3987,
      "step": 219300
    },
    {
      "epoch": 2.1337375819965088,
      "grad_norm": 0.27688363194465637,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 0.3979,
      "step": 219400
    },
    {
      "epoch": 2.134710112862207,
      "grad_norm": 0.3042011260986328,
      "learning_rate": 1.2708333333333333e-05,
      "loss": 0.3978,
      "step": 219500
    },
    {
      "epoch": 2.1356826437279053,
      "grad_norm": 0.2693803608417511,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.3981,
      "step": 219600
    },
    {
      "epoch": 2.1366551745936038,
      "grad_norm": 0.2914591431617737,
      "learning_rate": 1.2625e-05,
      "loss": 0.3988,
      "step": 219700
    },
    {
      "epoch": 2.137627705459302,
      "grad_norm": 0.26175981760025024,
      "learning_rate": 1.2583333333333334e-05,
      "loss": 0.3978,
      "step": 219800
    },
    {
      "epoch": 2.1386002363250003,
      "grad_norm": 0.2773490846157074,
      "learning_rate": 1.2541666666666669e-05,
      "loss": 0.3972,
      "step": 219900
    },
    {
      "epoch": 2.1395727671906988,
      "grad_norm": 0.2743099331855774,
      "learning_rate": 1.25e-05,
      "loss": 0.3975,
      "step": 220000
    },
    {
      "epoch": 2.1395727671906988,
      "eval_accuracy": 0.6701566890963317,
      "eval_loss": 0.39260342717170715,
      "eval_runtime": 3807.4234,
      "eval_samples_per_second": 600.078,
      "eval_steps_per_second": 6.001,
      "step": 220000
    },
    {
      "epoch": 2.1405452980563973,
      "grad_norm": 0.2752749025821686,
      "learning_rate": 1.2458333333333334e-05,
      "loss": 0.3972,
      "step": 220100
    },
    {
      "epoch": 2.1415178289220953,
      "grad_norm": 0.26212698221206665,
      "learning_rate": 1.2416666666666667e-05,
      "loss": 0.3983,
      "step": 220200
    },
    {
      "epoch": 2.142490359787794,
      "grad_norm": 0.2749654948711395,
      "learning_rate": 1.2375000000000001e-05,
      "loss": 0.3986,
      "step": 220300
    },
    {
      "epoch": 2.1434628906534923,
      "grad_norm": 0.26889365911483765,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.3979,
      "step": 220400
    },
    {
      "epoch": 2.1444354215191903,
      "grad_norm": 0.2884691655635834,
      "learning_rate": 1.2291666666666666e-05,
      "loss": 0.3978,
      "step": 220500
    },
    {
      "epoch": 2.145407952384889,
      "grad_norm": 0.2900453805923462,
      "learning_rate": 1.225e-05,
      "loss": 0.3976,
      "step": 220600
    },
    {
      "epoch": 2.1463804832505873,
      "grad_norm": 0.30221596360206604,
      "learning_rate": 1.2208333333333335e-05,
      "loss": 0.3979,
      "step": 220700
    },
    {
      "epoch": 2.1473530141162853,
      "grad_norm": 0.28173547983169556,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.3991,
      "step": 220800
    },
    {
      "epoch": 2.148325544981984,
      "grad_norm": 0.2864886224269867,
      "learning_rate": 1.2125e-05,
      "loss": 0.3985,
      "step": 220900
    },
    {
      "epoch": 2.1492980758476823,
      "grad_norm": 0.29046112298965454,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.398,
      "step": 221000
    },
    {
      "epoch": 2.1502706067133808,
      "grad_norm": 0.2690776288509369,
      "learning_rate": 1.2041666666666669e-05,
      "loss": 0.3982,
      "step": 221100
    },
    {
      "epoch": 2.151243137579079,
      "grad_norm": 0.28787073493003845,
      "learning_rate": 1.2e-05,
      "loss": 0.398,
      "step": 221200
    },
    {
      "epoch": 2.1522156684447773,
      "grad_norm": 0.27526959776878357,
      "learning_rate": 1.1958333333333334e-05,
      "loss": 0.3986,
      "step": 221300
    },
    {
      "epoch": 2.153188199310476,
      "grad_norm": 0.29698213934898376,
      "learning_rate": 1.1916666666666667e-05,
      "loss": 0.396,
      "step": 221400
    },
    {
      "epoch": 2.154160730176174,
      "grad_norm": 0.28958168625831604,
      "learning_rate": 1.1875e-05,
      "loss": 0.3976,
      "step": 221500
    },
    {
      "epoch": 2.1551332610418723,
      "grad_norm": 0.29927870631217957,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.3984,
      "step": 221600
    },
    {
      "epoch": 2.156105791907571,
      "grad_norm": 0.30214032530784607,
      "learning_rate": 1.1791666666666668e-05,
      "loss": 0.3979,
      "step": 221700
    },
    {
      "epoch": 2.157078322773269,
      "grad_norm": 0.2775464653968811,
      "learning_rate": 1.175e-05,
      "loss": 0.398,
      "step": 221800
    },
    {
      "epoch": 2.1580508536389673,
      "grad_norm": 0.2964783012866974,
      "learning_rate": 1.1708333333333334e-05,
      "loss": 0.3979,
      "step": 221900
    },
    {
      "epoch": 2.159023384504666,
      "grad_norm": 0.26734068989753723,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.399,
      "step": 222000
    },
    {
      "epoch": 2.159995915370364,
      "grad_norm": 0.2805178761482239,
      "learning_rate": 1.1625000000000001e-05,
      "loss": 0.3969,
      "step": 222100
    },
    {
      "epoch": 2.1609684462360623,
      "grad_norm": 0.2804393768310547,
      "learning_rate": 1.1583333333333333e-05,
      "loss": 0.3986,
      "step": 222200
    },
    {
      "epoch": 2.161940977101761,
      "grad_norm": 0.2927398979663849,
      "learning_rate": 1.1541666666666667e-05,
      "loss": 0.398,
      "step": 222300
    },
    {
      "epoch": 2.1629135079674593,
      "grad_norm": 0.2899635136127472,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.3981,
      "step": 222400
    },
    {
      "epoch": 2.1638860388331573,
      "grad_norm": 0.2997301518917084,
      "learning_rate": 1.1458333333333333e-05,
      "loss": 0.3971,
      "step": 222500
    },
    {
      "epoch": 2.164858569698856,
      "grad_norm": 0.2870500683784485,
      "learning_rate": 1.1416666666666667e-05,
      "loss": 0.3985,
      "step": 222600
    },
    {
      "epoch": 2.1658311005645543,
      "grad_norm": 0.2780407667160034,
      "learning_rate": 1.1375e-05,
      "loss": 0.3973,
      "step": 222700
    },
    {
      "epoch": 2.1668036314302523,
      "grad_norm": 0.27420517802238464,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.3985,
      "step": 222800
    },
    {
      "epoch": 2.167776162295951,
      "grad_norm": 0.2797645628452301,
      "learning_rate": 1.1291666666666667e-05,
      "loss": 0.3975,
      "step": 222900
    },
    {
      "epoch": 2.1687486931616493,
      "grad_norm": 0.27654287219047546,
      "learning_rate": 1.125e-05,
      "loss": 0.3975,
      "step": 223000
    },
    {
      "epoch": 2.169721224027348,
      "grad_norm": 0.2678142189979553,
      "learning_rate": 1.1208333333333332e-05,
      "loss": 0.3992,
      "step": 223100
    },
    {
      "epoch": 2.170693754893046,
      "grad_norm": 0.29161569476127625,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.3975,
      "step": 223200
    },
    {
      "epoch": 2.1716662857587443,
      "grad_norm": 0.27425068616867065,
      "learning_rate": 1.1125000000000001e-05,
      "loss": 0.3971,
      "step": 223300
    },
    {
      "epoch": 2.172638816624443,
      "grad_norm": 0.2592032551765442,
      "learning_rate": 1.1083333333333335e-05,
      "loss": 0.397,
      "step": 223400
    },
    {
      "epoch": 2.173611347490141,
      "grad_norm": 0.2708098888397217,
      "learning_rate": 1.1041666666666666e-05,
      "loss": 0.398,
      "step": 223500
    },
    {
      "epoch": 2.1745838783558393,
      "grad_norm": 0.27403753995895386,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.3972,
      "step": 223600
    },
    {
      "epoch": 2.175556409221538,
      "grad_norm": 0.2815339267253876,
      "learning_rate": 1.0958333333333335e-05,
      "loss": 0.3981,
      "step": 223700
    },
    {
      "epoch": 2.176528940087236,
      "grad_norm": 0.28176456689834595,
      "learning_rate": 1.0916666666666667e-05,
      "loss": 0.397,
      "step": 223800
    },
    {
      "epoch": 2.1775014709529343,
      "grad_norm": 0.28213346004486084,
      "learning_rate": 1.0875e-05,
      "loss": 0.3982,
      "step": 223900
    },
    {
      "epoch": 2.178474001818633,
      "grad_norm": 0.2874598801136017,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.3974,
      "step": 224000
    },
    {
      "epoch": 2.179446532684331,
      "grad_norm": 0.30616334080696106,
      "learning_rate": 1.0791666666666667e-05,
      "loss": 0.3965,
      "step": 224100
    },
    {
      "epoch": 2.1804190635500293,
      "grad_norm": 0.2854715585708618,
      "learning_rate": 1.075e-05,
      "loss": 0.3974,
      "step": 224200
    },
    {
      "epoch": 2.181391594415728,
      "grad_norm": 0.2964795231819153,
      "learning_rate": 1.0708333333333334e-05,
      "loss": 0.398,
      "step": 224300
    },
    {
      "epoch": 2.1823641252814263,
      "grad_norm": 0.29702210426330566,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.3974,
      "step": 224400
    },
    {
      "epoch": 2.1833366561471244,
      "grad_norm": 0.29097992181777954,
      "learning_rate": 1.0625e-05,
      "loss": 0.3967,
      "step": 224500
    },
    {
      "epoch": 2.184309187012823,
      "grad_norm": 0.2824926972389221,
      "learning_rate": 1.0583333333333334e-05,
      "loss": 0.3973,
      "step": 224600
    },
    {
      "epoch": 2.1852817178785213,
      "grad_norm": 0.28456875681877136,
      "learning_rate": 1.0541666666666668e-05,
      "loss": 0.3977,
      "step": 224700
    },
    {
      "epoch": 2.1862542487442194,
      "grad_norm": 0.28135496377944946,
      "learning_rate": 1.05e-05,
      "loss": 0.3963,
      "step": 224800
    },
    {
      "epoch": 2.187226779609918,
      "grad_norm": 0.29989632964134216,
      "learning_rate": 1.0458333333333335e-05,
      "loss": 0.3975,
      "step": 224900
    },
    {
      "epoch": 2.1881993104756163,
      "grad_norm": 0.2755480408668518,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.3978,
      "step": 225000
    },
    {
      "epoch": 2.1891718413413144,
      "grad_norm": 0.27331218123435974,
      "learning_rate": 1.0375e-05,
      "loss": 0.3976,
      "step": 225100
    },
    {
      "epoch": 2.190144372207013,
      "grad_norm": 0.2998513877391815,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.3975,
      "step": 225200
    },
    {
      "epoch": 2.1911169030727113,
      "grad_norm": 0.29721957445144653,
      "learning_rate": 1.0291666666666667e-05,
      "loss": 0.3971,
      "step": 225300
    },
    {
      "epoch": 2.19208943393841,
      "grad_norm": 0.29379162192344666,
      "learning_rate": 1.025e-05,
      "loss": 0.3972,
      "step": 225400
    },
    {
      "epoch": 2.193061964804108,
      "grad_norm": 0.26175403594970703,
      "learning_rate": 1.0208333333333334e-05,
      "loss": 0.3964,
      "step": 225500
    },
    {
      "epoch": 2.1940344956698064,
      "grad_norm": 0.2802569568157196,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.3973,
      "step": 225600
    },
    {
      "epoch": 2.195007026535505,
      "grad_norm": 0.2840346693992615,
      "learning_rate": 1.0125e-05,
      "loss": 0.3973,
      "step": 225700
    },
    {
      "epoch": 2.195979557401203,
      "grad_norm": 0.26536405086517334,
      "learning_rate": 1.0083333333333334e-05,
      "loss": 0.398,
      "step": 225800
    },
    {
      "epoch": 2.1969520882669014,
      "grad_norm": 0.29133355617523193,
      "learning_rate": 1.0041666666666667e-05,
      "loss": 0.3969,
      "step": 225900
    },
    {
      "epoch": 2.1979246191326,
      "grad_norm": 0.2933201789855957,
      "learning_rate": 1e-05,
      "loss": 0.3976,
      "step": 226000
    },
    {
      "epoch": 2.198897149998298,
      "grad_norm": 0.2792310118675232,
      "learning_rate": 9.958333333333333e-06,
      "loss": 0.3978,
      "step": 226100
    },
    {
      "epoch": 2.1998696808639964,
      "grad_norm": 0.3065313696861267,
      "learning_rate": 9.916666666666668e-06,
      "loss": 0.3963,
      "step": 226200
    },
    {
      "epoch": 2.200842211729695,
      "grad_norm": 0.276154100894928,
      "learning_rate": 9.875000000000001e-06,
      "loss": 0.3985,
      "step": 226300
    },
    {
      "epoch": 2.201814742595393,
      "grad_norm": 0.27237391471862793,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.3971,
      "step": 226400
    },
    {
      "epoch": 2.2027872734610914,
      "grad_norm": 0.2659684121608734,
      "learning_rate": 9.791666666666666e-06,
      "loss": 0.3978,
      "step": 226500
    },
    {
      "epoch": 2.20375980432679,
      "grad_norm": 0.29978039860725403,
      "learning_rate": 9.750000000000002e-06,
      "loss": 0.3981,
      "step": 226600
    },
    {
      "epoch": 2.2047323351924883,
      "grad_norm": 0.27108126878738403,
      "learning_rate": 9.708333333333333e-06,
      "loss": 0.3971,
      "step": 226700
    },
    {
      "epoch": 2.2057048660581864,
      "grad_norm": 0.26906052231788635,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.3968,
      "step": 226800
    },
    {
      "epoch": 2.206677396923885,
      "grad_norm": 0.3004188537597656,
      "learning_rate": 9.625e-06,
      "loss": 0.3981,
      "step": 226900
    },
    {
      "epoch": 2.2076499277895834,
      "grad_norm": 0.3017527759075165,
      "learning_rate": 9.583333333333334e-06,
      "loss": 0.3972,
      "step": 227000
    },
    {
      "epoch": 2.2086224586552814,
      "grad_norm": 0.27251091599464417,
      "learning_rate": 9.541666666666667e-06,
      "loss": 0.3962,
      "step": 227100
    },
    {
      "epoch": 2.20959498952098,
      "grad_norm": 0.2803677022457123,
      "learning_rate": 9.5e-06,
      "loss": 0.3961,
      "step": 227200
    },
    {
      "epoch": 2.2105675203866784,
      "grad_norm": 0.28793472051620483,
      "learning_rate": 9.458333333333334e-06,
      "loss": 0.3968,
      "step": 227300
    },
    {
      "epoch": 2.211540051252377,
      "grad_norm": 0.2781749963760376,
      "learning_rate": 9.416666666666667e-06,
      "loss": 0.3971,
      "step": 227400
    },
    {
      "epoch": 2.212512582118075,
      "grad_norm": 0.2909204065799713,
      "learning_rate": 9.375000000000001e-06,
      "loss": 0.3983,
      "step": 227500
    },
    {
      "epoch": 2.2134851129837734,
      "grad_norm": 0.29110971093177795,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.397,
      "step": 227600
    },
    {
      "epoch": 2.214457643849472,
      "grad_norm": 0.2906396985054016,
      "learning_rate": 9.291666666666666e-06,
      "loss": 0.3974,
      "step": 227700
    },
    {
      "epoch": 2.21543017471517,
      "grad_norm": 0.2895795404911041,
      "learning_rate": 9.25e-06,
      "loss": 0.3967,
      "step": 227800
    },
    {
      "epoch": 2.2164027055808684,
      "grad_norm": 0.31509828567504883,
      "learning_rate": 9.208333333333335e-06,
      "loss": 0.3979,
      "step": 227900
    },
    {
      "epoch": 2.217375236446567,
      "grad_norm": 0.27624621987342834,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.3967,
      "step": 228000
    },
    {
      "epoch": 2.218347767312265,
      "grad_norm": 0.2957896292209625,
      "learning_rate": 9.125e-06,
      "loss": 0.397,
      "step": 228100
    },
    {
      "epoch": 2.2193202981779634,
      "grad_norm": 0.2756018042564392,
      "learning_rate": 9.083333333333333e-06,
      "loss": 0.3966,
      "step": 228200
    },
    {
      "epoch": 2.220292829043662,
      "grad_norm": 0.26745089888572693,
      "learning_rate": 9.041666666666668e-06,
      "loss": 0.3978,
      "step": 228300
    },
    {
      "epoch": 2.22126535990936,
      "grad_norm": 0.275363028049469,
      "learning_rate": 9e-06,
      "loss": 0.3962,
      "step": 228400
    },
    {
      "epoch": 2.2222378907750584,
      "grad_norm": 0.30799800157546997,
      "learning_rate": 8.958333333333334e-06,
      "loss": 0.3977,
      "step": 228500
    },
    {
      "epoch": 2.223210421640757,
      "grad_norm": 0.2953745722770691,
      "learning_rate": 8.916666666666667e-06,
      "loss": 0.3973,
      "step": 228600
    },
    {
      "epoch": 2.2241829525064554,
      "grad_norm": 0.2820061445236206,
      "learning_rate": 8.875e-06,
      "loss": 0.3968,
      "step": 228700
    },
    {
      "epoch": 2.2251554833721534,
      "grad_norm": 0.2581007778644562,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.3952,
      "step": 228800
    },
    {
      "epoch": 2.226128014237852,
      "grad_norm": 0.2780108153820038,
      "learning_rate": 8.791666666666667e-06,
      "loss": 0.3958,
      "step": 228900
    },
    {
      "epoch": 2.2271005451035504,
      "grad_norm": 0.30183666944503784,
      "learning_rate": 8.75e-06,
      "loss": 0.3977,
      "step": 229000
    },
    {
      "epoch": 2.2280730759692484,
      "grad_norm": 0.2826554775238037,
      "learning_rate": 8.708333333333334e-06,
      "loss": 0.3958,
      "step": 229100
    },
    {
      "epoch": 2.229045606834947,
      "grad_norm": 0.29252171516418457,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.3957,
      "step": 229200
    },
    {
      "epoch": 2.2300181377006454,
      "grad_norm": 0.2849203646183014,
      "learning_rate": 8.625e-06,
      "loss": 0.3975,
      "step": 229300
    },
    {
      "epoch": 2.2309906685663434,
      "grad_norm": 0.29180899262428284,
      "learning_rate": 8.583333333333333e-06,
      "loss": 0.397,
      "step": 229400
    },
    {
      "epoch": 2.231963199432042,
      "grad_norm": 0.3034037947654724,
      "learning_rate": 8.541666666666666e-06,
      "loss": 0.3967,
      "step": 229500
    },
    {
      "epoch": 2.2329357302977404,
      "grad_norm": 0.28227031230926514,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.3978,
      "step": 229600
    },
    {
      "epoch": 2.233908261163439,
      "grad_norm": 0.2992814779281616,
      "learning_rate": 8.458333333333333e-06,
      "loss": 0.3975,
      "step": 229700
    },
    {
      "epoch": 2.234880792029137,
      "grad_norm": 0.30107173323631287,
      "learning_rate": 8.416666666666667e-06,
      "loss": 0.3962,
      "step": 229800
    },
    {
      "epoch": 2.2358533228948354,
      "grad_norm": 0.2867450416088104,
      "learning_rate": 8.375e-06,
      "loss": 0.3977,
      "step": 229900
    },
    {
      "epoch": 2.236825853760534,
      "grad_norm": 0.2706664800643921,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.3955,
      "step": 230000
    },
    {
      "epoch": 2.236825853760534,
      "eval_accuracy": 0.6704088476546226,
      "eval_loss": 0.39095449447631836,
      "eval_runtime": 3736.6438,
      "eval_samples_per_second": 611.444,
      "eval_steps_per_second": 6.115,
      "step": 230000
    },
    {
      "epoch": 2.237798384626232,
      "grad_norm": 0.2833651006221771,
      "learning_rate": 8.291666666666667e-06,
      "loss": 0.3969,
      "step": 230100
    },
    {
      "epoch": 2.2387709154919304,
      "grad_norm": 0.29959166049957275,
      "learning_rate": 8.25e-06,
      "loss": 0.3958,
      "step": 230200
    },
    {
      "epoch": 2.239743446357629,
      "grad_norm": 0.27934879064559937,
      "learning_rate": 8.208333333333332e-06,
      "loss": 0.3982,
      "step": 230300
    },
    {
      "epoch": 2.240715977223327,
      "grad_norm": 0.2983906865119934,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.3959,
      "step": 230400
    },
    {
      "epoch": 2.2416885080890254,
      "grad_norm": 0.2728358209133148,
      "learning_rate": 8.125000000000001e-06,
      "loss": 0.396,
      "step": 230500
    },
    {
      "epoch": 2.242661038954724,
      "grad_norm": 0.26908791065216064,
      "learning_rate": 8.083333333333333e-06,
      "loss": 0.3963,
      "step": 230600
    },
    {
      "epoch": 2.243633569820422,
      "grad_norm": 0.2810545563697815,
      "learning_rate": 8.041666666666666e-06,
      "loss": 0.3962,
      "step": 230700
    },
    {
      "epoch": 2.2446061006861204,
      "grad_norm": 0.29555442929267883,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.3954,
      "step": 230800
    },
    {
      "epoch": 2.245578631551819,
      "grad_norm": 0.28471100330352783,
      "learning_rate": 7.958333333333335e-06,
      "loss": 0.3962,
      "step": 230900
    },
    {
      "epoch": 2.2465511624175174,
      "grad_norm": 0.2879732549190521,
      "learning_rate": 7.916666666666667e-06,
      "loss": 0.3964,
      "step": 231000
    },
    {
      "epoch": 2.2475236932832154,
      "grad_norm": 0.2826412320137024,
      "learning_rate": 7.875e-06,
      "loss": 0.3958,
      "step": 231100
    },
    {
      "epoch": 2.248496224148914,
      "grad_norm": 0.2748869061470032,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.3957,
      "step": 231200
    },
    {
      "epoch": 2.2494687550146124,
      "grad_norm": 0.2692415714263916,
      "learning_rate": 7.791666666666667e-06,
      "loss": 0.396,
      "step": 231300
    },
    {
      "epoch": 2.2504412858803104,
      "grad_norm": 0.3261725604534149,
      "learning_rate": 7.75e-06,
      "loss": 0.3951,
      "step": 231400
    },
    {
      "epoch": 2.251413816746009,
      "grad_norm": 0.2903096377849579,
      "learning_rate": 7.708333333333334e-06,
      "loss": 0.3964,
      "step": 231500
    },
    {
      "epoch": 2.2523863476117074,
      "grad_norm": 0.3099885880947113,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.3964,
      "step": 231600
    },
    {
      "epoch": 2.253358878477406,
      "grad_norm": 0.2825689911842346,
      "learning_rate": 7.625e-06,
      "loss": 0.3963,
      "step": 231700
    },
    {
      "epoch": 2.254331409343104,
      "grad_norm": 0.2826392948627472,
      "learning_rate": 7.583333333333334e-06,
      "loss": 0.3971,
      "step": 231800
    },
    {
      "epoch": 2.2553039402088024,
      "grad_norm": 0.28216803073883057,
      "learning_rate": 7.541666666666668e-06,
      "loss": 0.3958,
      "step": 231900
    },
    {
      "epoch": 2.256276471074501,
      "grad_norm": 0.2772959768772125,
      "learning_rate": 7.5e-06,
      "loss": 0.3953,
      "step": 232000
    },
    {
      "epoch": 2.257249001940199,
      "grad_norm": 0.29310792684555054,
      "learning_rate": 7.458333333333334e-06,
      "loss": 0.3962,
      "step": 232100
    },
    {
      "epoch": 2.2582215328058974,
      "grad_norm": 0.320904940366745,
      "learning_rate": 7.416666666666668e-06,
      "loss": 0.3976,
      "step": 232200
    },
    {
      "epoch": 2.259194063671596,
      "grad_norm": 0.3043757975101471,
      "learning_rate": 7.375e-06,
      "loss": 0.3961,
      "step": 232300
    },
    {
      "epoch": 2.260166594537294,
      "grad_norm": 0.2890740931034088,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.396,
      "step": 232400
    },
    {
      "epoch": 2.2611391254029924,
      "grad_norm": 0.28456711769104004,
      "learning_rate": 7.2916666666666674e-06,
      "loss": 0.3958,
      "step": 232500
    },
    {
      "epoch": 2.262111656268691,
      "grad_norm": 0.29374998807907104,
      "learning_rate": 7.25e-06,
      "loss": 0.3954,
      "step": 232600
    },
    {
      "epoch": 2.263084187134389,
      "grad_norm": 0.295523464679718,
      "learning_rate": 7.2083333333333335e-06,
      "loss": 0.3967,
      "step": 232700
    },
    {
      "epoch": 2.2640567180000875,
      "grad_norm": 0.29159727692604065,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.3958,
      "step": 232800
    },
    {
      "epoch": 2.265029248865786,
      "grad_norm": 0.2882337272167206,
      "learning_rate": 7.1249999999999995e-06,
      "loss": 0.3969,
      "step": 232900
    },
    {
      "epoch": 2.266001779731484,
      "grad_norm": 0.2842516005039215,
      "learning_rate": 7.083333333333334e-06,
      "loss": 0.3969,
      "step": 233000
    },
    {
      "epoch": 2.2669743105971825,
      "grad_norm": 0.3011073172092438,
      "learning_rate": 7.041666666666667e-06,
      "loss": 0.3961,
      "step": 233100
    },
    {
      "epoch": 2.267946841462881,
      "grad_norm": 0.27607664465904236,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.3959,
      "step": 233200
    },
    {
      "epoch": 2.2689193723285794,
      "grad_norm": 0.28110548853874207,
      "learning_rate": 6.958333333333333e-06,
      "loss": 0.3967,
      "step": 233300
    },
    {
      "epoch": 2.2698919031942775,
      "grad_norm": 0.29447293281555176,
      "learning_rate": 6.916666666666667e-06,
      "loss": 0.3961,
      "step": 233400
    },
    {
      "epoch": 2.270864434059976,
      "grad_norm": 0.30118757486343384,
      "learning_rate": 6.875000000000001e-06,
      "loss": 0.3958,
      "step": 233500
    },
    {
      "epoch": 2.2718369649256744,
      "grad_norm": 0.28205960988998413,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.3955,
      "step": 233600
    },
    {
      "epoch": 2.272809495791373,
      "grad_norm": 0.30185553431510925,
      "learning_rate": 6.791666666666667e-06,
      "loss": 0.3961,
      "step": 233700
    },
    {
      "epoch": 2.273782026657071,
      "grad_norm": 0.2703497111797333,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.3964,
      "step": 233800
    },
    {
      "epoch": 2.2747545575227694,
      "grad_norm": 0.2918744683265686,
      "learning_rate": 6.708333333333333e-06,
      "loss": 0.3953,
      "step": 233900
    },
    {
      "epoch": 2.275727088388468,
      "grad_norm": 0.2825208008289337,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.3962,
      "step": 234000
    },
    {
      "epoch": 2.276699619254166,
      "grad_norm": 0.30477175116539,
      "learning_rate": 6.625000000000001e-06,
      "loss": 0.3959,
      "step": 234100
    },
    {
      "epoch": 2.2776721501198645,
      "grad_norm": 0.2781221568584442,
      "learning_rate": 6.583333333333333e-06,
      "loss": 0.3955,
      "step": 234200
    },
    {
      "epoch": 2.278644680985563,
      "grad_norm": 0.28423210978507996,
      "learning_rate": 6.541666666666667e-06,
      "loss": 0.3966,
      "step": 234300
    },
    {
      "epoch": 2.279617211851261,
      "grad_norm": 0.2756262719631195,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.3966,
      "step": 234400
    },
    {
      "epoch": 2.2805897427169595,
      "grad_norm": 0.2933197021484375,
      "learning_rate": 6.458333333333334e-06,
      "loss": 0.3958,
      "step": 234500
    },
    {
      "epoch": 2.281562273582658,
      "grad_norm": 0.2801559269428253,
      "learning_rate": 6.4166666666666665e-06,
      "loss": 0.3957,
      "step": 234600
    },
    {
      "epoch": 2.282534804448356,
      "grad_norm": 0.29948365688323975,
      "learning_rate": 6.375000000000001e-06,
      "loss": 0.3965,
      "step": 234700
    },
    {
      "epoch": 2.2835073353140545,
      "grad_norm": 0.2886398434638977,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.3962,
      "step": 234800
    },
    {
      "epoch": 2.284479866179753,
      "grad_norm": 0.3140013515949249,
      "learning_rate": 6.291666666666667e-06,
      "loss": 0.3951,
      "step": 234900
    },
    {
      "epoch": 2.285452397045451,
      "grad_norm": 0.28385278582572937,
      "learning_rate": 6.25e-06,
      "loss": 0.395,
      "step": 235000
    },
    {
      "epoch": 2.2864249279111495,
      "grad_norm": 0.27994298934936523,
      "learning_rate": 6.208333333333334e-06,
      "loss": 0.3962,
      "step": 235100
    },
    {
      "epoch": 2.287397458776848,
      "grad_norm": 0.29762113094329834,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.396,
      "step": 235200
    },
    {
      "epoch": 2.2883699896425465,
      "grad_norm": 0.28959330916404724,
      "learning_rate": 6.125e-06,
      "loss": 0.3956,
      "step": 235300
    },
    {
      "epoch": 2.2893425205082445,
      "grad_norm": 0.2798841595649719,
      "learning_rate": 6.083333333333334e-06,
      "loss": 0.3959,
      "step": 235400
    },
    {
      "epoch": 2.290315051373943,
      "grad_norm": 0.28553158044815063,
      "learning_rate": 6.041666666666667e-06,
      "loss": 0.3961,
      "step": 235500
    },
    {
      "epoch": 2.2912875822396415,
      "grad_norm": 0.29201269149780273,
      "learning_rate": 6e-06,
      "loss": 0.3953,
      "step": 235600
    },
    {
      "epoch": 2.2922601131053395,
      "grad_norm": 0.28962835669517517,
      "learning_rate": 5.958333333333334e-06,
      "loss": 0.3952,
      "step": 235700
    },
    {
      "epoch": 2.293232643971038,
      "grad_norm": 0.2753866910934448,
      "learning_rate": 5.916666666666667e-06,
      "loss": 0.3963,
      "step": 235800
    },
    {
      "epoch": 2.2942051748367365,
      "grad_norm": 0.2729218900203705,
      "learning_rate": 5.875e-06,
      "loss": 0.3956,
      "step": 235900
    },
    {
      "epoch": 2.295177705702435,
      "grad_norm": 0.28383058309555054,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.3958,
      "step": 236000
    },
    {
      "epoch": 2.296150236568133,
      "grad_norm": 0.2780441641807556,
      "learning_rate": 5.7916666666666666e-06,
      "loss": 0.3943,
      "step": 236100
    },
    {
      "epoch": 2.2971227674338315,
      "grad_norm": 0.27261218428611755,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.3946,
      "step": 236200
    },
    {
      "epoch": 2.29809529829953,
      "grad_norm": 0.2938636541366577,
      "learning_rate": 5.7083333333333335e-06,
      "loss": 0.396,
      "step": 236300
    },
    {
      "epoch": 2.299067829165228,
      "grad_norm": 0.30007094144821167,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.3962,
      "step": 236400
    },
    {
      "epoch": 2.3000403600309265,
      "grad_norm": 0.2909797132015228,
      "learning_rate": 5.625e-06,
      "loss": 0.3952,
      "step": 236500
    },
    {
      "epoch": 2.301012890896625,
      "grad_norm": 0.27847927808761597,
      "learning_rate": 5.583333333333334e-06,
      "loss": 0.3961,
      "step": 236600
    },
    {
      "epoch": 2.301985421762323,
      "grad_norm": 0.27435681223869324,
      "learning_rate": 5.541666666666667e-06,
      "loss": 0.3952,
      "step": 236700
    },
    {
      "epoch": 2.3029579526280215,
      "grad_norm": 0.29340991377830505,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.3951,
      "step": 236800
    },
    {
      "epoch": 2.30393048349372,
      "grad_norm": 0.2985413074493408,
      "learning_rate": 5.458333333333333e-06,
      "loss": 0.3961,
      "step": 236900
    },
    {
      "epoch": 2.304903014359418,
      "grad_norm": 0.29709592461586,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.3959,
      "step": 237000
    },
    {
      "epoch": 2.3058755452251165,
      "grad_norm": 0.2871498763561249,
      "learning_rate": 5.375e-06,
      "loss": 0.3955,
      "step": 237100
    },
    {
      "epoch": 2.306848076090815,
      "grad_norm": 0.3031529188156128,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.3962,
      "step": 237200
    },
    {
      "epoch": 2.3078206069565135,
      "grad_norm": 0.3079105615615845,
      "learning_rate": 5.291666666666667e-06,
      "loss": 0.3956,
      "step": 237300
    },
    {
      "epoch": 2.3087931378222115,
      "grad_norm": 0.3125442564487457,
      "learning_rate": 5.25e-06,
      "loss": 0.3954,
      "step": 237400
    },
    {
      "epoch": 2.30976566868791,
      "grad_norm": 0.3017728626728058,
      "learning_rate": 5.208333333333334e-06,
      "loss": 0.3949,
      "step": 237500
    },
    {
      "epoch": 2.3107381995536085,
      "grad_norm": 0.2765635550022125,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.3951,
      "step": 237600
    },
    {
      "epoch": 2.3117107304193065,
      "grad_norm": 0.287539005279541,
      "learning_rate": 5.125e-06,
      "loss": 0.3963,
      "step": 237700
    },
    {
      "epoch": 2.312683261285005,
      "grad_norm": 0.2819834053516388,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.3953,
      "step": 237800
    },
    {
      "epoch": 2.3136557921507035,
      "grad_norm": 0.2973949611186981,
      "learning_rate": 5.041666666666667e-06,
      "loss": 0.3958,
      "step": 237900
    },
    {
      "epoch": 2.314628323016402,
      "grad_norm": 0.28050824999809265,
      "learning_rate": 5e-06,
      "loss": 0.3944,
      "step": 238000
    },
    {
      "epoch": 2.3156008538821,
      "grad_norm": 0.27916550636291504,
      "learning_rate": 4.958333333333334e-06,
      "loss": 0.3949,
      "step": 238100
    },
    {
      "epoch": 2.3165733847477985,
      "grad_norm": 0.2665116786956787,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 0.3957,
      "step": 238200
    },
    {
      "epoch": 2.317545915613497,
      "grad_norm": 0.2775810956954956,
      "learning_rate": 4.875000000000001e-06,
      "loss": 0.3955,
      "step": 238300
    },
    {
      "epoch": 2.318518446479195,
      "grad_norm": 0.2862388491630554,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.3953,
      "step": 238400
    },
    {
      "epoch": 2.3194909773448935,
      "grad_norm": 0.28696104884147644,
      "learning_rate": 4.791666666666667e-06,
      "loss": 0.3948,
      "step": 238500
    },
    {
      "epoch": 2.320463508210592,
      "grad_norm": 0.2716658413410187,
      "learning_rate": 4.75e-06,
      "loss": 0.3952,
      "step": 238600
    },
    {
      "epoch": 2.32143603907629,
      "grad_norm": 0.27632543444633484,
      "learning_rate": 4.708333333333334e-06,
      "loss": 0.3948,
      "step": 238700
    },
    {
      "epoch": 2.3224085699419885,
      "grad_norm": 0.30261391401290894,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.3957,
      "step": 238800
    },
    {
      "epoch": 2.323381100807687,
      "grad_norm": 0.303960382938385,
      "learning_rate": 4.625e-06,
      "loss": 0.3956,
      "step": 238900
    },
    {
      "epoch": 2.324353631673385,
      "grad_norm": 0.2883833646774292,
      "learning_rate": 4.583333333333333e-06,
      "loss": 0.3956,
      "step": 239000
    },
    {
      "epoch": 2.3253261625390835,
      "grad_norm": 0.2931555211544037,
      "learning_rate": 4.541666666666667e-06,
      "loss": 0.3958,
      "step": 239100
    },
    {
      "epoch": 2.326298693404782,
      "grad_norm": 0.277803897857666,
      "learning_rate": 4.5e-06,
      "loss": 0.3952,
      "step": 239200
    },
    {
      "epoch": 2.32727122427048,
      "grad_norm": 0.28739434480667114,
      "learning_rate": 4.4583333333333336e-06,
      "loss": 0.3954,
      "step": 239300
    },
    {
      "epoch": 2.3282437551361785,
      "grad_norm": 0.28708550333976746,
      "learning_rate": 4.416666666666667e-06,
      "loss": 0.396,
      "step": 239400
    },
    {
      "epoch": 2.329216286001877,
      "grad_norm": 0.29529696702957153,
      "learning_rate": 4.375e-06,
      "loss": 0.393,
      "step": 239500
    },
    {
      "epoch": 2.3301888168675755,
      "grad_norm": 0.28380686044692993,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.3945,
      "step": 239600
    },
    {
      "epoch": 2.3311613477332735,
      "grad_norm": 0.2823566198348999,
      "learning_rate": 4.2916666666666665e-06,
      "loss": 0.3956,
      "step": 239700
    },
    {
      "epoch": 2.332133878598972,
      "grad_norm": 0.29703280329704285,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.3953,
      "step": 239800
    },
    {
      "epoch": 2.3331064094646705,
      "grad_norm": 0.30976077914237976,
      "learning_rate": 4.208333333333333e-06,
      "loss": 0.3946,
      "step": 239900
    },
    {
      "epoch": 2.3340789403303686,
      "grad_norm": 0.2960081100463867,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.395,
      "step": 240000
    },
    {
      "epoch": 2.3340789403303686,
      "eval_accuracy": 0.6705778421142979,
      "eval_loss": 0.3895508944988251,
      "eval_runtime": 3755.847,
      "eval_samples_per_second": 608.318,
      "eval_steps_per_second": 6.083,
      "step": 240000
    },
    {
      "epoch": 2.335051471196067,
      "grad_norm": 0.2822492718696594,
      "learning_rate": 4.125e-06,
      "loss": 0.3962,
      "step": 240100
    },
    {
      "epoch": 2.3360240020617655,
      "grad_norm": 0.3001575171947479,
      "learning_rate": 4.083333333333334e-06,
      "loss": 0.3951,
      "step": 240200
    },
    {
      "epoch": 2.336996532927464,
      "grad_norm": 0.277620404958725,
      "learning_rate": 4.041666666666666e-06,
      "loss": 0.3945,
      "step": 240300
    },
    {
      "epoch": 2.337969063793162,
      "grad_norm": 0.2856244742870331,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.3951,
      "step": 240400
    },
    {
      "epoch": 2.3389415946588605,
      "grad_norm": 0.2842472791671753,
      "learning_rate": 3.958333333333333e-06,
      "loss": 0.3951,
      "step": 240500
    },
    {
      "epoch": 2.339914125524559,
      "grad_norm": 0.2930125296115875,
      "learning_rate": 3.916666666666667e-06,
      "loss": 0.3945,
      "step": 240600
    },
    {
      "epoch": 2.340886656390257,
      "grad_norm": 0.29863154888153076,
      "learning_rate": 3.875e-06,
      "loss": 0.3953,
      "step": 240700
    },
    {
      "epoch": 2.3418591872559555,
      "grad_norm": 0.303213894367218,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.3955,
      "step": 240800
    },
    {
      "epoch": 2.342831718121654,
      "grad_norm": 0.29872384667396545,
      "learning_rate": 3.791666666666667e-06,
      "loss": 0.3946,
      "step": 240900
    },
    {
      "epoch": 2.343804248987352,
      "grad_norm": 0.30256491899490356,
      "learning_rate": 3.75e-06,
      "loss": 0.3952,
      "step": 241000
    },
    {
      "epoch": 2.3447767798530506,
      "grad_norm": 0.2917299270629883,
      "learning_rate": 3.708333333333334e-06,
      "loss": 0.395,
      "step": 241100
    },
    {
      "epoch": 2.345749310718749,
      "grad_norm": 0.2984364628791809,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.3952,
      "step": 241200
    },
    {
      "epoch": 2.346721841584447,
      "grad_norm": 0.28237295150756836,
      "learning_rate": 3.625e-06,
      "loss": 0.3956,
      "step": 241300
    },
    {
      "epoch": 2.3476943724501456,
      "grad_norm": 0.27978551387786865,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 0.3937,
      "step": 241400
    },
    {
      "epoch": 2.348666903315844,
      "grad_norm": 0.27114787697792053,
      "learning_rate": 3.541666666666667e-06,
      "loss": 0.3964,
      "step": 241500
    },
    {
      "epoch": 2.3496394341815425,
      "grad_norm": 0.2935703694820404,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.3944,
      "step": 241600
    },
    {
      "epoch": 2.3506119650472406,
      "grad_norm": 0.28230106830596924,
      "learning_rate": 3.4583333333333334e-06,
      "loss": 0.3945,
      "step": 241700
    },
    {
      "epoch": 2.351584495912939,
      "grad_norm": 0.3207782804965973,
      "learning_rate": 3.4166666666666664e-06,
      "loss": 0.3945,
      "step": 241800
    },
    {
      "epoch": 2.3525570267786375,
      "grad_norm": 0.27795085310935974,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 0.3946,
      "step": 241900
    },
    {
      "epoch": 2.3535295576443356,
      "grad_norm": 0.3225453495979309,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.3949,
      "step": 242000
    },
    {
      "epoch": 2.354502088510034,
      "grad_norm": 0.2887141704559326,
      "learning_rate": 3.2916666666666664e-06,
      "loss": 0.3949,
      "step": 242100
    },
    {
      "epoch": 2.3554746193757325,
      "grad_norm": 0.2779887616634369,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 0.3952,
      "step": 242200
    },
    {
      "epoch": 2.356447150241431,
      "grad_norm": 0.29276323318481445,
      "learning_rate": 3.2083333333333332e-06,
      "loss": 0.3946,
      "step": 242300
    },
    {
      "epoch": 2.357419681107129,
      "grad_norm": 0.297276109457016,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.3944,
      "step": 242400
    },
    {
      "epoch": 2.3583922119728276,
      "grad_norm": 0.277701199054718,
      "learning_rate": 3.125e-06,
      "loss": 0.3941,
      "step": 242500
    },
    {
      "epoch": 2.359364742838526,
      "grad_norm": 0.27552345395088196,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.3945,
      "step": 242600
    },
    {
      "epoch": 2.360337273704224,
      "grad_norm": 0.2835453152656555,
      "learning_rate": 3.041666666666667e-06,
      "loss": 0.3953,
      "step": 242700
    },
    {
      "epoch": 2.3613098045699226,
      "grad_norm": 0.2981678247451782,
      "learning_rate": 3e-06,
      "loss": 0.3948,
      "step": 242800
    },
    {
      "epoch": 2.362282335435621,
      "grad_norm": 0.3055354654788971,
      "learning_rate": 2.9583333333333335e-06,
      "loss": 0.3916,
      "step": 242900
    },
    {
      "epoch": 2.363254866301319,
      "grad_norm": 0.2987009286880493,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.3887,
      "step": 243000
    },
    {
      "epoch": 2.3642273971670176,
      "grad_norm": 0.29202553629875183,
      "learning_rate": 2.8750000000000004e-06,
      "loss": 0.3892,
      "step": 243100
    },
    {
      "epoch": 2.365199928032716,
      "grad_norm": 0.2799304723739624,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.3899,
      "step": 243200
    },
    {
      "epoch": 2.366172458898414,
      "grad_norm": 0.3144552409648895,
      "learning_rate": 2.791666666666667e-06,
      "loss": 0.3906,
      "step": 243300
    },
    {
      "epoch": 2.3671449897641126,
      "grad_norm": 0.3122982382774353,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.3906,
      "step": 243400
    },
    {
      "epoch": 2.368117520629811,
      "grad_norm": 0.27194106578826904,
      "learning_rate": 2.7083333333333334e-06,
      "loss": 0.3907,
      "step": 243500
    },
    {
      "epoch": 2.369090051495509,
      "grad_norm": 0.29400354623794556,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.3905,
      "step": 243600
    },
    {
      "epoch": 2.3700625823612076,
      "grad_norm": 0.2817358374595642,
      "learning_rate": 2.625e-06,
      "loss": 0.3897,
      "step": 243700
    },
    {
      "epoch": 2.371035113226906,
      "grad_norm": 0.3032090961933136,
      "learning_rate": 2.5833333333333333e-06,
      "loss": 0.39,
      "step": 243800
    },
    {
      "epoch": 2.3720076440926046,
      "grad_norm": 0.3091162443161011,
      "learning_rate": 2.5416666666666668e-06,
      "loss": 0.3899,
      "step": 243900
    },
    {
      "epoch": 2.3729801749583026,
      "grad_norm": 0.27798154950141907,
      "learning_rate": 2.5e-06,
      "loss": 0.3905,
      "step": 244000
    },
    {
      "epoch": 2.373952705824001,
      "grad_norm": 0.2665862441062927,
      "learning_rate": 2.4583333333333332e-06,
      "loss": 0.39,
      "step": 244100
    },
    {
      "epoch": 2.3749252366896996,
      "grad_norm": 0.2812109589576721,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.3904,
      "step": 244200
    },
    {
      "epoch": 2.3758977675553976,
      "grad_norm": 0.2774258553981781,
      "learning_rate": 2.375e-06,
      "loss": 0.3907,
      "step": 244300
    },
    {
      "epoch": 2.376870298421096,
      "grad_norm": 0.29215437173843384,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.3906,
      "step": 244400
    },
    {
      "epoch": 2.3778428292867946,
      "grad_norm": 0.29828593134880066,
      "learning_rate": 2.2916666666666666e-06,
      "loss": 0.3906,
      "step": 244500
    },
    {
      "epoch": 2.378815360152493,
      "grad_norm": 0.29417502880096436,
      "learning_rate": 2.25e-06,
      "loss": 0.3905,
      "step": 244600
    },
    {
      "epoch": 2.379787891018191,
      "grad_norm": 0.2894226312637329,
      "learning_rate": 2.2083333333333335e-06,
      "loss": 0.3899,
      "step": 244700
    },
    {
      "epoch": 2.3807604218838896,
      "grad_norm": 0.2884227931499481,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.3902,
      "step": 244800
    },
    {
      "epoch": 2.381732952749588,
      "grad_norm": 0.3000844419002533,
      "learning_rate": 2.1250000000000004e-06,
      "loss": 0.3895,
      "step": 244900
    },
    {
      "epoch": 2.382705483615286,
      "grad_norm": 0.27101579308509827,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.3901,
      "step": 245000
    },
    {
      "epoch": 2.3836780144809846,
      "grad_norm": 0.3166503608226776,
      "learning_rate": 2.041666666666667e-06,
      "loss": 0.3904,
      "step": 245100
    },
    {
      "epoch": 2.384650545346683,
      "grad_norm": 0.28739601373672485,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.3908,
      "step": 245200
    },
    {
      "epoch": 2.385623076212381,
      "grad_norm": 0.2930181920528412,
      "learning_rate": 1.9583333333333334e-06,
      "loss": 0.3899,
      "step": 245300
    },
    {
      "epoch": 2.3865956070780796,
      "grad_norm": 0.2785412073135376,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.3899,
      "step": 245400
    },
    {
      "epoch": 2.387568137943778,
      "grad_norm": 0.3065173029899597,
      "learning_rate": 1.875e-06,
      "loss": 0.3894,
      "step": 245500
    },
    {
      "epoch": 2.388540668809476,
      "grad_norm": 0.3255196511745453,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.3891,
      "step": 245600
    },
    {
      "epoch": 2.3895131996751746,
      "grad_norm": 0.3205992877483368,
      "learning_rate": 1.7916666666666667e-06,
      "loss": 0.39,
      "step": 245700
    },
    {
      "epoch": 2.390485730540873,
      "grad_norm": 0.25848644971847534,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.3902,
      "step": 245800
    },
    {
      "epoch": 2.3914582614065716,
      "grad_norm": 0.28606748580932617,
      "learning_rate": 1.7083333333333332e-06,
      "loss": 0.3892,
      "step": 245900
    },
    {
      "epoch": 2.3924307922722696,
      "grad_norm": 0.27753525972366333,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.3897,
      "step": 246000
    },
    {
      "epoch": 2.393403323137968,
      "grad_norm": 0.29733631014823914,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 0.3906,
      "step": 246100
    },
    {
      "epoch": 2.3943758540036666,
      "grad_norm": 0.28094708919525146,
      "learning_rate": 1.5833333333333336e-06,
      "loss": 0.391,
      "step": 246200
    },
    {
      "epoch": 2.3953483848693646,
      "grad_norm": 0.3240448832511902,
      "learning_rate": 1.5416666666666668e-06,
      "loss": 0.3899,
      "step": 246300
    },
    {
      "epoch": 2.396320915735063,
      "grad_norm": 0.292303204536438,
      "learning_rate": 1.5e-06,
      "loss": 0.3903,
      "step": 246400
    },
    {
      "epoch": 2.3972934466007616,
      "grad_norm": 0.2949414551258087,
      "learning_rate": 1.4583333333333335e-06,
      "loss": 0.39,
      "step": 246500
    },
    {
      "epoch": 2.39826597746646,
      "grad_norm": 0.3051081597805023,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 0.39,
      "step": 246600
    },
    {
      "epoch": 2.399238508332158,
      "grad_norm": 0.27979108691215515,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 0.3895,
      "step": 246700
    },
    {
      "epoch": 2.4002110391978566,
      "grad_norm": 0.2798682451248169,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.3906,
      "step": 246800
    },
    {
      "epoch": 2.401183570063555,
      "grad_norm": 0.2885388731956482,
      "learning_rate": 1.2916666666666667e-06,
      "loss": 0.39,
      "step": 246900
    },
    {
      "epoch": 2.402156100929253,
      "grad_norm": 0.2803615629673004,
      "learning_rate": 1.25e-06,
      "loss": 0.3894,
      "step": 247000
    },
    {
      "epoch": 2.4031286317949516,
      "grad_norm": 0.3065323829650879,
      "learning_rate": 1.2083333333333333e-06,
      "loss": 0.3902,
      "step": 247100
    },
    {
      "epoch": 2.40410116266065,
      "grad_norm": 0.2921239733695984,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.3899,
      "step": 247200
    },
    {
      "epoch": 2.405073693526348,
      "grad_norm": 0.28402575850486755,
      "learning_rate": 1.125e-06,
      "loss": 0.3898,
      "step": 247300
    },
    {
      "epoch": 2.4060462243920466,
      "grad_norm": 0.2907876670360565,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 0.3899,
      "step": 247400
    },
    {
      "epoch": 2.407018755257745,
      "grad_norm": 0.29149696230888367,
      "learning_rate": 1.0416666666666667e-06,
      "loss": 0.3897,
      "step": 247500
    },
    {
      "epoch": 2.407991286123443,
      "grad_norm": 0.2855728566646576,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.3891,
      "step": 247600
    },
    {
      "epoch": 2.4089638169891416,
      "grad_norm": 0.28538453578948975,
      "learning_rate": 9.583333333333334e-07,
      "loss": 0.3904,
      "step": 247700
    },
    {
      "epoch": 2.40993634785484,
      "grad_norm": 0.2912917733192444,
      "learning_rate": 9.166666666666667e-07,
      "loss": 0.3902,
      "step": 247800
    },
    {
      "epoch": 2.410908878720538,
      "grad_norm": 0.28227829933166504,
      "learning_rate": 8.750000000000001e-07,
      "loss": 0.3898,
      "step": 247900
    },
    {
      "epoch": 2.4118814095862366,
      "grad_norm": 0.3205602467060089,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.3904,
      "step": 248000
    },
    {
      "epoch": 2.412853940451935,
      "grad_norm": 0.28860822319984436,
      "learning_rate": 7.916666666666668e-07,
      "loss": 0.3903,
      "step": 248100
    },
    {
      "epoch": 2.4138264713176336,
      "grad_norm": 0.308244913816452,
      "learning_rate": 7.5e-07,
      "loss": 0.3897,
      "step": 248200
    },
    {
      "epoch": 2.4147990021833317,
      "grad_norm": 0.28750431537628174,
      "learning_rate": 7.083333333333334e-07,
      "loss": 0.39,
      "step": 248300
    },
    {
      "epoch": 2.41577153304903,
      "grad_norm": 0.29881083965301514,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.3895,
      "step": 248400
    },
    {
      "epoch": 2.4167440639147286,
      "grad_norm": 0.2902835011482239,
      "learning_rate": 6.25e-07,
      "loss": 0.3899,
      "step": 248500
    },
    {
      "epoch": 2.4177165947804267,
      "grad_norm": 0.28722330927848816,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.3906,
      "step": 248600
    },
    {
      "epoch": 2.418689125646125,
      "grad_norm": 0.28238579630851746,
      "learning_rate": 5.416666666666667e-07,
      "loss": 0.3897,
      "step": 248700
    },
    {
      "epoch": 2.4196616565118236,
      "grad_norm": 0.27874282002449036,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.3895,
      "step": 248800
    },
    {
      "epoch": 2.420634187377522,
      "grad_norm": 0.29273271560668945,
      "learning_rate": 4.583333333333334e-07,
      "loss": 0.3897,
      "step": 248900
    },
    {
      "epoch": 2.42160671824322,
      "grad_norm": 0.29720640182495117,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.3892,
      "step": 249000
    },
    {
      "epoch": 2.4225792491089186,
      "grad_norm": 0.29878300428390503,
      "learning_rate": 3.75e-07,
      "loss": 0.3904,
      "step": 249100
    },
    {
      "epoch": 2.423551779974617,
      "grad_norm": 0.3081071972846985,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.3912,
      "step": 249200
    },
    {
      "epoch": 2.424524310840315,
      "grad_norm": 0.30329737067222595,
      "learning_rate": 2.916666666666667e-07,
      "loss": 0.391,
      "step": 249300
    },
    {
      "epoch": 2.4254968417060137,
      "grad_norm": 0.31612491607666016,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 0.3907,
      "step": 249400
    },
    {
      "epoch": 2.426469372571712,
      "grad_norm": 0.2896081507205963,
      "learning_rate": 2.0833333333333333e-07,
      "loss": 0.391,
      "step": 249500
    },
    {
      "epoch": 2.42744190343741,
      "grad_norm": 0.27958300709724426,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.3893,
      "step": 249600
    },
    {
      "epoch": 2.4284144343031087,
      "grad_norm": 0.2824617922306061,
      "learning_rate": 1.2500000000000002e-07,
      "loss": 0.3894,
      "step": 249700
    },
    {
      "epoch": 2.429386965168807,
      "grad_norm": 0.28157174587249756,
      "learning_rate": 8.333333333333334e-08,
      "loss": 0.3894,
      "step": 249800
    },
    {
      "epoch": 2.430359496034505,
      "grad_norm": 0.2766910493373871,
      "learning_rate": 4.166666666666667e-08,
      "loss": 0.39,
      "step": 249900
    },
    {
      "epoch": 2.4313320269002037,
      "grad_norm": 0.3069501221179962,
      "learning_rate": 0.0,
      "loss": 0.3889,
      "step": 250000
    },
    {
      "epoch": 2.4313320269002037,
      "eval_accuracy": 0.6707121510694664,
      "eval_loss": 0.38872531056404114,
      "eval_runtime": 3700.7395,
      "eval_samples_per_second": 617.377,
      "eval_steps_per_second": 6.174,
      "step": 250000
    },
    {
      "epoch": 2.4313320269002037,
      "step": 250000,
      "total_flos": 2.000976342137913e+18,
      "train_loss": 0.17685218869018554,
      "train_runtime": 149203.3505,
      "train_samples_per_second": 335.113,
      "train_steps_per_second": 1.676
    }
  ],
  "logging_steps": 100,
  "max_steps": 250000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.000976342137913e+18,
  "train_batch_size": 100,
  "trial_name": null,
  "trial_params": null
}