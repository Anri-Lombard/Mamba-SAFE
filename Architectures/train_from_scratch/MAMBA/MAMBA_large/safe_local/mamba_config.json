{
  "d_model": 768,
  "n_layer": 12,
  "d_intermediate": 2048,
  "hidden_act": "silu",
  "ssm_cfg": {
    "layer": "Mamba2"
  },
  "dropout_rate": 0.3,
  "rms_norm": true,
  "residual_in_fp32": true,
  "vocab_size": 1880,
  "eos_token_id": 2,
  "bos_token_id": 1,
  "pad_token_id": 3,
  "pad_vocab_size_multiple": 8,
  "fused_add_norm": true,
  "num_labels": 1,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "summary_activation": "tanh",
  "summary_proj_to_labels": true,
  "summary_first_dropout": 0.4,
  "summary_hidden_size": 128
}
