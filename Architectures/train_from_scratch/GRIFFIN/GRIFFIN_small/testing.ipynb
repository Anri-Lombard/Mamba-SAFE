{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anrilombard/Desktop/UCT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections.abc import Sequence\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from flax.training import orbax_utils\n",
    "import orbax.checkpoint\n",
    "from recurrentgemma import jax as recurrentgemma\n",
    "from datasets import DatasetDict\n",
    "from safe.tokenizer import SAFETokenizer\n",
    "from time import perf_counter\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define flags\n",
    "# DEBUG_MODE = flags.DEFINE_bool(\"debug\", False, \"Debug mode.\")\n",
    "# SAVE_PATH = flags.DEFINE_string(\"save_path\", \"saves/recurrent_gemma_model.ckpt\", \"Path to save model checkpoints.\")\n",
    "# KEY = flags.DEFINE_integer(\"key\", 1241312, \"Key to use for randomization.\")\n",
    "DEBUG_MODE = False\n",
    "SAVE_PATH = \"saves/recurrent_gemma_model.ckpt\"\n",
    "KEY = 1241312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# EPOCHS = flags.DEFINE_integer(\"epochs\", 10, \"Number of training epochs.\")\n",
    "# STEPS_PER_EPOCH = flags.DEFINE_integer(\"steps_per_epoch\", 2500, \"Number of steps per epoch.\")\n",
    "# BATCH_SIZE = flags.DEFINE_integer(\"batch_size\", 64, \"Batch size for training.\")\n",
    "# SEQ_LENGTH = flags.DEFINE_integer(\"seq_length\", 69, \"Sequence length for training.\")\n",
    "# LEARNING_RATE = flags.DEFINE_float(\"learning_rate\", 1e-3, \"Initial learning rate.\")\n",
    "EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 2500\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LENGTH = 69\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_train_state(rng, config):\n",
    "#     \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "#     model = recurrentgemma.Griffin(config)\n",
    "#     params = model.init(rng, jnp.ones((1, SEQ_LENGTH.value), dtype=jnp.int32))\n",
    "#     tx = optax.adam(learning_rate=LEARNING_RATE.value)\n",
    "#     return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "def create_train_state(rng, config):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    model = recurrentgemma.Griffin(config)\n",
    "    dummy_input = jnp.ones((1, SEQ_LENGTH), dtype=jnp.int32)\n",
    "    dummy_segment_pos = jnp.zeros((1, SEQ_LENGTH), dtype=jnp.int32)\n",
    "    params = model.init(rng, dummy_input, segment_pos=dummy_segment_pos)\n",
    "    tx = optax.adam(learning_rate=LEARNING_RATE)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_parameters(params):\n",
    "#     \"\"\"Count the number of trainable parameters in the model.\"\"\"\n",
    "#     return sum(jnp.prod(p.shape).item() for p in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "# def count_parameters(params):\n",
    "#     \"\"\"Count the number of trainable parameters in the model.\"\"\"\n",
    "#     return sum(jnp.prod(p.shape).item() for p in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "def count_parameters(params):\n",
    "    \"\"\"Count the number of trainable parameters in the model.\"\"\"\n",
    "    total_params = 0\n",
    "    for param in jax.tree_util.tree_leaves(params):\n",
    "        if isinstance(param, jnp.ndarray):\n",
    "            total_params += jnp.prod(param.shape).item()\n",
    "        elif isinstance(param, tuple):\n",
    "            total_params += sum(jnp.prod(p.shape).item() for p in param if isinstance(p, jnp.ndarray))\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset():\n",
    "#     \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "#     dataset = DatasetDict.load_from_disk('../../Datasets/MOSES/datasets')\n",
    "#     tokenizer = SAFETokenizer.from_pretrained(\"./tokenizer.json\")\n",
    "\n",
    "#     def tokenize_function(examples):\n",
    "#         return {\"input_ids\": tokenizer.encode(examples[\"SAFE\"], ids_only=True)}\n",
    "\n",
    "#     tokenized_dataset = dataset.map(tokenize_function, batched=False, remove_columns=dataset['train'].column_names)\n",
    "#     return tokenized_dataset, tokenizer\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "    dataset = DatasetDict.load_from_disk('../../Datasets/MOSES/datasets')\n",
    "    tokenizer = SAFETokenizer.from_pretrained(\"./tokenizer.json\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return {\"input_ids\": tokenizer.encode(examples[\"SAFE\"], ids_only=True)}\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=False, remove_columns=dataset['train'].column_names)\n",
    "    return tokenized_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_len):\n",
    "    \"\"\"Pad sequences to the same length.\"\"\"\n",
    "    return [seq + [0] * (max_len - len(seq)) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch_size, seq_length):\n",
    "    \"\"\"Get a random batch from the dataset.\"\"\"\n",
    "    idx = np.random.randint(0, len(dataset), batch_size)\n",
    "    batch = [dataset[i] for i in idx]\n",
    "    \n",
    "    # Truncate or pad sequences\n",
    "    batch = [seq[:seq_length] for seq in batch]\n",
    "    batch = pad_sequences(batch, seq_length)\n",
    "    \n",
    "    return jnp.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Perform a single training step.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(params, batch)\n",
    "        targets = jnp.roll(batch, -1, axis=-1)\n",
    "        mask = jnp.where(targets != 0, 1, 0)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets) * mask\n",
    "        return jnp.sum(loss) / jnp.sum(mask)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    return state.apply_gradients(grads=grads), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(state, dataset, steps):\n",
    "#     \"\"\"Train for a single epoch.\"\"\"\n",
    "#     batch_size, seq_length = BATCH_SIZE.value, SEQ_LENGTH.value\n",
    "    \n",
    "#     epoch_loss = 0\n",
    "#     for _ in range(steps):\n",
    "#         batch = get_batch(dataset, batch_size, seq_length)\n",
    "#         state, loss = train_step(state, batch)\n",
    "#         epoch_loss += loss\n",
    "    \n",
    "#     return state, epoch_loss / steps\n",
    "\n",
    "def train_epoch(state, dataset, steps):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    epoch_loss = 0\n",
    "    for _ in range(steps):\n",
    "        batch = get_batch(dataset, BATCH_SIZE, SEQ_LENGTH)\n",
    "        state, loss = train_step(state, batch)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    return state, epoch_loss / steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_molecule(state, tokenizer, max_length=100, temperature=0.8, top_k=None):\n",
    "    \"\"\"Generate a single molecule.\"\"\"\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    start_token = jnp.array(tokenizer.encode(\"[START]\", ids_only=True))\n",
    "    input_ids = start_token[None, :]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        logits = state.apply_fn(state.params, input_ids)\n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        if top_k is not None:\n",
    "            top_k_logits, top_k_indices = jax.lax.top_k(next_token_logits, top_k)\n",
    "            next_token_logits = jnp.where(\n",
    "                jnp.expand_dims(jnp.arange(next_token_logits.shape[-1]), 0) == top_k_indices,\n",
    "                top_k_logits,\n",
    "                float('-inf')\n",
    "            )\n",
    "        \n",
    "        next_token = jax.random.categorical(rng, next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, next_token[:, None]], axis=-1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.encode(\"[END]\", ids_only=True)[0]:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_novo_generation(state, tokenizer, num_molecules=10):\n",
    "    \"\"\"Perform de novo generation of molecules.\"\"\"\n",
    "    print(f\"Generating {num_molecules} new molecules:\")\n",
    "    for i in range(num_molecules):\n",
    "        molecule = generate_molecule(state, tokenizer)\n",
    "        print(f\"Molecule {i+1}: {molecule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = recurrentgemma.GriffinConfig(\n",
    "    vocab_size=1180,\n",
    "    width=128,\n",
    "    mlp_expanded_width=3 * 128,\n",
    "    lru_width=256,\n",
    "    num_heads=2,\n",
    "    block_types=(\n",
    "        recurrentgemma.TemporalBlockType.RECURRENT,\n",
    "        recurrentgemma.TemporalBlockType.ATTENTION,\n",
    "    ),\n",
    "    embeddings_scale_by_sqrt_dim=True,\n",
    "    attention_window_size=2048,\n",
    "    logits_soft_cap=30.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and tokenizer\n",
    "dataset, tokenizer = load_dataset()\n",
    "train_data = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize model and training state\n",
    "# rng = jax.random.PRNGKey(KEY.value)\n",
    "# state = create_train_state(rng, config)\n",
    "\n",
    "# Initialize model and training state\n",
    "rng = jax.random.PRNGKey(KEY)\n",
    "state = create_train_state(rng, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count and print the number of trainable parameters\n",
    "# num_params = count_parameters(state.params)\n",
    "# logging.info(f\"Number of trainable parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong key type: '535065' of type '<class 'numpy.int64'>'. Expected one of int, slice, range, str or Iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m t1_start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m---> 34\u001b[0m state, epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(epoch_loss))\n\u001b[1;32m     37\u001b[0m t1_stop \u001b[38;5;241m=\u001b[39m perf_counter()\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(state, dataset, steps)\u001b[0m\n\u001b[1;32m     15\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m---> 17\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEQ_LENGTH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     state, loss \u001b[38;5;241m=\u001b[39m train_step(state, batch)\n\u001b[1;32m     19\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(dataset, batch_size, seq_length)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a random batch from the dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset), batch_size)\n\u001b[0;32m----> 4\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Truncate or pad sequences\u001b[39;00m\n\u001b[1;32m      7\u001b[0m batch \u001b[38;5;241m=\u001b[39m [seq[:seq_length] \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m batch]\n",
      "File \u001b[0;32m~/Desktop/UCT/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2866\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UCT/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2851\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2849\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2850\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2851\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/Desktop/UCT/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:630\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m table\n\u001b[0;32m--> 630\u001b[0m query_type \u001b[38;5;241m=\u001b[39m \u001b[43mkey_to_query_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/UCT/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:550\u001b[0m, in \u001b[0;36mkey_to_query_type\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (\u001b[38;5;28mslice\u001b[39m, \u001b[38;5;28mrange\u001b[39m, Iterable)):\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 550\u001b[0m \u001b[43m_raise_bad_key_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UCT/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:46\u001b[0m, in \u001b[0;36m_raise_bad_key_type\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_bad_key_type\u001b[39m(key: Any):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong key type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Expected one of int, slice, range, str or Iterable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong key type: '535065' of type '<class 'numpy.int64'>'. Expected one of int, slice, range, str or Iterable."
     ]
    }
   ],
   "source": [
    "# # Training loop\n",
    "# losses = []\n",
    "# t0_start = perf_counter()\n",
    "# for epoch in range(EPOCHS.value):\n",
    "#     logging.info(f\"Starting epoch {epoch + 1}/{EPOCHS.value}\")\n",
    "#     t1_start = perf_counter()\n",
    "    \n",
    "#     state, epoch_loss = train_epoch(state, train_data, STEPS_PER_EPOCH.value)\n",
    "#     losses.append(float(epoch_loss))\n",
    "    \n",
    "#     t1_stop = perf_counter()\n",
    "#     logging.info(f\"Epoch {epoch + 1} completed. Loss: {epoch_loss:.5f}. Time: {t1_stop - t1_start:.2f} sec\")\n",
    "\n",
    "#     # Save checkpoint\n",
    "#     ckpt = {\n",
    "#         \"model\": state.params,\n",
    "#         \"config\": config,\n",
    "#         \"epoch\": epoch + 1,\n",
    "#     }\n",
    "#     orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "#     save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "#     orbax_checkpointer.save(f\"{SAVE_PATH.value}_{epoch + 1}\", ckpt, save_args=save_args)\n",
    "\n",
    "# t0_stop = perf_counter()\n",
    "# logging.info(f\"Training completed in {t0_stop - t0_start:.2f} seconds\")\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "t0_start = perf_counter()\n",
    "for epoch in range(EPOCHS):\n",
    "    logging.info(f\"Starting epoch {epoch + 1}/{EPOCHS}\")\n",
    "    t1_start = perf_counter()\n",
    "    \n",
    "    state, epoch_loss = train_epoch(state, train_data, STEPS_PER_EPOCH)\n",
    "    losses.append(float(epoch_loss))\n",
    "    \n",
    "    t1_stop = perf_counter()\n",
    "    logging.info(f\"Epoch {epoch + 1} completed. Loss: {epoch_loss:.5f}. Time: {t1_stop - t1_start:.2f} sec\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt = {\n",
    "        \"model\": state.params,\n",
    "        \"config\": config,\n",
    "        \"epoch\": epoch + 1,\n",
    "    }\n",
    "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "    save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "    orbax_checkpointer.save(f\"{SAVE_PATH}_{epoch + 1}\", ckpt, save_args=save_args)\n",
    "\n",
    "t0_stop = perf_counter()\n",
    "logging.info(f\"Training completed in {t0_stop - t0_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save loss history\n",
    "with open('recurrent_gemma_loss_history.json', 'w') as f:\n",
    "    json.dump(losses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
