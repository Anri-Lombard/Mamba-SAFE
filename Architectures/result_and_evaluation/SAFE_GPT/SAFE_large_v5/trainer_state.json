{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500.0,
  "global_step": 50000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 38.44445037841797,
      "learning_rate": 5e-08,
      "loss": 6.841,
      "step": 500
    },
    {
      "epoch": 0.02,
      "grad_norm": 6.4877753257751465,
      "learning_rate": 1e-07,
      "loss": 4.3835,
      "step": 1000
    },
    {
      "epoch": 0.03,
      "grad_norm": 6.467931747436523,
      "learning_rate": 1.5e-07,
      "loss": 3.3806,
      "step": 1500
    },
    {
      "epoch": 0.04,
      "grad_norm": 5.236299514770508,
      "learning_rate": 2e-07,
      "loss": 2.8247,
      "step": 2000
    },
    {
      "epoch": 0.05,
      "grad_norm": 5.700149059295654,
      "learning_rate": 2.5e-07,
      "loss": 2.5317,
      "step": 2500
    },
    {
      "epoch": 0.06,
      "grad_norm": 5.550288677215576,
      "learning_rate": 3e-07,
      "loss": 2.3653,
      "step": 3000
    },
    {
      "epoch": 0.07,
      "grad_norm": 9.093704223632812,
      "learning_rate": 3.5e-07,
      "loss": 2.2426,
      "step": 3500
    },
    {
      "epoch": 0.08,
      "grad_norm": 5.227163314819336,
      "learning_rate": 4e-07,
      "loss": 2.134,
      "step": 4000
    },
    {
      "epoch": 0.09,
      "grad_norm": 5.844882011413574,
      "learning_rate": 4.5e-07,
      "loss": 2.0322,
      "step": 4500
    },
    {
      "epoch": 0.1,
      "grad_norm": 4.96788215637207,
      "learning_rate": 5e-07,
      "loss": 1.9336,
      "step": 5000
    },
    {
      "epoch": 0.11,
      "grad_norm": 6.551884651184082,
      "learning_rate": 5.5e-07,
      "loss": 1.8413,
      "step": 5500
    },
    {
      "epoch": 0.12,
      "grad_norm": 7.4743523597717285,
      "learning_rate": 6e-07,
      "loss": 1.7539,
      "step": 6000
    },
    {
      "epoch": 0.13,
      "grad_norm": 6.0163893699646,
      "learning_rate": 6.5e-07,
      "loss": 1.665,
      "step": 6500
    },
    {
      "epoch": 0.14,
      "grad_norm": 6.539722919464111,
      "learning_rate": 7e-07,
      "loss": 1.5892,
      "step": 7000
    },
    {
      "epoch": 0.15,
      "grad_norm": 6.3940629959106445,
      "learning_rate": 7.5e-07,
      "loss": 1.5152,
      "step": 7500
    },
    {
      "epoch": 0.16,
      "grad_norm": 4.574140548706055,
      "learning_rate": 8e-07,
      "loss": 1.4486,
      "step": 8000
    },
    {
      "epoch": 0.17,
      "grad_norm": 5.6923956871032715,
      "learning_rate": 8.499999999999999e-07,
      "loss": 1.3887,
      "step": 8500
    },
    {
      "epoch": 0.18,
      "grad_norm": 6.922948837280273,
      "learning_rate": 9e-07,
      "loss": 1.3337,
      "step": 9000
    },
    {
      "epoch": 0.19,
      "grad_norm": 6.032220363616943,
      "learning_rate": 9.499999999999999e-07,
      "loss": 1.2869,
      "step": 9500
    },
    {
      "epoch": 0.2,
      "grad_norm": 6.1115217208862305,
      "learning_rate": 1e-06,
      "loss": 1.2411,
      "step": 10000
    },
    {
      "epoch": 0.21,
      "grad_norm": 6.640834331512451,
      "learning_rate": 9.875e-07,
      "loss": 1.2015,
      "step": 10500
    },
    {
      "epoch": 0.22,
      "grad_norm": 8.368191719055176,
      "learning_rate": 9.75e-07,
      "loss": 1.1718,
      "step": 11000
    },
    {
      "epoch": 0.23,
      "grad_norm": 8.537383079528809,
      "learning_rate": 9.624999999999999e-07,
      "loss": 1.1435,
      "step": 11500
    },
    {
      "epoch": 0.24,
      "grad_norm": 6.745502948760986,
      "learning_rate": 9.499999999999999e-07,
      "loss": 1.1198,
      "step": 12000
    },
    {
      "epoch": 0.25,
      "grad_norm": 4.803701877593994,
      "learning_rate": 9.374999999999999e-07,
      "loss": 1.1015,
      "step": 12500
    },
    {
      "epoch": 0.26,
      "grad_norm": 5.452188968658447,
      "learning_rate": 9.25e-07,
      "loss": 1.0822,
      "step": 13000
    },
    {
      "epoch": 0.27,
      "grad_norm": 4.633382320404053,
      "learning_rate": 9.124999999999999e-07,
      "loss": 1.0634,
      "step": 13500
    },
    {
      "epoch": 0.28,
      "grad_norm": 5.125351428985596,
      "learning_rate": 9e-07,
      "loss": 1.0519,
      "step": 14000
    },
    {
      "epoch": 0.29,
      "grad_norm": 6.748693943023682,
      "learning_rate": 8.874999999999999e-07,
      "loss": 1.0382,
      "step": 14500
    },
    {
      "epoch": 0.3,
      "grad_norm": 6.321127414703369,
      "learning_rate": 8.75e-07,
      "loss": 1.0246,
      "step": 15000
    },
    {
      "epoch": 0.31,
      "grad_norm": 4.9751105308532715,
      "learning_rate": 8.625e-07,
      "loss": 1.0148,
      "step": 15500
    },
    {
      "epoch": 0.32,
      "grad_norm": 5.66453742980957,
      "learning_rate": 8.499999999999999e-07,
      "loss": 1.0042,
      "step": 16000
    },
    {
      "epoch": 0.33,
      "grad_norm": 5.681591987609863,
      "learning_rate": 8.375e-07,
      "loss": 0.9983,
      "step": 16500
    },
    {
      "epoch": 0.34,
      "grad_norm": 6.321434020996094,
      "learning_rate": 8.249999999999999e-07,
      "loss": 0.988,
      "step": 17000
    },
    {
      "epoch": 0.35,
      "grad_norm": 4.993677616119385,
      "learning_rate": 8.125e-07,
      "loss": 0.979,
      "step": 17500
    },
    {
      "epoch": 0.36,
      "grad_norm": 5.782954216003418,
      "learning_rate": 8e-07,
      "loss": 0.9732,
      "step": 18000
    },
    {
      "epoch": 0.37,
      "grad_norm": 4.839466571807861,
      "learning_rate": 7.875e-07,
      "loss": 0.9651,
      "step": 18500
    },
    {
      "epoch": 0.38,
      "grad_norm": 6.009923934936523,
      "learning_rate": 7.75e-07,
      "loss": 0.9598,
      "step": 19000
    },
    {
      "epoch": 0.39,
      "grad_norm": 6.065587997436523,
      "learning_rate": 7.624999999999999e-07,
      "loss": 0.951,
      "step": 19500
    },
    {
      "epoch": 0.4,
      "grad_norm": 5.321629524230957,
      "learning_rate": 7.5e-07,
      "loss": 0.9479,
      "step": 20000
    },
    {
      "epoch": 0.41,
      "grad_norm": 6.447431564331055,
      "learning_rate": 7.375e-07,
      "loss": 0.9411,
      "step": 20500
    },
    {
      "epoch": 0.42,
      "grad_norm": 5.194344997406006,
      "learning_rate": 7.249999999999999e-07,
      "loss": 0.9367,
      "step": 21000
    },
    {
      "epoch": 0.43,
      "grad_norm": 4.859065532684326,
      "learning_rate": 7.125e-07,
      "loss": 0.9296,
      "step": 21500
    },
    {
      "epoch": 0.44,
      "grad_norm": 4.416916370391846,
      "learning_rate": 7e-07,
      "loss": 0.9265,
      "step": 22000
    },
    {
      "epoch": 0.45,
      "grad_norm": 4.434675693511963,
      "learning_rate": 6.875e-07,
      "loss": 0.922,
      "step": 22500
    },
    {
      "epoch": 0.46,
      "grad_norm": 4.781727313995361,
      "learning_rate": 6.75e-07,
      "loss": 0.9167,
      "step": 23000
    },
    {
      "epoch": 0.47,
      "grad_norm": 5.261031627655029,
      "learning_rate": 6.624999999999999e-07,
      "loss": 0.9157,
      "step": 23500
    },
    {
      "epoch": 0.48,
      "grad_norm": 5.8311920166015625,
      "learning_rate": 6.5e-07,
      "loss": 0.9114,
      "step": 24000
    },
    {
      "epoch": 0.49,
      "grad_norm": 4.862998008728027,
      "learning_rate": 6.374999999999999e-07,
      "loss": 0.9061,
      "step": 24500
    },
    {
      "epoch": 0.5,
      "grad_norm": 4.347824573516846,
      "learning_rate": 6.249999999999999e-07,
      "loss": 0.9044,
      "step": 25000
    },
    {
      "epoch": 0.51,
      "grad_norm": 4.702270030975342,
      "learning_rate": 6.125000000000001e-07,
      "loss": 0.8997,
      "step": 25500
    },
    {
      "epoch": 0.52,
      "grad_norm": 5.276803493499756,
      "learning_rate": 6e-07,
      "loss": 0.8967,
      "step": 26000
    },
    {
      "epoch": 0.53,
      "grad_norm": 5.314545631408691,
      "learning_rate": 5.875e-07,
      "loss": 0.8938,
      "step": 26500
    },
    {
      "epoch": 0.54,
      "grad_norm": 4.694545269012451,
      "learning_rate": 5.749999999999999e-07,
      "loss": 0.8929,
      "step": 27000
    },
    {
      "epoch": 0.55,
      "grad_norm": 4.457617282867432,
      "learning_rate": 5.625e-07,
      "loss": 0.8892,
      "step": 27500
    },
    {
      "epoch": 0.56,
      "grad_norm": 4.736658096313477,
      "learning_rate": 5.5e-07,
      "loss": 0.8868,
      "step": 28000
    },
    {
      "epoch": 0.57,
      "grad_norm": 4.796227931976318,
      "learning_rate": 5.374999999999999e-07,
      "loss": 0.8844,
      "step": 28500
    },
    {
      "epoch": 0.58,
      "grad_norm": 5.607601642608643,
      "learning_rate": 5.25e-07,
      "loss": 0.8815,
      "step": 29000
    },
    {
      "epoch": 0.59,
      "grad_norm": 5.097726821899414,
      "learning_rate": 5.125e-07,
      "loss": 0.8798,
      "step": 29500
    },
    {
      "epoch": 0.6,
      "grad_norm": 4.400300025939941,
      "learning_rate": 5e-07,
      "loss": 0.8774,
      "step": 30000
    },
    {
      "epoch": 0.61,
      "grad_norm": 5.623958110809326,
      "learning_rate": 4.875e-07,
      "loss": 0.8762,
      "step": 30500
    },
    {
      "epoch": 0.62,
      "grad_norm": 5.638601303100586,
      "learning_rate": 4.7499999999999995e-07,
      "loss": 0.8748,
      "step": 31000
    },
    {
      "epoch": 0.63,
      "grad_norm": 4.657592296600342,
      "learning_rate": 4.625e-07,
      "loss": 0.873,
      "step": 31500
    },
    {
      "epoch": 0.64,
      "grad_norm": 4.369668006896973,
      "learning_rate": 4.5e-07,
      "loss": 0.8689,
      "step": 32000
    },
    {
      "epoch": 0.65,
      "grad_norm": 4.275276184082031,
      "learning_rate": 4.375e-07,
      "loss": 0.8676,
      "step": 32500
    },
    {
      "epoch": 0.66,
      "grad_norm": 5.246971607208252,
      "learning_rate": 4.2499999999999995e-07,
      "loss": 0.8669,
      "step": 33000
    },
    {
      "epoch": 0.67,
      "grad_norm": 5.949439525604248,
      "learning_rate": 4.1249999999999997e-07,
      "loss": 0.8636,
      "step": 33500
    },
    {
      "epoch": 0.68,
      "grad_norm": 6.129522800445557,
      "learning_rate": 4e-07,
      "loss": 0.8618,
      "step": 34000
    },
    {
      "epoch": 0.69,
      "grad_norm": 5.997241497039795,
      "learning_rate": 3.875e-07,
      "loss": 0.8624,
      "step": 34500
    },
    {
      "epoch": 0.7,
      "grad_norm": 4.346686840057373,
      "learning_rate": 3.75e-07,
      "loss": 0.8586,
      "step": 35000
    },
    {
      "epoch": 0.71,
      "grad_norm": 5.455142021179199,
      "learning_rate": 3.6249999999999997e-07,
      "loss": 0.8588,
      "step": 35500
    },
    {
      "epoch": 0.72,
      "grad_norm": 5.742190361022949,
      "learning_rate": 3.5e-07,
      "loss": 0.8574,
      "step": 36000
    },
    {
      "epoch": 0.73,
      "grad_norm": 5.84987735748291,
      "learning_rate": 3.375e-07,
      "loss": 0.8564,
      "step": 36500
    },
    {
      "epoch": 0.74,
      "grad_norm": 4.5533246994018555,
      "learning_rate": 3.25e-07,
      "loss": 0.8537,
      "step": 37000
    },
    {
      "epoch": 0.75,
      "grad_norm": 5.136107921600342,
      "learning_rate": 3.1249999999999997e-07,
      "loss": 0.8509,
      "step": 37500
    },
    {
      "epoch": 0.76,
      "grad_norm": 4.540552139282227,
      "learning_rate": 3e-07,
      "loss": 0.8528,
      "step": 38000
    },
    {
      "epoch": 0.77,
      "grad_norm": 5.904707908630371,
      "learning_rate": 2.8749999999999995e-07,
      "loss": 0.8512,
      "step": 38500
    },
    {
      "epoch": 0.78,
      "grad_norm": 4.901312351226807,
      "learning_rate": 2.75e-07,
      "loss": 0.8508,
      "step": 39000
    },
    {
      "epoch": 0.79,
      "grad_norm": 4.724414348602295,
      "learning_rate": 2.625e-07,
      "loss": 0.8492,
      "step": 39500
    },
    {
      "epoch": 0.8,
      "grad_norm": 5.784249305725098,
      "learning_rate": 2.5e-07,
      "loss": 0.8483,
      "step": 40000
    },
    {
      "epoch": 0.81,
      "grad_norm": 5.044711112976074,
      "learning_rate": 2.3749999999999998e-07,
      "loss": 0.8472,
      "step": 40500
    },
    {
      "epoch": 0.82,
      "grad_norm": 4.964502334594727,
      "learning_rate": 2.25e-07,
      "loss": 0.848,
      "step": 41000
    },
    {
      "epoch": 0.83,
      "grad_norm": 4.822344779968262,
      "learning_rate": 2.1249999999999998e-07,
      "loss": 0.8447,
      "step": 41500
    },
    {
      "epoch": 0.84,
      "grad_norm": 5.5992255210876465,
      "learning_rate": 2e-07,
      "loss": 0.8459,
      "step": 42000
    },
    {
      "epoch": 0.85,
      "grad_norm": 4.710227966308594,
      "learning_rate": 1.875e-07,
      "loss": 0.8443,
      "step": 42500
    },
    {
      "epoch": 0.86,
      "grad_norm": 5.172890663146973,
      "learning_rate": 1.75e-07,
      "loss": 0.8455,
      "step": 43000
    },
    {
      "epoch": 0.87,
      "grad_norm": 5.605441093444824,
      "learning_rate": 1.625e-07,
      "loss": 0.8428,
      "step": 43500
    },
    {
      "epoch": 0.88,
      "grad_norm": 5.136120319366455,
      "learning_rate": 1.5e-07,
      "loss": 0.8441,
      "step": 44000
    },
    {
      "epoch": 0.89,
      "grad_norm": 5.7181596755981445,
      "learning_rate": 1.375e-07,
      "loss": 0.8407,
      "step": 44500
    },
    {
      "epoch": 0.9,
      "grad_norm": 5.067420959472656,
      "learning_rate": 1.25e-07,
      "loss": 0.8404,
      "step": 45000
    },
    {
      "epoch": 0.91,
      "grad_norm": 4.768031597137451,
      "learning_rate": 1.125e-07,
      "loss": 0.8427,
      "step": 45500
    },
    {
      "epoch": 0.92,
      "grad_norm": 4.977381229400635,
      "learning_rate": 1e-07,
      "loss": 0.8413,
      "step": 46000
    },
    {
      "epoch": 0.93,
      "grad_norm": 4.966160297393799,
      "learning_rate": 8.75e-08,
      "loss": 0.8422,
      "step": 46500
    },
    {
      "epoch": 0.94,
      "grad_norm": 4.440672874450684,
      "learning_rate": 7.5e-08,
      "loss": 0.8412,
      "step": 47000
    },
    {
      "epoch": 0.95,
      "grad_norm": 5.056884765625,
      "learning_rate": 6.25e-08,
      "loss": 0.84,
      "step": 47500
    },
    {
      "epoch": 0.96,
      "grad_norm": 4.885088920593262,
      "learning_rate": 5e-08,
      "loss": 0.8412,
      "step": 48000
    },
    {
      "epoch": 0.97,
      "grad_norm": 4.866405010223389,
      "learning_rate": 3.75e-08,
      "loss": 0.8402,
      "step": 48500
    },
    {
      "epoch": 0.98,
      "grad_norm": 4.939864158630371,
      "learning_rate": 2.5e-08,
      "loss": 0.8411,
      "step": 49000
    },
    {
      "epoch": 0.99,
      "grad_norm": 5.093091011047363,
      "learning_rate": 1.25e-08,
      "loss": 0.839,
      "step": 49500
    },
    {
      "epoch": 1.0,
      "grad_norm": 4.716949462890625,
      "learning_rate": 0.0,
      "loss": 0.839,
      "step": 50000
    },
    {
      "epoch": 1.0,
      "step": 50000,
      "total_flos": 6.041190979823558e+16,
      "train_loss": 1.1865958911132812,
      "train_runtime": 9631.9425,
      "train_samples_per_second": 332.228,
      "train_steps_per_second": 5.191
    }
  ],
  "logging_steps": 500,
  "max_steps": 50000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.041190979823558e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
