{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.000738516666667,
  "eval_steps": 500.0,
  "global_step": 245000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.6666666666666667e-05,
      "grad_norm": 2.525944232940674,
      "learning_rate": 0.000125,
      "loss": 1.8424,
      "step": 500
    },
    {
      "epoch": 3.3333333333333335e-05,
      "grad_norm": 2.959821939468384,
      "learning_rate": 0.00025,
      "loss": 0.8275,
      "step": 1000
    },
    {
      "epoch": 5e-05,
      "grad_norm": 1.6075856685638428,
      "learning_rate": 0.000375,
      "loss": 0.7262,
      "step": 1500
    },
    {
      "epoch": 6.666666666666667e-05,
      "grad_norm": 1.2588224411010742,
      "learning_rate": 0.0005,
      "loss": 0.7226,
      "step": 2000
    },
    {
      "epoch": 8.333333333333333e-05,
      "grad_norm": 1.031587839126587,
      "learning_rate": 0.0004999916661110742,
      "loss": 0.6775,
      "step": 2500
    },
    {
      "epoch": 0.0001,
      "grad_norm": 0.7260088920593262,
      "learning_rate": 0.0004999833322221482,
      "loss": 0.6357,
      "step": 3000
    },
    {
      "epoch": 0.00011666666666666667,
      "grad_norm": 0.5721416473388672,
      "learning_rate": 0.0004999749983332222,
      "loss": 0.6258,
      "step": 3500
    },
    {
      "epoch": 0.00013333333333333334,
      "grad_norm": 0.8517795205116272,
      "learning_rate": 0.0004999666644442963,
      "loss": 0.6169,
      "step": 4000
    },
    {
      "epoch": 0.00015,
      "grad_norm": 0.6086206436157227,
      "learning_rate": 0.0004999583305553704,
      "loss": 0.6177,
      "step": 4500
    },
    {
      "epoch": 0.00016666666666666666,
      "grad_norm": 0.6733903884887695,
      "learning_rate": 0.0004999499966664444,
      "loss": 0.6092,
      "step": 5000
    },
    {
      "epoch": 0.00018333333333333334,
      "grad_norm": 0.45445334911346436,
      "learning_rate": 0.0004999416627775185,
      "loss": 0.596,
      "step": 5500
    },
    {
      "epoch": 0.0002,
      "grad_norm": 0.5999668836593628,
      "learning_rate": 0.0004999333288885926,
      "loss": 0.5958,
      "step": 6000
    },
    {
      "epoch": 0.00021666666666666666,
      "grad_norm": 0.4424344599246979,
      "learning_rate": 0.0004999249949996666,
      "loss": 0.5671,
      "step": 6500
    },
    {
      "epoch": 0.00023333333333333333,
      "grad_norm": 0.931814432144165,
      "learning_rate": 0.0004999166611107408,
      "loss": 0.5995,
      "step": 7000
    },
    {
      "epoch": 0.00025,
      "grad_norm": 0.3403550386428833,
      "learning_rate": 0.0004999083272218148,
      "loss": 0.5748,
      "step": 7500
    },
    {
      "epoch": 0.0002666666666666667,
      "grad_norm": 0.40399014949798584,
      "learning_rate": 0.0004998999933328889,
      "loss": 0.5753,
      "step": 8000
    },
    {
      "epoch": 0.00028333333333333335,
      "grad_norm": 0.5537791848182678,
      "learning_rate": 0.0004998916594439629,
      "loss": 0.575,
      "step": 8500
    },
    {
      "epoch": 0.0003,
      "grad_norm": 0.4340948760509491,
      "learning_rate": 0.000499883325555037,
      "loss": 0.5657,
      "step": 9000
    },
    {
      "epoch": 0.00031666666666666665,
      "grad_norm": 0.4760831892490387,
      "learning_rate": 0.0004998749916661111,
      "loss": 0.5765,
      "step": 9500
    },
    {
      "epoch": 0.0003333333333333333,
      "grad_norm": 0.3069644868373871,
      "learning_rate": 0.0004998666577771851,
      "loss": 0.5646,
      "step": 10000
    },
    {
      "epoch": 0.00035,
      "grad_norm": 0.5491994619369507,
      "learning_rate": 0.0004998583238882593,
      "loss": 0.5606,
      "step": 10500
    },
    {
      "epoch": 0.00036666666666666667,
      "grad_norm": 0.3876720666885376,
      "learning_rate": 0.0004998499899993333,
      "loss": 0.5763,
      "step": 11000
    },
    {
      "epoch": 0.00038333333333333334,
      "grad_norm": 0.3757576048374176,
      "learning_rate": 0.0004998416561104074,
      "loss": 0.565,
      "step": 11500
    },
    {
      "epoch": 0.0004,
      "grad_norm": 0.29073071479797363,
      "learning_rate": 0.0004998333222214815,
      "loss": 0.5511,
      "step": 12000
    },
    {
      "epoch": 0.0004166666666666667,
      "grad_norm": 0.3138662874698639,
      "learning_rate": 0.0004998249883325555,
      "loss": 0.561,
      "step": 12500
    },
    {
      "epoch": 0.0004333333333333333,
      "grad_norm": 0.48935264348983765,
      "learning_rate": 0.0004998166544436295,
      "loss": 0.5621,
      "step": 13000
    },
    {
      "epoch": 0.00045,
      "grad_norm": 0.23926950991153717,
      "learning_rate": 0.0004998083205547037,
      "loss": 0.5818,
      "step": 13500
    },
    {
      "epoch": 0.00046666666666666666,
      "grad_norm": 0.37741154432296753,
      "learning_rate": 0.0004997999866657777,
      "loss": 0.5689,
      "step": 14000
    },
    {
      "epoch": 0.00048333333333333334,
      "grad_norm": 0.3123525381088257,
      "learning_rate": 0.0004997916527768518,
      "loss": 0.5468,
      "step": 14500
    },
    {
      "epoch": 0.0005,
      "grad_norm": 0.24133522808551788,
      "learning_rate": 0.0004997833188879259,
      "loss": 0.548,
      "step": 15000
    },
    {
      "epoch": 0.0005166666666666667,
      "grad_norm": 0.4836028516292572,
      "learning_rate": 0.0004997749849989999,
      "loss": 0.5464,
      "step": 15500
    },
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 0.3392070233821869,
      "learning_rate": 0.0004997666511100741,
      "loss": 0.5325,
      "step": 16000
    },
    {
      "epoch": 0.00055,
      "grad_norm": 0.19961610436439514,
      "learning_rate": 0.0004997583172211481,
      "loss": 0.5514,
      "step": 16500
    },
    {
      "epoch": 0.0005666666666666667,
      "grad_norm": 0.3093241751194,
      "learning_rate": 0.0004997499833322221,
      "loss": 0.5388,
      "step": 17000
    },
    {
      "epoch": 0.0005833333333333334,
      "grad_norm": 0.30977755784988403,
      "learning_rate": 0.0004997416494432963,
      "loss": 0.553,
      "step": 17500
    },
    {
      "epoch": 0.0006,
      "grad_norm": 0.22052064538002014,
      "learning_rate": 0.0004997333155543703,
      "loss": 0.5489,
      "step": 18000
    },
    {
      "epoch": 0.0006166666666666666,
      "grad_norm": 0.2865675687789917,
      "learning_rate": 0.0004997249816654444,
      "loss": 0.5466,
      "step": 18500
    },
    {
      "epoch": 0.0006333333333333333,
      "grad_norm": 0.2544333338737488,
      "learning_rate": 0.0004997166477765184,
      "loss": 0.5288,
      "step": 19000
    },
    {
      "epoch": 0.00065,
      "grad_norm": 0.1907673180103302,
      "learning_rate": 0.0004997083138875925,
      "loss": 0.5434,
      "step": 19500
    },
    {
      "epoch": 0.0006666666666666666,
      "grad_norm": 0.31047338247299194,
      "learning_rate": 0.0004996999799986666,
      "loss": 0.5389,
      "step": 20000
    },
    {
      "epoch": 0.0006833333333333333,
      "grad_norm": 0.2972049117088318,
      "learning_rate": 0.0004996916461097407,
      "loss": 0.5591,
      "step": 20500
    },
    {
      "epoch": 0.0007,
      "grad_norm": 0.2153702974319458,
      "learning_rate": 0.0004996833122208147,
      "loss": 0.5247,
      "step": 21000
    },
    {
      "epoch": 0.0007166666666666667,
      "grad_norm": 0.22747057676315308,
      "learning_rate": 0.0004996749783318888,
      "loss": 0.5393,
      "step": 21500
    },
    {
      "epoch": 0.0007333333333333333,
      "grad_norm": 0.22309668362140656,
      "learning_rate": 0.0004996666444429629,
      "loss": 0.5515,
      "step": 22000
    },
    {
      "epoch": 0.00075,
      "grad_norm": 0.23977069556713104,
      "learning_rate": 0.000499658310554037,
      "loss": 0.5368,
      "step": 22500
    },
    {
      "epoch": 0.0007666666666666667,
      "grad_norm": 0.1991029530763626,
      "learning_rate": 0.000499649976665111,
      "loss": 0.5512,
      "step": 23000
    },
    {
      "epoch": 0.0007833333333333334,
      "grad_norm": 0.20421342551708221,
      "learning_rate": 0.000499641642776185,
      "loss": 0.5478,
      "step": 23500
    },
    {
      "epoch": 0.0008,
      "grad_norm": 0.1641843020915985,
      "learning_rate": 0.0004996333088872592,
      "loss": 0.5371,
      "step": 24000
    },
    {
      "epoch": 0.0008166666666666667,
      "grad_norm": 0.16723549365997314,
      "learning_rate": 0.0004996249749983332,
      "loss": 0.5415,
      "step": 24500
    },
    {
      "epoch": 1.0000079833333333,
      "grad_norm": 0.19436708092689514,
      "learning_rate": 0.0004996166411094074,
      "loss": 0.5499,
      "step": 25000
    },
    {
      "epoch": 1.00002465,
      "grad_norm": 0.2534274160861969,
      "learning_rate": 0.0004996083072204814,
      "loss": 0.5279,
      "step": 25500
    },
    {
      "epoch": 1.0000413166666666,
      "grad_norm": 0.21610771119594574,
      "learning_rate": 0.0004995999733315554,
      "loss": 0.5472,
      "step": 26000
    },
    {
      "epoch": 1.0000579833333334,
      "grad_norm": 0.17208001017570496,
      "learning_rate": 0.0004995916394426296,
      "loss": 0.5412,
      "step": 26500
    },
    {
      "epoch": 1.00007465,
      "grad_norm": 0.18527594208717346,
      "learning_rate": 0.0004995833055537036,
      "loss": 0.5309,
      "step": 27000
    },
    {
      "epoch": 1.0000913166666667,
      "grad_norm": 0.17918500304222107,
      "learning_rate": 0.0004995749716647776,
      "loss": 0.5301,
      "step": 27500
    },
    {
      "epoch": 1.0001079833333333,
      "grad_norm": 0.17402301728725433,
      "learning_rate": 0.0004995666377758517,
      "loss": 0.5301,
      "step": 28000
    },
    {
      "epoch": 1.00012465,
      "grad_norm": 0.20331566035747528,
      "learning_rate": 0.0004995583038869258,
      "loss": 0.5336,
      "step": 28500
    },
    {
      "epoch": 1.0001413166666666,
      "grad_norm": 0.16429239511489868,
      "learning_rate": 0.0004995499699979998,
      "loss": 0.532,
      "step": 29000
    },
    {
      "epoch": 1.0001579833333334,
      "grad_norm": 0.20939520001411438,
      "learning_rate": 0.000499541636109074,
      "loss": 0.5216,
      "step": 29500
    },
    {
      "epoch": 1.00017465,
      "grad_norm": 0.21960698068141937,
      "learning_rate": 0.000499533302220148,
      "loss": 0.5411,
      "step": 30000
    },
    {
      "epoch": 1.0001913166666667,
      "grad_norm": 0.14219388365745544,
      "learning_rate": 0.0004995249683312221,
      "loss": 0.5235,
      "step": 30500
    },
    {
      "epoch": 1.0002079833333333,
      "grad_norm": 0.20686379075050354,
      "learning_rate": 0.0004995166344422962,
      "loss": 0.5267,
      "step": 31000
    },
    {
      "epoch": 1.00022465,
      "grad_norm": 0.180345356464386,
      "learning_rate": 0.0004995083005533702,
      "loss": 0.5222,
      "step": 31500
    },
    {
      "epoch": 1.0002413166666666,
      "grad_norm": 0.18581220507621765,
      "learning_rate": 0.0004994999666644443,
      "loss": 0.5241,
      "step": 32000
    },
    {
      "epoch": 1.0002579833333334,
      "grad_norm": 0.1656821221113205,
      "learning_rate": 0.0004994916327755184,
      "loss": 0.5355,
      "step": 32500
    },
    {
      "epoch": 1.00027465,
      "grad_norm": 0.163687065243721,
      "learning_rate": 0.0004994832988865925,
      "loss": 0.5128,
      "step": 33000
    },
    {
      "epoch": 1.0002913166666667,
      "grad_norm": 0.11820054799318314,
      "learning_rate": 0.0004994749649976665,
      "loss": 0.5347,
      "step": 33500
    },
    {
      "epoch": 1.0003079833333333,
      "grad_norm": 0.1650761514902115,
      "learning_rate": 0.0004994666311087406,
      "loss": 0.524,
      "step": 34000
    },
    {
      "epoch": 1.00032465,
      "grad_norm": 0.17554055154323578,
      "learning_rate": 0.0004994582972198147,
      "loss": 0.5158,
      "step": 34500
    },
    {
      "epoch": 1.0003413166666666,
      "grad_norm": 0.24690403044223785,
      "learning_rate": 0.0004994499633308887,
      "loss": 0.5348,
      "step": 35000
    },
    {
      "epoch": 1.0003579833333334,
      "grad_norm": 0.17205363512039185,
      "learning_rate": 0.0004994416294419628,
      "loss": 0.5238,
      "step": 35500
    },
    {
      "epoch": 1.00037465,
      "grad_norm": 0.11036866903305054,
      "learning_rate": 0.0004994332955530369,
      "loss": 0.5382,
      "step": 36000
    },
    {
      "epoch": 1.0003913166666667,
      "grad_norm": 0.13775616884231567,
      "learning_rate": 0.0004994249616641109,
      "loss": 0.5323,
      "step": 36500
    },
    {
      "epoch": 1.0004079833333333,
      "grad_norm": 0.13864684104919434,
      "learning_rate": 0.0004994166277751851,
      "loss": 0.5456,
      "step": 37000
    },
    {
      "epoch": 1.00042465,
      "grad_norm": 0.16853056848049164,
      "learning_rate": 0.0004994082938862591,
      "loss": 0.5299,
      "step": 37500
    },
    {
      "epoch": 1.0004413166666666,
      "grad_norm": 0.15650279819965363,
      "learning_rate": 0.0004993999599973331,
      "loss": 0.5478,
      "step": 38000
    },
    {
      "epoch": 1.0004579833333334,
      "grad_norm": 0.17165839672088623,
      "learning_rate": 0.0004993916261084072,
      "loss": 0.5131,
      "step": 38500
    },
    {
      "epoch": 1.00047465,
      "grad_norm": 0.18162278831005096,
      "learning_rate": 0.0004993832922194813,
      "loss": 0.5227,
      "step": 39000
    },
    {
      "epoch": 1.0004913166666667,
      "grad_norm": 0.17358505725860596,
      "learning_rate": 0.0004993749583305554,
      "loss": 0.5158,
      "step": 39500
    },
    {
      "epoch": 1.0005079833333332,
      "grad_norm": 0.13011685013771057,
      "learning_rate": 0.0004993666244416295,
      "loss": 0.5382,
      "step": 40000
    },
    {
      "epoch": 1.00052465,
      "grad_norm": 0.16431021690368652,
      "learning_rate": 0.0004993582905527035,
      "loss": 0.5318,
      "step": 40500
    },
    {
      "epoch": 1.0005413166666666,
      "grad_norm": 0.14242678880691528,
      "learning_rate": 0.0004993499566637776,
      "loss": 0.5223,
      "step": 41000
    },
    {
      "epoch": 1.0005579833333333,
      "grad_norm": 0.1227688416838646,
      "learning_rate": 0.0004993416227748517,
      "loss": 0.5198,
      "step": 41500
    },
    {
      "epoch": 1.00057465,
      "grad_norm": 0.12904536724090576,
      "learning_rate": 0.0004993332888859257,
      "loss": 0.5253,
      "step": 42000
    },
    {
      "epoch": 1.0005913166666667,
      "grad_norm": 0.25484269857406616,
      "learning_rate": 0.0004993249549969998,
      "loss": 0.5255,
      "step": 42500
    },
    {
      "epoch": 1.0006079833333332,
      "grad_norm": 0.13788571953773499,
      "learning_rate": 0.0004993166211080739,
      "loss": 0.5223,
      "step": 43000
    },
    {
      "epoch": 1.00062465,
      "grad_norm": 0.12936188280582428,
      "learning_rate": 0.0004993082872191479,
      "loss": 0.508,
      "step": 43500
    },
    {
      "epoch": 1.0006413166666666,
      "grad_norm": 0.11771363019943237,
      "learning_rate": 0.0004992999533302221,
      "loss": 0.5246,
      "step": 44000
    },
    {
      "epoch": 1.0006579833333333,
      "grad_norm": 0.12302890419960022,
      "learning_rate": 0.0004992916194412961,
      "loss": 0.5113,
      "step": 44500
    },
    {
      "epoch": 1.00067465,
      "grad_norm": 0.1550741344690323,
      "learning_rate": 0.0004992832855523702,
      "loss": 0.5236,
      "step": 45000
    },
    {
      "epoch": 1.0006913166666667,
      "grad_norm": 0.1512061506509781,
      "learning_rate": 0.0004992749516634442,
      "loss": 0.5174,
      "step": 45500
    },
    {
      "epoch": 1.0007079833333334,
      "grad_norm": 0.18470175564289093,
      "learning_rate": 0.0004992666177745183,
      "loss": 0.5161,
      "step": 46000
    },
    {
      "epoch": 1.00072465,
      "grad_norm": 0.1903432160615921,
      "learning_rate": 0.0004992582838855924,
      "loss": 0.522,
      "step": 46500
    },
    {
      "epoch": 1.0007413166666668,
      "grad_norm": 0.17968708276748657,
      "learning_rate": 0.0004992499499966664,
      "loss": 0.5272,
      "step": 47000
    },
    {
      "epoch": 1.0007579833333333,
      "grad_norm": 0.15185245871543884,
      "learning_rate": 0.0004992416161077405,
      "loss": 0.5128,
      "step": 47500
    },
    {
      "epoch": 1.00077465,
      "grad_norm": 0.14307790994644165,
      "learning_rate": 0.0004992332822188146,
      "loss": 0.5145,
      "step": 48000
    },
    {
      "epoch": 1.0007913166666667,
      "grad_norm": 0.18639637529850006,
      "learning_rate": 0.0004992249483298887,
      "loss": 0.5279,
      "step": 48500
    },
    {
      "epoch": 1.0008079833333334,
      "grad_norm": 0.10773248970508575,
      "learning_rate": 0.0004992166144409628,
      "loss": 0.519,
      "step": 49000
    },
    {
      "epoch": 1.00082465,
      "grad_norm": 0.11098728328943253,
      "learning_rate": 0.0004992082805520368,
      "loss": 0.5247,
      "step": 49500
    },
    {
      "epoch": 2.0000159666666666,
      "grad_norm": 0.13572710752487183,
      "learning_rate": 0.0004991999466631108,
      "loss": 0.504,
      "step": 50000
    },
    {
      "epoch": 2.0000326333333334,
      "grad_norm": 0.20072899758815765,
      "learning_rate": 0.000499191612774185,
      "loss": 0.5057,
      "step": 50500
    },
    {
      "epoch": 2.0000493,
      "grad_norm": 0.13405822217464447,
      "learning_rate": 0.000499183278885259,
      "loss": 0.5167,
      "step": 51000
    },
    {
      "epoch": 2.0000659666666665,
      "grad_norm": 0.11436880379915237,
      "learning_rate": 0.000499174944996333,
      "loss": 0.5145,
      "step": 51500
    },
    {
      "epoch": 2.0000826333333332,
      "grad_norm": 0.122079998254776,
      "learning_rate": 0.0004991666111074072,
      "loss": 0.5112,
      "step": 52000
    },
    {
      "epoch": 2.0000993,
      "grad_norm": 0.1603037267923355,
      "learning_rate": 0.0004991582772184812,
      "loss": 0.5283,
      "step": 52500
    },
    {
      "epoch": 2.000115966666667,
      "grad_norm": 0.1394503265619278,
      "learning_rate": 0.0004991499433295554,
      "loss": 0.5002,
      "step": 53000
    },
    {
      "epoch": 2.000132633333333,
      "grad_norm": 0.15587322413921356,
      "learning_rate": 0.0004991416094406294,
      "loss": 0.5086,
      "step": 53500
    },
    {
      "epoch": 2.0001493,
      "grad_norm": 0.16498330235481262,
      "learning_rate": 0.0004991332755517034,
      "loss": 0.5051,
      "step": 54000
    },
    {
      "epoch": 2.0001659666666667,
      "grad_norm": 0.14812913537025452,
      "learning_rate": 0.0004991249416627776,
      "loss": 0.5206,
      "step": 54500
    },
    {
      "epoch": 2.0001826333333335,
      "grad_norm": 0.18272103369235992,
      "learning_rate": 0.0004991166077738516,
      "loss": 0.526,
      "step": 55000
    },
    {
      "epoch": 2.0001993,
      "grad_norm": 0.11598959565162659,
      "learning_rate": 0.0004991082738849256,
      "loss": 0.4973,
      "step": 55500
    },
    {
      "epoch": 2.0002159666666666,
      "grad_norm": 0.13714590668678284,
      "learning_rate": 0.0004990999399959997,
      "loss": 0.5101,
      "step": 56000
    },
    {
      "epoch": 2.0002326333333333,
      "grad_norm": 0.12116168439388275,
      "learning_rate": 0.0004990916061070738,
      "loss": 0.5028,
      "step": 56500
    },
    {
      "epoch": 2.0002493,
      "grad_norm": 0.11389432102441788,
      "learning_rate": 0.0004990832722181479,
      "loss": 0.5167,
      "step": 57000
    },
    {
      "epoch": 2.0002659666666665,
      "grad_norm": 0.14552216231822968,
      "learning_rate": 0.000499074938329222,
      "loss": 0.5154,
      "step": 57500
    },
    {
      "epoch": 2.0002826333333332,
      "grad_norm": 0.11753116548061371,
      "learning_rate": 0.000499066604440296,
      "loss": 0.5205,
      "step": 58000
    },
    {
      "epoch": 2.0002993,
      "grad_norm": 0.133879616856575,
      "learning_rate": 0.0004990582705513701,
      "loss": 0.5043,
      "step": 58500
    },
    {
      "epoch": 2.000315966666667,
      "grad_norm": 0.13363508880138397,
      "learning_rate": 0.0004990499366624442,
      "loss": 0.5016,
      "step": 59000
    },
    {
      "epoch": 2.0003326333333336,
      "grad_norm": 0.1112310141324997,
      "learning_rate": 0.0004990416027735183,
      "loss": 0.5252,
      "step": 59500
    },
    {
      "epoch": 2.0003493,
      "grad_norm": 0.11062963306903839,
      "learning_rate": 0.0004990332688845923,
      "loss": 0.5308,
      "step": 60000
    },
    {
      "epoch": 2.0003659666666667,
      "grad_norm": 0.12244955450296402,
      "learning_rate": 0.0004990249349956663,
      "loss": 0.5251,
      "step": 60500
    },
    {
      "epoch": 2.0003826333333334,
      "grad_norm": 0.1307399868965149,
      "learning_rate": 0.0004990166011067405,
      "loss": 0.5199,
      "step": 61000
    },
    {
      "epoch": 2.0003993,
      "grad_norm": 0.1836928278207779,
      "learning_rate": 0.0004990082672178145,
      "loss": 0.5136,
      "step": 61500
    },
    {
      "epoch": 2.0004159666666665,
      "grad_norm": 0.16484445333480835,
      "learning_rate": 0.0004989999333288886,
      "loss": 0.5131,
      "step": 62000
    },
    {
      "epoch": 2.0004326333333333,
      "grad_norm": 0.14271564781665802,
      "learning_rate": 0.0004989915994399627,
      "loss": 0.5067,
      "step": 62500
    },
    {
      "epoch": 2.0004493,
      "grad_norm": 0.18172340095043182,
      "learning_rate": 0.0004989832655510367,
      "loss": 0.512,
      "step": 63000
    },
    {
      "epoch": 2.000465966666667,
      "grad_norm": 0.15875178575515747,
      "learning_rate": 0.0004989749316621109,
      "loss": 0.5138,
      "step": 63500
    },
    {
      "epoch": 2.000482633333333,
      "grad_norm": 0.1550641506910324,
      "learning_rate": 0.0004989665977731849,
      "loss": 0.5264,
      "step": 64000
    },
    {
      "epoch": 2.0004993,
      "grad_norm": 0.12185290455818176,
      "learning_rate": 0.0004989582638842589,
      "loss": 0.5038,
      "step": 64500
    },
    {
      "epoch": 2.0005159666666668,
      "grad_norm": 0.161433145403862,
      "learning_rate": 0.000498949929995333,
      "loss": 0.5226,
      "step": 65000
    },
    {
      "epoch": 2.0005326333333335,
      "grad_norm": 0.17462237179279327,
      "learning_rate": 0.0004989415961064071,
      "loss": 0.5279,
      "step": 65500
    },
    {
      "epoch": 2.0005493,
      "grad_norm": 0.13087806105613708,
      "learning_rate": 0.0004989332622174813,
      "loss": 0.5154,
      "step": 66000
    },
    {
      "epoch": 2.0005659666666666,
      "grad_norm": 0.12471126765012741,
      "learning_rate": 0.0004989249283285553,
      "loss": 0.5238,
      "step": 66500
    },
    {
      "epoch": 2.0005826333333334,
      "grad_norm": 0.09948164224624634,
      "learning_rate": 0.0004989165944396293,
      "loss": 0.5053,
      "step": 67000
    },
    {
      "epoch": 2.0005993,
      "grad_norm": 0.14608526229858398,
      "learning_rate": 0.0004989082605507034,
      "loss": 0.5063,
      "step": 67500
    },
    {
      "epoch": 2.0006159666666665,
      "grad_norm": 0.11540653556585312,
      "learning_rate": 0.0004988999266617775,
      "loss": 0.5144,
      "step": 68000
    },
    {
      "epoch": 2.0006326333333333,
      "grad_norm": 0.1168479397892952,
      "learning_rate": 0.0004988915927728515,
      "loss": 0.5114,
      "step": 68500
    },
    {
      "epoch": 2.0006493,
      "grad_norm": 0.12215118110179901,
      "learning_rate": 0.0004988832588839256,
      "loss": 0.5121,
      "step": 69000
    },
    {
      "epoch": 2.000665966666667,
      "grad_norm": 0.14382800459861755,
      "learning_rate": 0.0004988749249949997,
      "loss": 0.4947,
      "step": 69500
    },
    {
      "epoch": 2.000682633333333,
      "grad_norm": 0.12561240792274475,
      "learning_rate": 0.0004988665911060737,
      "loss": 0.5145,
      "step": 70000
    },
    {
      "epoch": 2.0006993,
      "grad_norm": 0.11011038720607758,
      "learning_rate": 0.0004988582572171479,
      "loss": 0.5079,
      "step": 70500
    },
    {
      "epoch": 2.0007159666666667,
      "grad_norm": 0.1552480310201645,
      "learning_rate": 0.0004988499233282219,
      "loss": 0.5126,
      "step": 71000
    },
    {
      "epoch": 2.0007326333333335,
      "grad_norm": 0.09911241382360458,
      "learning_rate": 0.000498841589439296,
      "loss": 0.5201,
      "step": 71500
    },
    {
      "epoch": 2.0007493,
      "grad_norm": 0.16598893702030182,
      "learning_rate": 0.00049883325555037,
      "loss": 0.5178,
      "step": 72000
    },
    {
      "epoch": 2.0007659666666666,
      "grad_norm": 0.12831008434295654,
      "learning_rate": 0.0004988249216614441,
      "loss": 0.515,
      "step": 72500
    },
    {
      "epoch": 2.0007826333333334,
      "grad_norm": 0.13237036764621735,
      "learning_rate": 0.0004988165877725182,
      "loss": 0.509,
      "step": 73000
    },
    {
      "epoch": 2.0007993,
      "grad_norm": 0.1379414051771164,
      "learning_rate": 0.0004988082538835922,
      "loss": 0.5153,
      "step": 73500
    },
    {
      "epoch": 2.0008159666666665,
      "grad_norm": 0.10922642052173615,
      "learning_rate": 0.0004987999199946664,
      "loss": 0.5029,
      "step": 74000
    },
    {
      "epoch": 3.0000072833333333,
      "grad_norm": 0.11715913563966751,
      "learning_rate": 0.0004987915861057404,
      "loss": 0.5121,
      "step": 74500
    },
    {
      "epoch": 3.00002395,
      "grad_norm": 0.09988059848546982,
      "learning_rate": 0.0004987832522168145,
      "loss": 0.501,
      "step": 75000
    },
    {
      "epoch": 3.000040616666667,
      "grad_norm": 0.14040575921535492,
      "learning_rate": 0.0004987749183278886,
      "loss": 0.4893,
      "step": 75500
    },
    {
      "epoch": 3.000057283333333,
      "grad_norm": 0.10135674476623535,
      "learning_rate": 0.0004987665844389626,
      "loss": 0.5022,
      "step": 76000
    },
    {
      "epoch": 3.00007395,
      "grad_norm": 0.13899530470371246,
      "learning_rate": 0.0004987582505500366,
      "loss": 0.4983,
      "step": 76500
    },
    {
      "epoch": 3.0000906166666668,
      "grad_norm": 0.09998220950365067,
      "learning_rate": 0.0004987499166611108,
      "loss": 0.5062,
      "step": 77000
    },
    {
      "epoch": 3.0001072833333335,
      "grad_norm": 0.11027289181947708,
      "learning_rate": 0.0004987415827721848,
      "loss": 0.5075,
      "step": 77500
    },
    {
      "epoch": 3.00012395,
      "grad_norm": 0.14127539098262787,
      "learning_rate": 0.0004987332488832588,
      "loss": 0.5011,
      "step": 78000
    },
    {
      "epoch": 3.0001406166666666,
      "grad_norm": 0.11448643356561661,
      "learning_rate": 0.000498724914994333,
      "loss": 0.5217,
      "step": 78500
    },
    {
      "epoch": 3.0001572833333334,
      "grad_norm": 0.12158645689487457,
      "learning_rate": 0.000498716581105407,
      "loss": 0.5099,
      "step": 79000
    },
    {
      "epoch": 3.00017395,
      "grad_norm": 0.09740091860294342,
      "learning_rate": 0.0004987082472164811,
      "loss": 0.5243,
      "step": 79500
    },
    {
      "epoch": 3.0001906166666665,
      "grad_norm": 0.12505486607551575,
      "learning_rate": 0.0004986999133275552,
      "loss": 0.4898,
      "step": 80000
    },
    {
      "epoch": 3.0002072833333333,
      "grad_norm": 0.10445640236139297,
      "learning_rate": 0.0004986915794386292,
      "loss": 0.4913,
      "step": 80500
    },
    {
      "epoch": 3.00022395,
      "grad_norm": 0.15126246213912964,
      "learning_rate": 0.0004986832455497034,
      "loss": 0.5114,
      "step": 81000
    },
    {
      "epoch": 3.000240616666667,
      "grad_norm": 0.10633877664804459,
      "learning_rate": 0.0004986749116607774,
      "loss": 0.4953,
      "step": 81500
    },
    {
      "epoch": 3.000257283333333,
      "grad_norm": 0.11075803637504578,
      "learning_rate": 0.0004986665777718515,
      "loss": 0.5236,
      "step": 82000
    },
    {
      "epoch": 3.00027395,
      "grad_norm": 0.14428989589214325,
      "learning_rate": 0.0004986582438829255,
      "loss": 0.5044,
      "step": 82500
    },
    {
      "epoch": 3.0002906166666667,
      "grad_norm": 0.11421027779579163,
      "learning_rate": 0.0004986499099939996,
      "loss": 0.5038,
      "step": 83000
    },
    {
      "epoch": 3.0003072833333335,
      "grad_norm": 0.12961487472057343,
      "learning_rate": 0.0004986415761050737,
      "loss": 0.5241,
      "step": 83500
    },
    {
      "epoch": 3.00032395,
      "grad_norm": 0.13614989817142487,
      "learning_rate": 0.0004986332422161477,
      "loss": 0.5155,
      "step": 84000
    },
    {
      "epoch": 3.0003406166666666,
      "grad_norm": 0.10954741388559341,
      "learning_rate": 0.0004986249083272218,
      "loss": 0.5072,
      "step": 84500
    },
    {
      "epoch": 3.0003572833333334,
      "grad_norm": 0.1101946011185646,
      "learning_rate": 0.0004986165744382959,
      "loss": 0.5208,
      "step": 85000
    },
    {
      "epoch": 3.00037395,
      "grad_norm": 0.10347022861242294,
      "learning_rate": 0.00049860824054937,
      "loss": 0.499,
      "step": 85500
    },
    {
      "epoch": 3.0003906166666665,
      "grad_norm": 0.11708083748817444,
      "learning_rate": 0.0004985999066604441,
      "loss": 0.5294,
      "step": 86000
    },
    {
      "epoch": 3.0004072833333333,
      "grad_norm": 0.12153756618499756,
      "learning_rate": 0.0004985915727715181,
      "loss": 0.5153,
      "step": 86500
    },
    {
      "epoch": 3.00042395,
      "grad_norm": 0.09323564171791077,
      "learning_rate": 0.0004985832388825921,
      "loss": 0.5238,
      "step": 87000
    },
    {
      "epoch": 3.000440616666667,
      "grad_norm": 0.11702670902013779,
      "learning_rate": 0.0004985749049936663,
      "loss": 0.5065,
      "step": 87500
    },
    {
      "epoch": 3.000457283333333,
      "grad_norm": 0.09608989208936691,
      "learning_rate": 0.0004985665711047403,
      "loss": 0.4983,
      "step": 88000
    },
    {
      "epoch": 3.00047395,
      "grad_norm": 0.1614898145198822,
      "learning_rate": 0.0004985582372158143,
      "loss": 0.4995,
      "step": 88500
    },
    {
      "epoch": 3.0004906166666667,
      "grad_norm": 0.154231458902359,
      "learning_rate": 0.0004985499033268885,
      "loss": 0.5016,
      "step": 89000
    },
    {
      "epoch": 3.0005072833333335,
      "grad_norm": 0.10689470916986465,
      "learning_rate": 0.0004985415694379625,
      "loss": 0.5039,
      "step": 89500
    },
    {
      "epoch": 3.00052395,
      "grad_norm": 0.10663019120693207,
      "learning_rate": 0.0004985332355490367,
      "loss": 0.5074,
      "step": 90000
    },
    {
      "epoch": 3.0005406166666666,
      "grad_norm": 0.10099863260984421,
      "learning_rate": 0.0004985249016601107,
      "loss": 0.4977,
      "step": 90500
    },
    {
      "epoch": 3.0005572833333334,
      "grad_norm": 0.13027717173099518,
      "learning_rate": 0.0004985165677711847,
      "loss": 0.5048,
      "step": 91000
    },
    {
      "epoch": 3.00057395,
      "grad_norm": 0.11482946574687958,
      "learning_rate": 0.0004985082338822589,
      "loss": 0.4984,
      "step": 91500
    },
    {
      "epoch": 3.0005906166666665,
      "grad_norm": 0.09548457711935043,
      "learning_rate": 0.0004984998999933329,
      "loss": 0.51,
      "step": 92000
    },
    {
      "epoch": 3.0006072833333333,
      "grad_norm": 0.14352157711982727,
      "learning_rate": 0.0004984915661044069,
      "loss": 0.4951,
      "step": 92500
    },
    {
      "epoch": 3.00062395,
      "grad_norm": 0.125238835811615,
      "learning_rate": 0.000498483232215481,
      "loss": 0.4963,
      "step": 93000
    },
    {
      "epoch": 3.000640616666667,
      "grad_norm": 0.12364764511585236,
      "learning_rate": 0.0004984748983265551,
      "loss": 0.498,
      "step": 93500
    },
    {
      "epoch": 3.000657283333333,
      "grad_norm": 0.14016610383987427,
      "learning_rate": 0.0004984665644376292,
      "loss": 0.5108,
      "step": 94000
    },
    {
      "epoch": 3.00067395,
      "grad_norm": 0.10969480127096176,
      "learning_rate": 0.0004984582305487033,
      "loss": 0.512,
      "step": 94500
    },
    {
      "epoch": 3.0006906166666667,
      "grad_norm": 0.13796895742416382,
      "learning_rate": 0.0004984498966597773,
      "loss": 0.5054,
      "step": 95000
    },
    {
      "epoch": 3.0007072833333335,
      "grad_norm": 0.09735652804374695,
      "learning_rate": 0.0004984415627708514,
      "loss": 0.5083,
      "step": 95500
    },
    {
      "epoch": 3.00072395,
      "grad_norm": 0.11182783544063568,
      "learning_rate": 0.0004984332288819255,
      "loss": 0.5094,
      "step": 96000
    },
    {
      "epoch": 3.0007406166666666,
      "grad_norm": 0.09881012886762619,
      "learning_rate": 0.0004984248949929995,
      "loss": 0.4983,
      "step": 96500
    },
    {
      "epoch": 3.0007572833333334,
      "grad_norm": 0.15566657483577728,
      "learning_rate": 0.0004984165611040736,
      "loss": 0.4966,
      "step": 97000
    },
    {
      "epoch": 3.00077395,
      "grad_norm": 0.14154233038425446,
      "learning_rate": 0.0004984082272151476,
      "loss": 0.5164,
      "step": 97500
    },
    {
      "epoch": 3.0007906166666665,
      "grad_norm": 0.15929555892944336,
      "learning_rate": 0.0004983998933262218,
      "loss": 0.5055,
      "step": 98000
    },
    {
      "epoch": 3.0008072833333332,
      "grad_norm": 0.20754003524780273,
      "learning_rate": 0.0004983915594372958,
      "loss": 0.5092,
      "step": 98500
    },
    {
      "epoch": 3.00082395,
      "grad_norm": 0.12343036383390427,
      "learning_rate": 0.0004983832255483699,
      "loss": 0.5044,
      "step": 99000
    },
    {
      "epoch": 4.000015266666667,
      "grad_norm": 0.11899963021278381,
      "learning_rate": 0.000498374891659444,
      "loss": 0.5008,
      "step": 99500
    },
    {
      "epoch": 4.000031933333333,
      "grad_norm": 0.12955987453460693,
      "learning_rate": 0.000498366557770518,
      "loss": 0.505,
      "step": 100000
    },
    {
      "epoch": 4.0000486,
      "grad_norm": 0.12805217504501343,
      "learning_rate": 0.0004983582238815922,
      "loss": 0.5025,
      "step": 100500
    },
    {
      "epoch": 4.000065266666667,
      "grad_norm": 0.09346239268779755,
      "learning_rate": 0.0004983498899926662,
      "loss": 0.4971,
      "step": 101000
    },
    {
      "epoch": 4.000081933333333,
      "grad_norm": 0.15130937099456787,
      "learning_rate": 0.0004983415561037402,
      "loss": 0.5001,
      "step": 101500
    },
    {
      "epoch": 4.0000986,
      "grad_norm": 0.10144880414009094,
      "learning_rate": 0.0004983332222148143,
      "loss": 0.4868,
      "step": 102000
    },
    {
      "epoch": 4.000115266666667,
      "grad_norm": 0.1373555064201355,
      "learning_rate": 0.0004983248883258884,
      "loss": 0.5104,
      "step": 102500
    },
    {
      "epoch": 4.000131933333333,
      "grad_norm": 0.12892888486385345,
      "learning_rate": 0.0004983165544369625,
      "loss": 0.4945,
      "step": 103000
    },
    {
      "epoch": 4.0001486,
      "grad_norm": 0.12814046442508698,
      "learning_rate": 0.0004983082205480366,
      "loss": 0.4949,
      "step": 103500
    },
    {
      "epoch": 4.0001652666666665,
      "grad_norm": 0.1137513667345047,
      "learning_rate": 0.0004982998866591106,
      "loss": 0.5091,
      "step": 104000
    },
    {
      "epoch": 4.000181933333334,
      "grad_norm": 0.1040620431303978,
      "learning_rate": 0.0004982915527701846,
      "loss": 0.4974,
      "step": 104500
    },
    {
      "epoch": 4.0001986,
      "grad_norm": 0.11345245689153671,
      "learning_rate": 0.0004982832188812588,
      "loss": 0.5184,
      "step": 105000
    },
    {
      "epoch": 4.000215266666666,
      "grad_norm": 0.11980816721916199,
      "learning_rate": 0.0004982748849923328,
      "loss": 0.4947,
      "step": 105500
    },
    {
      "epoch": 4.000231933333334,
      "grad_norm": 0.15591755509376526,
      "learning_rate": 0.0004982665511034069,
      "loss": 0.5019,
      "step": 106000
    },
    {
      "epoch": 4.0002486,
      "grad_norm": 0.11826740950345993,
      "learning_rate": 0.000498258217214481,
      "loss": 0.4973,
      "step": 106500
    },
    {
      "epoch": 4.000265266666666,
      "grad_norm": 0.10465603321790695,
      "learning_rate": 0.000498249883325555,
      "loss": 0.4975,
      "step": 107000
    },
    {
      "epoch": 4.0002819333333335,
      "grad_norm": 0.12639306485652924,
      "learning_rate": 0.0004982415494366292,
      "loss": 0.5081,
      "step": 107500
    },
    {
      "epoch": 4.0002986,
      "grad_norm": 0.08846205472946167,
      "learning_rate": 0.0004982332155477032,
      "loss": 0.5114,
      "step": 108000
    },
    {
      "epoch": 4.000315266666667,
      "grad_norm": 0.11422135680913925,
      "learning_rate": 0.0004982248816587773,
      "loss": 0.4785,
      "step": 108500
    },
    {
      "epoch": 4.000331933333333,
      "grad_norm": 0.12758806347846985,
      "learning_rate": 0.0004982165477698513,
      "loss": 0.517,
      "step": 109000
    },
    {
      "epoch": 4.0003486,
      "grad_norm": 0.12573428452014923,
      "learning_rate": 0.0004982082138809254,
      "loss": 0.4996,
      "step": 109500
    },
    {
      "epoch": 4.000365266666667,
      "grad_norm": 0.12604889273643494,
      "learning_rate": 0.0004981998799919995,
      "loss": 0.5126,
      "step": 110000
    },
    {
      "epoch": 4.000381933333333,
      "grad_norm": 0.10070759803056717,
      "learning_rate": 0.0004981915461030735,
      "loss": 0.4916,
      "step": 110500
    },
    {
      "epoch": 4.0003986,
      "grad_norm": 0.08192973583936691,
      "learning_rate": 0.0004981832122141476,
      "loss": 0.5054,
      "step": 111000
    },
    {
      "epoch": 4.000415266666667,
      "grad_norm": 0.17697639763355255,
      "learning_rate": 0.0004981748783252217,
      "loss": 0.5056,
      "step": 111500
    },
    {
      "epoch": 4.000431933333333,
      "grad_norm": 0.0895695686340332,
      "learning_rate": 0.0004981665444362958,
      "loss": 0.5064,
      "step": 112000
    },
    {
      "epoch": 4.0004486,
      "grad_norm": 0.10446173697710037,
      "learning_rate": 0.0004981582105473699,
      "loss": 0.499,
      "step": 112500
    },
    {
      "epoch": 4.000465266666667,
      "grad_norm": 0.11105731874704361,
      "learning_rate": 0.0004981498766584439,
      "loss": 0.4907,
      "step": 113000
    },
    {
      "epoch": 4.000481933333333,
      "grad_norm": 0.17902767658233643,
      "learning_rate": 0.0004981415427695179,
      "loss": 0.5123,
      "step": 113500
    },
    {
      "epoch": 4.0004986,
      "grad_norm": 0.15805183351039886,
      "learning_rate": 0.0004981332088805921,
      "loss": 0.5095,
      "step": 114000
    },
    {
      "epoch": 4.000515266666667,
      "grad_norm": 0.10879934579133987,
      "learning_rate": 0.0004981248749916661,
      "loss": 0.4975,
      "step": 114500
    },
    {
      "epoch": 4.000531933333333,
      "grad_norm": 0.12631550431251526,
      "learning_rate": 0.0004981165411027401,
      "loss": 0.5008,
      "step": 115000
    },
    {
      "epoch": 4.0005486,
      "grad_norm": 0.09307848662137985,
      "learning_rate": 0.0004981082072138143,
      "loss": 0.5163,
      "step": 115500
    },
    {
      "epoch": 4.0005652666666665,
      "grad_norm": 0.13327570259571075,
      "learning_rate": 0.0004980998733248883,
      "loss": 0.5097,
      "step": 116000
    },
    {
      "epoch": 4.000581933333334,
      "grad_norm": 0.13997240364551544,
      "learning_rate": 0.0004980915394359625,
      "loss": 0.4931,
      "step": 116500
    },
    {
      "epoch": 4.0005986,
      "grad_norm": 0.13947707414627075,
      "learning_rate": 0.0004980832055470365,
      "loss": 0.5079,
      "step": 117000
    },
    {
      "epoch": 4.000615266666666,
      "grad_norm": 0.13003845512866974,
      "learning_rate": 0.0004980748716581105,
      "loss": 0.4932,
      "step": 117500
    },
    {
      "epoch": 4.000631933333334,
      "grad_norm": 0.10368097573518753,
      "learning_rate": 0.0004980665377691847,
      "loss": 0.5144,
      "step": 118000
    },
    {
      "epoch": 4.0006486,
      "grad_norm": 0.11696897447109222,
      "learning_rate": 0.0004980582038802587,
      "loss": 0.5004,
      "step": 118500
    },
    {
      "epoch": 4.000665266666667,
      "grad_norm": 0.14525854587554932,
      "learning_rate": 0.0004980498699913327,
      "loss": 0.4849,
      "step": 119000
    },
    {
      "epoch": 4.000681933333333,
      "grad_norm": 0.1257287859916687,
      "learning_rate": 0.0004980415361024068,
      "loss": 0.4971,
      "step": 119500
    },
    {
      "epoch": 4.0006986,
      "grad_norm": 0.12349390983581543,
      "learning_rate": 0.0004980332022134809,
      "loss": 0.4908,
      "step": 120000
    },
    {
      "epoch": 4.000715266666667,
      "grad_norm": 0.10592661798000336,
      "learning_rate": 0.000498024868324555,
      "loss": 0.5016,
      "step": 120500
    },
    {
      "epoch": 4.000731933333333,
      "grad_norm": 0.12864843010902405,
      "learning_rate": 0.0004980165344356291,
      "loss": 0.4989,
      "step": 121000
    },
    {
      "epoch": 4.0007486,
      "grad_norm": 0.16795076429843903,
      "learning_rate": 0.0004980082005467031,
      "loss": 0.4953,
      "step": 121500
    },
    {
      "epoch": 4.000765266666667,
      "grad_norm": 0.11569545418024063,
      "learning_rate": 0.0004979998666577772,
      "loss": 0.5035,
      "step": 122000
    },
    {
      "epoch": 4.000781933333333,
      "grad_norm": 0.1354602724313736,
      "learning_rate": 0.0004979915327688513,
      "loss": 0.5066,
      "step": 122500
    },
    {
      "epoch": 4.0007986,
      "grad_norm": 0.12527142465114594,
      "learning_rate": 0.0004979831988799254,
      "loss": 0.4924,
      "step": 123000
    },
    {
      "epoch": 4.000815266666667,
      "grad_norm": 0.12657012045383453,
      "learning_rate": 0.0004979748649909994,
      "loss": 0.4949,
      "step": 123500
    },
    {
      "epoch": 5.000006583333334,
      "grad_norm": 0.11986567825078964,
      "learning_rate": 0.0004979665311020734,
      "loss": 0.4943,
      "step": 124000
    },
    {
      "epoch": 5.00002325,
      "grad_norm": 0.11787496507167816,
      "learning_rate": 0.0004979581972131476,
      "loss": 0.4989,
      "step": 124500
    },
    {
      "epoch": 5.000039916666666,
      "grad_norm": 0.09769067913293839,
      "learning_rate": 0.0004979498633242216,
      "loss": 0.5104,
      "step": 125000
    },
    {
      "epoch": 5.0000565833333335,
      "grad_norm": 0.12465673685073853,
      "learning_rate": 0.0004979415294352957,
      "loss": 0.5052,
      "step": 125500
    },
    {
      "epoch": 5.00007325,
      "grad_norm": 0.1398012489080429,
      "learning_rate": 0.0004979331955463698,
      "loss": 0.4967,
      "step": 126000
    },
    {
      "epoch": 5.000089916666667,
      "grad_norm": 0.08807983249425888,
      "learning_rate": 0.0004979248616574438,
      "loss": 0.4942,
      "step": 126500
    },
    {
      "epoch": 5.000106583333333,
      "grad_norm": 0.12891969084739685,
      "learning_rate": 0.000497916527768518,
      "loss": 0.4983,
      "step": 127000
    },
    {
      "epoch": 5.00012325,
      "grad_norm": 0.12461962550878525,
      "learning_rate": 0.000497908193879592,
      "loss": 0.4981,
      "step": 127500
    },
    {
      "epoch": 5.000139916666667,
      "grad_norm": 0.11974582821130753,
      "learning_rate": 0.000497899859990666,
      "loss": 0.5063,
      "step": 128000
    },
    {
      "epoch": 5.000156583333333,
      "grad_norm": 0.12075697630643845,
      "learning_rate": 0.0004978915261017401,
      "loss": 0.5094,
      "step": 128500
    },
    {
      "epoch": 5.00017325,
      "grad_norm": 0.11611577868461609,
      "learning_rate": 0.0004978831922128142,
      "loss": 0.4989,
      "step": 129000
    },
    {
      "epoch": 5.000189916666667,
      "grad_norm": 0.13601495325565338,
      "learning_rate": 0.0004978748583238882,
      "loss": 0.499,
      "step": 129500
    },
    {
      "epoch": 5.000206583333333,
      "grad_norm": 0.10752883553504944,
      "learning_rate": 0.0004978665244349624,
      "loss": 0.4959,
      "step": 130000
    },
    {
      "epoch": 5.00022325,
      "grad_norm": 0.1042650118470192,
      "learning_rate": 0.0004978581905460364,
      "loss": 0.4802,
      "step": 130500
    },
    {
      "epoch": 5.000239916666667,
      "grad_norm": 0.13419273495674133,
      "learning_rate": 0.0004978498566571105,
      "loss": 0.4897,
      "step": 131000
    },
    {
      "epoch": 5.000256583333333,
      "grad_norm": 0.14560183882713318,
      "learning_rate": 0.0004978415227681846,
      "loss": 0.4985,
      "step": 131500
    },
    {
      "epoch": 5.00027325,
      "grad_norm": 0.10342065244913101,
      "learning_rate": 0.0004978331888792586,
      "loss": 0.4924,
      "step": 132000
    },
    {
      "epoch": 5.000289916666667,
      "grad_norm": 0.1497325748205185,
      "learning_rate": 0.0004978248549903327,
      "loss": 0.4836,
      "step": 132500
    },
    {
      "epoch": 5.000306583333334,
      "grad_norm": 0.20469798147678375,
      "learning_rate": 0.0004978165211014068,
      "loss": 0.4989,
      "step": 133000
    },
    {
      "epoch": 5.00032325,
      "grad_norm": 0.13139817118644714,
      "learning_rate": 0.0004978081872124808,
      "loss": 0.4843,
      "step": 133500
    },
    {
      "epoch": 5.000339916666666,
      "grad_norm": 0.10928898304700851,
      "learning_rate": 0.0004977998533235549,
      "loss": 0.5068,
      "step": 134000
    },
    {
      "epoch": 5.000356583333334,
      "grad_norm": 0.10305245965719223,
      "learning_rate": 0.000497791519434629,
      "loss": 0.4904,
      "step": 134500
    },
    {
      "epoch": 5.00037325,
      "grad_norm": 0.12034566700458527,
      "learning_rate": 0.0004977831855457031,
      "loss": 0.485,
      "step": 135000
    },
    {
      "epoch": 5.000389916666666,
      "grad_norm": 0.1872764527797699,
      "learning_rate": 0.0004977748516567771,
      "loss": 0.5142,
      "step": 135500
    },
    {
      "epoch": 5.0004065833333335,
      "grad_norm": 0.1379832774400711,
      "learning_rate": 0.0004977665177678512,
      "loss": 0.4951,
      "step": 136000
    },
    {
      "epoch": 5.00042325,
      "grad_norm": 0.12769894301891327,
      "learning_rate": 0.0004977581838789253,
      "loss": 0.5087,
      "step": 136500
    },
    {
      "epoch": 5.000439916666667,
      "grad_norm": 0.1022060289978981,
      "learning_rate": 0.0004977498499899993,
      "loss": 0.5036,
      "step": 137000
    },
    {
      "epoch": 5.000456583333333,
      "grad_norm": 0.11077611893415451,
      "learning_rate": 0.0004977415161010734,
      "loss": 0.4794,
      "step": 137500
    },
    {
      "epoch": 5.00047325,
      "grad_norm": 0.10508996248245239,
      "learning_rate": 0.0004977331822121475,
      "loss": 0.5049,
      "step": 138000
    },
    {
      "epoch": 5.000489916666667,
      "grad_norm": 0.12943673133850098,
      "learning_rate": 0.0004977248483232215,
      "loss": 0.4975,
      "step": 138500
    },
    {
      "epoch": 5.000506583333333,
      "grad_norm": 0.09943576902151108,
      "learning_rate": 0.0004977165144342957,
      "loss": 0.4813,
      "step": 139000
    },
    {
      "epoch": 5.00052325,
      "grad_norm": 0.14027589559555054,
      "learning_rate": 0.0004977081805453697,
      "loss": 0.4915,
      "step": 139500
    },
    {
      "epoch": 5.000539916666667,
      "grad_norm": 0.180355504155159,
      "learning_rate": 0.0004976998466564438,
      "loss": 0.5138,
      "step": 140000
    },
    {
      "epoch": 5.000556583333333,
      "grad_norm": 0.11836904287338257,
      "learning_rate": 0.0004976915127675179,
      "loss": 0.5064,
      "step": 140500
    },
    {
      "epoch": 5.00057325,
      "grad_norm": 0.08462950587272644,
      "learning_rate": 0.0004976831788785919,
      "loss": 0.4924,
      "step": 141000
    },
    {
      "epoch": 5.000589916666667,
      "grad_norm": 0.11227858066558838,
      "learning_rate": 0.0004976748449896659,
      "loss": 0.5135,
      "step": 141500
    },
    {
      "epoch": 5.000606583333333,
      "grad_norm": 0.10095331072807312,
      "learning_rate": 0.0004976665111007401,
      "loss": 0.496,
      "step": 142000
    },
    {
      "epoch": 5.00062325,
      "grad_norm": 0.11805472522974014,
      "learning_rate": 0.0004976581772118141,
      "loss": 0.4879,
      "step": 142500
    },
    {
      "epoch": 5.000639916666667,
      "grad_norm": 0.21374362707138062,
      "learning_rate": 0.0004976498433228882,
      "loss": 0.4862,
      "step": 143000
    },
    {
      "epoch": 5.000656583333333,
      "grad_norm": 0.14500202238559723,
      "learning_rate": 0.0004976415094339623,
      "loss": 0.498,
      "step": 143500
    },
    {
      "epoch": 5.00067325,
      "grad_norm": 0.12204201519489288,
      "learning_rate": 0.0004976331755450363,
      "loss": 0.4947,
      "step": 144000
    },
    {
      "epoch": 5.0006899166666665,
      "grad_norm": 0.12969742715358734,
      "learning_rate": 0.0004976248416561105,
      "loss": 0.5056,
      "step": 144500
    },
    {
      "epoch": 5.000706583333334,
      "grad_norm": 0.11909934878349304,
      "learning_rate": 0.0004976165077671845,
      "loss": 0.494,
      "step": 145000
    },
    {
      "epoch": 5.00072325,
      "grad_norm": 0.11378882080316544,
      "learning_rate": 0.0004976081738782585,
      "loss": 0.4864,
      "step": 145500
    },
    {
      "epoch": 5.000739916666666,
      "grad_norm": 0.13654902577400208,
      "learning_rate": 0.0004975998399893326,
      "loss": 0.4933,
      "step": 146000
    },
    {
      "epoch": 5.000756583333334,
      "grad_norm": 0.15423578023910522,
      "learning_rate": 0.0004975915061004067,
      "loss": 0.4904,
      "step": 146500
    },
    {
      "epoch": 5.00077325,
      "grad_norm": 0.09825745970010757,
      "learning_rate": 0.0004975831722114808,
      "loss": 0.5127,
      "step": 147000
    },
    {
      "epoch": 5.000789916666666,
      "grad_norm": 0.12199290841817856,
      "learning_rate": 0.0004975748383225548,
      "loss": 0.4897,
      "step": 147500
    },
    {
      "epoch": 5.0008065833333335,
      "grad_norm": 0.12620779871940613,
      "learning_rate": 0.0004975665044336289,
      "loss": 0.5097,
      "step": 148000
    },
    {
      "epoch": 5.00082325,
      "grad_norm": 0.11557506769895554,
      "learning_rate": 0.000497558170544703,
      "loss": 0.5077,
      "step": 148500
    },
    {
      "epoch": 6.000014566666667,
      "grad_norm": 0.1643439382314682,
      "learning_rate": 0.0004975498366557771,
      "loss": 0.4922,
      "step": 149000
    },
    {
      "epoch": 6.000031233333333,
      "grad_norm": 0.17156921327114105,
      "learning_rate": 0.0004975415027668512,
      "loss": 0.4924,
      "step": 149500
    },
    {
      "epoch": 6.0000479,
      "grad_norm": 0.15455399453639984,
      "learning_rate": 0.0004975331688779252,
      "loss": 0.5036,
      "step": 150000
    },
    {
      "epoch": 6.0000645666666665,
      "grad_norm": 0.14313964545726776,
      "learning_rate": 0.0004975248349889992,
      "loss": 0.4863,
      "step": 150500
    },
    {
      "epoch": 6.000081233333334,
      "grad_norm": 0.10154742002487183,
      "learning_rate": 0.0004975165011000734,
      "loss": 0.4971,
      "step": 151000
    },
    {
      "epoch": 6.0000979,
      "grad_norm": 0.12556812167167664,
      "learning_rate": 0.0004975081672111474,
      "loss": 0.5038,
      "step": 151500
    },
    {
      "epoch": 6.000114566666666,
      "grad_norm": 0.14688333868980408,
      "learning_rate": 0.0004974998333222214,
      "loss": 0.4934,
      "step": 152000
    },
    {
      "epoch": 6.000131233333334,
      "grad_norm": 0.13123853504657745,
      "learning_rate": 0.0004974914994332956,
      "loss": 0.4926,
      "step": 152500
    },
    {
      "epoch": 6.0001479,
      "grad_norm": 0.08971355110406876,
      "learning_rate": 0.0004974831655443696,
      "loss": 0.4903,
      "step": 153000
    },
    {
      "epoch": 6.000164566666666,
      "grad_norm": 0.1027284562587738,
      "learning_rate": 0.0004974748316554438,
      "loss": 0.5023,
      "step": 153500
    },
    {
      "epoch": 6.0001812333333335,
      "grad_norm": 0.14640603959560394,
      "learning_rate": 0.0004974664977665178,
      "loss": 0.4987,
      "step": 154000
    },
    {
      "epoch": 6.0001979,
      "grad_norm": 0.1368297040462494,
      "learning_rate": 0.0004974581638775918,
      "loss": 0.511,
      "step": 154500
    },
    {
      "epoch": 6.000214566666667,
      "grad_norm": 0.1117890328168869,
      "learning_rate": 0.000497449829988666,
      "loss": 0.5065,
      "step": 155000
    },
    {
      "epoch": 6.000231233333333,
      "grad_norm": 0.0995812639594078,
      "learning_rate": 0.00049744149609974,
      "loss": 0.476,
      "step": 155500
    },
    {
      "epoch": 6.0002479,
      "grad_norm": 0.12992191314697266,
      "learning_rate": 0.000497433162210814,
      "loss": 0.489,
      "step": 156000
    },
    {
      "epoch": 6.000264566666667,
      "grad_norm": 0.13317348062992096,
      "learning_rate": 0.0004974248283218881,
      "loss": 0.4919,
      "step": 156500
    },
    {
      "epoch": 6.000281233333333,
      "grad_norm": 0.12540583312511444,
      "learning_rate": 0.0004974164944329622,
      "loss": 0.4901,
      "step": 157000
    },
    {
      "epoch": 6.0002979,
      "grad_norm": 0.125107541680336,
      "learning_rate": 0.0004974081605440363,
      "loss": 0.4807,
      "step": 157500
    },
    {
      "epoch": 6.000314566666667,
      "grad_norm": 0.11676689982414246,
      "learning_rate": 0.0004973998266551104,
      "loss": 0.5005,
      "step": 158000
    },
    {
      "epoch": 6.000331233333333,
      "grad_norm": 0.14529408514499664,
      "learning_rate": 0.0004973914927661844,
      "loss": 0.4928,
      "step": 158500
    },
    {
      "epoch": 6.0003479,
      "grad_norm": 0.0883282944560051,
      "learning_rate": 0.0004973831588772585,
      "loss": 0.4872,
      "step": 159000
    },
    {
      "epoch": 6.000364566666667,
      "grad_norm": 0.10131336003541946,
      "learning_rate": 0.0004973748249883326,
      "loss": 0.4867,
      "step": 159500
    },
    {
      "epoch": 6.000381233333333,
      "grad_norm": 0.0999913364648819,
      "learning_rate": 0.0004973664910994066,
      "loss": 0.5019,
      "step": 160000
    },
    {
      "epoch": 6.0003979,
      "grad_norm": 0.12251252681016922,
      "learning_rate": 0.0004973581572104807,
      "loss": 0.5038,
      "step": 160500
    },
    {
      "epoch": 6.000414566666667,
      "grad_norm": 0.08340492099523544,
      "learning_rate": 0.0004973498233215547,
      "loss": 0.5129,
      "step": 161000
    },
    {
      "epoch": 6.000431233333333,
      "grad_norm": 0.14674320816993713,
      "learning_rate": 0.0004973414894326289,
      "loss": 0.4866,
      "step": 161500
    },
    {
      "epoch": 6.0004479,
      "grad_norm": 0.1301731914281845,
      "learning_rate": 0.0004973331555437029,
      "loss": 0.4873,
      "step": 162000
    },
    {
      "epoch": 6.0004645666666665,
      "grad_norm": 0.13997066020965576,
      "learning_rate": 0.000497324821654777,
      "loss": 0.4952,
      "step": 162500
    },
    {
      "epoch": 6.000481233333334,
      "grad_norm": 0.13096533715724945,
      "learning_rate": 0.0004973164877658511,
      "loss": 0.5128,
      "step": 163000
    },
    {
      "epoch": 6.0004979,
      "grad_norm": 0.12613935768604279,
      "learning_rate": 0.0004973081538769251,
      "loss": 0.4888,
      "step": 163500
    },
    {
      "epoch": 6.000514566666666,
      "grad_norm": 0.1232742890715599,
      "learning_rate": 0.0004972998199879993,
      "loss": 0.4957,
      "step": 164000
    },
    {
      "epoch": 6.000531233333334,
      "grad_norm": 0.13818015158176422,
      "learning_rate": 0.0004972914860990733,
      "loss": 0.498,
      "step": 164500
    },
    {
      "epoch": 6.0005479,
      "grad_norm": 0.08734957128763199,
      "learning_rate": 0.0004972831522101473,
      "loss": 0.497,
      "step": 165000
    },
    {
      "epoch": 6.000564566666666,
      "grad_norm": 0.11509250104427338,
      "learning_rate": 0.0004972748183212214,
      "loss": 0.4965,
      "step": 165500
    },
    {
      "epoch": 6.0005812333333335,
      "grad_norm": 0.12645508348941803,
      "learning_rate": 0.0004972664844322955,
      "loss": 0.4905,
      "step": 166000
    },
    {
      "epoch": 6.0005979,
      "grad_norm": 0.11931170523166656,
      "learning_rate": 0.0004972581505433696,
      "loss": 0.4914,
      "step": 166500
    },
    {
      "epoch": 6.000614566666667,
      "grad_norm": 0.10142982006072998,
      "learning_rate": 0.0004972498166544437,
      "loss": 0.485,
      "step": 167000
    },
    {
      "epoch": 6.000631233333333,
      "grad_norm": 0.1446675807237625,
      "learning_rate": 0.0004972414827655177,
      "loss": 0.4982,
      "step": 167500
    },
    {
      "epoch": 6.0006479,
      "grad_norm": 0.1196429431438446,
      "learning_rate": 0.0004972331488765917,
      "loss": 0.4999,
      "step": 168000
    },
    {
      "epoch": 6.000664566666667,
      "grad_norm": 0.11217500269412994,
      "learning_rate": 0.0004972248149876659,
      "loss": 0.4789,
      "step": 168500
    },
    {
      "epoch": 6.000681233333333,
      "grad_norm": 0.12107454985380173,
      "learning_rate": 0.0004972164810987399,
      "loss": 0.4828,
      "step": 169000
    },
    {
      "epoch": 6.0006979,
      "grad_norm": 0.1255781054496765,
      "learning_rate": 0.000497208147209814,
      "loss": 0.4836,
      "step": 169500
    },
    {
      "epoch": 6.000714566666667,
      "grad_norm": 0.14919868111610413,
      "learning_rate": 0.0004971998133208881,
      "loss": 0.4958,
      "step": 170000
    },
    {
      "epoch": 6.000731233333333,
      "grad_norm": 0.13198266923427582,
      "learning_rate": 0.0004971914794319621,
      "loss": 0.4865,
      "step": 170500
    },
    {
      "epoch": 6.0007479,
      "grad_norm": 0.14237529039382935,
      "learning_rate": 0.0004971831455430363,
      "loss": 0.506,
      "step": 171000
    },
    {
      "epoch": 6.000764566666667,
      "grad_norm": 0.10690242797136307,
      "learning_rate": 0.0004971748116541103,
      "loss": 0.4923,
      "step": 171500
    },
    {
      "epoch": 6.000781233333333,
      "grad_norm": 0.1153525710105896,
      "learning_rate": 0.0004971664777651844,
      "loss": 0.4954,
      "step": 172000
    },
    {
      "epoch": 6.0007979,
      "grad_norm": 0.10325203090906143,
      "learning_rate": 0.0004971581438762584,
      "loss": 0.501,
      "step": 172500
    },
    {
      "epoch": 6.000814566666667,
      "grad_norm": 0.16269129514694214,
      "learning_rate": 0.0004971498099873325,
      "loss": 0.4801,
      "step": 173000
    },
    {
      "epoch": 7.000005883333333,
      "grad_norm": 0.11243969202041626,
      "learning_rate": 0.0004971414760984066,
      "loss": 0.4952,
      "step": 173500
    },
    {
      "epoch": 7.00002255,
      "grad_norm": 0.09860371053218842,
      "learning_rate": 0.0004971331422094806,
      "loss": 0.4803,
      "step": 174000
    },
    {
      "epoch": 7.000039216666667,
      "grad_norm": 0.15214449167251587,
      "learning_rate": 0.0004971248083205547,
      "loss": 0.4936,
      "step": 174500
    },
    {
      "epoch": 7.000055883333333,
      "grad_norm": 0.1232222318649292,
      "learning_rate": 0.0004971164744316288,
      "loss": 0.4891,
      "step": 175000
    },
    {
      "epoch": 7.00007255,
      "grad_norm": 0.11140116304159164,
      "learning_rate": 0.0004971081405427029,
      "loss": 0.4849,
      "step": 175500
    },
    {
      "epoch": 7.000089216666667,
      "grad_norm": 0.109984390437603,
      "learning_rate": 0.000497099806653777,
      "loss": 0.4993,
      "step": 176000
    },
    {
      "epoch": 7.000105883333333,
      "grad_norm": 0.10225635021924973,
      "learning_rate": 0.000497091472764851,
      "loss": 0.4916,
      "step": 176500
    },
    {
      "epoch": 7.00012255,
      "grad_norm": 0.12234975397586823,
      "learning_rate": 0.000497083138875925,
      "loss": 0.4878,
      "step": 177000
    },
    {
      "epoch": 7.000139216666667,
      "grad_norm": 0.1192040741443634,
      "learning_rate": 0.0004970748049869992,
      "loss": 0.4763,
      "step": 177500
    },
    {
      "epoch": 7.000155883333333,
      "grad_norm": 0.09212227910757065,
      "learning_rate": 0.0004970664710980732,
      "loss": 0.4856,
      "step": 178000
    },
    {
      "epoch": 7.00017255,
      "grad_norm": 0.10056095570325851,
      "learning_rate": 0.0004970581372091472,
      "loss": 0.5012,
      "step": 178500
    },
    {
      "epoch": 7.000189216666667,
      "grad_norm": 0.12367971986532211,
      "learning_rate": 0.0004970498033202214,
      "loss": 0.4899,
      "step": 179000
    },
    {
      "epoch": 7.000205883333333,
      "grad_norm": 0.10731524229049683,
      "learning_rate": 0.0004970414694312954,
      "loss": 0.49,
      "step": 179500
    },
    {
      "epoch": 7.00022255,
      "grad_norm": 0.13893331587314606,
      "learning_rate": 0.0004970331355423696,
      "loss": 0.4961,
      "step": 180000
    },
    {
      "epoch": 7.0002392166666665,
      "grad_norm": 0.22231930494308472,
      "learning_rate": 0.0004970248016534436,
      "loss": 0.4906,
      "step": 180500
    },
    {
      "epoch": 7.000255883333334,
      "grad_norm": 0.09487113356590271,
      "learning_rate": 0.0004970164677645176,
      "loss": 0.4928,
      "step": 181000
    },
    {
      "epoch": 7.00027255,
      "grad_norm": 0.12484840303659439,
      "learning_rate": 0.0004970081338755918,
      "loss": 0.494,
      "step": 181500
    },
    {
      "epoch": 7.000289216666666,
      "grad_norm": 0.14670415222644806,
      "learning_rate": 0.0004969997999866658,
      "loss": 0.4994,
      "step": 182000
    },
    {
      "epoch": 7.000305883333334,
      "grad_norm": 0.13610903918743134,
      "learning_rate": 0.0004969914660977398,
      "loss": 0.5021,
      "step": 182500
    },
    {
      "epoch": 7.00032255,
      "grad_norm": 0.12179548293352127,
      "learning_rate": 0.0004969831322088139,
      "loss": 0.4942,
      "step": 183000
    },
    {
      "epoch": 7.000339216666666,
      "grad_norm": 0.15519694983959198,
      "learning_rate": 0.000496974798319888,
      "loss": 0.495,
      "step": 183500
    },
    {
      "epoch": 7.0003558833333335,
      "grad_norm": 0.12017299979925156,
      "learning_rate": 0.0004969664644309621,
      "loss": 0.4922,
      "step": 184000
    },
    {
      "epoch": 7.00037255,
      "grad_norm": 0.1302720010280609,
      "learning_rate": 0.0004969581305420362,
      "loss": 0.4887,
      "step": 184500
    },
    {
      "epoch": 7.000389216666667,
      "grad_norm": 0.1101987361907959,
      "learning_rate": 0.0004969497966531102,
      "loss": 0.4906,
      "step": 185000
    },
    {
      "epoch": 7.000405883333333,
      "grad_norm": 0.11997566372156143,
      "learning_rate": 0.0004969414627641843,
      "loss": 0.4839,
      "step": 185500
    },
    {
      "epoch": 7.00042255,
      "grad_norm": 0.12741775810718536,
      "learning_rate": 0.0004969331288752584,
      "loss": 0.4794,
      "step": 186000
    },
    {
      "epoch": 7.000439216666667,
      "grad_norm": 0.15523473918437958,
      "learning_rate": 0.0004969247949863324,
      "loss": 0.503,
      "step": 186500
    },
    {
      "epoch": 7.000455883333333,
      "grad_norm": 0.1732999086380005,
      "learning_rate": 0.0004969164610974065,
      "loss": 0.4923,
      "step": 187000
    },
    {
      "epoch": 7.00047255,
      "grad_norm": 0.1274457424879074,
      "learning_rate": 0.0004969081272084805,
      "loss": 0.4993,
      "step": 187500
    },
    {
      "epoch": 7.000489216666667,
      "grad_norm": 0.09319540858268738,
      "learning_rate": 0.0004968997933195547,
      "loss": 0.4901,
      "step": 188000
    },
    {
      "epoch": 7.000505883333333,
      "grad_norm": 0.13236916065216064,
      "learning_rate": 0.0004968914594306287,
      "loss": 0.5013,
      "step": 188500
    },
    {
      "epoch": 7.00052255,
      "grad_norm": 0.12705928087234497,
      "learning_rate": 0.0004968831255417027,
      "loss": 0.4867,
      "step": 189000
    },
    {
      "epoch": 7.000539216666667,
      "grad_norm": 0.17629685997962952,
      "learning_rate": 0.0004968747916527769,
      "loss": 0.4932,
      "step": 189500
    },
    {
      "epoch": 7.000555883333333,
      "grad_norm": 0.14725695550441742,
      "learning_rate": 0.0004968664577638509,
      "loss": 0.4716,
      "step": 190000
    },
    {
      "epoch": 7.00057255,
      "grad_norm": 0.12515783309936523,
      "learning_rate": 0.000496858123874925,
      "loss": 0.5004,
      "step": 190500
    },
    {
      "epoch": 7.000589216666667,
      "grad_norm": 0.13207456469535828,
      "learning_rate": 0.0004968497899859991,
      "loss": 0.4851,
      "step": 191000
    },
    {
      "epoch": 7.000605883333333,
      "grad_norm": 0.14540719985961914,
      "learning_rate": 0.0004968414560970731,
      "loss": 0.5007,
      "step": 191500
    },
    {
      "epoch": 7.00062255,
      "grad_norm": 0.12877798080444336,
      "learning_rate": 0.0004968331222081472,
      "loss": 0.4932,
      "step": 192000
    },
    {
      "epoch": 7.000639216666666,
      "grad_norm": 0.2152692675590515,
      "learning_rate": 0.0004968247883192213,
      "loss": 0.4923,
      "step": 192500
    },
    {
      "epoch": 7.000655883333334,
      "grad_norm": 0.11141239106655121,
      "learning_rate": 0.0004968164544302953,
      "loss": 0.4955,
      "step": 193000
    },
    {
      "epoch": 7.00067255,
      "grad_norm": 0.0899854525923729,
      "learning_rate": 0.0004968081205413694,
      "loss": 0.5049,
      "step": 193500
    },
    {
      "epoch": 7.000689216666666,
      "grad_norm": 0.1135578379034996,
      "learning_rate": 0.0004967997866524435,
      "loss": 0.4798,
      "step": 194000
    },
    {
      "epoch": 7.0007058833333335,
      "grad_norm": 0.12831243872642517,
      "learning_rate": 0.0004967914527635175,
      "loss": 0.4945,
      "step": 194500
    },
    {
      "epoch": 7.00072255,
      "grad_norm": 0.13131988048553467,
      "learning_rate": 0.0004967831188745917,
      "loss": 0.4876,
      "step": 195000
    },
    {
      "epoch": 7.000739216666667,
      "grad_norm": 0.10218589752912521,
      "learning_rate": 0.0004967747849856657,
      "loss": 0.4866,
      "step": 195500
    },
    {
      "epoch": 7.000755883333333,
      "grad_norm": 0.10987427830696106,
      "learning_rate": 0.0004967664510967398,
      "loss": 0.4953,
      "step": 196000
    },
    {
      "epoch": 7.00077255,
      "grad_norm": 0.11019060760736465,
      "learning_rate": 0.0004967581172078139,
      "loss": 0.4953,
      "step": 196500
    },
    {
      "epoch": 7.000789216666667,
      "grad_norm": 0.13531164824962616,
      "learning_rate": 0.0004967497833188879,
      "loss": 0.4897,
      "step": 197000
    },
    {
      "epoch": 7.000805883333333,
      "grad_norm": 0.13519884645938873,
      "learning_rate": 0.000496741449429962,
      "loss": 0.4711,
      "step": 197500
    },
    {
      "epoch": 7.00082255,
      "grad_norm": 0.11731791496276855,
      "learning_rate": 0.000496733115541036,
      "loss": 0.4946,
      "step": 198000
    },
    {
      "epoch": 8.000013866666666,
      "grad_norm": 0.11000527441501617,
      "learning_rate": 0.0004967247816521102,
      "loss": 0.4823,
      "step": 198500
    },
    {
      "epoch": 8.000030533333334,
      "grad_norm": 0.12886132299900055,
      "learning_rate": 0.0004967164477631842,
      "loss": 0.479,
      "step": 199000
    },
    {
      "epoch": 8.0000472,
      "grad_norm": 0.10760315507650375,
      "learning_rate": 0.0004967081138742583,
      "loss": 0.4724,
      "step": 199500
    },
    {
      "epoch": 8.000063866666666,
      "grad_norm": 0.10595723986625671,
      "learning_rate": 0.0004966997799853324,
      "loss": 0.4824,
      "step": 200000
    },
    {
      "epoch": 8.000080533333334,
      "grad_norm": 0.10434435307979584,
      "learning_rate": 0.0004966914460964064,
      "loss": 0.477,
      "step": 200500
    },
    {
      "epoch": 8.0000972,
      "grad_norm": 0.14977291226387024,
      "learning_rate": 0.0004966831122074805,
      "loss": 0.4755,
      "step": 201000
    },
    {
      "epoch": 8.000113866666666,
      "grad_norm": 0.1096048504114151,
      "learning_rate": 0.0004966747783185546,
      "loss": 0.4883,
      "step": 201500
    },
    {
      "epoch": 8.000130533333333,
      "grad_norm": 0.1563759744167328,
      "learning_rate": 0.0004966664444296286,
      "loss": 0.5081,
      "step": 202000
    },
    {
      "epoch": 8.0001472,
      "grad_norm": 0.16574139893054962,
      "learning_rate": 0.0004966581105407027,
      "loss": 0.5053,
      "step": 202500
    },
    {
      "epoch": 8.000163866666666,
      "grad_norm": 0.16659586131572723,
      "learning_rate": 0.0004966497766517768,
      "loss": 0.5017,
      "step": 203000
    },
    {
      "epoch": 8.000180533333333,
      "grad_norm": 0.10405364632606506,
      "learning_rate": 0.0004966414427628509,
      "loss": 0.4841,
      "step": 203500
    },
    {
      "epoch": 8.0001972,
      "grad_norm": 0.11449775099754333,
      "learning_rate": 0.000496633108873925,
      "loss": 0.4894,
      "step": 204000
    },
    {
      "epoch": 8.000213866666666,
      "grad_norm": 0.10592445731163025,
      "learning_rate": 0.000496624774984999,
      "loss": 0.4831,
      "step": 204500
    },
    {
      "epoch": 8.000230533333333,
      "grad_norm": 0.12454038113355637,
      "learning_rate": 0.000496616441096073,
      "loss": 0.498,
      "step": 205000
    },
    {
      "epoch": 8.0002472,
      "grad_norm": 0.12378893792629242,
      "learning_rate": 0.0004966081072071472,
      "loss": 0.4832,
      "step": 205500
    },
    {
      "epoch": 8.000263866666666,
      "grad_norm": 0.14683648943901062,
      "learning_rate": 0.0004965997733182212,
      "loss": 0.484,
      "step": 206000
    },
    {
      "epoch": 8.000280533333333,
      "grad_norm": 0.10909034311771393,
      "learning_rate": 0.0004965914394292953,
      "loss": 0.4973,
      "step": 206500
    },
    {
      "epoch": 8.0002972,
      "grad_norm": 0.1560579091310501,
      "learning_rate": 0.0004965831055403694,
      "loss": 0.4905,
      "step": 207000
    },
    {
      "epoch": 8.000313866666668,
      "grad_norm": 0.1573159098625183,
      "learning_rate": 0.0004965747716514434,
      "loss": 0.4828,
      "step": 207500
    },
    {
      "epoch": 8.000330533333333,
      "grad_norm": 0.14233441650867462,
      "learning_rate": 0.0004965664377625176,
      "loss": 0.4815,
      "step": 208000
    },
    {
      "epoch": 8.0003472,
      "grad_norm": 0.10555263608694077,
      "learning_rate": 0.0004965581038735916,
      "loss": 0.4877,
      "step": 208500
    },
    {
      "epoch": 8.000363866666667,
      "grad_norm": 0.1118994876742363,
      "learning_rate": 0.0004965497699846656,
      "loss": 0.4939,
      "step": 209000
    },
    {
      "epoch": 8.000380533333333,
      "grad_norm": 0.14046582579612732,
      "learning_rate": 0.0004965414360957397,
      "loss": 0.4889,
      "step": 209500
    },
    {
      "epoch": 8.0003972,
      "grad_norm": 0.13197998702526093,
      "learning_rate": 0.0004965331022068138,
      "loss": 0.4756,
      "step": 210000
    },
    {
      "epoch": 8.000413866666667,
      "grad_norm": 0.12490449100732803,
      "learning_rate": 0.0004965247683178879,
      "loss": 0.4902,
      "step": 210500
    },
    {
      "epoch": 8.000430533333333,
      "grad_norm": 0.16887561976909637,
      "learning_rate": 0.0004965164344289619,
      "loss": 0.5034,
      "step": 211000
    },
    {
      "epoch": 8.0004472,
      "grad_norm": 0.09103460609912872,
      "learning_rate": 0.000496508100540036,
      "loss": 0.4695,
      "step": 211500
    },
    {
      "epoch": 8.000463866666667,
      "grad_norm": 0.11141331493854523,
      "learning_rate": 0.0004964997666511101,
      "loss": 0.488,
      "step": 212000
    },
    {
      "epoch": 8.000480533333333,
      "grad_norm": 0.15117745101451874,
      "learning_rate": 0.0004964914327621842,
      "loss": 0.4949,
      "step": 212500
    },
    {
      "epoch": 8.0004972,
      "grad_norm": 0.13818739354610443,
      "learning_rate": 0.0004964830988732583,
      "loss": 0.4999,
      "step": 213000
    },
    {
      "epoch": 8.000513866666667,
      "grad_norm": 0.12299641966819763,
      "learning_rate": 0.0004964747649843323,
      "loss": 0.4746,
      "step": 213500
    },
    {
      "epoch": 8.000530533333333,
      "grad_norm": 0.12770229578018188,
      "learning_rate": 0.0004964664310954063,
      "loss": 0.4949,
      "step": 214000
    },
    {
      "epoch": 8.0005472,
      "grad_norm": 0.12835004925727844,
      "learning_rate": 0.0004964580972064805,
      "loss": 0.4866,
      "step": 214500
    },
    {
      "epoch": 8.000563866666667,
      "grad_norm": 0.09600386768579483,
      "learning_rate": 0.0004964497633175545,
      "loss": 0.489,
      "step": 215000
    },
    {
      "epoch": 8.000580533333334,
      "grad_norm": 0.14421667158603668,
      "learning_rate": 0.0004964414294286285,
      "loss": 0.4965,
      "step": 215500
    },
    {
      "epoch": 8.0005972,
      "grad_norm": 0.10348251461982727,
      "learning_rate": 0.0004964330955397027,
      "loss": 0.4995,
      "step": 216000
    },
    {
      "epoch": 8.000613866666667,
      "grad_norm": 0.10345834493637085,
      "learning_rate": 0.0004964247616507767,
      "loss": 0.4843,
      "step": 216500
    },
    {
      "epoch": 8.000630533333334,
      "grad_norm": 0.11169987171888351,
      "learning_rate": 0.0004964164277618509,
      "loss": 0.4928,
      "step": 217000
    },
    {
      "epoch": 8.0006472,
      "grad_norm": 0.12204772979021072,
      "learning_rate": 0.0004964080938729249,
      "loss": 0.4988,
      "step": 217500
    },
    {
      "epoch": 8.000663866666667,
      "grad_norm": 0.13760477304458618,
      "learning_rate": 0.0004963997599839989,
      "loss": 0.5039,
      "step": 218000
    },
    {
      "epoch": 8.000680533333334,
      "grad_norm": 0.13064995408058167,
      "learning_rate": 0.0004963914260950731,
      "loss": 0.4905,
      "step": 218500
    },
    {
      "epoch": 8.0006972,
      "grad_norm": 0.12142452597618103,
      "learning_rate": 0.0004963830922061471,
      "loss": 0.4906,
      "step": 219000
    },
    {
      "epoch": 8.000713866666667,
      "grad_norm": 0.1255778968334198,
      "learning_rate": 0.0004963747583172211,
      "loss": 0.495,
      "step": 219500
    },
    {
      "epoch": 8.000730533333334,
      "grad_norm": 0.1056341677904129,
      "learning_rate": 0.0004963664244282952,
      "loss": 0.4917,
      "step": 220000
    },
    {
      "epoch": 8.0007472,
      "grad_norm": 0.11729542911052704,
      "learning_rate": 0.0004963580905393693,
      "loss": 0.4888,
      "step": 220500
    },
    {
      "epoch": 8.000763866666667,
      "grad_norm": 0.15883544087409973,
      "learning_rate": 0.0004963497566504434,
      "loss": 0.4906,
      "step": 221000
    },
    {
      "epoch": 8.000780533333334,
      "grad_norm": 0.09842231124639511,
      "learning_rate": 0.0004963414227615175,
      "loss": 0.4942,
      "step": 221500
    },
    {
      "epoch": 8.0007972,
      "grad_norm": 0.10798793286085129,
      "learning_rate": 0.0004963330888725915,
      "loss": 0.481,
      "step": 222000
    },
    {
      "epoch": 8.000813866666666,
      "grad_norm": 0.13659314811229706,
      "learning_rate": 0.0004963247549836656,
      "loss": 0.4881,
      "step": 222500
    },
    {
      "epoch": 9.000005183333334,
      "grad_norm": 0.1506999135017395,
      "learning_rate": 0.0004963164210947397,
      "loss": 0.49,
      "step": 223000
    },
    {
      "epoch": 9.00002185,
      "grad_norm": 0.11729342490434647,
      "learning_rate": 0.0004963080872058137,
      "loss": 0.49,
      "step": 223500
    },
    {
      "epoch": 9.000038516666667,
      "grad_norm": 0.16315555572509766,
      "learning_rate": 0.0004962997533168878,
      "loss": 0.4938,
      "step": 224000
    },
    {
      "epoch": 9.000055183333334,
      "grad_norm": 0.0958498939871788,
      "learning_rate": 0.0004962914194279618,
      "loss": 0.488,
      "step": 224500
    },
    {
      "epoch": 9.00007185,
      "grad_norm": 0.10677436739206314,
      "learning_rate": 0.000496283085539036,
      "loss": 0.48,
      "step": 225000
    },
    {
      "epoch": 9.000088516666667,
      "grad_norm": 0.10178399085998535,
      "learning_rate": 0.00049627475165011,
      "loss": 0.4816,
      "step": 225500
    },
    {
      "epoch": 9.000105183333334,
      "grad_norm": 0.12345056980848312,
      "learning_rate": 0.0004962664177611841,
      "loss": 0.48,
      "step": 226000
    },
    {
      "epoch": 9.00012185,
      "grad_norm": 0.1388818323612213,
      "learning_rate": 0.0004962580838722582,
      "loss": 0.4828,
      "step": 226500
    },
    {
      "epoch": 9.000138516666667,
      "grad_norm": 0.10970653593540192,
      "learning_rate": 0.0004962497499833322,
      "loss": 0.4785,
      "step": 227000
    },
    {
      "epoch": 9.000155183333334,
      "grad_norm": 0.11171630769968033,
      "learning_rate": 0.0004962414160944063,
      "loss": 0.4838,
      "step": 227500
    },
    {
      "epoch": 9.00017185,
      "grad_norm": 0.12090753018856049,
      "learning_rate": 0.0004962330822054804,
      "loss": 0.4928,
      "step": 228000
    },
    {
      "epoch": 9.000188516666666,
      "grad_norm": 0.11527956277132034,
      "learning_rate": 0.0004962247483165544,
      "loss": 0.4908,
      "step": 228500
    },
    {
      "epoch": 9.000205183333334,
      "grad_norm": 0.11642647534608841,
      "learning_rate": 0.0004962164144276285,
      "loss": 0.4978,
      "step": 229000
    },
    {
      "epoch": 9.00022185,
      "grad_norm": 0.11424482613801956,
      "learning_rate": 0.0004962080805387026,
      "loss": 0.477,
      "step": 229500
    },
    {
      "epoch": 9.000238516666666,
      "grad_norm": 0.1410171240568161,
      "learning_rate": 0.0004961997466497766,
      "loss": 0.4965,
      "step": 230000
    },
    {
      "epoch": 9.000255183333334,
      "grad_norm": 0.09853269159793854,
      "learning_rate": 0.0004961914127608508,
      "loss": 0.4813,
      "step": 230500
    },
    {
      "epoch": 9.00027185,
      "grad_norm": 0.12964828312397003,
      "learning_rate": 0.0004961830788719248,
      "loss": 0.4802,
      "step": 231000
    },
    {
      "epoch": 9.000288516666666,
      "grad_norm": 0.12092413008213043,
      "learning_rate": 0.0004961747449829988,
      "loss": 0.4793,
      "step": 231500
    },
    {
      "epoch": 9.000305183333333,
      "grad_norm": 0.1257631927728653,
      "learning_rate": 0.000496166411094073,
      "loss": 0.4838,
      "step": 232000
    },
    {
      "epoch": 9.00032185,
      "grad_norm": 0.10822311788797379,
      "learning_rate": 0.000496158077205147,
      "loss": 0.4934,
      "step": 232500
    },
    {
      "epoch": 9.000338516666666,
      "grad_norm": 0.1299700289964676,
      "learning_rate": 0.0004961497433162211,
      "loss": 0.5082,
      "step": 233000
    },
    {
      "epoch": 9.000355183333333,
      "grad_norm": 0.1543768048286438,
      "learning_rate": 0.0004961414094272952,
      "loss": 0.4819,
      "step": 233500
    },
    {
      "epoch": 9.00037185,
      "grad_norm": 0.1481214463710785,
      "learning_rate": 0.0004961330755383692,
      "loss": 0.4842,
      "step": 234000
    },
    {
      "epoch": 9.000388516666666,
      "grad_norm": 0.11860131472349167,
      "learning_rate": 0.0004961247416494433,
      "loss": 0.4778,
      "step": 234500
    },
    {
      "epoch": 9.000405183333333,
      "grad_norm": 0.12704402208328247,
      "learning_rate": 0.0004961164077605174,
      "loss": 0.4769,
      "step": 235000
    },
    {
      "epoch": 9.00042185,
      "grad_norm": 0.08800746500492096,
      "learning_rate": 0.0004961080738715914,
      "loss": 0.4832,
      "step": 235500
    },
    {
      "epoch": 9.000438516666666,
      "grad_norm": 0.13688862323760986,
      "learning_rate": 0.0004960997399826655,
      "loss": 0.4854,
      "step": 236000
    },
    {
      "epoch": 9.000455183333333,
      "grad_norm": 0.08492344617843628,
      "learning_rate": 0.0004960914060937396,
      "loss": 0.4867,
      "step": 236500
    },
    {
      "epoch": 9.00047185,
      "grad_norm": 0.1117526963353157,
      "learning_rate": 0.0004960830722048137,
      "loss": 0.4799,
      "step": 237000
    },
    {
      "epoch": 9.000488516666667,
      "grad_norm": 0.1271693855524063,
      "learning_rate": 0.0004960747383158877,
      "loss": 0.4869,
      "step": 237500
    },
    {
      "epoch": 9.000505183333333,
      "grad_norm": 0.1429857760667801,
      "learning_rate": 0.0004960664044269618,
      "loss": 0.4855,
      "step": 238000
    },
    {
      "epoch": 9.00052185,
      "grad_norm": 0.11292526125907898,
      "learning_rate": 0.0004960580705380359,
      "loss": 0.483,
      "step": 238500
    },
    {
      "epoch": 9.000538516666667,
      "grad_norm": 0.10973038524389267,
      "learning_rate": 0.0004960497366491099,
      "loss": 0.4742,
      "step": 239000
    },
    {
      "epoch": 9.000555183333333,
      "grad_norm": 0.1315021812915802,
      "learning_rate": 0.000496041402760184,
      "loss": 0.49,
      "step": 239500
    },
    {
      "epoch": 9.00057185,
      "grad_norm": 0.10392142832279205,
      "learning_rate": 0.0004960330688712581,
      "loss": 0.4919,
      "step": 240000
    },
    {
      "epoch": 9.000588516666667,
      "grad_norm": 0.11976916342973709,
      "learning_rate": 0.0004960247349823322,
      "loss": 0.4797,
      "step": 240500
    },
    {
      "epoch": 9.000605183333333,
      "grad_norm": 0.12616346776485443,
      "learning_rate": 0.0004960164010934063,
      "loss": 0.5065,
      "step": 241000
    },
    {
      "epoch": 9.00062185,
      "grad_norm": 0.1510964184999466,
      "learning_rate": 0.0004960080672044803,
      "loss": 0.49,
      "step": 241500
    },
    {
      "epoch": 9.000638516666667,
      "grad_norm": 0.09251285344362259,
      "learning_rate": 0.0004959997333155543,
      "loss": 0.4945,
      "step": 242000
    },
    {
      "epoch": 9.000655183333333,
      "grad_norm": 0.2196112871170044,
      "learning_rate": 0.0004959913994266285,
      "loss": 0.4979,
      "step": 242500
    },
    {
      "epoch": 9.00067185,
      "grad_norm": 0.09953481703996658,
      "learning_rate": 0.0004959830655377025,
      "loss": 0.4962,
      "step": 243000
    },
    {
      "epoch": 9.000688516666667,
      "grad_norm": 0.1249212920665741,
      "learning_rate": 0.0004959747316487765,
      "loss": 0.4944,
      "step": 243500
    },
    {
      "epoch": 9.000705183333332,
      "grad_norm": 0.12400777637958527,
      "learning_rate": 0.0004959663977598507,
      "loss": 0.4911,
      "step": 244000
    },
    {
      "epoch": 9.00072185,
      "grad_norm": 0.1186387836933136,
      "learning_rate": 0.0004959580638709247,
      "loss": 0.4878,
      "step": 244500
    },
    {
      "epoch": 9.000738516666667,
      "grad_norm": 0.08852367848157883,
      "learning_rate": 0.0004959497299819989,
      "loss": 0.4987,
      "step": 245000
    }
  ],
  "logging_steps": 500,
  "max_steps": 30000000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.123841544023014e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
