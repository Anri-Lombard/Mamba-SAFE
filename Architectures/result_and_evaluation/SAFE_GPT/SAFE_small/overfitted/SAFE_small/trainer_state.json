{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.99939008274544,
  "eval_steps": 500.0,
  "global_step": 122960,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0406611503039421,
      "grad_norm": 0.9126878976821899,
      "learning_rate": 0.0004979668184775537,
      "loss": 1.2372,
      "step": 500
    },
    {
      "epoch": 0.0813223006078842,
      "grad_norm": 1.125605583190918,
      "learning_rate": 0.0004959336369551073,
      "loss": 0.7786,
      "step": 1000
    },
    {
      "epoch": 0.1219834509118263,
      "grad_norm": 0.669577419757843,
      "learning_rate": 0.000493900455432661,
      "loss": 0.6818,
      "step": 1500
    },
    {
      "epoch": 0.1626446012157684,
      "grad_norm": 0.40963035821914673,
      "learning_rate": 0.0004918672739102147,
      "loss": 0.6347,
      "step": 2000
    },
    {
      "epoch": 0.2033057515197105,
      "grad_norm": 0.420190691947937,
      "learning_rate": 0.0004898340923877684,
      "loss": 0.6049,
      "step": 2500
    },
    {
      "epoch": 0.2439669018236526,
      "grad_norm": 0.387763112783432,
      "learning_rate": 0.00048780091086532206,
      "loss": 0.5867,
      "step": 3000
    },
    {
      "epoch": 0.2846280521275947,
      "grad_norm": 0.31300053000450134,
      "learning_rate": 0.0004857677293428757,
      "loss": 0.5717,
      "step": 3500
    },
    {
      "epoch": 0.3252892024315368,
      "grad_norm": 0.37694600224494934,
      "learning_rate": 0.0004837345478204294,
      "loss": 0.56,
      "step": 4000
    },
    {
      "epoch": 0.3659503527354789,
      "grad_norm": 0.2835025489330292,
      "learning_rate": 0.0004817013662979831,
      "loss": 0.5512,
      "step": 4500
    },
    {
      "epoch": 0.406611503039421,
      "grad_norm": 0.3534022569656372,
      "learning_rate": 0.00047966818477553677,
      "loss": 0.5419,
      "step": 5000
    },
    {
      "epoch": 0.4472726533433631,
      "grad_norm": 0.3030935227870941,
      "learning_rate": 0.00047763500325309047,
      "loss": 0.5377,
      "step": 5500
    },
    {
      "epoch": 0.4879338036473052,
      "grad_norm": 0.26907554268836975,
      "learning_rate": 0.00047560182173064417,
      "loss": 0.5304,
      "step": 6000
    },
    {
      "epoch": 0.5285949539512472,
      "grad_norm": 0.2724861800670624,
      "learning_rate": 0.00047356864020819776,
      "loss": 0.5258,
      "step": 6500
    },
    {
      "epoch": 0.5692561042551894,
      "grad_norm": 0.2619284391403198,
      "learning_rate": 0.00047153545868575147,
      "loss": 0.521,
      "step": 7000
    },
    {
      "epoch": 0.6099172545591315,
      "grad_norm": 0.24096304178237915,
      "learning_rate": 0.0004695022771633051,
      "loss": 0.5185,
      "step": 7500
    },
    {
      "epoch": 0.6505784048630736,
      "grad_norm": 0.29177626967430115,
      "learning_rate": 0.0004674690956408588,
      "loss": 0.5145,
      "step": 8000
    },
    {
      "epoch": 0.6912395551670156,
      "grad_norm": 0.22095970809459686,
      "learning_rate": 0.0004654359141184125,
      "loss": 0.511,
      "step": 8500
    },
    {
      "epoch": 0.7319007054709578,
      "grad_norm": 0.24671639502048492,
      "learning_rate": 0.00046340273259596617,
      "loss": 0.5078,
      "step": 9000
    },
    {
      "epoch": 0.7725618557748999,
      "grad_norm": 0.25528988242149353,
      "learning_rate": 0.00046136955107351987,
      "loss": 0.5056,
      "step": 9500
    },
    {
      "epoch": 0.813223006078842,
      "grad_norm": 0.23724937438964844,
      "learning_rate": 0.0004593363695510736,
      "loss": 0.5023,
      "step": 10000
    },
    {
      "epoch": 0.853884156382784,
      "grad_norm": 0.20000125467777252,
      "learning_rate": 0.00045730318802862717,
      "loss": 0.5001,
      "step": 10500
    },
    {
      "epoch": 0.8945453066867262,
      "grad_norm": 0.21473318338394165,
      "learning_rate": 0.00045527000650618087,
      "loss": 0.4987,
      "step": 11000
    },
    {
      "epoch": 0.9352064569906683,
      "grad_norm": 0.20878182351589203,
      "learning_rate": 0.0004532368249837345,
      "loss": 0.4955,
      "step": 11500
    },
    {
      "epoch": 0.9758676072946104,
      "grad_norm": 0.24409621953964233,
      "learning_rate": 0.0004512036434612882,
      "loss": 0.4929,
      "step": 12000
    },
    {
      "epoch": 1.0165287575985524,
      "grad_norm": 0.23116344213485718,
      "learning_rate": 0.0004491704619388419,
      "loss": 0.4904,
      "step": 12500
    },
    {
      "epoch": 1.0571899079024947,
      "grad_norm": 0.20665714144706726,
      "learning_rate": 0.0004471372804163956,
      "loss": 0.4885,
      "step": 13000
    },
    {
      "epoch": 1.0978510582064367,
      "grad_norm": 0.20569254457950592,
      "learning_rate": 0.0004451040988939493,
      "loss": 0.4863,
      "step": 13500
    },
    {
      "epoch": 1.1385122085103787,
      "grad_norm": 0.19000732898712158,
      "learning_rate": 0.000443070917371503,
      "loss": 0.4847,
      "step": 14000
    },
    {
      "epoch": 1.179173358814321,
      "grad_norm": 0.21367965638637543,
      "learning_rate": 0.0004410377358490566,
      "loss": 0.4823,
      "step": 14500
    },
    {
      "epoch": 1.219834509118263,
      "grad_norm": 0.18642973899841309,
      "learning_rate": 0.0004390045543266103,
      "loss": 0.4825,
      "step": 15000
    },
    {
      "epoch": 1.260495659422205,
      "grad_norm": 0.1693432778120041,
      "learning_rate": 0.000436971372804164,
      "loss": 0.4811,
      "step": 15500
    },
    {
      "epoch": 1.3011568097261472,
      "grad_norm": 0.20802824199199677,
      "learning_rate": 0.0004349381912817176,
      "loss": 0.4799,
      "step": 16000
    },
    {
      "epoch": 1.3418179600300892,
      "grad_norm": 0.17543518543243408,
      "learning_rate": 0.00043290500975927133,
      "loss": 0.4777,
      "step": 16500
    },
    {
      "epoch": 1.3824791103340313,
      "grad_norm": 0.20687276124954224,
      "learning_rate": 0.000430871828236825,
      "loss": 0.4763,
      "step": 17000
    },
    {
      "epoch": 1.4231402606379735,
      "grad_norm": 0.22278213500976562,
      "learning_rate": 0.0004288386467143787,
      "loss": 0.4759,
      "step": 17500
    },
    {
      "epoch": 1.4638014109419155,
      "grad_norm": 0.19406689703464508,
      "learning_rate": 0.0004268054651919324,
      "loss": 0.4742,
      "step": 18000
    },
    {
      "epoch": 1.5044625612458575,
      "grad_norm": 0.2005782425403595,
      "learning_rate": 0.000424772283669486,
      "loss": 0.4737,
      "step": 18500
    },
    {
      "epoch": 1.5451237115497998,
      "grad_norm": 0.15505832433700562,
      "learning_rate": 0.0004227391021470397,
      "loss": 0.4722,
      "step": 19000
    },
    {
      "epoch": 1.5857848618537418,
      "grad_norm": 0.18010932207107544,
      "learning_rate": 0.0004207059206245934,
      "loss": 0.4719,
      "step": 19500
    },
    {
      "epoch": 1.6264460121576838,
      "grad_norm": 0.18862284719944,
      "learning_rate": 0.00041867273910214703,
      "loss": 0.471,
      "step": 20000
    },
    {
      "epoch": 1.667107162461626,
      "grad_norm": 0.17331869900226593,
      "learning_rate": 0.00041663955757970073,
      "loss": 0.4697,
      "step": 20500
    },
    {
      "epoch": 1.7077683127655683,
      "grad_norm": 0.19420155882835388,
      "learning_rate": 0.0004146063760572544,
      "loss": 0.47,
      "step": 21000
    },
    {
      "epoch": 1.74842946306951,
      "grad_norm": 0.17144861817359924,
      "learning_rate": 0.0004125731945348081,
      "loss": 0.468,
      "step": 21500
    },
    {
      "epoch": 1.7890906133734523,
      "grad_norm": 0.2515861690044403,
      "learning_rate": 0.0004105400130123618,
      "loss": 0.4672,
      "step": 22000
    },
    {
      "epoch": 1.8297517636773946,
      "grad_norm": 0.19447265565395355,
      "learning_rate": 0.00040850683148991544,
      "loss": 0.466,
      "step": 22500
    },
    {
      "epoch": 1.8704129139813366,
      "grad_norm": 0.15504635870456696,
      "learning_rate": 0.0004064736499674691,
      "loss": 0.4648,
      "step": 23000
    },
    {
      "epoch": 1.9110740642852786,
      "grad_norm": 0.18758589029312134,
      "learning_rate": 0.0004044404684450228,
      "loss": 0.4647,
      "step": 23500
    },
    {
      "epoch": 1.9517352145892208,
      "grad_norm": 0.15694241225719452,
      "learning_rate": 0.00040240728692257643,
      "loss": 0.4639,
      "step": 24000
    },
    {
      "epoch": 1.9923963648931629,
      "grad_norm": 0.16800472140312195,
      "learning_rate": 0.00040037410540013014,
      "loss": 0.4627,
      "step": 24500
    },
    {
      "epoch": 2.033057515197105,
      "grad_norm": 0.17835763096809387,
      "learning_rate": 0.0003983409238776838,
      "loss": 0.4592,
      "step": 25000
    },
    {
      "epoch": 2.073718665501047,
      "grad_norm": 0.18502116203308105,
      "learning_rate": 0.0003963077423552375,
      "loss": 0.4586,
      "step": 25500
    },
    {
      "epoch": 2.1143798158049893,
      "grad_norm": 0.16063180565834045,
      "learning_rate": 0.0003942745608327912,
      "loss": 0.4582,
      "step": 26000
    },
    {
      "epoch": 2.155040966108931,
      "grad_norm": 0.16945964097976685,
      "learning_rate": 0.00039224137931034484,
      "loss": 0.4576,
      "step": 26500
    },
    {
      "epoch": 2.1957021164128734,
      "grad_norm": 0.17870818078517914,
      "learning_rate": 0.0003902081977878985,
      "loss": 0.4568,
      "step": 27000
    },
    {
      "epoch": 2.2363632667168156,
      "grad_norm": 0.15557564795017242,
      "learning_rate": 0.0003881750162654522,
      "loss": 0.4566,
      "step": 27500
    },
    {
      "epoch": 2.2770244170207574,
      "grad_norm": 0.2083691656589508,
      "learning_rate": 0.00038614183474300584,
      "loss": 0.4554,
      "step": 28000
    },
    {
      "epoch": 2.3176855673246997,
      "grad_norm": 0.17280414700508118,
      "learning_rate": 0.00038410865322055954,
      "loss": 0.455,
      "step": 28500
    },
    {
      "epoch": 2.358346717628642,
      "grad_norm": 0.17014074325561523,
      "learning_rate": 0.00038207547169811324,
      "loss": 0.4553,
      "step": 29000
    },
    {
      "epoch": 2.3990078679325837,
      "grad_norm": 0.18984355032444,
      "learning_rate": 0.0003800422901756669,
      "loss": 0.4548,
      "step": 29500
    },
    {
      "epoch": 2.439669018236526,
      "grad_norm": 0.1443738341331482,
      "learning_rate": 0.0003780091086532206,
      "loss": 0.4543,
      "step": 30000
    },
    {
      "epoch": 2.480330168540468,
      "grad_norm": 0.14728009700775146,
      "learning_rate": 0.00037597592713077424,
      "loss": 0.4537,
      "step": 30500
    },
    {
      "epoch": 2.52099131884441,
      "grad_norm": 0.15199014544487,
      "learning_rate": 0.0003739427456083279,
      "loss": 0.4532,
      "step": 31000
    },
    {
      "epoch": 2.561652469148352,
      "grad_norm": 0.18151119351387024,
      "learning_rate": 0.0003719095640858816,
      "loss": 0.4526,
      "step": 31500
    },
    {
      "epoch": 2.6023136194522944,
      "grad_norm": 0.16835644841194153,
      "learning_rate": 0.00036987638256343524,
      "loss": 0.4519,
      "step": 32000
    },
    {
      "epoch": 2.6429747697562362,
      "grad_norm": 0.2460826188325882,
      "learning_rate": 0.00036784320104098895,
      "loss": 0.4516,
      "step": 32500
    },
    {
      "epoch": 2.6836359200601785,
      "grad_norm": 0.18480932712554932,
      "learning_rate": 0.00036581001951854265,
      "loss": 0.4511,
      "step": 33000
    },
    {
      "epoch": 2.7242970703641207,
      "grad_norm": 0.16550271213054657,
      "learning_rate": 0.0003637768379960963,
      "loss": 0.4507,
      "step": 33500
    },
    {
      "epoch": 2.7649582206680625,
      "grad_norm": 0.17012819647789001,
      "learning_rate": 0.00036174365647365,
      "loss": 0.4512,
      "step": 34000
    },
    {
      "epoch": 2.8056193709720048,
      "grad_norm": 0.1565174162387848,
      "learning_rate": 0.00035971047495120365,
      "loss": 0.4495,
      "step": 34500
    },
    {
      "epoch": 2.846280521275947,
      "grad_norm": 0.1472208946943283,
      "learning_rate": 0.0003576772934287573,
      "loss": 0.4494,
      "step": 35000
    },
    {
      "epoch": 2.886941671579889,
      "grad_norm": 0.14568108320236206,
      "learning_rate": 0.000355644111906311,
      "loss": 0.4489,
      "step": 35500
    },
    {
      "epoch": 2.927602821883831,
      "grad_norm": 0.15385504066944122,
      "learning_rate": 0.00035361093038386465,
      "loss": 0.4489,
      "step": 36000
    },
    {
      "epoch": 2.9682639721877733,
      "grad_norm": 0.14504793286323547,
      "learning_rate": 0.00035157774886141835,
      "loss": 0.4483,
      "step": 36500
    },
    {
      "epoch": 3.0089251224917155,
      "grad_norm": 0.16838602721691132,
      "learning_rate": 0.00034954456733897205,
      "loss": 0.4466,
      "step": 37000
    },
    {
      "epoch": 3.0495862727956573,
      "grad_norm": 0.16665443778038025,
      "learning_rate": 0.0003475113858165257,
      "loss": 0.4437,
      "step": 37500
    },
    {
      "epoch": 3.0902474230995995,
      "grad_norm": 0.17031730711460114,
      "learning_rate": 0.0003454782042940794,
      "loss": 0.4435,
      "step": 38000
    },
    {
      "epoch": 3.130908573403542,
      "grad_norm": 0.1455339938402176,
      "learning_rate": 0.00034344502277163305,
      "loss": 0.4433,
      "step": 38500
    },
    {
      "epoch": 3.1715697237074836,
      "grad_norm": 0.15864136815071106,
      "learning_rate": 0.00034141184124918675,
      "loss": 0.4429,
      "step": 39000
    },
    {
      "epoch": 3.212230874011426,
      "grad_norm": 0.16138137876987457,
      "learning_rate": 0.0003393786597267404,
      "loss": 0.4428,
      "step": 39500
    },
    {
      "epoch": 3.252892024315368,
      "grad_norm": 0.18813593685626984,
      "learning_rate": 0.00033734547820429405,
      "loss": 0.4428,
      "step": 40000
    },
    {
      "epoch": 3.29355317461931,
      "grad_norm": 0.14282578229904175,
      "learning_rate": 0.00033531229668184775,
      "loss": 0.4426,
      "step": 40500
    },
    {
      "epoch": 3.334214324923252,
      "grad_norm": 0.175524041056633,
      "learning_rate": 0.00033327911515940146,
      "loss": 0.4422,
      "step": 41000
    },
    {
      "epoch": 3.3748754752271943,
      "grad_norm": 0.1571110039949417,
      "learning_rate": 0.0003312459336369551,
      "loss": 0.4422,
      "step": 41500
    },
    {
      "epoch": 3.415536625531136,
      "grad_norm": 0.14891865849494934,
      "learning_rate": 0.0003292127521145088,
      "loss": 0.4412,
      "step": 42000
    },
    {
      "epoch": 3.4561977758350784,
      "grad_norm": 0.23025919497013092,
      "learning_rate": 0.0003271795705920625,
      "loss": 0.441,
      "step": 42500
    },
    {
      "epoch": 3.4968589261390206,
      "grad_norm": 0.1544857621192932,
      "learning_rate": 0.00032514638906961616,
      "loss": 0.4403,
      "step": 43000
    },
    {
      "epoch": 3.537520076442963,
      "grad_norm": 0.16251464188098907,
      "learning_rate": 0.0003231132075471698,
      "loss": 0.4406,
      "step": 43500
    },
    {
      "epoch": 3.5781812267469046,
      "grad_norm": 0.15851745009422302,
      "learning_rate": 0.00032108002602472346,
      "loss": 0.4407,
      "step": 44000
    },
    {
      "epoch": 3.618842377050847,
      "grad_norm": 0.1416431963443756,
      "learning_rate": 0.00031904684450227716,
      "loss": 0.4399,
      "step": 44500
    },
    {
      "epoch": 3.659503527354789,
      "grad_norm": 0.13668249547481537,
      "learning_rate": 0.00031701366297983086,
      "loss": 0.4396,
      "step": 45000
    },
    {
      "epoch": 3.700164677658731,
      "grad_norm": 0.13083374500274658,
      "learning_rate": 0.0003149804814573845,
      "loss": 0.4387,
      "step": 45500
    },
    {
      "epoch": 3.740825827962673,
      "grad_norm": 0.12908069789409637,
      "learning_rate": 0.0003129472999349382,
      "loss": 0.4397,
      "step": 46000
    },
    {
      "epoch": 3.7814869782666154,
      "grad_norm": 0.1492447406053543,
      "learning_rate": 0.0003109141184124919,
      "loss": 0.4384,
      "step": 46500
    },
    {
      "epoch": 3.822148128570557,
      "grad_norm": 0.14105381071567535,
      "learning_rate": 0.00030888093689004556,
      "loss": 0.4377,
      "step": 47000
    },
    {
      "epoch": 3.8628092788744994,
      "grad_norm": 0.17152534425258636,
      "learning_rate": 0.0003068477553675992,
      "loss": 0.4385,
      "step": 47500
    },
    {
      "epoch": 3.9034704291784417,
      "grad_norm": 0.1401442438364029,
      "learning_rate": 0.00030481457384515286,
      "loss": 0.4375,
      "step": 48000
    },
    {
      "epoch": 3.9441315794823835,
      "grad_norm": 0.13888157904148102,
      "learning_rate": 0.00030278139232270656,
      "loss": 0.4382,
      "step": 48500
    },
    {
      "epoch": 3.9847927297863257,
      "grad_norm": 0.1188613623380661,
      "learning_rate": 0.00030074821080026026,
      "loss": 0.4377,
      "step": 49000
    },
    {
      "epoch": 4.025453880090268,
      "grad_norm": 0.16203472018241882,
      "learning_rate": 0.0002987150292778139,
      "loss": 0.4343,
      "step": 49500
    },
    {
      "epoch": 4.06611503039421,
      "grad_norm": 0.13807375729084015,
      "learning_rate": 0.0002966818477553676,
      "loss": 0.4326,
      "step": 50000
    },
    {
      "epoch": 4.1067761806981515,
      "grad_norm": 0.1463489532470703,
      "learning_rate": 0.0002946486662329213,
      "loss": 0.4325,
      "step": 50500
    },
    {
      "epoch": 4.147437331002094,
      "grad_norm": 0.16584663093090057,
      "learning_rate": 0.00029261548471047497,
      "loss": 0.4331,
      "step": 51000
    },
    {
      "epoch": 4.188098481306036,
      "grad_norm": 0.15331850945949554,
      "learning_rate": 0.0002905823031880286,
      "loss": 0.4326,
      "step": 51500
    },
    {
      "epoch": 4.228759631609979,
      "grad_norm": 0.14794577658176422,
      "learning_rate": 0.00028854912166558226,
      "loss": 0.4328,
      "step": 52000
    },
    {
      "epoch": 4.2694207819139205,
      "grad_norm": 0.1706523299217224,
      "learning_rate": 0.00028651594014313597,
      "loss": 0.4324,
      "step": 52500
    },
    {
      "epoch": 4.310081932217862,
      "grad_norm": 0.1393173336982727,
      "learning_rate": 0.00028448275862068967,
      "loss": 0.4318,
      "step": 53000
    },
    {
      "epoch": 4.350743082521804,
      "grad_norm": 0.15986081957817078,
      "learning_rate": 0.0002824495770982433,
      "loss": 0.4327,
      "step": 53500
    },
    {
      "epoch": 4.391404232825747,
      "grad_norm": 0.14407797157764435,
      "learning_rate": 0.000280416395575797,
      "loss": 0.4326,
      "step": 54000
    },
    {
      "epoch": 4.432065383129689,
      "grad_norm": 0.1420121192932129,
      "learning_rate": 0.0002783832140533507,
      "loss": 0.4318,
      "step": 54500
    },
    {
      "epoch": 4.472726533433631,
      "grad_norm": 0.1391310840845108,
      "learning_rate": 0.00027635003253090437,
      "loss": 0.4316,
      "step": 55000
    },
    {
      "epoch": 4.513387683737573,
      "grad_norm": 0.12902508676052094,
      "learning_rate": 0.000274316851008458,
      "loss": 0.4314,
      "step": 55500
    },
    {
      "epoch": 4.554048834041515,
      "grad_norm": 0.14633876085281372,
      "learning_rate": 0.0002722836694860117,
      "loss": 0.4309,
      "step": 56000
    },
    {
      "epoch": 4.5947099843454575,
      "grad_norm": 0.13238203525543213,
      "learning_rate": 0.00027025048796356537,
      "loss": 0.4316,
      "step": 56500
    },
    {
      "epoch": 4.635371134649399,
      "grad_norm": 0.11797436326742172,
      "learning_rate": 0.00026821730644111907,
      "loss": 0.4315,
      "step": 57000
    },
    {
      "epoch": 4.676032284953341,
      "grad_norm": 0.1447109580039978,
      "learning_rate": 0.0002661841249186727,
      "loss": 0.4309,
      "step": 57500
    },
    {
      "epoch": 4.716693435257284,
      "grad_norm": 0.16093820333480835,
      "learning_rate": 0.0002641509433962264,
      "loss": 0.4308,
      "step": 58000
    },
    {
      "epoch": 4.757354585561226,
      "grad_norm": 0.1315252035856247,
      "learning_rate": 0.0002621177618737801,
      "loss": 0.4302,
      "step": 58500
    },
    {
      "epoch": 4.798015735865167,
      "grad_norm": 0.13017301261425018,
      "learning_rate": 0.0002600845803513338,
      "loss": 0.4311,
      "step": 59000
    },
    {
      "epoch": 4.83867688616911,
      "grad_norm": 0.1376570761203766,
      "learning_rate": 0.0002580513988288875,
      "loss": 0.4303,
      "step": 59500
    },
    {
      "epoch": 4.879338036473052,
      "grad_norm": 0.14120328426361084,
      "learning_rate": 0.0002560182173064411,
      "loss": 0.4297,
      "step": 60000
    },
    {
      "epoch": 4.919999186776994,
      "grad_norm": 0.14256829023361206,
      "learning_rate": 0.0002539850357839948,
      "loss": 0.4293,
      "step": 60500
    },
    {
      "epoch": 4.960660337080936,
      "grad_norm": 0.1415029913187027,
      "learning_rate": 0.0002519518542615485,
      "loss": 0.4299,
      "step": 61000
    },
    {
      "epoch": 5.001321487384878,
      "grad_norm": 0.13668765127658844,
      "learning_rate": 0.0002499186727391022,
      "loss": 0.4296,
      "step": 61500
    },
    {
      "epoch": 5.04198263768882,
      "grad_norm": 0.12427297234535217,
      "learning_rate": 0.00024788549121665583,
      "loss": 0.4241,
      "step": 62000
    },
    {
      "epoch": 5.082643787992763,
      "grad_norm": 0.15353281795978546,
      "learning_rate": 0.0002458523096942095,
      "loss": 0.4246,
      "step": 62500
    },
    {
      "epoch": 5.123304938296704,
      "grad_norm": 0.1468183845281601,
      "learning_rate": 0.00024381912817176318,
      "loss": 0.4249,
      "step": 63000
    },
    {
      "epoch": 5.163966088600646,
      "grad_norm": 0.15031425654888153,
      "learning_rate": 0.00024178594664931685,
      "loss": 0.4252,
      "step": 63500
    },
    {
      "epoch": 5.204627238904589,
      "grad_norm": 0.1375075876712799,
      "learning_rate": 0.00023975276512687053,
      "loss": 0.4253,
      "step": 64000
    },
    {
      "epoch": 5.245288389208531,
      "grad_norm": 0.15997067093849182,
      "learning_rate": 0.0002377195836044242,
      "loss": 0.425,
      "step": 64500
    },
    {
      "epoch": 5.2859495395124725,
      "grad_norm": 0.1392591893672943,
      "learning_rate": 0.00023568640208197788,
      "loss": 0.4248,
      "step": 65000
    },
    {
      "epoch": 5.326610689816415,
      "grad_norm": 0.15236729383468628,
      "learning_rate": 0.00023365322055953156,
      "loss": 0.4246,
      "step": 65500
    },
    {
      "epoch": 5.367271840120357,
      "grad_norm": 0.15187418460845947,
      "learning_rate": 0.00023162003903708523,
      "loss": 0.4241,
      "step": 66000
    },
    {
      "epoch": 5.407932990424299,
      "grad_norm": 0.120088130235672,
      "learning_rate": 0.00022958685751463893,
      "loss": 0.4249,
      "step": 66500
    },
    {
      "epoch": 5.448594140728241,
      "grad_norm": 0.15874408185482025,
      "learning_rate": 0.00022755367599219258,
      "loss": 0.4244,
      "step": 67000
    },
    {
      "epoch": 5.489255291032183,
      "grad_norm": 0.13289546966552734,
      "learning_rate": 0.00022552049446974626,
      "loss": 0.4241,
      "step": 67500
    },
    {
      "epoch": 5.529916441336125,
      "grad_norm": 0.11867862194776535,
      "learning_rate": 0.00022348731294729993,
      "loss": 0.4248,
      "step": 68000
    },
    {
      "epoch": 5.570577591640068,
      "grad_norm": 0.14459770917892456,
      "learning_rate": 0.00022145413142485364,
      "loss": 0.4241,
      "step": 68500
    },
    {
      "epoch": 5.6112387419440095,
      "grad_norm": 0.1469273418188095,
      "learning_rate": 0.00021942094990240728,
      "loss": 0.4236,
      "step": 69000
    },
    {
      "epoch": 5.651899892247951,
      "grad_norm": 0.13912631571292877,
      "learning_rate": 0.00021738776837996096,
      "loss": 0.4238,
      "step": 69500
    },
    {
      "epoch": 5.692561042551894,
      "grad_norm": 0.15847812592983246,
      "learning_rate": 0.00021535458685751464,
      "loss": 0.4241,
      "step": 70000
    },
    {
      "epoch": 5.733222192855836,
      "grad_norm": 0.14226746559143066,
      "learning_rate": 0.00021332140533506834,
      "loss": 0.4239,
      "step": 70500
    },
    {
      "epoch": 5.773883343159778,
      "grad_norm": 0.1396360695362091,
      "learning_rate": 0.000211288223812622,
      "loss": 0.4235,
      "step": 71000
    },
    {
      "epoch": 5.81454449346372,
      "grad_norm": 0.13349628448486328,
      "learning_rate": 0.00020925504229017566,
      "loss": 0.4235,
      "step": 71500
    },
    {
      "epoch": 5.855205643767662,
      "grad_norm": 0.14102940261363983,
      "learning_rate": 0.00020722186076772934,
      "loss": 0.4229,
      "step": 72000
    },
    {
      "epoch": 5.895866794071605,
      "grad_norm": 0.12578976154327393,
      "learning_rate": 0.00020518867924528304,
      "loss": 0.4227,
      "step": 72500
    },
    {
      "epoch": 5.9365279443755465,
      "grad_norm": 0.13039162755012512,
      "learning_rate": 0.0002031554977228367,
      "loss": 0.4226,
      "step": 73000
    },
    {
      "epoch": 5.977189094679488,
      "grad_norm": 0.14325791597366333,
      "learning_rate": 0.00020112231620039036,
      "loss": 0.4225,
      "step": 73500
    },
    {
      "epoch": 6.017850244983431,
      "grad_norm": 0.14128656685352325,
      "learning_rate": 0.00019908913467794404,
      "loss": 0.4198,
      "step": 74000
    },
    {
      "epoch": 6.058511395287373,
      "grad_norm": 0.16624687612056732,
      "learning_rate": 0.00019705595315549774,
      "loss": 0.4181,
      "step": 74500
    },
    {
      "epoch": 6.099172545591315,
      "grad_norm": 0.15094023942947388,
      "learning_rate": 0.00019502277163305142,
      "loss": 0.4182,
      "step": 75000
    },
    {
      "epoch": 6.139833695895257,
      "grad_norm": 0.1291974037885666,
      "learning_rate": 0.00019298959011060507,
      "loss": 0.4181,
      "step": 75500
    },
    {
      "epoch": 6.180494846199199,
      "grad_norm": 0.14130571484565735,
      "learning_rate": 0.00019095640858815874,
      "loss": 0.418,
      "step": 76000
    },
    {
      "epoch": 6.221155996503141,
      "grad_norm": 0.13269858062267303,
      "learning_rate": 0.00018892322706571244,
      "loss": 0.4183,
      "step": 76500
    },
    {
      "epoch": 6.261817146807084,
      "grad_norm": 0.137922465801239,
      "learning_rate": 0.00018689004554326612,
      "loss": 0.4176,
      "step": 77000
    },
    {
      "epoch": 6.302478297111025,
      "grad_norm": 0.16310982406139374,
      "learning_rate": 0.00018485686402081977,
      "loss": 0.4179,
      "step": 77500
    },
    {
      "epoch": 6.343139447414967,
      "grad_norm": 0.13678468763828278,
      "learning_rate": 0.00018282368249837344,
      "loss": 0.418,
      "step": 78000
    },
    {
      "epoch": 6.38380059771891,
      "grad_norm": 0.15564872324466705,
      "learning_rate": 0.00018079050097592715,
      "loss": 0.4185,
      "step": 78500
    },
    {
      "epoch": 6.424461748022852,
      "grad_norm": 0.14421895146369934,
      "learning_rate": 0.00017875731945348082,
      "loss": 0.4183,
      "step": 79000
    },
    {
      "epoch": 6.465122898326793,
      "grad_norm": 0.18508906662464142,
      "learning_rate": 0.00017672413793103447,
      "loss": 0.4182,
      "step": 79500
    },
    {
      "epoch": 6.505784048630736,
      "grad_norm": 0.13689059019088745,
      "learning_rate": 0.00017469095640858817,
      "loss": 0.4181,
      "step": 80000
    },
    {
      "epoch": 6.546445198934678,
      "grad_norm": 0.14093104004859924,
      "learning_rate": 0.00017265777488614185,
      "loss": 0.418,
      "step": 80500
    },
    {
      "epoch": 6.58710634923862,
      "grad_norm": 0.1384076029062271,
      "learning_rate": 0.00017062459336369552,
      "loss": 0.4178,
      "step": 81000
    },
    {
      "epoch": 6.627767499542562,
      "grad_norm": 0.14006026089191437,
      "learning_rate": 0.00016859141184124917,
      "loss": 0.4179,
      "step": 81500
    },
    {
      "epoch": 6.668428649846504,
      "grad_norm": 0.1509217619895935,
      "learning_rate": 0.00016655823031880288,
      "loss": 0.4169,
      "step": 82000
    },
    {
      "epoch": 6.709089800150446,
      "grad_norm": 0.16623668372631073,
      "learning_rate": 0.00016452504879635655,
      "loss": 0.4171,
      "step": 82500
    },
    {
      "epoch": 6.749750950454389,
      "grad_norm": 0.1296483725309372,
      "learning_rate": 0.00016249186727391023,
      "loss": 0.4171,
      "step": 83000
    },
    {
      "epoch": 6.7904121007583305,
      "grad_norm": 0.12451576441526413,
      "learning_rate": 0.00016045868575146387,
      "loss": 0.4172,
      "step": 83500
    },
    {
      "epoch": 6.831073251062272,
      "grad_norm": 0.17879655957221985,
      "learning_rate": 0.00015842550422901758,
      "loss": 0.4175,
      "step": 84000
    },
    {
      "epoch": 6.871734401366215,
      "grad_norm": 0.13943268358707428,
      "learning_rate": 0.00015639232270657125,
      "loss": 0.4167,
      "step": 84500
    },
    {
      "epoch": 6.912395551670157,
      "grad_norm": 0.11901506781578064,
      "learning_rate": 0.00015435914118412493,
      "loss": 0.4168,
      "step": 85000
    },
    {
      "epoch": 6.9530567019740985,
      "grad_norm": 0.15162479877471924,
      "learning_rate": 0.00015232595966167858,
      "loss": 0.4169,
      "step": 85500
    },
    {
      "epoch": 6.993717852278041,
      "grad_norm": 0.1420772820711136,
      "learning_rate": 0.00015029277813923228,
      "loss": 0.4169,
      "step": 86000
    },
    {
      "epoch": 7.034379002581983,
      "grad_norm": 0.13834314048290253,
      "learning_rate": 0.00014825959661678595,
      "loss": 0.4123,
      "step": 86500
    },
    {
      "epoch": 7.075040152885925,
      "grad_norm": 0.13194847106933594,
      "learning_rate": 0.00014622641509433963,
      "loss": 0.4119,
      "step": 87000
    },
    {
      "epoch": 7.1157013031898675,
      "grad_norm": 0.14098207652568817,
      "learning_rate": 0.00014419323357189328,
      "loss": 0.4125,
      "step": 87500
    },
    {
      "epoch": 7.156362453493809,
      "grad_norm": 0.13114899396896362,
      "learning_rate": 0.00014216005204944698,
      "loss": 0.412,
      "step": 88000
    },
    {
      "epoch": 7.197023603797751,
      "grad_norm": 0.138233482837677,
      "learning_rate": 0.00014012687052700066,
      "loss": 0.4127,
      "step": 88500
    },
    {
      "epoch": 7.237684754101694,
      "grad_norm": 0.13871434330940247,
      "learning_rate": 0.00013809368900455433,
      "loss": 0.4122,
      "step": 89000
    },
    {
      "epoch": 7.278345904405636,
      "grad_norm": 0.11958198994398117,
      "learning_rate": 0.00013606050748210798,
      "loss": 0.4121,
      "step": 89500
    },
    {
      "epoch": 7.319007054709577,
      "grad_norm": 0.1299075186252594,
      "learning_rate": 0.00013402732595966168,
      "loss": 0.4125,
      "step": 90000
    },
    {
      "epoch": 7.35966820501352,
      "grad_norm": 0.14280715584754944,
      "learning_rate": 0.00013199414443721536,
      "loss": 0.4121,
      "step": 90500
    },
    {
      "epoch": 7.400329355317462,
      "grad_norm": 0.12981531023979187,
      "learning_rate": 0.00012996096291476903,
      "loss": 0.4121,
      "step": 91000
    },
    {
      "epoch": 7.440990505621404,
      "grad_norm": 0.14154154062271118,
      "learning_rate": 0.0001279277813923227,
      "loss": 0.4123,
      "step": 91500
    },
    {
      "epoch": 7.481651655925346,
      "grad_norm": 0.12891964614391327,
      "learning_rate": 0.00012589459986987639,
      "loss": 0.4122,
      "step": 92000
    },
    {
      "epoch": 7.522312806229288,
      "grad_norm": 0.12419825047254562,
      "learning_rate": 0.00012386141834743006,
      "loss": 0.4123,
      "step": 92500
    },
    {
      "epoch": 7.562973956533231,
      "grad_norm": 0.1399717926979065,
      "learning_rate": 0.00012182823682498374,
      "loss": 0.4126,
      "step": 93000
    },
    {
      "epoch": 7.603635106837173,
      "grad_norm": 0.13419051468372345,
      "learning_rate": 0.00011979505530253741,
      "loss": 0.412,
      "step": 93500
    },
    {
      "epoch": 7.644296257141114,
      "grad_norm": 0.13296350836753845,
      "learning_rate": 0.0001177618737800911,
      "loss": 0.4115,
      "step": 94000
    },
    {
      "epoch": 7.684957407445056,
      "grad_norm": 0.1471552699804306,
      "learning_rate": 0.00011572869225764476,
      "loss": 0.4121,
      "step": 94500
    },
    {
      "epoch": 7.725618557748999,
      "grad_norm": 0.12354033440351486,
      "learning_rate": 0.00011369551073519845,
      "loss": 0.4115,
      "step": 95000
    },
    {
      "epoch": 7.766279708052941,
      "grad_norm": 0.13873358070850372,
      "learning_rate": 0.00011166232921275211,
      "loss": 0.4117,
      "step": 95500
    },
    {
      "epoch": 7.806940858356883,
      "grad_norm": 0.13700750470161438,
      "learning_rate": 0.0001096291476903058,
      "loss": 0.412,
      "step": 96000
    },
    {
      "epoch": 7.847602008660825,
      "grad_norm": 0.17274750769138336,
      "learning_rate": 0.00010759596616785946,
      "loss": 0.4114,
      "step": 96500
    },
    {
      "epoch": 7.888263158964767,
      "grad_norm": 0.16871590912342072,
      "learning_rate": 0.00010556278464541315,
      "loss": 0.4115,
      "step": 97000
    },
    {
      "epoch": 7.928924309268709,
      "grad_norm": 0.15303291380405426,
      "learning_rate": 0.00010352960312296682,
      "loss": 0.4115,
      "step": 97500
    },
    {
      "epoch": 7.969585459572651,
      "grad_norm": 0.12185352295637131,
      "learning_rate": 0.0001014964216005205,
      "loss": 0.4112,
      "step": 98000
    },
    {
      "epoch": 8.010246609876594,
      "grad_norm": 0.14206130802631378,
      "learning_rate": 9.946324007807417e-05,
      "loss": 0.4097,
      "step": 98500
    },
    {
      "epoch": 8.050907760180536,
      "grad_norm": 0.14555904269218445,
      "learning_rate": 9.743005855562786e-05,
      "loss": 0.4065,
      "step": 99000
    },
    {
      "epoch": 8.091568910484478,
      "grad_norm": 0.1422388106584549,
      "learning_rate": 9.539687703318152e-05,
      "loss": 0.4067,
      "step": 99500
    },
    {
      "epoch": 8.13223006078842,
      "grad_norm": 0.15586915612220764,
      "learning_rate": 9.336369551073521e-05,
      "loss": 0.4067,
      "step": 100000
    },
    {
      "epoch": 8.172891211092361,
      "grad_norm": 0.1465553492307663,
      "learning_rate": 9.133051398828887e-05,
      "loss": 0.4072,
      "step": 100500
    },
    {
      "epoch": 8.213552361396303,
      "grad_norm": 0.12635929882526398,
      "learning_rate": 8.929733246584256e-05,
      "loss": 0.4073,
      "step": 101000
    },
    {
      "epoch": 8.254213511700247,
      "grad_norm": 0.15106834471225739,
      "learning_rate": 8.726415094339622e-05,
      "loss": 0.4074,
      "step": 101500
    },
    {
      "epoch": 8.294874662004188,
      "grad_norm": 0.1712779700756073,
      "learning_rate": 8.523096942094991e-05,
      "loss": 0.4069,
      "step": 102000
    },
    {
      "epoch": 8.33553581230813,
      "grad_norm": 0.13283351063728333,
      "learning_rate": 8.319778789850357e-05,
      "loss": 0.4073,
      "step": 102500
    },
    {
      "epoch": 8.376196962612072,
      "grad_norm": 0.1339014172554016,
      "learning_rate": 8.116460637605726e-05,
      "loss": 0.407,
      "step": 103000
    },
    {
      "epoch": 8.416858112916014,
      "grad_norm": 0.14681334793567657,
      "learning_rate": 7.913142485361092e-05,
      "loss": 0.4067,
      "step": 103500
    },
    {
      "epoch": 8.457519263219957,
      "grad_norm": 0.13544636964797974,
      "learning_rate": 7.709824333116461e-05,
      "loss": 0.4076,
      "step": 104000
    },
    {
      "epoch": 8.4981804135239,
      "grad_norm": 0.15028634667396545,
      "learning_rate": 7.506506180871829e-05,
      "loss": 0.4066,
      "step": 104500
    },
    {
      "epoch": 8.538841563827841,
      "grad_norm": 0.1351308375597,
      "learning_rate": 7.303188028627196e-05,
      "loss": 0.4069,
      "step": 105000
    },
    {
      "epoch": 8.579502714131783,
      "grad_norm": 0.17839321494102478,
      "learning_rate": 7.099869876382564e-05,
      "loss": 0.4067,
      "step": 105500
    },
    {
      "epoch": 8.620163864435725,
      "grad_norm": 0.15265263617038727,
      "learning_rate": 6.896551724137931e-05,
      "loss": 0.407,
      "step": 106000
    },
    {
      "epoch": 8.660825014739666,
      "grad_norm": 0.15105929970741272,
      "learning_rate": 6.693233571893299e-05,
      "loss": 0.407,
      "step": 106500
    },
    {
      "epoch": 8.701486165043608,
      "grad_norm": 0.1458972841501236,
      "learning_rate": 6.489915419648666e-05,
      "loss": 0.4064,
      "step": 107000
    },
    {
      "epoch": 8.742147315347552,
      "grad_norm": 0.13423025608062744,
      "learning_rate": 6.286597267404034e-05,
      "loss": 0.4063,
      "step": 107500
    },
    {
      "epoch": 8.782808465651494,
      "grad_norm": 0.14012755453586578,
      "learning_rate": 6.0832791151594015e-05,
      "loss": 0.4069,
      "step": 108000
    },
    {
      "epoch": 8.823469615955435,
      "grad_norm": 0.14462992548942566,
      "learning_rate": 5.879960962914769e-05,
      "loss": 0.4065,
      "step": 108500
    },
    {
      "epoch": 8.864130766259377,
      "grad_norm": 0.177517831325531,
      "learning_rate": 5.6766428106701366e-05,
      "loss": 0.4063,
      "step": 109000
    },
    {
      "epoch": 8.904791916563319,
      "grad_norm": 0.1270572692155838,
      "learning_rate": 5.473324658425504e-05,
      "loss": 0.4064,
      "step": 109500
    },
    {
      "epoch": 8.945453066867262,
      "grad_norm": 0.1461610198020935,
      "learning_rate": 5.270006506180872e-05,
      "loss": 0.4063,
      "step": 110000
    },
    {
      "epoch": 8.986114217171204,
      "grad_norm": 0.14243601262569427,
      "learning_rate": 5.066688353936239e-05,
      "loss": 0.406,
      "step": 110500
    },
    {
      "epoch": 9.026775367475146,
      "grad_norm": 0.15172889828681946,
      "learning_rate": 4.8633702016916075e-05,
      "loss": 0.4037,
      "step": 111000
    },
    {
      "epoch": 9.067436517779088,
      "grad_norm": 0.12431073188781738,
      "learning_rate": 4.660052049446975e-05,
      "loss": 0.4027,
      "step": 111500
    },
    {
      "epoch": 9.10809766808303,
      "grad_norm": 0.13495855033397675,
      "learning_rate": 4.4567338972023426e-05,
      "loss": 0.4027,
      "step": 112000
    },
    {
      "epoch": 9.148758818386971,
      "grad_norm": 0.12493375688791275,
      "learning_rate": 4.25341574495771e-05,
      "loss": 0.4025,
      "step": 112500
    },
    {
      "epoch": 9.189419968690915,
      "grad_norm": 0.12687838077545166,
      "learning_rate": 4.050097592713078e-05,
      "loss": 0.4027,
      "step": 113000
    },
    {
      "epoch": 9.230081118994857,
      "grad_norm": 0.14100006222724915,
      "learning_rate": 3.846779440468445e-05,
      "loss": 0.403,
      "step": 113500
    },
    {
      "epoch": 9.270742269298799,
      "grad_norm": 0.1428811103105545,
      "learning_rate": 3.643461288223813e-05,
      "loss": 0.4028,
      "step": 114000
    },
    {
      "epoch": 9.31140341960274,
      "grad_norm": 0.13930854201316833,
      "learning_rate": 3.4401431359791804e-05,
      "loss": 0.4028,
      "step": 114500
    },
    {
      "epoch": 9.352064569906682,
      "grad_norm": 0.14519809186458588,
      "learning_rate": 3.236824983734548e-05,
      "loss": 0.4023,
      "step": 115000
    },
    {
      "epoch": 9.392725720210624,
      "grad_norm": 0.1437297910451889,
      "learning_rate": 3.0335068314899155e-05,
      "loss": 0.4028,
      "step": 115500
    },
    {
      "epoch": 9.433386870514568,
      "grad_norm": 0.14054960012435913,
      "learning_rate": 2.830188679245283e-05,
      "loss": 0.4025,
      "step": 116000
    },
    {
      "epoch": 9.47404802081851,
      "grad_norm": 0.14748068153858185,
      "learning_rate": 2.6268705270006506e-05,
      "loss": 0.4023,
      "step": 116500
    },
    {
      "epoch": 9.514709171122451,
      "grad_norm": 0.13375982642173767,
      "learning_rate": 2.423552374756018e-05,
      "loss": 0.4023,
      "step": 117000
    },
    {
      "epoch": 9.555370321426393,
      "grad_norm": 0.12512274086475372,
      "learning_rate": 2.220234222511386e-05,
      "loss": 0.4024,
      "step": 117500
    },
    {
      "epoch": 9.596031471730335,
      "grad_norm": 0.14309744536876678,
      "learning_rate": 2.0169160702667536e-05,
      "loss": 0.4017,
      "step": 118000
    },
    {
      "epoch": 9.636692622034277,
      "grad_norm": 0.1292697787284851,
      "learning_rate": 1.813597918022121e-05,
      "loss": 0.4026,
      "step": 118500
    },
    {
      "epoch": 9.67735377233822,
      "grad_norm": 0.15145105123519897,
      "learning_rate": 1.6102797657774887e-05,
      "loss": 0.4024,
      "step": 119000
    },
    {
      "epoch": 9.718014922642162,
      "grad_norm": 0.14660604298114777,
      "learning_rate": 1.4069616135328564e-05,
      "loss": 0.4028,
      "step": 119500
    },
    {
      "epoch": 9.758676072946104,
      "grad_norm": 0.15511435270309448,
      "learning_rate": 1.203643461288224e-05,
      "loss": 0.4022,
      "step": 120000
    },
    {
      "epoch": 9.799337223250046,
      "grad_norm": 0.132461816072464,
      "learning_rate": 1.0003253090435915e-05,
      "loss": 0.4013,
      "step": 120500
    },
    {
      "epoch": 9.839998373553987,
      "grad_norm": 0.130512535572052,
      "learning_rate": 7.97007156798959e-06,
      "loss": 0.4015,
      "step": 121000
    },
    {
      "epoch": 9.88065952385793,
      "grad_norm": 0.17279957234859467,
      "learning_rate": 5.936890045543267e-06,
      "loss": 0.4015,
      "step": 121500
    },
    {
      "epoch": 9.921320674161873,
      "grad_norm": 0.12243900448083878,
      "learning_rate": 3.903708523096942e-06,
      "loss": 0.4016,
      "step": 122000
    },
    {
      "epoch": 9.961981824465814,
      "grad_norm": 0.13101397454738617,
      "learning_rate": 1.8705270006506181e-06,
      "loss": 0.4021,
      "step": 122500
    },
    {
      "epoch": 9.99939008274544,
      "step": 122960,
      "total_flos": 1.0041556435152235e+17,
      "train_loss": 0.44395857893497664,
      "train_runtime": 24680.6365,
      "train_samples_per_second": 637.74,
      "train_steps_per_second": 4.982
    }
  ],
  "logging_steps": 500,
  "max_steps": 122960,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "total_flos": 1.0041556435152235e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
