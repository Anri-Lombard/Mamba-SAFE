{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 139.22377864069776,
  "eval_steps": 500.0,
  "global_step": 1712000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0406611503039421,
      "grad_norm": 1.1594756841659546,
      "learning_rate": 0.0004999796681847755,
      "loss": 1.2224,
      "step": 500
    },
    {
      "epoch": 0.0813223006078842,
      "grad_norm": 0.8752167224884033,
      "learning_rate": 0.0004999593363695512,
      "loss": 0.7587,
      "step": 1000
    },
    {
      "epoch": 0.1219834509118263,
      "grad_norm": 0.6502133011817932,
      "learning_rate": 0.0004999390045543267,
      "loss": 0.6704,
      "step": 1500
    },
    {
      "epoch": 0.1626446012157684,
      "grad_norm": 0.5201811194419861,
      "learning_rate": 0.0004999186727391022,
      "loss": 0.6309,
      "step": 2000
    },
    {
      "epoch": 0.2033057515197105,
      "grad_norm": 0.4612445533275604,
      "learning_rate": 0.0004998983409238777,
      "loss": 0.6049,
      "step": 2500
    },
    {
      "epoch": 0.2439669018236526,
      "grad_norm": 0.5367523431777954,
      "learning_rate": 0.0004998780091086532,
      "loss": 0.5898,
      "step": 3000
    },
    {
      "epoch": 0.2846280521275947,
      "grad_norm": 0.37784919142723083,
      "learning_rate": 0.0004998576772934288,
      "loss": 0.5767,
      "step": 3500
    },
    {
      "epoch": 0.3252892024315368,
      "grad_norm": 0.3702698349952698,
      "learning_rate": 0.0004998373454782044,
      "loss": 0.5667,
      "step": 4000
    },
    {
      "epoch": 0.3659503527354789,
      "grad_norm": 0.3228362202644348,
      "learning_rate": 0.0004998170136629799,
      "loss": 0.5586,
      "step": 4500
    },
    {
      "epoch": 0.406611503039421,
      "grad_norm": 0.33456262946128845,
      "learning_rate": 0.0004997966818477554,
      "loss": 0.5507,
      "step": 5000
    },
    {
      "epoch": 0.4472726533433631,
      "grad_norm": 0.3053341507911682,
      "learning_rate": 0.0004997763500325309,
      "loss": 0.547,
      "step": 5500
    },
    {
      "epoch": 0.4879338036473052,
      "grad_norm": 0.3189201354980469,
      "learning_rate": 0.0004997560182173064,
      "loss": 0.5399,
      "step": 6000
    },
    {
      "epoch": 0.5285949539512472,
      "grad_norm": 0.3366231918334961,
      "learning_rate": 0.000499735686402082,
      "loss": 0.5358,
      "step": 6500
    },
    {
      "epoch": 0.5692561042551894,
      "grad_norm": 0.30668193101882935,
      "learning_rate": 0.0004997153545868576,
      "loss": 0.5312,
      "step": 7000
    },
    {
      "epoch": 0.6099172545591315,
      "grad_norm": 0.2654882073402405,
      "learning_rate": 0.0004996950227716331,
      "loss": 0.529,
      "step": 7500
    },
    {
      "epoch": 0.6505784048630736,
      "grad_norm": 0.32270917296409607,
      "learning_rate": 0.0004996746909564086,
      "loss": 0.525,
      "step": 8000
    },
    {
      "epoch": 0.6912395551670156,
      "grad_norm": 0.24908936023712158,
      "learning_rate": 0.0004996543591411841,
      "loss": 0.5219,
      "step": 8500
    },
    {
      "epoch": 0.7319007054709578,
      "grad_norm": 0.26098835468292236,
      "learning_rate": 0.0004996340273259597,
      "loss": 0.5191,
      "step": 9000
    },
    {
      "epoch": 0.7725618557748999,
      "grad_norm": 0.26346802711486816,
      "learning_rate": 0.0004996136955107353,
      "loss": 0.5168,
      "step": 9500
    },
    {
      "epoch": 0.813223006078842,
      "grad_norm": 0.21929965913295746,
      "learning_rate": 0.0004995933636955108,
      "loss": 0.514,
      "step": 10000
    },
    {
      "epoch": 0.853884156382784,
      "grad_norm": 0.2119971215724945,
      "learning_rate": 0.0004995730318802863,
      "loss": 0.5116,
      "step": 10500
    },
    {
      "epoch": 0.8945453066867262,
      "grad_norm": 0.22667571902275085,
      "learning_rate": 0.0004995527000650618,
      "loss": 0.5103,
      "step": 11000
    },
    {
      "epoch": 0.9352064569906683,
      "grad_norm": 0.21666154265403748,
      "learning_rate": 0.0004995323682498373,
      "loss": 0.5076,
      "step": 11500
    },
    {
      "epoch": 0.9758676072946104,
      "grad_norm": 0.24485228955745697,
      "learning_rate": 0.000499512036434613,
      "loss": 0.5051,
      "step": 12000
    },
    {
      "epoch": 1.0165287575985524,
      "grad_norm": 0.23953576385974884,
      "learning_rate": 0.0004994917046193885,
      "loss": 0.5027,
      "step": 12500
    },
    {
      "epoch": 1.0571899079024947,
      "grad_norm": 0.2290162593126297,
      "learning_rate": 0.000499471372804164,
      "loss": 0.5011,
      "step": 13000
    },
    {
      "epoch": 1.0978510582064367,
      "grad_norm": 0.24104535579681396,
      "learning_rate": 0.0004994510409889395,
      "loss": 0.4988,
      "step": 13500
    },
    {
      "epoch": 1.1385122085103787,
      "grad_norm": 0.22788989543914795,
      "learning_rate": 0.000499430709173715,
      "loss": 0.4974,
      "step": 14000
    },
    {
      "epoch": 1.179173358814321,
      "grad_norm": 0.2251431941986084,
      "learning_rate": 0.0004994103773584906,
      "loss": 0.4949,
      "step": 14500
    },
    {
      "epoch": 1.219834509118263,
      "grad_norm": 0.1763388067483902,
      "learning_rate": 0.0004993900455432662,
      "loss": 0.4955,
      "step": 15000
    },
    {
      "epoch": 1.260495659422205,
      "grad_norm": 0.19645534455776215,
      "learning_rate": 0.0004993697137280417,
      "loss": 0.494,
      "step": 15500
    },
    {
      "epoch": 1.3011568097261472,
      "grad_norm": 0.21773721277713776,
      "learning_rate": 0.0004993493819128172,
      "loss": 0.4929,
      "step": 16000
    },
    {
      "epoch": 1.3418179600300892,
      "grad_norm": 0.1823216676712036,
      "learning_rate": 0.0004993290500975927,
      "loss": 0.491,
      "step": 16500
    },
    {
      "epoch": 1.3824791103340313,
      "grad_norm": 0.23446302115917206,
      "learning_rate": 0.0004993087182823682,
      "loss": 0.4894,
      "step": 17000
    },
    {
      "epoch": 1.4231402606379735,
      "grad_norm": 0.2204621583223343,
      "learning_rate": 0.0004992883864671438,
      "loss": 0.489,
      "step": 17500
    },
    {
      "epoch": 1.4638014109419155,
      "grad_norm": 0.19991014897823334,
      "learning_rate": 0.0004992680546519194,
      "loss": 0.4878,
      "step": 18000
    },
    {
      "epoch": 1.5044625612458575,
      "grad_norm": 0.17701977491378784,
      "learning_rate": 0.0004992477228366949,
      "loss": 0.487,
      "step": 18500
    },
    {
      "epoch": 1.5451237115497998,
      "grad_norm": 0.17688480019569397,
      "learning_rate": 0.0004992273910214704,
      "loss": 0.4858,
      "step": 19000
    },
    {
      "epoch": 1.5857848618537418,
      "grad_norm": 0.18928392231464386,
      "learning_rate": 0.0004992070592062459,
      "loss": 0.4851,
      "step": 19500
    },
    {
      "epoch": 1.6264460121576838,
      "grad_norm": 0.18944045901298523,
      "learning_rate": 0.0004991867273910215,
      "loss": 0.4847,
      "step": 20000
    },
    {
      "epoch": 1.667107162461626,
      "grad_norm": 0.18259218335151672,
      "learning_rate": 0.000499166395575797,
      "loss": 0.4835,
      "step": 20500
    },
    {
      "epoch": 1.7077683127655683,
      "grad_norm": 0.20468157529830933,
      "learning_rate": 0.0004991460637605726,
      "loss": 0.4837,
      "step": 21000
    },
    {
      "epoch": 1.74842946306951,
      "grad_norm": 0.18065324425697327,
      "learning_rate": 0.0004991257319453481,
      "loss": 0.4814,
      "step": 21500
    },
    {
      "epoch": 1.7890906133734523,
      "grad_norm": 0.20687061548233032,
      "learning_rate": 0.0004991054001301236,
      "loss": 0.4805,
      "step": 22000
    },
    {
      "epoch": 1.8297517636773946,
      "grad_norm": 0.18783782422542572,
      "learning_rate": 0.0004990850683148991,
      "loss": 0.4799,
      "step": 22500
    },
    {
      "epoch": 1.8704129139813366,
      "grad_norm": 0.16696326434612274,
      "learning_rate": 0.0004990647364996747,
      "loss": 0.4788,
      "step": 23000
    },
    {
      "epoch": 1.9110740642852786,
      "grad_norm": 0.1800985485315323,
      "learning_rate": 0.0004990444046844503,
      "loss": 0.4789,
      "step": 23500
    },
    {
      "epoch": 1.9517352145892208,
      "grad_norm": 0.16685675084590912,
      "learning_rate": 0.0004990240728692258,
      "loss": 0.478,
      "step": 24000
    },
    {
      "epoch": 1.9923963648931629,
      "grad_norm": 0.1707664430141449,
      "learning_rate": 0.0004990037410540013,
      "loss": 0.4767,
      "step": 24500
    },
    {
      "epoch": 2.033057515197105,
      "grad_norm": 0.1751834601163864,
      "learning_rate": 0.0004989834092387768,
      "loss": 0.4737,
      "step": 25000
    },
    {
      "epoch": 2.073718665501047,
      "grad_norm": 0.18333835899829865,
      "learning_rate": 0.0004989630774235524,
      "loss": 0.4736,
      "step": 25500
    },
    {
      "epoch": 2.1143798158049893,
      "grad_norm": 0.1706211417913437,
      "learning_rate": 0.000498942745608328,
      "loss": 0.4732,
      "step": 26000
    },
    {
      "epoch": 2.155040966108931,
      "grad_norm": 0.16132813692092896,
      "learning_rate": 0.0004989224137931035,
      "loss": 0.4721,
      "step": 26500
    },
    {
      "epoch": 2.1957021164128734,
      "grad_norm": 0.164530411362648,
      "learning_rate": 0.000498902081977879,
      "loss": 0.4715,
      "step": 27000
    },
    {
      "epoch": 2.2363632667168156,
      "grad_norm": 0.14891862869262695,
      "learning_rate": 0.0004988817501626545,
      "loss": 0.4715,
      "step": 27500
    },
    {
      "epoch": 2.2770244170207574,
      "grad_norm": 0.20396539568901062,
      "learning_rate": 0.0004988614183474301,
      "loss": 0.47,
      "step": 28000
    },
    {
      "epoch": 2.3176855673246997,
      "grad_norm": 0.1763211041688919,
      "learning_rate": 0.0004988410865322056,
      "loss": 0.47,
      "step": 28500
    },
    {
      "epoch": 2.358346717628642,
      "grad_norm": 0.16517002880573273,
      "learning_rate": 0.0004988207547169812,
      "loss": 0.47,
      "step": 29000
    },
    {
      "epoch": 2.3990078679325837,
      "grad_norm": 0.16254419088363647,
      "learning_rate": 0.0004988004229017567,
      "loss": 0.4695,
      "step": 29500
    },
    {
      "epoch": 2.439669018236526,
      "grad_norm": 0.16542893648147583,
      "learning_rate": 0.0004987800910865322,
      "loss": 0.4695,
      "step": 30000
    },
    {
      "epoch": 2.480330168540468,
      "grad_norm": 0.13969485461711884,
      "learning_rate": 0.0004987597592713077,
      "loss": 0.4685,
      "step": 30500
    },
    {
      "epoch": 2.52099131884441,
      "grad_norm": 0.15823926031589508,
      "learning_rate": 0.0004987394274560833,
      "loss": 0.4684,
      "step": 31000
    },
    {
      "epoch": 2.561652469148352,
      "grad_norm": 0.14922913908958435,
      "learning_rate": 0.0004987190956408588,
      "loss": 0.4677,
      "step": 31500
    },
    {
      "epoch": 2.6023136194522944,
      "grad_norm": 0.1666596680879593,
      "learning_rate": 0.0004986987638256344,
      "loss": 0.467,
      "step": 32000
    },
    {
      "epoch": 2.6429747697562362,
      "grad_norm": 0.21886135637760162,
      "learning_rate": 0.0004986784320104099,
      "loss": 0.4667,
      "step": 32500
    },
    {
      "epoch": 2.6836359200601785,
      "grad_norm": 0.17621101438999176,
      "learning_rate": 0.0004986581001951854,
      "loss": 0.4665,
      "step": 33000
    },
    {
      "epoch": 2.7242970703641207,
      "grad_norm": 0.15418364107608795,
      "learning_rate": 0.000498637768379961,
      "loss": 0.4661,
      "step": 33500
    },
    {
      "epoch": 2.7649582206680625,
      "grad_norm": 0.15041746199131012,
      "learning_rate": 0.0004986174365647365,
      "loss": 0.4663,
      "step": 34000
    },
    {
      "epoch": 2.8056193709720048,
      "grad_norm": 0.15329086780548096,
      "learning_rate": 0.000498597104749512,
      "loss": 0.4647,
      "step": 34500
    },
    {
      "epoch": 2.846280521275947,
      "grad_norm": 0.13919217884540558,
      "learning_rate": 0.0004985767729342876,
      "loss": 0.4648,
      "step": 35000
    },
    {
      "epoch": 2.886941671579889,
      "grad_norm": 0.14307470619678497,
      "learning_rate": 0.0004985564411190631,
      "loss": 0.4641,
      "step": 35500
    },
    {
      "epoch": 2.927602821883831,
      "grad_norm": 0.1528572291135788,
      "learning_rate": 0.0004985361093038386,
      "loss": 0.464,
      "step": 36000
    },
    {
      "epoch": 2.9682639721877733,
      "grad_norm": 0.1415516436100006,
      "learning_rate": 0.0004985157774886142,
      "loss": 0.4638,
      "step": 36500
    },
    {
      "epoch": 3.0089251224917155,
      "grad_norm": 0.15252573788166046,
      "learning_rate": 0.0004984954456733897,
      "loss": 0.4623,
      "step": 37000
    },
    {
      "epoch": 3.0495862727956573,
      "grad_norm": 0.14597287774085999,
      "learning_rate": 0.0004984751138581653,
      "loss": 0.4602,
      "step": 37500
    },
    {
      "epoch": 3.0902474230995995,
      "grad_norm": 0.15593469142913818,
      "learning_rate": 0.0004984547820429408,
      "loss": 0.4599,
      "step": 38000
    },
    {
      "epoch": 3.130908573403542,
      "grad_norm": 0.13799510896205902,
      "learning_rate": 0.0004984344502277163,
      "loss": 0.4597,
      "step": 38500
    },
    {
      "epoch": 3.1715697237074836,
      "grad_norm": 0.16021530330181122,
      "learning_rate": 0.0004984141184124919,
      "loss": 0.4594,
      "step": 39000
    },
    {
      "epoch": 3.212230874011426,
      "grad_norm": 0.14201819896697998,
      "learning_rate": 0.0004983937865972674,
      "loss": 0.459,
      "step": 39500
    },
    {
      "epoch": 3.252892024315368,
      "grad_norm": 0.1502692550420761,
      "learning_rate": 0.000498373454782043,
      "loss": 0.4596,
      "step": 40000
    },
    {
      "epoch": 3.29355317461931,
      "grad_norm": 0.1381065398454666,
      "learning_rate": 0.0004983531229668185,
      "loss": 0.4588,
      "step": 40500
    },
    {
      "epoch": 3.334214324923252,
      "grad_norm": 0.14958252012729645,
      "learning_rate": 0.000498332791151594,
      "loss": 0.4587,
      "step": 41000
    },
    {
      "epoch": 3.3748754752271943,
      "grad_norm": 0.14295990765094757,
      "learning_rate": 0.0004983124593363695,
      "loss": 0.4586,
      "step": 41500
    },
    {
      "epoch": 3.415536625531136,
      "grad_norm": 0.13805817067623138,
      "learning_rate": 0.0004982921275211451,
      "loss": 0.4577,
      "step": 42000
    },
    {
      "epoch": 3.4561977758350784,
      "grad_norm": 0.19972757995128632,
      "learning_rate": 0.0004982717957059206,
      "loss": 0.4577,
      "step": 42500
    },
    {
      "epoch": 3.4968589261390206,
      "grad_norm": 0.13831301033496857,
      "learning_rate": 0.0004982514638906962,
      "loss": 0.457,
      "step": 43000
    },
    {
      "epoch": 3.537520076442963,
      "grad_norm": 0.1536189168691635,
      "learning_rate": 0.0004982311320754717,
      "loss": 0.4571,
      "step": 43500
    },
    {
      "epoch": 3.5781812267469046,
      "grad_norm": 0.1292436718940735,
      "learning_rate": 0.0004982108002602472,
      "loss": 0.4573,
      "step": 44000
    },
    {
      "epoch": 3.618842377050847,
      "grad_norm": 0.12425103783607483,
      "learning_rate": 0.0004981904684450228,
      "loss": 0.4567,
      "step": 44500
    },
    {
      "epoch": 3.659503527354789,
      "grad_norm": 0.14019586145877838,
      "learning_rate": 0.0004981701366297983,
      "loss": 0.4565,
      "step": 45000
    },
    {
      "epoch": 3.700164677658731,
      "grad_norm": 0.1261669397354126,
      "learning_rate": 0.0004981498048145738,
      "loss": 0.4555,
      "step": 45500
    },
    {
      "epoch": 3.740825827962673,
      "grad_norm": 0.11357975751161575,
      "learning_rate": 0.0004981294729993494,
      "loss": 0.4563,
      "step": 46000
    },
    {
      "epoch": 3.7814869782666154,
      "grad_norm": 0.13529203832149506,
      "learning_rate": 0.0004981091411841249,
      "loss": 0.4549,
      "step": 46500
    },
    {
      "epoch": 3.822148128570557,
      "grad_norm": 0.13030056655406952,
      "learning_rate": 0.0004980888093689004,
      "loss": 0.4547,
      "step": 47000
    },
    {
      "epoch": 3.8628092788744994,
      "grad_norm": 0.13806310296058655,
      "learning_rate": 0.000498068477553676,
      "loss": 0.4552,
      "step": 47500
    },
    {
      "epoch": 3.9034704291784417,
      "grad_norm": 0.14335176348686218,
      "learning_rate": 0.0004980481457384515,
      "loss": 0.4545,
      "step": 48000
    },
    {
      "epoch": 3.9441315794823835,
      "grad_norm": 0.12348199635744095,
      "learning_rate": 0.0004980278139232271,
      "loss": 0.4551,
      "step": 48500
    },
    {
      "epoch": 3.9847927297863257,
      "grad_norm": 0.10704018175601959,
      "learning_rate": 0.0004980074821080026,
      "loss": 0.4543,
      "step": 49000
    },
    {
      "epoch": 4.025453880090268,
      "grad_norm": 0.14555642008781433,
      "learning_rate": 0.0004979871502927781,
      "loss": 0.452,
      "step": 49500
    },
    {
      "epoch": 4.06611503039421,
      "grad_norm": 0.11884826421737671,
      "learning_rate": 0.0004979668184775537,
      "loss": 0.4506,
      "step": 50000
    },
    {
      "epoch": 4.1067761806981515,
      "grad_norm": 0.14210106432437897,
      "learning_rate": 0.0004979464866623292,
      "loss": 0.4507,
      "step": 50500
    },
    {
      "epoch": 4.147437331002094,
      "grad_norm": 0.14239142835140228,
      "learning_rate": 0.0004979261548471047,
      "loss": 0.4509,
      "step": 51000
    },
    {
      "epoch": 4.188098481306036,
      "grad_norm": 0.12802687287330627,
      "learning_rate": 0.0004979058230318803,
      "loss": 0.4508,
      "step": 51500
    },
    {
      "epoch": 4.228759631609979,
      "grad_norm": 0.1300562620162964,
      "learning_rate": 0.0004978854912166558,
      "loss": 0.4508,
      "step": 52000
    },
    {
      "epoch": 4.2694207819139205,
      "grad_norm": 0.14299774169921875,
      "learning_rate": 0.0004978651594014313,
      "loss": 0.4507,
      "step": 52500
    },
    {
      "epoch": 4.310081932217862,
      "grad_norm": 0.11311045289039612,
      "learning_rate": 0.0004978448275862069,
      "loss": 0.4502,
      "step": 53000
    },
    {
      "epoch": 4.350743082521804,
      "grad_norm": 0.13018986582756042,
      "learning_rate": 0.0004978244957709824,
      "loss": 0.4508,
      "step": 53500
    },
    {
      "epoch": 4.391404232825747,
      "grad_norm": 0.13365581631660461,
      "learning_rate": 0.000497804163955758,
      "loss": 0.4507,
      "step": 54000
    },
    {
      "epoch": 4.432065383129689,
      "grad_norm": 0.11755722016096115,
      "learning_rate": 0.0004977838321405335,
      "loss": 0.4501,
      "step": 54500
    },
    {
      "epoch": 4.472726533433631,
      "grad_norm": 0.12928612530231476,
      "learning_rate": 0.000497763500325309,
      "loss": 0.4496,
      "step": 55000
    },
    {
      "epoch": 4.513387683737573,
      "grad_norm": 0.11930295825004578,
      "learning_rate": 0.0004977431685100846,
      "loss": 0.4494,
      "step": 55500
    },
    {
      "epoch": 4.554048834041515,
      "grad_norm": 0.13037686049938202,
      "learning_rate": 0.0004977228366948601,
      "loss": 0.4493,
      "step": 56000
    },
    {
      "epoch": 4.5947099843454575,
      "grad_norm": 0.11779075115919113,
      "learning_rate": 0.0004977025048796356,
      "loss": 0.4497,
      "step": 56500
    },
    {
      "epoch": 4.635371134649399,
      "grad_norm": 0.10557811707258224,
      "learning_rate": 0.0004976821730644113,
      "loss": 0.4499,
      "step": 57000
    },
    {
      "epoch": 4.676032284953341,
      "grad_norm": 0.1148463785648346,
      "learning_rate": 0.0004976618412491868,
      "loss": 0.4491,
      "step": 57500
    },
    {
      "epoch": 4.716693435257284,
      "grad_norm": 0.13532279431819916,
      "learning_rate": 0.0004976415094339623,
      "loss": 0.4488,
      "step": 58000
    },
    {
      "epoch": 4.757354585561226,
      "grad_norm": 0.10845331847667694,
      "learning_rate": 0.0004976211776187378,
      "loss": 0.4486,
      "step": 58500
    },
    {
      "epoch": 4.798015735865167,
      "grad_norm": 0.1149093359708786,
      "learning_rate": 0.0004976008458035133,
      "loss": 0.4494,
      "step": 59000
    },
    {
      "epoch": 4.83867688616911,
      "grad_norm": 0.1259121596813202,
      "learning_rate": 0.000497580513988289,
      "loss": 0.4487,
      "step": 59500
    },
    {
      "epoch": 4.879338036473052,
      "grad_norm": 0.11271313577890396,
      "learning_rate": 0.0004975601821730645,
      "loss": 0.4481,
      "step": 60000
    },
    {
      "epoch": 4.919999186776994,
      "grad_norm": 0.11989408731460571,
      "learning_rate": 0.00049753985035784,
      "loss": 0.4478,
      "step": 60500
    },
    {
      "epoch": 4.960660337080936,
      "grad_norm": 0.11693480610847473,
      "learning_rate": 0.0004975195185426155,
      "loss": 0.448,
      "step": 61000
    },
    {
      "epoch": 5.001321487384878,
      "grad_norm": 0.11274531483650208,
      "learning_rate": 0.000497499186727391,
      "loss": 0.4485,
      "step": 61500
    },
    {
      "epoch": 5.04198263768882,
      "grad_norm": 0.11414124816656113,
      "learning_rate": 0.0004974788549121665,
      "loss": 0.4437,
      "step": 62000
    },
    {
      "epoch": 5.082643787992763,
      "grad_norm": 0.1211622953414917,
      "learning_rate": 0.0004974585230969422,
      "loss": 0.4446,
      "step": 62500
    },
    {
      "epoch": 5.123304938296704,
      "grad_norm": 0.12379593402147293,
      "learning_rate": 0.0004974381912817177,
      "loss": 0.4445,
      "step": 63000
    },
    {
      "epoch": 5.163966088600646,
      "grad_norm": 0.12115342915058136,
      "learning_rate": 0.0004974178594664932,
      "loss": 0.4449,
      "step": 63500
    },
    {
      "epoch": 5.204627238904589,
      "grad_norm": 0.11674907803535461,
      "learning_rate": 0.0004973975276512687,
      "loss": 0.4452,
      "step": 64000
    },
    {
      "epoch": 5.245288389208531,
      "grad_norm": 0.12502864003181458,
      "learning_rate": 0.0004973771958360442,
      "loss": 0.4448,
      "step": 64500
    },
    {
      "epoch": 5.2859495395124725,
      "grad_norm": 0.11657688766717911,
      "learning_rate": 0.0004973568640208199,
      "loss": 0.4446,
      "step": 65000
    },
    {
      "epoch": 5.326610689816415,
      "grad_norm": 0.11694400757551193,
      "learning_rate": 0.0004973365322055954,
      "loss": 0.4443,
      "step": 65500
    },
    {
      "epoch": 5.367271840120357,
      "grad_norm": 0.11388961225748062,
      "learning_rate": 0.0004973162003903709,
      "loss": 0.4436,
      "step": 66000
    },
    {
      "epoch": 5.407932990424299,
      "grad_norm": 0.09977695345878601,
      "learning_rate": 0.0004972958685751464,
      "loss": 0.4449,
      "step": 66500
    },
    {
      "epoch": 5.448594140728241,
      "grad_norm": 0.1229487732052803,
      "learning_rate": 0.0004972755367599219,
      "loss": 0.4444,
      "step": 67000
    },
    {
      "epoch": 5.489255291032183,
      "grad_norm": 0.11015788465738297,
      "learning_rate": 0.0004972552049446974,
      "loss": 0.4438,
      "step": 67500
    },
    {
      "epoch": 5.529916441336125,
      "grad_norm": 0.10320780426263809,
      "learning_rate": 0.0004972348731294731,
      "loss": 0.4446,
      "step": 68000
    },
    {
      "epoch": 5.570577591640068,
      "grad_norm": 0.12923568487167358,
      "learning_rate": 0.0004972145413142486,
      "loss": 0.4439,
      "step": 68500
    },
    {
      "epoch": 5.6112387419440095,
      "grad_norm": 0.1073794737458229,
      "learning_rate": 0.0004971942094990241,
      "loss": 0.4437,
      "step": 69000
    },
    {
      "epoch": 5.651899892247951,
      "grad_norm": 0.11280898749828339,
      "learning_rate": 0.0004971738776837996,
      "loss": 0.4436,
      "step": 69500
    },
    {
      "epoch": 5.692561042551894,
      "grad_norm": 0.13132210075855255,
      "learning_rate": 0.0004971535458685751,
      "loss": 0.4442,
      "step": 70000
    },
    {
      "epoch": 5.733222192855836,
      "grad_norm": 0.12494178861379623,
      "learning_rate": 0.0004971332140533508,
      "loss": 0.4439,
      "step": 70500
    },
    {
      "epoch": 5.773883343159778,
      "grad_norm": 0.11286670714616776,
      "learning_rate": 0.0004971128822381263,
      "loss": 0.4434,
      "step": 71000
    },
    {
      "epoch": 5.81454449346372,
      "grad_norm": 0.11650262027978897,
      "learning_rate": 0.0004970925504229018,
      "loss": 0.4435,
      "step": 71500
    },
    {
      "epoch": 5.855205643767662,
      "grad_norm": 0.10874370485544205,
      "learning_rate": 0.0004970722186076773,
      "loss": 0.4431,
      "step": 72000
    },
    {
      "epoch": 5.895866794071605,
      "grad_norm": 0.11177099496126175,
      "learning_rate": 0.0004970518867924528,
      "loss": 0.4432,
      "step": 72500
    },
    {
      "epoch": 5.9365279443755465,
      "grad_norm": 0.10938670486211777,
      "learning_rate": 0.0004970315549772283,
      "loss": 0.4431,
      "step": 73000
    },
    {
      "epoch": 5.977189094679488,
      "grad_norm": 0.1177130714058876,
      "learning_rate": 0.000497011223162004,
      "loss": 0.4428,
      "step": 73500
    },
    {
      "epoch": 6.017850244983431,
      "grad_norm": 0.12016227096319199,
      "learning_rate": 0.0004969908913467795,
      "loss": 0.4405,
      "step": 74000
    },
    {
      "epoch": 6.058511395287373,
      "grad_norm": 0.11889824271202087,
      "learning_rate": 0.000496970559531555,
      "loss": 0.4392,
      "step": 74500
    },
    {
      "epoch": 6.099172545591315,
      "grad_norm": 0.12657710909843445,
      "learning_rate": 0.0004969502277163305,
      "loss": 0.4398,
      "step": 75000
    },
    {
      "epoch": 6.139833695895257,
      "grad_norm": 0.10360235720872879,
      "learning_rate": 0.000496929895901106,
      "loss": 0.4398,
      "step": 75500
    },
    {
      "epoch": 6.180494846199199,
      "grad_norm": 0.11890801787376404,
      "learning_rate": 0.0004969095640858817,
      "loss": 0.4398,
      "step": 76000
    },
    {
      "epoch": 6.221155996503141,
      "grad_norm": 0.11770696192979813,
      "learning_rate": 0.0004968892322706572,
      "loss": 0.44,
      "step": 76500
    },
    {
      "epoch": 6.261817146807084,
      "grad_norm": 0.11513417214155197,
      "learning_rate": 0.0004968689004554327,
      "loss": 0.4391,
      "step": 77000
    },
    {
      "epoch": 6.302478297111025,
      "grad_norm": 0.13554424047470093,
      "learning_rate": 0.0004968485686402082,
      "loss": 0.4399,
      "step": 77500
    },
    {
      "epoch": 6.343139447414967,
      "grad_norm": 0.1161181777715683,
      "learning_rate": 0.0004968282368249837,
      "loss": 0.4395,
      "step": 78000
    },
    {
      "epoch": 6.38380059771891,
      "grad_norm": 0.11436823010444641,
      "learning_rate": 0.0004968079050097593,
      "loss": 0.4403,
      "step": 78500
    },
    {
      "epoch": 6.424461748022852,
      "grad_norm": 0.11189858615398407,
      "learning_rate": 0.0004967875731945349,
      "loss": 0.4398,
      "step": 79000
    },
    {
      "epoch": 6.465122898326793,
      "grad_norm": 0.13024422526359558,
      "learning_rate": 0.0004967672413793104,
      "loss": 0.4398,
      "step": 79500
    },
    {
      "epoch": 6.505784048630736,
      "grad_norm": 0.11976958066225052,
      "learning_rate": 0.0004967469095640859,
      "loss": 0.4401,
      "step": 80000
    },
    {
      "epoch": 6.546445198934678,
      "grad_norm": 0.13054615259170532,
      "learning_rate": 0.0004967265777488614,
      "loss": 0.4398,
      "step": 80500
    },
    {
      "epoch": 6.58710634923862,
      "grad_norm": 0.11019480973482132,
      "learning_rate": 0.0004967062459336369,
      "loss": 0.4396,
      "step": 81000
    },
    {
      "epoch": 6.627767499542562,
      "grad_norm": 0.1134670302271843,
      "learning_rate": 0.0004966859141184125,
      "loss": 0.4402,
      "step": 81500
    },
    {
      "epoch": 6.668428649846504,
      "grad_norm": 0.10757160931825638,
      "learning_rate": 0.0004966655823031881,
      "loss": 0.439,
      "step": 82000
    },
    {
      "epoch": 6.709089800150446,
      "grad_norm": 0.1112813875079155,
      "learning_rate": 0.0004966452504879636,
      "loss": 0.4391,
      "step": 82500
    },
    {
      "epoch": 6.749750950454389,
      "grad_norm": 0.10619596391916275,
      "learning_rate": 0.0004966249186727391,
      "loss": 0.4393,
      "step": 83000
    },
    {
      "epoch": 6.7904121007583305,
      "grad_norm": 0.099098801612854,
      "learning_rate": 0.0004966045868575146,
      "loss": 0.4393,
      "step": 83500
    },
    {
      "epoch": 6.831073251062272,
      "grad_norm": 0.12034938484430313,
      "learning_rate": 0.0004965842550422902,
      "loss": 0.4391,
      "step": 84000
    },
    {
      "epoch": 6.871734401366215,
      "grad_norm": 0.1142091453075409,
      "learning_rate": 0.0004965639232270658,
      "loss": 0.4388,
      "step": 84500
    },
    {
      "epoch": 6.912395551670157,
      "grad_norm": 0.10176819562911987,
      "learning_rate": 0.0004965435914118413,
      "loss": 0.4389,
      "step": 85000
    },
    {
      "epoch": 6.9530567019740985,
      "grad_norm": 0.1252099573612213,
      "learning_rate": 0.0004965232595966168,
      "loss": 0.4393,
      "step": 85500
    },
    {
      "epoch": 6.993717852278041,
      "grad_norm": 0.10410419851541519,
      "learning_rate": 0.0004965029277813923,
      "loss": 0.4389,
      "step": 86000
    },
    {
      "epoch": 7.034379002581983,
      "grad_norm": 0.11432410031557083,
      "learning_rate": 0.0004964825959661678,
      "loss": 0.435,
      "step": 86500
    },
    {
      "epoch": 7.075040152885925,
      "grad_norm": 0.10315091162919998,
      "learning_rate": 0.0004964622641509434,
      "loss": 0.4353,
      "step": 87000
    },
    {
      "epoch": 7.1157013031898675,
      "grad_norm": 0.10634006559848785,
      "learning_rate": 0.000496441932335719,
      "loss": 0.436,
      "step": 87500
    },
    {
      "epoch": 7.156362453493809,
      "grad_norm": 0.1245163083076477,
      "learning_rate": 0.0004964216005204945,
      "loss": 0.4354,
      "step": 88000
    },
    {
      "epoch": 7.197023603797751,
      "grad_norm": 0.11303935199975967,
      "learning_rate": 0.00049640126870527,
      "loss": 0.4365,
      "step": 88500
    },
    {
      "epoch": 7.237684754101694,
      "grad_norm": 0.10360915213823318,
      "learning_rate": 0.0004963809368900455,
      "loss": 0.4356,
      "step": 89000
    },
    {
      "epoch": 7.278345904405636,
      "grad_norm": 0.10940296202898026,
      "learning_rate": 0.0004963606050748211,
      "loss": 0.4355,
      "step": 89500
    },
    {
      "epoch": 7.319007054709577,
      "grad_norm": 0.1102888360619545,
      "learning_rate": 0.0004963402732595967,
      "loss": 0.4361,
      "step": 90000
    },
    {
      "epoch": 7.35966820501352,
      "grad_norm": 0.10817267745733261,
      "learning_rate": 0.0004963199414443722,
      "loss": 0.4357,
      "step": 90500
    },
    {
      "epoch": 7.400329355317462,
      "grad_norm": 0.11569657176733017,
      "learning_rate": 0.0004962996096291477,
      "loss": 0.4354,
      "step": 91000
    },
    {
      "epoch": 7.440990505621404,
      "grad_norm": 0.10868166387081146,
      "learning_rate": 0.0004962792778139232,
      "loss": 0.4359,
      "step": 91500
    },
    {
      "epoch": 7.481651655925346,
      "grad_norm": 0.11054500937461853,
      "learning_rate": 0.0004962589459986987,
      "loss": 0.4361,
      "step": 92000
    },
    {
      "epoch": 7.522312806229288,
      "grad_norm": 0.10198114067316055,
      "learning_rate": 0.0004962386141834743,
      "loss": 0.4363,
      "step": 92500
    },
    {
      "epoch": 7.562973956533231,
      "grad_norm": 0.10496341437101364,
      "learning_rate": 0.0004962182823682499,
      "loss": 0.4364,
      "step": 93000
    },
    {
      "epoch": 7.603635106837173,
      "grad_norm": 0.10286951065063477,
      "learning_rate": 0.0004961979505530254,
      "loss": 0.4361,
      "step": 93500
    },
    {
      "epoch": 7.644296257141114,
      "grad_norm": 0.10336124151945114,
      "learning_rate": 0.0004961776187378009,
      "loss": 0.4355,
      "step": 94000
    },
    {
      "epoch": 7.684957407445056,
      "grad_norm": 0.12498342990875244,
      "learning_rate": 0.0004961572869225764,
      "loss": 0.4358,
      "step": 94500
    },
    {
      "epoch": 7.725618557748999,
      "grad_norm": 0.09745609760284424,
      "learning_rate": 0.000496136955107352,
      "loss": 0.4352,
      "step": 95000
    },
    {
      "epoch": 7.766279708052941,
      "grad_norm": 0.10767543315887451,
      "learning_rate": 0.0004961166232921276,
      "loss": 0.4359,
      "step": 95500
    },
    {
      "epoch": 7.806940858356883,
      "grad_norm": 0.10920374095439911,
      "learning_rate": 0.0004960962914769031,
      "loss": 0.4361,
      "step": 96000
    },
    {
      "epoch": 7.847602008660825,
      "grad_norm": 0.1127408891916275,
      "learning_rate": 0.0004960759596616786,
      "loss": 0.4355,
      "step": 96500
    },
    {
      "epoch": 7.888263158964767,
      "grad_norm": 0.10670078545808792,
      "learning_rate": 0.0004960556278464541,
      "loss": 0.4352,
      "step": 97000
    },
    {
      "epoch": 7.928924309268709,
      "grad_norm": 0.12056296318769455,
      "learning_rate": 0.0004960352960312296,
      "loss": 0.4356,
      "step": 97500
    },
    {
      "epoch": 7.969585459572651,
      "grad_norm": 0.10861450433731079,
      "learning_rate": 0.0004960149642160052,
      "loss": 0.4354,
      "step": 98000
    },
    {
      "epoch": 8.010246609876594,
      "grad_norm": 0.10955146700143814,
      "learning_rate": 0.0004959946324007808,
      "loss": 0.4344,
      "step": 98500
    },
    {
      "epoch": 8.050907760180536,
      "grad_norm": 0.12215941399335861,
      "learning_rate": 0.0004959743005855563,
      "loss": 0.4314,
      "step": 99000
    },
    {
      "epoch": 8.091568910484478,
      "grad_norm": 0.10745657235383987,
      "learning_rate": 0.0004959539687703318,
      "loss": 0.4319,
      "step": 99500
    },
    {
      "epoch": 8.13223006078842,
      "grad_norm": 0.11510854959487915,
      "learning_rate": 0.0004959336369551073,
      "loss": 0.4319,
      "step": 100000
    },
    {
      "epoch": 8.172891211092361,
      "grad_norm": 0.10763733834028244,
      "learning_rate": 0.0004959133051398829,
      "loss": 0.4326,
      "step": 100500
    },
    {
      "epoch": 8.213552361396303,
      "grad_norm": 0.0986662209033966,
      "learning_rate": 0.0004958929733246584,
      "loss": 0.4325,
      "step": 101000
    },
    {
      "epoch": 8.254213511700247,
      "grad_norm": 0.10055758059024811,
      "learning_rate": 0.000495872641509434,
      "loss": 0.4328,
      "step": 101500
    },
    {
      "epoch": 8.294874662004188,
      "grad_norm": 0.11140119284391403,
      "learning_rate": 0.0004958523096942095,
      "loss": 0.4326,
      "step": 102000
    },
    {
      "epoch": 8.33553581230813,
      "grad_norm": 0.10836227983236313,
      "learning_rate": 0.000495831977878985,
      "loss": 0.4331,
      "step": 102500
    },
    {
      "epoch": 8.376196962612072,
      "grad_norm": 0.10658354312181473,
      "learning_rate": 0.0004958116460637605,
      "loss": 0.4327,
      "step": 103000
    },
    {
      "epoch": 8.416858112916014,
      "grad_norm": 0.10360255092382431,
      "learning_rate": 0.0004957913142485361,
      "loss": 0.4325,
      "step": 103500
    },
    {
      "epoch": 8.457519263219957,
      "grad_norm": 0.10652085393667221,
      "learning_rate": 0.0004957709824333117,
      "loss": 0.4332,
      "step": 104000
    },
    {
      "epoch": 8.4981804135239,
      "grad_norm": 0.12264808267354965,
      "learning_rate": 0.0004957506506180872,
      "loss": 0.4327,
      "step": 104500
    },
    {
      "epoch": 8.538841563827841,
      "grad_norm": 0.10255347192287445,
      "learning_rate": 0.0004957303188028627,
      "loss": 0.4326,
      "step": 105000
    },
    {
      "epoch": 8.579502714131783,
      "grad_norm": 0.1309748739004135,
      "learning_rate": 0.0004957099869876382,
      "loss": 0.4328,
      "step": 105500
    },
    {
      "epoch": 8.620163864435725,
      "grad_norm": 0.10883399844169617,
      "learning_rate": 0.0004956896551724138,
      "loss": 0.4328,
      "step": 106000
    },
    {
      "epoch": 8.660825014739666,
      "grad_norm": 0.10340609401464462,
      "learning_rate": 0.0004956693233571893,
      "loss": 0.4331,
      "step": 106500
    },
    {
      "epoch": 8.701486165043608,
      "grad_norm": 0.11330083012580872,
      "learning_rate": 0.0004956489915419649,
      "loss": 0.4326,
      "step": 107000
    },
    {
      "epoch": 8.742147315347552,
      "grad_norm": 0.10380294173955917,
      "learning_rate": 0.0004956286597267404,
      "loss": 0.4324,
      "step": 107500
    },
    {
      "epoch": 8.782808465651494,
      "grad_norm": 0.10355594754219055,
      "learning_rate": 0.0004956083279115159,
      "loss": 0.4329,
      "step": 108000
    },
    {
      "epoch": 8.823469615955435,
      "grad_norm": 0.10323372483253479,
      "learning_rate": 0.0004955879960962914,
      "loss": 0.4327,
      "step": 108500
    },
    {
      "epoch": 8.864130766259377,
      "grad_norm": 0.11585895717144012,
      "learning_rate": 0.000495567664281067,
      "loss": 0.4329,
      "step": 109000
    },
    {
      "epoch": 8.904791916563319,
      "grad_norm": 0.09119106084108353,
      "learning_rate": 0.0004955473324658426,
      "loss": 0.4327,
      "step": 109500
    },
    {
      "epoch": 8.945453066867262,
      "grad_norm": 0.12235744297504425,
      "learning_rate": 0.0004955270006506181,
      "loss": 0.4329,
      "step": 110000
    },
    {
      "epoch": 8.986114217171204,
      "grad_norm": 0.12625978887081146,
      "learning_rate": 0.0004955066688353936,
      "loss": 0.4325,
      "step": 110500
    },
    {
      "epoch": 9.026775367475146,
      "grad_norm": 0.11385050415992737,
      "learning_rate": 0.0004954863370201691,
      "loss": 0.4297,
      "step": 111000
    },
    {
      "epoch": 9.067436517779088,
      "grad_norm": 0.10161738842725754,
      "learning_rate": 0.0004954660052049447,
      "loss": 0.4291,
      "step": 111500
    },
    {
      "epoch": 9.10809766808303,
      "grad_norm": 0.11173569411039352,
      "learning_rate": 0.0004954456733897202,
      "loss": 0.429,
      "step": 112000
    },
    {
      "epoch": 9.148758818386971,
      "grad_norm": 0.09806609153747559,
      "learning_rate": 0.0004954253415744958,
      "loss": 0.4292,
      "step": 112500
    },
    {
      "epoch": 9.189419968690915,
      "grad_norm": 0.11399625241756439,
      "learning_rate": 0.0004954050097592713,
      "loss": 0.4296,
      "step": 113000
    },
    {
      "epoch": 9.230081118994857,
      "grad_norm": 0.10606830567121506,
      "learning_rate": 0.0004953846779440468,
      "loss": 0.4299,
      "step": 113500
    },
    {
      "epoch": 9.270742269298799,
      "grad_norm": 0.11006039381027222,
      "learning_rate": 0.0004953643461288224,
      "loss": 0.4299,
      "step": 114000
    },
    {
      "epoch": 9.31140341960274,
      "grad_norm": 0.10196885466575623,
      "learning_rate": 0.0004953440143135979,
      "loss": 0.4303,
      "step": 114500
    },
    {
      "epoch": 9.352064569906682,
      "grad_norm": 0.12181632220745087,
      "learning_rate": 0.0004953236824983735,
      "loss": 0.4297,
      "step": 115000
    },
    {
      "epoch": 9.392725720210624,
      "grad_norm": 0.11586597561836243,
      "learning_rate": 0.0004953033506831491,
      "loss": 0.4304,
      "step": 115500
    },
    {
      "epoch": 9.433386870514568,
      "grad_norm": 0.10114433616399765,
      "learning_rate": 0.0004952830188679246,
      "loss": 0.4299,
      "step": 116000
    },
    {
      "epoch": 9.47404802081851,
      "grad_norm": 0.12480611354112625,
      "learning_rate": 0.0004952626870527001,
      "loss": 0.4299,
      "step": 116500
    },
    {
      "epoch": 9.514709171122451,
      "grad_norm": 0.09250401705503464,
      "learning_rate": 0.0004952423552374756,
      "loss": 0.4303,
      "step": 117000
    },
    {
      "epoch": 9.555370321426393,
      "grad_norm": 0.09017285704612732,
      "learning_rate": 0.0004952220234222511,
      "loss": 0.4304,
      "step": 117500
    },
    {
      "epoch": 9.596031471730335,
      "grad_norm": 0.09815777838230133,
      "learning_rate": 0.0004952016916070267,
      "loss": 0.4295,
      "step": 118000
    },
    {
      "epoch": 9.636692622034277,
      "grad_norm": 0.0947328433394432,
      "learning_rate": 0.0004951813597918023,
      "loss": 0.4307,
      "step": 118500
    },
    {
      "epoch": 9.67735377233822,
      "grad_norm": 0.1031947210431099,
      "learning_rate": 0.0004951610279765778,
      "loss": 0.4302,
      "step": 119000
    },
    {
      "epoch": 9.718014922642162,
      "grad_norm": 0.1070941686630249,
      "learning_rate": 0.0004951406961613533,
      "loss": 0.4311,
      "step": 119500
    },
    {
      "epoch": 9.758676072946104,
      "grad_norm": 0.1060430109500885,
      "learning_rate": 0.0004951203643461288,
      "loss": 0.4304,
      "step": 120000
    },
    {
      "epoch": 9.799337223250046,
      "grad_norm": 0.11220379173755646,
      "learning_rate": 0.0004951000325309043,
      "loss": 0.4291,
      "step": 120500
    },
    {
      "epoch": 9.839998373553987,
      "grad_norm": 0.10914801061153412,
      "learning_rate": 0.00049507970071568,
      "loss": 0.4291,
      "step": 121000
    },
    {
      "epoch": 9.88065952385793,
      "grad_norm": 0.12345198541879654,
      "learning_rate": 0.0004950593689004555,
      "loss": 0.4297,
      "step": 121500
    },
    {
      "epoch": 9.921320674161873,
      "grad_norm": 0.10003925859928131,
      "learning_rate": 0.000495039037085231,
      "loss": 0.4298,
      "step": 122000
    },
    {
      "epoch": 9.961981824465814,
      "grad_norm": 0.09698808938264847,
      "learning_rate": 0.0004950187052700065,
      "loss": 0.4306,
      "step": 122500
    },
    {
      "epoch": 10.002642974769756,
      "grad_norm": 0.10954813659191132,
      "learning_rate": 0.000494998373454782,
      "loss": 0.4293,
      "step": 123000
    },
    {
      "epoch": 10.043304125073698,
      "grad_norm": 0.09936698526144028,
      "learning_rate": 0.0004949780416395576,
      "loss": 0.4264,
      "step": 123500
    },
    {
      "epoch": 10.08396527537764,
      "grad_norm": 0.10444498062133789,
      "learning_rate": 0.0004949577098243332,
      "loss": 0.4265,
      "step": 124000
    },
    {
      "epoch": 10.124626425681582,
      "grad_norm": 0.10931270569562912,
      "learning_rate": 0.0004949373780091087,
      "loss": 0.4265,
      "step": 124500
    },
    {
      "epoch": 10.165287575985525,
      "grad_norm": 0.11956574767827988,
      "learning_rate": 0.0004949170461938842,
      "loss": 0.4266,
      "step": 125000
    },
    {
      "epoch": 10.205948726289467,
      "grad_norm": 0.10470316559076309,
      "learning_rate": 0.0004948967143786597,
      "loss": 0.4271,
      "step": 125500
    },
    {
      "epoch": 10.246609876593409,
      "grad_norm": 0.1116630882024765,
      "learning_rate": 0.0004948763825634352,
      "loss": 0.4275,
      "step": 126000
    },
    {
      "epoch": 10.28727102689735,
      "grad_norm": 0.11120101064443588,
      "learning_rate": 0.0004948560507482109,
      "loss": 0.4274,
      "step": 126500
    },
    {
      "epoch": 10.327932177201292,
      "grad_norm": 0.11334138363599777,
      "learning_rate": 0.0004948357189329864,
      "loss": 0.4276,
      "step": 127000
    },
    {
      "epoch": 10.368593327505234,
      "grad_norm": 0.11203379184007645,
      "learning_rate": 0.0004948153871177619,
      "loss": 0.4282,
      "step": 127500
    },
    {
      "epoch": 10.409254477809178,
      "grad_norm": 0.10586849600076675,
      "learning_rate": 0.0004947950553025374,
      "loss": 0.4278,
      "step": 128000
    },
    {
      "epoch": 10.44991562811312,
      "grad_norm": 0.10872996598482132,
      "learning_rate": 0.0004947747234873129,
      "loss": 0.4276,
      "step": 128500
    },
    {
      "epoch": 10.490576778417061,
      "grad_norm": 0.11406950652599335,
      "learning_rate": 0.0004947543916720886,
      "loss": 0.4271,
      "step": 129000
    },
    {
      "epoch": 10.531237928721003,
      "grad_norm": 0.12409369647502899,
      "learning_rate": 0.0004947340598568641,
      "loss": 0.4281,
      "step": 129500
    },
    {
      "epoch": 10.571899079024945,
      "grad_norm": 0.1097634956240654,
      "learning_rate": 0.0004947137280416396,
      "loss": 0.4282,
      "step": 130000
    },
    {
      "epoch": 10.612560229328889,
      "grad_norm": 0.10517030209302902,
      "learning_rate": 0.0004946933962264151,
      "loss": 0.4279,
      "step": 130500
    },
    {
      "epoch": 10.65322137963283,
      "grad_norm": 0.10477179288864136,
      "learning_rate": 0.0004946730644111906,
      "loss": 0.4274,
      "step": 131000
    },
    {
      "epoch": 10.693882529936772,
      "grad_norm": 0.1014254242181778,
      "learning_rate": 0.0004946527325959661,
      "loss": 0.4268,
      "step": 131500
    },
    {
      "epoch": 10.734543680240714,
      "grad_norm": 0.10733803361654282,
      "learning_rate": 0.0004946324007807418,
      "loss": 0.428,
      "step": 132000
    },
    {
      "epoch": 10.775204830544656,
      "grad_norm": 0.11494232714176178,
      "learning_rate": 0.0004946120689655173,
      "loss": 0.4281,
      "step": 132500
    },
    {
      "epoch": 10.815865980848598,
      "grad_norm": 0.11886844784021378,
      "learning_rate": 0.0004945917371502928,
      "loss": 0.4276,
      "step": 133000
    },
    {
      "epoch": 10.856527131152541,
      "grad_norm": 0.09965606033802032,
      "learning_rate": 0.0004945714053350683,
      "loss": 0.4275,
      "step": 133500
    },
    {
      "epoch": 10.897188281456483,
      "grad_norm": 0.13317154347896576,
      "learning_rate": 0.0004945510735198438,
      "loss": 0.4278,
      "step": 134000
    },
    {
      "epoch": 10.937849431760425,
      "grad_norm": 0.11352197825908661,
      "learning_rate": 0.0004945307417046195,
      "loss": 0.4278,
      "step": 134500
    },
    {
      "epoch": 10.978510582064366,
      "grad_norm": 0.12010519206523895,
      "learning_rate": 0.000494510409889395,
      "loss": 0.4278,
      "step": 135000
    },
    {
      "epoch": 11.019171732368308,
      "grad_norm": 0.10727857798337936,
      "learning_rate": 0.0004944900780741705,
      "loss": 0.4255,
      "step": 135500
    },
    {
      "epoch": 11.05983288267225,
      "grad_norm": 0.10308265686035156,
      "learning_rate": 0.000494469746258946,
      "loss": 0.4243,
      "step": 136000
    },
    {
      "epoch": 11.100494032976194,
      "grad_norm": 0.10983289778232574,
      "learning_rate": 0.0004944494144437215,
      "loss": 0.4247,
      "step": 136500
    },
    {
      "epoch": 11.141155183280135,
      "grad_norm": 0.12185952067375183,
      "learning_rate": 0.000494429082628497,
      "loss": 0.4249,
      "step": 137000
    },
    {
      "epoch": 11.181816333584077,
      "grad_norm": 0.11113791167736053,
      "learning_rate": 0.0004944087508132727,
      "loss": 0.4253,
      "step": 137500
    },
    {
      "epoch": 11.222477483888019,
      "grad_norm": 0.1131310984492302,
      "learning_rate": 0.0004943884189980482,
      "loss": 0.4249,
      "step": 138000
    },
    {
      "epoch": 11.26313863419196,
      "grad_norm": 0.10922422260046005,
      "learning_rate": 0.0004943680871828237,
      "loss": 0.4252,
      "step": 138500
    },
    {
      "epoch": 11.303799784495903,
      "grad_norm": 0.10368145257234573,
      "learning_rate": 0.0004943477553675992,
      "loss": 0.4244,
      "step": 139000
    },
    {
      "epoch": 11.344460934799846,
      "grad_norm": 0.11536439508199692,
      "learning_rate": 0.0004943274235523747,
      "loss": 0.4256,
      "step": 139500
    },
    {
      "epoch": 11.385122085103788,
      "grad_norm": 0.10020996630191803,
      "learning_rate": 0.0004943070917371504,
      "loss": 0.4247,
      "step": 140000
    },
    {
      "epoch": 11.42578323540773,
      "grad_norm": 0.10624200105667114,
      "learning_rate": 0.0004942867599219259,
      "loss": 0.4254,
      "step": 140500
    },
    {
      "epoch": 11.466444385711672,
      "grad_norm": 0.11472848802804947,
      "learning_rate": 0.0004942664281067014,
      "loss": 0.4257,
      "step": 141000
    },
    {
      "epoch": 11.507105536015613,
      "grad_norm": 0.11000888049602509,
      "learning_rate": 0.0004942460962914769,
      "loss": 0.4256,
      "step": 141500
    },
    {
      "epoch": 11.547766686319555,
      "grad_norm": 0.11950813233852386,
      "learning_rate": 0.0004942257644762524,
      "loss": 0.4256,
      "step": 142000
    },
    {
      "epoch": 11.588427836623499,
      "grad_norm": 0.10455502569675446,
      "learning_rate": 0.0004942054326610279,
      "loss": 0.4255,
      "step": 142500
    },
    {
      "epoch": 11.62908898692744,
      "grad_norm": 0.1025424674153328,
      "learning_rate": 0.0004941851008458036,
      "loss": 0.4259,
      "step": 143000
    },
    {
      "epoch": 11.669750137231382,
      "grad_norm": 0.11041691154241562,
      "learning_rate": 0.0004941647690305791,
      "loss": 0.4253,
      "step": 143500
    },
    {
      "epoch": 11.710411287535324,
      "grad_norm": 0.10864188522100449,
      "learning_rate": 0.0004941444372153546,
      "loss": 0.426,
      "step": 144000
    },
    {
      "epoch": 11.751072437839266,
      "grad_norm": 0.09860215336084366,
      "learning_rate": 0.0004941241054001301,
      "loss": 0.4251,
      "step": 144500
    },
    {
      "epoch": 11.79173358814321,
      "grad_norm": 0.10680732876062393,
      "learning_rate": 0.0004941037735849056,
      "loss": 0.4255,
      "step": 145000
    },
    {
      "epoch": 11.832394738447151,
      "grad_norm": 0.10855472832918167,
      "learning_rate": 0.0004940834417696813,
      "loss": 0.4258,
      "step": 145500
    },
    {
      "epoch": 11.873055888751093,
      "grad_norm": 0.10571878403425217,
      "learning_rate": 0.0004940631099544568,
      "loss": 0.4252,
      "step": 146000
    },
    {
      "epoch": 11.913717039055035,
      "grad_norm": 0.11715884506702423,
      "learning_rate": 0.0004940427781392323,
      "loss": 0.4264,
      "step": 146500
    },
    {
      "epoch": 11.954378189358977,
      "grad_norm": 0.11658122390508652,
      "learning_rate": 0.0004940224463240078,
      "loss": 0.4258,
      "step": 147000
    },
    {
      "epoch": 11.995039339662918,
      "grad_norm": 0.1172405257821083,
      "learning_rate": 0.0004940021145087833,
      "loss": 0.4257,
      "step": 147500
    },
    {
      "epoch": 12.035700489966862,
      "grad_norm": 0.11309589445590973,
      "learning_rate": 0.0004939817826935588,
      "loss": 0.4219,
      "step": 148000
    },
    {
      "epoch": 12.076361640270804,
      "grad_norm": 0.10686273127794266,
      "learning_rate": 0.0004939614508783345,
      "loss": 0.4222,
      "step": 148500
    },
    {
      "epoch": 12.117022790574746,
      "grad_norm": 0.10383626818656921,
      "learning_rate": 0.00049394111906311,
      "loss": 0.4226,
      "step": 149000
    },
    {
      "epoch": 12.157683940878687,
      "grad_norm": 0.10846976935863495,
      "learning_rate": 0.0004939207872478855,
      "loss": 0.4227,
      "step": 149500
    },
    {
      "epoch": 12.19834509118263,
      "grad_norm": 0.10718560963869095,
      "learning_rate": 0.000493900455432661,
      "loss": 0.4227,
      "step": 150000
    },
    {
      "epoch": 12.239006241486571,
      "grad_norm": 0.10170914232730865,
      "learning_rate": 0.0004938801236174365,
      "loss": 0.4232,
      "step": 150500
    },
    {
      "epoch": 12.279667391790515,
      "grad_norm": 0.10082782804965973,
      "learning_rate": 0.0004938597918022122,
      "loss": 0.4234,
      "step": 151000
    },
    {
      "epoch": 12.320328542094456,
      "grad_norm": 0.10296519845724106,
      "learning_rate": 0.0004938394599869877,
      "loss": 0.4239,
      "step": 151500
    },
    {
      "epoch": 12.360989692398398,
      "grad_norm": 0.10066158324480057,
      "learning_rate": 0.0004938191281717632,
      "loss": 0.4238,
      "step": 152000
    },
    {
      "epoch": 12.40165084270234,
      "grad_norm": 0.10090676695108414,
      "learning_rate": 0.0004937987963565387,
      "loss": 0.4235,
      "step": 152500
    },
    {
      "epoch": 12.442311993006282,
      "grad_norm": 0.11070090532302856,
      "learning_rate": 0.0004937784645413142,
      "loss": 0.4236,
      "step": 153000
    },
    {
      "epoch": 12.482973143310224,
      "grad_norm": 0.10724471509456635,
      "learning_rate": 0.0004937581327260897,
      "loss": 0.4236,
      "step": 153500
    },
    {
      "epoch": 12.523634293614167,
      "grad_norm": 0.10428185015916824,
      "learning_rate": 0.0004937378009108654,
      "loss": 0.4242,
      "step": 154000
    },
    {
      "epoch": 12.564295443918109,
      "grad_norm": 0.11236604303121567,
      "learning_rate": 0.0004937174690956409,
      "loss": 0.4236,
      "step": 154500
    },
    {
      "epoch": 12.60495659422205,
      "grad_norm": 0.10619820654392242,
      "learning_rate": 0.0004936971372804164,
      "loss": 0.4234,
      "step": 155000
    },
    {
      "epoch": 12.645617744525993,
      "grad_norm": 0.10700272768735886,
      "learning_rate": 0.0004936768054651919,
      "loss": 0.4241,
      "step": 155500
    },
    {
      "epoch": 12.686278894829934,
      "grad_norm": 0.11815879493951797,
      "learning_rate": 0.0004936564736499674,
      "loss": 0.4237,
      "step": 156000
    },
    {
      "epoch": 12.726940045133876,
      "grad_norm": 0.11702511459589005,
      "learning_rate": 0.000493636141834743,
      "loss": 0.4237,
      "step": 156500
    },
    {
      "epoch": 12.76760119543782,
      "grad_norm": 0.10725228488445282,
      "learning_rate": 0.0004936158100195186,
      "loss": 0.423,
      "step": 157000
    },
    {
      "epoch": 12.808262345741761,
      "grad_norm": 0.10205312818288803,
      "learning_rate": 0.0004935954782042941,
      "loss": 0.4241,
      "step": 157500
    },
    {
      "epoch": 12.848923496045703,
      "grad_norm": 0.1057703047990799,
      "learning_rate": 0.0004935751463890696,
      "loss": 0.4238,
      "step": 158000
    },
    {
      "epoch": 12.889584646349645,
      "grad_norm": 0.10757807642221451,
      "learning_rate": 0.0004935548145738451,
      "loss": 0.4242,
      "step": 158500
    },
    {
      "epoch": 12.930245796653587,
      "grad_norm": 0.11353912949562073,
      "learning_rate": 0.0004935344827586206,
      "loss": 0.4236,
      "step": 159000
    },
    {
      "epoch": 12.970906946957529,
      "grad_norm": 0.11594543606042862,
      "learning_rate": 0.0004935141509433963,
      "loss": 0.424,
      "step": 159500
    },
    {
      "epoch": 13.011568097261472,
      "grad_norm": 0.11499164253473282,
      "learning_rate": 0.0004934938191281718,
      "loss": 0.4227,
      "step": 160000
    },
    {
      "epoch": 13.052229247565414,
      "grad_norm": 0.11297869682312012,
      "learning_rate": 0.0004934734873129473,
      "loss": 0.4196,
      "step": 160500
    },
    {
      "epoch": 13.092890397869356,
      "grad_norm": 0.10380573570728302,
      "learning_rate": 0.0004934531554977228,
      "loss": 0.4206,
      "step": 161000
    },
    {
      "epoch": 13.133551548173298,
      "grad_norm": 0.10712655633687973,
      "learning_rate": 0.0004934328236824983,
      "loss": 0.421,
      "step": 161500
    },
    {
      "epoch": 13.17421269847724,
      "grad_norm": 0.11449213325977325,
      "learning_rate": 0.000493412491867274,
      "loss": 0.4212,
      "step": 162000
    },
    {
      "epoch": 13.214873848781181,
      "grad_norm": 0.10457468777894974,
      "learning_rate": 0.0004933921600520495,
      "loss": 0.4214,
      "step": 162500
    },
    {
      "epoch": 13.255534999085125,
      "grad_norm": 0.11456211656332016,
      "learning_rate": 0.000493371828236825,
      "loss": 0.4211,
      "step": 163000
    },
    {
      "epoch": 13.296196149389067,
      "grad_norm": 0.11076699197292328,
      "learning_rate": 0.0004933514964216005,
      "loss": 0.4215,
      "step": 163500
    },
    {
      "epoch": 13.336857299693008,
      "grad_norm": 0.11725270003080368,
      "learning_rate": 0.000493331164606376,
      "loss": 0.4216,
      "step": 164000
    },
    {
      "epoch": 13.37751844999695,
      "grad_norm": 0.13216108083724976,
      "learning_rate": 0.0004933108327911516,
      "loss": 0.4222,
      "step": 164500
    },
    {
      "epoch": 13.418179600300892,
      "grad_norm": 0.11519351601600647,
      "learning_rate": 0.0004932905009759272,
      "loss": 0.4222,
      "step": 165000
    },
    {
      "epoch": 13.458840750604834,
      "grad_norm": 0.12107077240943909,
      "learning_rate": 0.0004932701691607027,
      "loss": 0.4217,
      "step": 165500
    },
    {
      "epoch": 13.499501900908777,
      "grad_norm": 0.11507555842399597,
      "learning_rate": 0.0004932498373454782,
      "loss": 0.4222,
      "step": 166000
    },
    {
      "epoch": 13.54016305121272,
      "grad_norm": 0.10507877171039581,
      "learning_rate": 0.0004932295055302537,
      "loss": 0.4225,
      "step": 166500
    },
    {
      "epoch": 13.580824201516661,
      "grad_norm": 0.10658847540616989,
      "learning_rate": 0.0004932091737150292,
      "loss": 0.4215,
      "step": 167000
    },
    {
      "epoch": 13.621485351820603,
      "grad_norm": 0.12420579791069031,
      "learning_rate": 0.0004931888418998048,
      "loss": 0.422,
      "step": 167500
    },
    {
      "epoch": 13.662146502124545,
      "grad_norm": 0.11269433796405792,
      "learning_rate": 0.0004931685100845804,
      "loss": 0.422,
      "step": 168000
    },
    {
      "epoch": 13.702807652428486,
      "grad_norm": 0.10154291987419128,
      "learning_rate": 0.0004931481782693559,
      "loss": 0.4213,
      "step": 168500
    },
    {
      "epoch": 13.74346880273243,
      "grad_norm": 0.13146252930164337,
      "learning_rate": 0.0004931278464541314,
      "loss": 0.4222,
      "step": 169000
    },
    {
      "epoch": 13.784129953036372,
      "grad_norm": 0.10392607748508453,
      "learning_rate": 0.0004931075146389069,
      "loss": 0.4218,
      "step": 169500
    },
    {
      "epoch": 13.824791103340313,
      "grad_norm": 0.1177564412355423,
      "learning_rate": 0.0004930871828236825,
      "loss": 0.4219,
      "step": 170000
    },
    {
      "epoch": 13.865452253644255,
      "grad_norm": 0.10700485855340958,
      "learning_rate": 0.000493066851008458,
      "loss": 0.4218,
      "step": 170500
    },
    {
      "epoch": 13.906113403948197,
      "grad_norm": 0.10787255316972733,
      "learning_rate": 0.0004930465191932336,
      "loss": 0.4226,
      "step": 171000
    },
    {
      "epoch": 13.94677455425214,
      "grad_norm": 0.11673352122306824,
      "learning_rate": 0.0004930261873780092,
      "loss": 0.4222,
      "step": 171500
    },
    {
      "epoch": 13.987435704556082,
      "grad_norm": 0.13983368873596191,
      "learning_rate": 0.0004930058555627847,
      "loss": 0.4226,
      "step": 172000
    },
    {
      "epoch": 14.028096854860024,
      "grad_norm": 0.12001468241214752,
      "learning_rate": 0.0004929855237475602,
      "loss": 0.4194,
      "step": 172500
    },
    {
      "epoch": 14.068758005163966,
      "grad_norm": 0.12213672697544098,
      "learning_rate": 0.0004929651919323357,
      "loss": 0.419,
      "step": 173000
    },
    {
      "epoch": 14.109419155467908,
      "grad_norm": 0.12151426821947098,
      "learning_rate": 0.0004929448601171113,
      "loss": 0.4186,
      "step": 173500
    },
    {
      "epoch": 14.15008030577185,
      "grad_norm": 0.09733441472053528,
      "learning_rate": 0.0004929245283018868,
      "loss": 0.4195,
      "step": 174000
    },
    {
      "epoch": 14.190741456075793,
      "grad_norm": 0.11293534189462662,
      "learning_rate": 0.0004929041964866624,
      "loss": 0.4201,
      "step": 174500
    },
    {
      "epoch": 14.231402606379735,
      "grad_norm": 0.11449959129095078,
      "learning_rate": 0.0004928838646714379,
      "loss": 0.4196,
      "step": 175000
    },
    {
      "epoch": 14.272063756683677,
      "grad_norm": 0.09883496910333633,
      "learning_rate": 0.0004928635328562134,
      "loss": 0.4197,
      "step": 175500
    },
    {
      "epoch": 14.312724906987619,
      "grad_norm": 0.12169615179300308,
      "learning_rate": 0.000492843201040989,
      "loss": 0.4201,
      "step": 176000
    },
    {
      "epoch": 14.35338605729156,
      "grad_norm": 0.11634638905525208,
      "learning_rate": 0.0004928228692257645,
      "loss": 0.4198,
      "step": 176500
    },
    {
      "epoch": 14.394047207595502,
      "grad_norm": 0.11075230687856674,
      "learning_rate": 0.0004928025374105401,
      "loss": 0.4201,
      "step": 177000
    },
    {
      "epoch": 14.434708357899446,
      "grad_norm": 0.12536689639091492,
      "learning_rate": 0.0004927822055953156,
      "loss": 0.4204,
      "step": 177500
    },
    {
      "epoch": 14.475369508203388,
      "grad_norm": 0.09493176639080048,
      "learning_rate": 0.0004927618737800911,
      "loss": 0.4205,
      "step": 178000
    },
    {
      "epoch": 14.51603065850733,
      "grad_norm": 0.11506402492523193,
      "learning_rate": 0.0004927415419648666,
      "loss": 0.4204,
      "step": 178500
    },
    {
      "epoch": 14.556691808811271,
      "grad_norm": 0.11528829485177994,
      "learning_rate": 0.0004927212101496422,
      "loss": 0.42,
      "step": 179000
    },
    {
      "epoch": 14.597352959115213,
      "grad_norm": 0.11061849445104599,
      "learning_rate": 0.0004927008783344178,
      "loss": 0.4204,
      "step": 179500
    },
    {
      "epoch": 14.638014109419155,
      "grad_norm": 0.10884998738765717,
      "learning_rate": 0.0004926805465191933,
      "loss": 0.4211,
      "step": 180000
    },
    {
      "epoch": 14.678675259723098,
      "grad_norm": 0.10493721067905426,
      "learning_rate": 0.0004926602147039688,
      "loss": 0.4204,
      "step": 180500
    },
    {
      "epoch": 14.71933641002704,
      "grad_norm": 0.1113685742020607,
      "learning_rate": 0.0004926398828887443,
      "loss": 0.4203,
      "step": 181000
    },
    {
      "epoch": 14.759997560330982,
      "grad_norm": 0.10777448862791061,
      "learning_rate": 0.0004926195510735198,
      "loss": 0.4207,
      "step": 181500
    },
    {
      "epoch": 14.800658710634924,
      "grad_norm": 0.11266008764505386,
      "learning_rate": 0.0004925992192582954,
      "loss": 0.4207,
      "step": 182000
    },
    {
      "epoch": 14.841319860938865,
      "grad_norm": 0.11259095370769501,
      "learning_rate": 0.000492578887443071,
      "loss": 0.4206,
      "step": 182500
    },
    {
      "epoch": 14.881981011242807,
      "grad_norm": 0.10975413024425507,
      "learning_rate": 0.0004925585556278465,
      "loss": 0.4204,
      "step": 183000
    },
    {
      "epoch": 14.92264216154675,
      "grad_norm": 0.1207125186920166,
      "learning_rate": 0.000492538223812622,
      "loss": 0.4206,
      "step": 183500
    },
    {
      "epoch": 14.963303311850693,
      "grad_norm": 0.09879135340452194,
      "learning_rate": 0.0004925178919973975,
      "loss": 0.4206,
      "step": 184000
    },
    {
      "epoch": 15.003964462154634,
      "grad_norm": 0.0999491885304451,
      "learning_rate": 0.000492497560182173,
      "loss": 0.4206,
      "step": 184500
    },
    {
      "epoch": 15.044625612458576,
      "grad_norm": 0.11211030930280685,
      "learning_rate": 0.0004924772283669487,
      "loss": 0.4169,
      "step": 185000
    },
    {
      "epoch": 15.085286762762518,
      "grad_norm": 0.10112523287534714,
      "learning_rate": 0.0004924568965517242,
      "loss": 0.4174,
      "step": 185500
    },
    {
      "epoch": 15.12594791306646,
      "grad_norm": 0.1091519296169281,
      "learning_rate": 0.0004924365647364997,
      "loss": 0.4174,
      "step": 186000
    },
    {
      "epoch": 15.166609063370403,
      "grad_norm": 0.10457146167755127,
      "learning_rate": 0.0004924162329212752,
      "loss": 0.4183,
      "step": 186500
    },
    {
      "epoch": 15.207270213674345,
      "grad_norm": 0.12348045408725739,
      "learning_rate": 0.0004923959011060507,
      "loss": 0.4178,
      "step": 187000
    },
    {
      "epoch": 15.247931363978287,
      "grad_norm": 0.14611469209194183,
      "learning_rate": 0.0004923755692908263,
      "loss": 0.4182,
      "step": 187500
    },
    {
      "epoch": 15.288592514282229,
      "grad_norm": 0.10858020186424255,
      "learning_rate": 0.0004923552374756019,
      "loss": 0.4185,
      "step": 188000
    },
    {
      "epoch": 15.32925366458617,
      "grad_norm": 0.11264913529157639,
      "learning_rate": 0.0004923349056603774,
      "loss": 0.4185,
      "step": 188500
    },
    {
      "epoch": 15.369914814890112,
      "grad_norm": 0.10373182594776154,
      "learning_rate": 0.0004923145738451529,
      "loss": 0.4185,
      "step": 189000
    },
    {
      "epoch": 15.410575965194056,
      "grad_norm": 0.11626571416854858,
      "learning_rate": 0.0004922942420299284,
      "loss": 0.4189,
      "step": 189500
    },
    {
      "epoch": 15.451237115497998,
      "grad_norm": 0.11881241202354431,
      "learning_rate": 0.000492273910214704,
      "loss": 0.4186,
      "step": 190000
    },
    {
      "epoch": 15.49189826580194,
      "grad_norm": 0.10249610245227814,
      "learning_rate": 0.0004922535783994796,
      "loss": 0.4194,
      "step": 190500
    },
    {
      "epoch": 15.532559416105881,
      "grad_norm": 0.12109971791505814,
      "learning_rate": 0.0004922332465842551,
      "loss": 0.4185,
      "step": 191000
    },
    {
      "epoch": 15.573220566409823,
      "grad_norm": 0.10645933449268341,
      "learning_rate": 0.0004922129147690306,
      "loss": 0.4193,
      "step": 191500
    },
    {
      "epoch": 15.613881716713767,
      "grad_norm": 0.11002834886312485,
      "learning_rate": 0.0004921925829538061,
      "loss": 0.4191,
      "step": 192000
    },
    {
      "epoch": 15.654542867017708,
      "grad_norm": 0.1173449382185936,
      "learning_rate": 0.0004921722511385816,
      "loss": 0.4187,
      "step": 192500
    },
    {
      "epoch": 15.69520401732165,
      "grad_norm": 0.11549808830022812,
      "learning_rate": 0.0004921519193233572,
      "loss": 0.419,
      "step": 193000
    },
    {
      "epoch": 15.735865167625592,
      "grad_norm": 0.10505618155002594,
      "learning_rate": 0.0004921315875081328,
      "loss": 0.4182,
      "step": 193500
    },
    {
      "epoch": 15.776526317929534,
      "grad_norm": 0.11237622052431107,
      "learning_rate": 0.0004921112556929083,
      "loss": 0.4193,
      "step": 194000
    },
    {
      "epoch": 15.817187468233476,
      "grad_norm": 0.11928604543209076,
      "learning_rate": 0.0004920909238776838,
      "loss": 0.4195,
      "step": 194500
    },
    {
      "epoch": 15.85784861853742,
      "grad_norm": 0.10364510118961334,
      "learning_rate": 0.0004920705920624593,
      "loss": 0.4197,
      "step": 195000
    },
    {
      "epoch": 15.898509768841361,
      "grad_norm": 0.12030990421772003,
      "learning_rate": 0.0004920502602472348,
      "loss": 0.4191,
      "step": 195500
    },
    {
      "epoch": 15.939170919145303,
      "grad_norm": 0.10937207192182541,
      "learning_rate": 0.0004920299284320105,
      "loss": 0.419,
      "step": 196000
    },
    {
      "epoch": 15.979832069449245,
      "grad_norm": 0.12937495112419128,
      "learning_rate": 0.000492009596616786,
      "loss": 0.4197,
      "step": 196500
    },
    {
      "epoch": 16.020493219753188,
      "grad_norm": 0.11371942609548569,
      "learning_rate": 0.0004919892648015615,
      "loss": 0.4172,
      "step": 197000
    },
    {
      "epoch": 16.061154370057128,
      "grad_norm": 0.11571323126554489,
      "learning_rate": 0.000491968932986337,
      "loss": 0.415,
      "step": 197500
    },
    {
      "epoch": 16.10181552036107,
      "grad_norm": 0.11726764589548111,
      "learning_rate": 0.0004919486011711125,
      "loss": 0.4163,
      "step": 198000
    },
    {
      "epoch": 16.142476670665012,
      "grad_norm": 0.10617057234048843,
      "learning_rate": 0.0004919282693558881,
      "loss": 0.4161,
      "step": 198500
    },
    {
      "epoch": 16.183137820968955,
      "grad_norm": 0.1190740317106247,
      "learning_rate": 0.0004919079375406637,
      "loss": 0.4161,
      "step": 199000
    },
    {
      "epoch": 16.2237989712729,
      "grad_norm": 0.11547718942165375,
      "learning_rate": 0.0004918876057254392,
      "loss": 0.417,
      "step": 199500
    },
    {
      "epoch": 16.26446012157684,
      "grad_norm": 0.12262813746929169,
      "learning_rate": 0.0004918672739102147,
      "loss": 0.4162,
      "step": 200000
    },
    {
      "epoch": 16.305121271880783,
      "grad_norm": 0.11519809812307358,
      "learning_rate": 0.0004918469420949902,
      "loss": 0.417,
      "step": 200500
    },
    {
      "epoch": 16.345782422184723,
      "grad_norm": 0.09977772831916809,
      "learning_rate": 0.0004918266102797657,
      "loss": 0.4172,
      "step": 201000
    },
    {
      "epoch": 16.386443572488666,
      "grad_norm": 0.13105303049087524,
      "learning_rate": 0.0004918062784645414,
      "loss": 0.4178,
      "step": 201500
    },
    {
      "epoch": 16.427104722792606,
      "grad_norm": 0.12847283482551575,
      "learning_rate": 0.0004917859466493169,
      "loss": 0.4175,
      "step": 202000
    },
    {
      "epoch": 16.46776587309655,
      "grad_norm": 0.10327736288309097,
      "learning_rate": 0.0004917656148340924,
      "loss": 0.4175,
      "step": 202500
    },
    {
      "epoch": 16.508427023400493,
      "grad_norm": 0.12829235196113586,
      "learning_rate": 0.0004917452830188679,
      "loss": 0.4172,
      "step": 203000
    },
    {
      "epoch": 16.549088173704433,
      "grad_norm": 0.13693943619728088,
      "learning_rate": 0.0004917249512036434,
      "loss": 0.4174,
      "step": 203500
    },
    {
      "epoch": 16.589749324008377,
      "grad_norm": 0.13096380233764648,
      "learning_rate": 0.000491704619388419,
      "loss": 0.418,
      "step": 204000
    },
    {
      "epoch": 16.630410474312317,
      "grad_norm": 0.1473420411348343,
      "learning_rate": 0.0004916842875731946,
      "loss": 0.4173,
      "step": 204500
    },
    {
      "epoch": 16.67107162461626,
      "grad_norm": 0.11201540380716324,
      "learning_rate": 0.0004916639557579701,
      "loss": 0.4179,
      "step": 205000
    },
    {
      "epoch": 16.711732774920204,
      "grad_norm": 0.12362091988325119,
      "learning_rate": 0.0004916436239427456,
      "loss": 0.418,
      "step": 205500
    },
    {
      "epoch": 16.752393925224144,
      "grad_norm": 0.11593713611364365,
      "learning_rate": 0.0004916232921275211,
      "loss": 0.4179,
      "step": 206000
    },
    {
      "epoch": 16.793055075528088,
      "grad_norm": 0.10897105932235718,
      "learning_rate": 0.0004916029603122966,
      "loss": 0.4185,
      "step": 206500
    },
    {
      "epoch": 16.833716225832028,
      "grad_norm": 0.11228257417678833,
      "learning_rate": 0.0004915826284970723,
      "loss": 0.4185,
      "step": 207000
    },
    {
      "epoch": 16.87437737613597,
      "grad_norm": 0.11371775716543198,
      "learning_rate": 0.0004915622966818478,
      "loss": 0.4176,
      "step": 207500
    },
    {
      "epoch": 16.915038526439915,
      "grad_norm": 0.10896144807338715,
      "learning_rate": 0.0004915419648666233,
      "loss": 0.4181,
      "step": 208000
    },
    {
      "epoch": 16.955699676743855,
      "grad_norm": 0.10772693157196045,
      "learning_rate": 0.0004915216330513988,
      "loss": 0.4175,
      "step": 208500
    },
    {
      "epoch": 16.9963608270478,
      "grad_norm": 0.11706867069005966,
      "learning_rate": 0.0004915013012361743,
      "loss": 0.4185,
      "step": 209000
    },
    {
      "epoch": 17.03702197735174,
      "grad_norm": 0.1016354113817215,
      "learning_rate": 0.0004914809694209498,
      "loss": 0.4138,
      "step": 209500
    },
    {
      "epoch": 17.077683127655682,
      "grad_norm": 0.12860643863677979,
      "learning_rate": 0.0004914606376057255,
      "loss": 0.4139,
      "step": 210000
    },
    {
      "epoch": 17.118344277959622,
      "grad_norm": 0.1248200461268425,
      "learning_rate": 0.000491440305790501,
      "loss": 0.4154,
      "step": 210500
    },
    {
      "epoch": 17.159005428263566,
      "grad_norm": 0.11839370429515839,
      "learning_rate": 0.0004914199739752765,
      "loss": 0.4151,
      "step": 211000
    },
    {
      "epoch": 17.19966657856751,
      "grad_norm": 0.09941811114549637,
      "learning_rate": 0.000491399642160052,
      "loss": 0.4153,
      "step": 211500
    },
    {
      "epoch": 17.24032772887145,
      "grad_norm": 0.128113254904747,
      "learning_rate": 0.0004913793103448275,
      "loss": 0.4151,
      "step": 212000
    },
    {
      "epoch": 17.280988879175393,
      "grad_norm": 0.11413142830133438,
      "learning_rate": 0.0004913589785296032,
      "loss": 0.4155,
      "step": 212500
    },
    {
      "epoch": 17.321650029479333,
      "grad_norm": 0.11509620398283005,
      "learning_rate": 0.0004913386467143787,
      "loss": 0.4161,
      "step": 213000
    },
    {
      "epoch": 17.362311179783276,
      "grad_norm": 0.11349500715732574,
      "learning_rate": 0.0004913183148991542,
      "loss": 0.4152,
      "step": 213500
    },
    {
      "epoch": 17.40297233008722,
      "grad_norm": 0.10104598850011826,
      "learning_rate": 0.0004912979830839297,
      "loss": 0.4158,
      "step": 214000
    },
    {
      "epoch": 17.44363348039116,
      "grad_norm": 0.11666278541088104,
      "learning_rate": 0.0004912776512687052,
      "loss": 0.4165,
      "step": 214500
    },
    {
      "epoch": 17.484294630695103,
      "grad_norm": 0.12281006574630737,
      "learning_rate": 0.0004912573194534809,
      "loss": 0.4161,
      "step": 215000
    },
    {
      "epoch": 17.524955780999043,
      "grad_norm": 0.1297619789838791,
      "learning_rate": 0.0004912369876382564,
      "loss": 0.4165,
      "step": 215500
    },
    {
      "epoch": 17.565616931302987,
      "grad_norm": 0.10708378255367279,
      "learning_rate": 0.0004912166558230319,
      "loss": 0.4157,
      "step": 216000
    },
    {
      "epoch": 17.606278081606927,
      "grad_norm": 0.1346481889486313,
      "learning_rate": 0.0004911963240078074,
      "loss": 0.4165,
      "step": 216500
    },
    {
      "epoch": 17.64693923191087,
      "grad_norm": 0.12218735367059708,
      "learning_rate": 0.0004911759921925829,
      "loss": 0.4162,
      "step": 217000
    },
    {
      "epoch": 17.687600382214814,
      "grad_norm": 0.10612179338932037,
      "learning_rate": 0.0004911556603773584,
      "loss": 0.4177,
      "step": 217500
    },
    {
      "epoch": 17.728261532518754,
      "grad_norm": 0.12176513671875,
      "learning_rate": 0.0004911353285621341,
      "loss": 0.4166,
      "step": 218000
    },
    {
      "epoch": 17.768922682822698,
      "grad_norm": 0.10531945526599884,
      "learning_rate": 0.0004911149967469096,
      "loss": 0.4166,
      "step": 218500
    },
    {
      "epoch": 17.809583833126638,
      "grad_norm": 0.1185809001326561,
      "learning_rate": 0.0004910946649316851,
      "loss": 0.4172,
      "step": 219000
    },
    {
      "epoch": 17.85024498343058,
      "grad_norm": 0.1078280434012413,
      "learning_rate": 0.0004910743331164606,
      "loss": 0.4166,
      "step": 219500
    },
    {
      "epoch": 17.890906133734525,
      "grad_norm": 0.1285877376794815,
      "learning_rate": 0.0004910540013012361,
      "loss": 0.4166,
      "step": 220000
    },
    {
      "epoch": 17.931567284038465,
      "grad_norm": 0.11982041597366333,
      "learning_rate": 0.0004910336694860118,
      "loss": 0.4172,
      "step": 220500
    },
    {
      "epoch": 17.97222843434241,
      "grad_norm": 0.11790259182453156,
      "learning_rate": 0.0004910133376707873,
      "loss": 0.4171,
      "step": 221000
    },
    {
      "epoch": 18.01288958464635,
      "grad_norm": 0.11692608147859573,
      "learning_rate": 0.0004909930058555628,
      "loss": 0.4154,
      "step": 221500
    },
    {
      "epoch": 18.053550734950292,
      "grad_norm": 0.10514786094427109,
      "learning_rate": 0.0004909726740403383,
      "loss": 0.4131,
      "step": 222000
    },
    {
      "epoch": 18.094211885254232,
      "grad_norm": 0.1318414956331253,
      "learning_rate": 0.0004909523422251138,
      "loss": 0.4138,
      "step": 222500
    },
    {
      "epoch": 18.134873035558176,
      "grad_norm": 0.11621122807264328,
      "learning_rate": 0.0004909320104098893,
      "loss": 0.4134,
      "step": 223000
    },
    {
      "epoch": 18.17553418586212,
      "grad_norm": 0.12467748671770096,
      "learning_rate": 0.000490911678594665,
      "loss": 0.4138,
      "step": 223500
    },
    {
      "epoch": 18.21619533616606,
      "grad_norm": 0.11849214136600494,
      "learning_rate": 0.0004908913467794405,
      "loss": 0.414,
      "step": 224000
    },
    {
      "epoch": 18.256856486470003,
      "grad_norm": 0.11792681366205215,
      "learning_rate": 0.000490871014964216,
      "loss": 0.4143,
      "step": 224500
    },
    {
      "epoch": 18.297517636773943,
      "grad_norm": 0.11743050813674927,
      "learning_rate": 0.0004908506831489915,
      "loss": 0.4138,
      "step": 225000
    },
    {
      "epoch": 18.338178787077887,
      "grad_norm": 0.10880151391029358,
      "learning_rate": 0.000490830351333767,
      "loss": 0.4147,
      "step": 225500
    },
    {
      "epoch": 18.37883993738183,
      "grad_norm": 0.12771470844745636,
      "learning_rate": 0.0004908100195185427,
      "loss": 0.415,
      "step": 226000
    },
    {
      "epoch": 18.41950108768577,
      "grad_norm": 0.12862570583820343,
      "learning_rate": 0.0004907896877033182,
      "loss": 0.4151,
      "step": 226500
    },
    {
      "epoch": 18.460162237989714,
      "grad_norm": 0.1272791475057602,
      "learning_rate": 0.0004907693558880937,
      "loss": 0.4149,
      "step": 227000
    },
    {
      "epoch": 18.500823388293654,
      "grad_norm": 0.11731404066085815,
      "learning_rate": 0.0004907490240728693,
      "loss": 0.4151,
      "step": 227500
    },
    {
      "epoch": 18.541484538597597,
      "grad_norm": 0.11867586523294449,
      "learning_rate": 0.0004907286922576448,
      "loss": 0.4152,
      "step": 228000
    },
    {
      "epoch": 18.58214568890154,
      "grad_norm": 0.12627798318862915,
      "learning_rate": 0.0004907083604424203,
      "loss": 0.4156,
      "step": 228500
    },
    {
      "epoch": 18.62280683920548,
      "grad_norm": 0.11678796261548996,
      "learning_rate": 0.0004906880286271959,
      "loss": 0.415,
      "step": 229000
    },
    {
      "epoch": 18.663467989509424,
      "grad_norm": 0.10271506011486053,
      "learning_rate": 0.0004906676968119714,
      "loss": 0.4151,
      "step": 229500
    },
    {
      "epoch": 18.704129139813364,
      "grad_norm": 0.11676883697509766,
      "learning_rate": 0.0004906473649967469,
      "loss": 0.4154,
      "step": 230000
    },
    {
      "epoch": 18.744790290117308,
      "grad_norm": 0.1491989642381668,
      "learning_rate": 0.0004906270331815225,
      "loss": 0.4152,
      "step": 230500
    },
    {
      "epoch": 18.785451440421248,
      "grad_norm": 0.12373504787683487,
      "learning_rate": 0.000490606701366298,
      "loss": 0.416,
      "step": 231000
    },
    {
      "epoch": 18.82611259072519,
      "grad_norm": 0.11612068861722946,
      "learning_rate": 0.0004905863695510735,
      "loss": 0.4157,
      "step": 231500
    },
    {
      "epoch": 18.866773741029135,
      "grad_norm": 0.1187199130654335,
      "learning_rate": 0.0004905660377358491,
      "loss": 0.4156,
      "step": 232000
    },
    {
      "epoch": 18.907434891333075,
      "grad_norm": 0.1304585188627243,
      "learning_rate": 0.0004905457059206246,
      "loss": 0.4155,
      "step": 232500
    },
    {
      "epoch": 18.94809604163702,
      "grad_norm": 0.12650923430919647,
      "learning_rate": 0.0004905253741054002,
      "loss": 0.4161,
      "step": 233000
    },
    {
      "epoch": 18.98875719194096,
      "grad_norm": 0.12708887457847595,
      "learning_rate": 0.0004905050422901757,
      "loss": 0.4156,
      "step": 233500
    },
    {
      "epoch": 19.029418342244902,
      "grad_norm": 0.1185210719704628,
      "learning_rate": 0.0004904847104749512,
      "loss": 0.4127,
      "step": 234000
    },
    {
      "epoch": 19.070079492548846,
      "grad_norm": 0.10659652948379517,
      "learning_rate": 0.0004904643786597268,
      "loss": 0.4125,
      "step": 234500
    },
    {
      "epoch": 19.110740642852786,
      "grad_norm": 0.13378705084323883,
      "learning_rate": 0.0004904440468445023,
      "loss": 0.4121,
      "step": 235000
    },
    {
      "epoch": 19.15140179315673,
      "grad_norm": 0.1259683221578598,
      "learning_rate": 0.0004904237150292779,
      "loss": 0.4121,
      "step": 235500
    },
    {
      "epoch": 19.19206294346067,
      "grad_norm": 0.12012219429016113,
      "learning_rate": 0.0004904033832140534,
      "loss": 0.4126,
      "step": 236000
    },
    {
      "epoch": 19.232724093764613,
      "grad_norm": 0.13067184388637543,
      "learning_rate": 0.0004903830513988289,
      "loss": 0.4132,
      "step": 236500
    },
    {
      "epoch": 19.273385244068553,
      "grad_norm": 0.11549446731805801,
      "learning_rate": 0.0004903627195836044,
      "loss": 0.4142,
      "step": 237000
    },
    {
      "epoch": 19.314046394372497,
      "grad_norm": 0.1103065088391304,
      "learning_rate": 0.00049034238776838,
      "loss": 0.4132,
      "step": 237500
    },
    {
      "epoch": 19.35470754467644,
      "grad_norm": 0.1224728673696518,
      "learning_rate": 0.0004903220559531555,
      "loss": 0.4136,
      "step": 238000
    },
    {
      "epoch": 19.39536869498038,
      "grad_norm": 0.12643815577030182,
      "learning_rate": 0.0004903017241379311,
      "loss": 0.4132,
      "step": 238500
    },
    {
      "epoch": 19.436029845284324,
      "grad_norm": 0.1275215893983841,
      "learning_rate": 0.0004902813923227066,
      "loss": 0.4144,
      "step": 239000
    },
    {
      "epoch": 19.476690995588264,
      "grad_norm": 0.11007944494485855,
      "learning_rate": 0.0004902610605074821,
      "loss": 0.4135,
      "step": 239500
    },
    {
      "epoch": 19.517352145892207,
      "grad_norm": 0.11214018613100052,
      "learning_rate": 0.0004902407286922577,
      "loss": 0.4144,
      "step": 240000
    },
    {
      "epoch": 19.55801329619615,
      "grad_norm": 0.13147035241127014,
      "learning_rate": 0.0004902203968770332,
      "loss": 0.4139,
      "step": 240500
    },
    {
      "epoch": 19.59867444650009,
      "grad_norm": 0.11412743479013443,
      "learning_rate": 0.0004902000650618088,
      "loss": 0.4143,
      "step": 241000
    },
    {
      "epoch": 19.639335596804035,
      "grad_norm": 0.12668190896511078,
      "learning_rate": 0.0004901797332465843,
      "loss": 0.414,
      "step": 241500
    },
    {
      "epoch": 19.679996747107975,
      "grad_norm": 0.10194442421197891,
      "learning_rate": 0.0004901594014313598,
      "loss": 0.4142,
      "step": 242000
    },
    {
      "epoch": 19.720657897411918,
      "grad_norm": 0.14442026615142822,
      "learning_rate": 0.0004901390696161353,
      "loss": 0.4149,
      "step": 242500
    },
    {
      "epoch": 19.76131904771586,
      "grad_norm": 0.12393015623092651,
      "learning_rate": 0.0004901187378009109,
      "loss": 0.4146,
      "step": 243000
    },
    {
      "epoch": 19.801980198019802,
      "grad_norm": 0.124044269323349,
      "learning_rate": 0.0004900984059856864,
      "loss": 0.4142,
      "step": 243500
    },
    {
      "epoch": 19.842641348323745,
      "grad_norm": 0.10925986617803574,
      "learning_rate": 0.000490078074170462,
      "loss": 0.4147,
      "step": 244000
    },
    {
      "epoch": 19.883302498627685,
      "grad_norm": 0.13136403262615204,
      "learning_rate": 0.0004900577423552375,
      "loss": 0.4145,
      "step": 244500
    },
    {
      "epoch": 19.92396364893163,
      "grad_norm": 0.12532024085521698,
      "learning_rate": 0.000490037410540013,
      "loss": 0.4143,
      "step": 245000
    },
    {
      "epoch": 19.96462479923557,
      "grad_norm": 0.11818792670965195,
      "learning_rate": 0.0004900170787247886,
      "loss": 0.4151,
      "step": 245500
    },
    {
      "epoch": 20.005285949539513,
      "grad_norm": 0.11314447224140167,
      "learning_rate": 0.0004899967469095641,
      "loss": 0.4139,
      "step": 246000
    },
    {
      "epoch": 20.045947099843456,
      "grad_norm": 0.14054566621780396,
      "learning_rate": 0.0004899764150943397,
      "loss": 0.4111,
      "step": 246500
    },
    {
      "epoch": 20.086608250147396,
      "grad_norm": 0.10060432553291321,
      "learning_rate": 0.0004899560832791152,
      "loss": 0.4113,
      "step": 247000
    },
    {
      "epoch": 20.12726940045134,
      "grad_norm": 0.12694744765758514,
      "learning_rate": 0.0004899357514638907,
      "loss": 0.4109,
      "step": 247500
    },
    {
      "epoch": 20.16793055075528,
      "grad_norm": 0.12025726586580276,
      "learning_rate": 0.0004899154196486662,
      "loss": 0.4114,
      "step": 248000
    },
    {
      "epoch": 20.208591701059223,
      "grad_norm": 0.13606753945350647,
      "learning_rate": 0.0004898950878334418,
      "loss": 0.4114,
      "step": 248500
    },
    {
      "epoch": 20.249252851363163,
      "grad_norm": 0.1125359907746315,
      "learning_rate": 0.0004898747560182173,
      "loss": 0.4115,
      "step": 249000
    },
    {
      "epoch": 20.289914001667107,
      "grad_norm": 0.12973642349243164,
      "learning_rate": 0.0004898544242029929,
      "loss": 0.4118,
      "step": 249500
    },
    {
      "epoch": 20.33057515197105,
      "grad_norm": 0.13278500735759735,
      "learning_rate": 0.0004898340923877684,
      "loss": 0.4125,
      "step": 250000
    },
    {
      "epoch": 20.37123630227499,
      "grad_norm": 0.12891769409179688,
      "learning_rate": 0.0004898137605725439,
      "loss": 0.4128,
      "step": 250500
    },
    {
      "epoch": 20.411897452578934,
      "grad_norm": 0.1163693219423294,
      "learning_rate": 0.0004897934287573194,
      "loss": 0.4129,
      "step": 251000
    },
    {
      "epoch": 20.452558602882874,
      "grad_norm": 0.12473420053720474,
      "learning_rate": 0.000489773096942095,
      "loss": 0.413,
      "step": 251500
    },
    {
      "epoch": 20.493219753186818,
      "grad_norm": 0.12356653809547424,
      "learning_rate": 0.0004897527651268706,
      "loss": 0.4131,
      "step": 252000
    },
    {
      "epoch": 20.53388090349076,
      "grad_norm": 0.13572165369987488,
      "learning_rate": 0.0004897324333116461,
      "loss": 0.413,
      "step": 252500
    },
    {
      "epoch": 20.5745420537947,
      "grad_norm": 0.12922044098377228,
      "learning_rate": 0.0004897121014964216,
      "loss": 0.4136,
      "step": 253000
    },
    {
      "epoch": 20.615203204098645,
      "grad_norm": 0.11032864451408386,
      "learning_rate": 0.0004896917696811971,
      "loss": 0.4135,
      "step": 253500
    },
    {
      "epoch": 20.655864354402585,
      "grad_norm": 0.13431476056575775,
      "learning_rate": 0.0004896714378659727,
      "loss": 0.4139,
      "step": 254000
    },
    {
      "epoch": 20.69652550470653,
      "grad_norm": 0.13097427785396576,
      "learning_rate": 0.0004896511060507482,
      "loss": 0.4132,
      "step": 254500
    },
    {
      "epoch": 20.73718665501047,
      "grad_norm": 0.12230173498392105,
      "learning_rate": 0.0004896307742355238,
      "loss": 0.4135,
      "step": 255000
    },
    {
      "epoch": 20.777847805314412,
      "grad_norm": 0.1263759732246399,
      "learning_rate": 0.0004896104424202993,
      "loss": 0.4136,
      "step": 255500
    },
    {
      "epoch": 20.818508955618356,
      "grad_norm": 0.12437676638364792,
      "learning_rate": 0.0004895901106050748,
      "loss": 0.4132,
      "step": 256000
    },
    {
      "epoch": 20.859170105922296,
      "grad_norm": 0.13441036641597748,
      "learning_rate": 0.0004895697787898503,
      "loss": 0.414,
      "step": 256500
    },
    {
      "epoch": 20.89983125622624,
      "grad_norm": 0.12476766854524612,
      "learning_rate": 0.0004895494469746259,
      "loss": 0.4139,
      "step": 257000
    },
    {
      "epoch": 20.94049240653018,
      "grad_norm": 0.11899435520172119,
      "learning_rate": 0.0004895291151594015,
      "loss": 0.4136,
      "step": 257500
    },
    {
      "epoch": 20.981153556834123,
      "grad_norm": 0.13410575687885284,
      "learning_rate": 0.000489508783344177,
      "loss": 0.4132,
      "step": 258000
    },
    {
      "epoch": 21.021814707138066,
      "grad_norm": 0.11196871101856232,
      "learning_rate": 0.0004894884515289525,
      "loss": 0.4111,
      "step": 258500
    },
    {
      "epoch": 21.062475857442006,
      "grad_norm": 0.12142281234264374,
      "learning_rate": 0.000489468119713728,
      "loss": 0.4102,
      "step": 259000
    },
    {
      "epoch": 21.10313700774595,
      "grad_norm": 0.13731171190738678,
      "learning_rate": 0.0004894477878985036,
      "loss": 0.41,
      "step": 259500
    },
    {
      "epoch": 21.14379815804989,
      "grad_norm": 0.1317751556634903,
      "learning_rate": 0.0004894274560832791,
      "loss": 0.4112,
      "step": 260000
    },
    {
      "epoch": 21.184459308353834,
      "grad_norm": 0.13290898501873016,
      "learning_rate": 0.0004894071242680547,
      "loss": 0.4111,
      "step": 260500
    },
    {
      "epoch": 21.225120458657777,
      "grad_norm": 0.13634726405143738,
      "learning_rate": 0.0004893867924528302,
      "loss": 0.4105,
      "step": 261000
    },
    {
      "epoch": 21.265781608961717,
      "grad_norm": 0.12030765414237976,
      "learning_rate": 0.0004893664606376057,
      "loss": 0.4105,
      "step": 261500
    },
    {
      "epoch": 21.30644275926566,
      "grad_norm": 0.13150709867477417,
      "learning_rate": 0.0004893461288223812,
      "loss": 0.4116,
      "step": 262000
    },
    {
      "epoch": 21.3471039095696,
      "grad_norm": 0.13905976712703705,
      "learning_rate": 0.0004893257970071568,
      "loss": 0.4116,
      "step": 262500
    },
    {
      "epoch": 21.387765059873544,
      "grad_norm": 0.13785220682621002,
      "learning_rate": 0.0004893054651919324,
      "loss": 0.4116,
      "step": 263000
    },
    {
      "epoch": 21.428426210177484,
      "grad_norm": 0.1189553439617157,
      "learning_rate": 0.0004892851333767079,
      "loss": 0.412,
      "step": 263500
    },
    {
      "epoch": 21.469087360481428,
      "grad_norm": 0.11838001757860184,
      "learning_rate": 0.0004892648015614834,
      "loss": 0.4115,
      "step": 264000
    },
    {
      "epoch": 21.50974851078537,
      "grad_norm": 0.12696006894111633,
      "learning_rate": 0.0004892444697462589,
      "loss": 0.4123,
      "step": 264500
    },
    {
      "epoch": 21.55040966108931,
      "grad_norm": 0.11704373359680176,
      "learning_rate": 0.0004892241379310344,
      "loss": 0.4121,
      "step": 265000
    },
    {
      "epoch": 21.591070811393255,
      "grad_norm": 0.13963079452514648,
      "learning_rate": 0.0004892038061158101,
      "loss": 0.4122,
      "step": 265500
    },
    {
      "epoch": 21.631731961697195,
      "grad_norm": 0.12289882451295853,
      "learning_rate": 0.0004891834743005856,
      "loss": 0.4122,
      "step": 266000
    },
    {
      "epoch": 21.67239311200114,
      "grad_norm": 0.13605907559394836,
      "learning_rate": 0.0004891631424853611,
      "loss": 0.4121,
      "step": 266500
    },
    {
      "epoch": 21.713054262305082,
      "grad_norm": 0.13085533678531647,
      "learning_rate": 0.0004891428106701366,
      "loss": 0.4125,
      "step": 267000
    },
    {
      "epoch": 21.753715412609022,
      "grad_norm": 0.1280885636806488,
      "learning_rate": 0.0004891224788549121,
      "loss": 0.4129,
      "step": 267500
    },
    {
      "epoch": 21.794376562912966,
      "grad_norm": 0.11166353523731232,
      "learning_rate": 0.0004891021470396877,
      "loss": 0.4129,
      "step": 268000
    },
    {
      "epoch": 21.835037713216906,
      "grad_norm": 0.13126938045024872,
      "learning_rate": 0.0004890818152244633,
      "loss": 0.4125,
      "step": 268500
    },
    {
      "epoch": 21.87569886352085,
      "grad_norm": 0.1292203962802887,
      "learning_rate": 0.0004890614834092388,
      "loss": 0.4129,
      "step": 269000
    },
    {
      "epoch": 21.916360013824793,
      "grad_norm": 0.11020104587078094,
      "learning_rate": 0.0004890411515940143,
      "loss": 0.4128,
      "step": 269500
    },
    {
      "epoch": 21.957021164128733,
      "grad_norm": 0.12464983761310577,
      "learning_rate": 0.0004890208197787898,
      "loss": 0.4127,
      "step": 270000
    },
    {
      "epoch": 21.997682314432677,
      "grad_norm": 0.12488393485546112,
      "learning_rate": 0.0004890004879635653,
      "loss": 0.4129,
      "step": 270500
    },
    {
      "epoch": 22.038343464736617,
      "grad_norm": 0.12443198263645172,
      "learning_rate": 0.000488980156148341,
      "loss": 0.4085,
      "step": 271000
    },
    {
      "epoch": 22.07900461504056,
      "grad_norm": 0.12524032592773438,
      "learning_rate": 0.0004889598243331165,
      "loss": 0.4096,
      "step": 271500
    },
    {
      "epoch": 22.1196657653445,
      "grad_norm": 0.13649322092533112,
      "learning_rate": 0.000488939492517892,
      "loss": 0.4101,
      "step": 272000
    },
    {
      "epoch": 22.160326915648444,
      "grad_norm": 0.1290174275636673,
      "learning_rate": 0.0004889191607026675,
      "loss": 0.4101,
      "step": 272500
    },
    {
      "epoch": 22.200988065952387,
      "grad_norm": 0.1277499943971634,
      "learning_rate": 0.000488898828887443,
      "loss": 0.4098,
      "step": 273000
    },
    {
      "epoch": 22.241649216256327,
      "grad_norm": 0.12859699130058289,
      "learning_rate": 0.0004888784970722186,
      "loss": 0.4098,
      "step": 273500
    },
    {
      "epoch": 22.28231036656027,
      "grad_norm": 0.13110090792179108,
      "learning_rate": 0.0004888581652569942,
      "loss": 0.4099,
      "step": 274000
    },
    {
      "epoch": 22.32297151686421,
      "grad_norm": 0.13024957478046417,
      "learning_rate": 0.0004888378334417697,
      "loss": 0.4106,
      "step": 274500
    },
    {
      "epoch": 22.363632667168154,
      "grad_norm": 0.1362798660993576,
      "learning_rate": 0.0004888175016265452,
      "loss": 0.4104,
      "step": 275000
    },
    {
      "epoch": 22.404293817472098,
      "grad_norm": 0.12117386609315872,
      "learning_rate": 0.0004887971698113207,
      "loss": 0.4105,
      "step": 275500
    },
    {
      "epoch": 22.444954967776038,
      "grad_norm": 0.11914926022291183,
      "learning_rate": 0.0004887768379960962,
      "loss": 0.4112,
      "step": 276000
    },
    {
      "epoch": 22.48561611807998,
      "grad_norm": 0.1292656660079956,
      "learning_rate": 0.0004887565061808719,
      "loss": 0.4115,
      "step": 276500
    },
    {
      "epoch": 22.52627726838392,
      "grad_norm": 0.1224568709731102,
      "learning_rate": 0.0004887361743656474,
      "loss": 0.4113,
      "step": 277000
    },
    {
      "epoch": 22.566938418687865,
      "grad_norm": 0.12840551137924194,
      "learning_rate": 0.0004887158425504229,
      "loss": 0.4113,
      "step": 277500
    },
    {
      "epoch": 22.607599568991805,
      "grad_norm": 0.1285388469696045,
      "learning_rate": 0.0004886955107351984,
      "loss": 0.4111,
      "step": 278000
    },
    {
      "epoch": 22.64826071929575,
      "grad_norm": 0.1346258521080017,
      "learning_rate": 0.0004886751789199739,
      "loss": 0.4118,
      "step": 278500
    },
    {
      "epoch": 22.688921869599692,
      "grad_norm": 0.12899070978164673,
      "learning_rate": 0.0004886548471047495,
      "loss": 0.4116,
      "step": 279000
    },
    {
      "epoch": 22.729583019903632,
      "grad_norm": 0.12082679569721222,
      "learning_rate": 0.0004886345152895251,
      "loss": 0.4119,
      "step": 279500
    },
    {
      "epoch": 22.770244170207576,
      "grad_norm": 0.11824723333120346,
      "learning_rate": 0.0004886141834743006,
      "loss": 0.4109,
      "step": 280000
    },
    {
      "epoch": 22.810905320511516,
      "grad_norm": 0.11324293166399002,
      "learning_rate": 0.0004885938516590761,
      "loss": 0.4118,
      "step": 280500
    },
    {
      "epoch": 22.85156647081546,
      "grad_norm": 0.15071351826190948,
      "learning_rate": 0.0004885735198438516,
      "loss": 0.4111,
      "step": 281000
    },
    {
      "epoch": 22.892227621119403,
      "grad_norm": 0.11593768000602722,
      "learning_rate": 0.0004885531880286271,
      "loss": 0.4122,
      "step": 281500
    },
    {
      "epoch": 22.932888771423343,
      "grad_norm": 0.14468611776828766,
      "learning_rate": 0.0004885328562134028,
      "loss": 0.4118,
      "step": 282000
    },
    {
      "epoch": 22.973549921727287,
      "grad_norm": 0.1216440349817276,
      "learning_rate": 0.0004885125243981783,
      "loss": 0.4117,
      "step": 282500
    },
    {
      "epoch": 23.014211072031227,
      "grad_norm": 0.1294257491827011,
      "learning_rate": 0.0004884921925829538,
      "loss": 0.4102,
      "step": 283000
    },
    {
      "epoch": 23.05487222233517,
      "grad_norm": 0.12326114624738693,
      "learning_rate": 0.0004884718607677293,
      "loss": 0.4082,
      "step": 283500
    },
    {
      "epoch": 23.09553337263911,
      "grad_norm": 0.10824123024940491,
      "learning_rate": 0.0004884515289525048,
      "loss": 0.4084,
      "step": 284000
    },
    {
      "epoch": 23.136194522943054,
      "grad_norm": 0.12181728333234787,
      "learning_rate": 0.0004884311971372803,
      "loss": 0.4086,
      "step": 284500
    },
    {
      "epoch": 23.176855673246997,
      "grad_norm": 0.12347202003002167,
      "learning_rate": 0.000488410865322056,
      "loss": 0.4087,
      "step": 285000
    },
    {
      "epoch": 23.217516823550937,
      "grad_norm": 0.12130201607942581,
      "learning_rate": 0.0004883905335068315,
      "loss": 0.4098,
      "step": 285500
    },
    {
      "epoch": 23.25817797385488,
      "grad_norm": 0.12117187678813934,
      "learning_rate": 0.0004883702016916071,
      "loss": 0.4094,
      "step": 286000
    },
    {
      "epoch": 23.29883912415882,
      "grad_norm": 0.1269018054008484,
      "learning_rate": 0.0004883498698763826,
      "loss": 0.4094,
      "step": 286500
    },
    {
      "epoch": 23.339500274462765,
      "grad_norm": 0.13720710575580597,
      "learning_rate": 0.0004883295380611581,
      "loss": 0.41,
      "step": 287000
    },
    {
      "epoch": 23.380161424766708,
      "grad_norm": 0.15567025542259216,
      "learning_rate": 0.0004883092062459337,
      "loss": 0.4098,
      "step": 287500
    },
    {
      "epoch": 23.42082257507065,
      "grad_norm": 0.12139659374952316,
      "learning_rate": 0.0004882888744307092,
      "loss": 0.4097,
      "step": 288000
    },
    {
      "epoch": 23.461483725374592,
      "grad_norm": 0.13901720941066742,
      "learning_rate": 0.0004882685426154847,
      "loss": 0.4103,
      "step": 288500
    },
    {
      "epoch": 23.502144875678532,
      "grad_norm": 0.13329026103019714,
      "learning_rate": 0.00048824821080026027,
      "loss": 0.4104,
      "step": 289000
    },
    {
      "epoch": 23.542806025982475,
      "grad_norm": 0.13858814537525177,
      "learning_rate": 0.0004882278789850358,
      "loss": 0.4105,
      "step": 289500
    },
    {
      "epoch": 23.583467176286415,
      "grad_norm": 0.13396355509757996,
      "learning_rate": 0.0004882075471698113,
      "loss": 0.4104,
      "step": 290000
    },
    {
      "epoch": 23.62412832659036,
      "grad_norm": 0.14034228026866913,
      "learning_rate": 0.00048818721535458687,
      "loss": 0.4105,
      "step": 290500
    },
    {
      "epoch": 23.664789476894303,
      "grad_norm": 0.117889903485775,
      "learning_rate": 0.0004881668835393624,
      "loss": 0.411,
      "step": 291000
    },
    {
      "epoch": 23.705450627198243,
      "grad_norm": 0.16166076064109802,
      "learning_rate": 0.00048814655172413796,
      "loss": 0.411,
      "step": 291500
    },
    {
      "epoch": 23.746111777502186,
      "grad_norm": 0.12660998106002808,
      "learning_rate": 0.0004881262199089135,
      "loss": 0.4102,
      "step": 292000
    },
    {
      "epoch": 23.786772927806126,
      "grad_norm": 0.1256795972585678,
      "learning_rate": 0.000488105888093689,
      "loss": 0.4111,
      "step": 292500
    },
    {
      "epoch": 23.82743407811007,
      "grad_norm": 0.13403823971748352,
      "learning_rate": 0.00048808555627846456,
      "loss": 0.4108,
      "step": 293000
    },
    {
      "epoch": 23.868095228414013,
      "grad_norm": 0.13155391812324524,
      "learning_rate": 0.0004880652244632401,
      "loss": 0.4107,
      "step": 293500
    },
    {
      "epoch": 23.908756378717953,
      "grad_norm": 0.12476622313261032,
      "learning_rate": 0.0004880448926480156,
      "loss": 0.4114,
      "step": 294000
    },
    {
      "epoch": 23.949417529021897,
      "grad_norm": 0.12652038037776947,
      "learning_rate": 0.00048802456083279117,
      "loss": 0.4105,
      "step": 294500
    },
    {
      "epoch": 23.990078679325837,
      "grad_norm": 0.1250935047864914,
      "learning_rate": 0.0004880042290175667,
      "loss": 0.4114,
      "step": 295000
    },
    {
      "epoch": 24.03073982962978,
      "grad_norm": 0.11659380048513412,
      "learning_rate": 0.0004879838972023422,
      "loss": 0.4082,
      "step": 295500
    },
    {
      "epoch": 24.071400979933724,
      "grad_norm": 0.1281898468732834,
      "learning_rate": 0.00048796356538711777,
      "loss": 0.4072,
      "step": 296000
    },
    {
      "epoch": 24.112062130237664,
      "grad_norm": 0.12642700970172882,
      "learning_rate": 0.0004879432335718933,
      "loss": 0.4078,
      "step": 296500
    },
    {
      "epoch": 24.152723280541608,
      "grad_norm": 0.12972787022590637,
      "learning_rate": 0.00048792290175666886,
      "loss": 0.408,
      "step": 297000
    },
    {
      "epoch": 24.193384430845548,
      "grad_norm": 0.12485610693693161,
      "learning_rate": 0.0004879025699414444,
      "loss": 0.408,
      "step": 297500
    },
    {
      "epoch": 24.23404558114949,
      "grad_norm": 0.1211557611823082,
      "learning_rate": 0.0004878822381262199,
      "loss": 0.4086,
      "step": 298000
    },
    {
      "epoch": 24.27470673145343,
      "grad_norm": 0.13456495106220245,
      "learning_rate": 0.00048786190631099546,
      "loss": 0.409,
      "step": 298500
    },
    {
      "epoch": 24.315367881757375,
      "grad_norm": 0.13105501234531403,
      "learning_rate": 0.000487841574495771,
      "loss": 0.4084,
      "step": 299000
    },
    {
      "epoch": 24.35602903206132,
      "grad_norm": 0.12470059096813202,
      "learning_rate": 0.0004878212426805465,
      "loss": 0.4091,
      "step": 299500
    },
    {
      "epoch": 24.39669018236526,
      "grad_norm": 0.12478616088628769,
      "learning_rate": 0.00048780091086532206,
      "loss": 0.4096,
      "step": 300000
    },
    {
      "epoch": 24.437351332669202,
      "grad_norm": 0.13355353474617004,
      "learning_rate": 0.0004877805790500976,
      "loss": 0.4092,
      "step": 300500
    },
    {
      "epoch": 24.478012482973142,
      "grad_norm": 0.12682616710662842,
      "learning_rate": 0.00048776024723487315,
      "loss": 0.4091,
      "step": 301000
    },
    {
      "epoch": 24.518673633277086,
      "grad_norm": 0.1173628717660904,
      "learning_rate": 0.00048773991541964867,
      "loss": 0.4094,
      "step": 301500
    },
    {
      "epoch": 24.55933478358103,
      "grad_norm": 0.12194788455963135,
      "learning_rate": 0.0004877195836044242,
      "loss": 0.4095,
      "step": 302000
    },
    {
      "epoch": 24.59999593388497,
      "grad_norm": 0.1539883315563202,
      "learning_rate": 0.00048769925178919975,
      "loss": 0.4096,
      "step": 302500
    },
    {
      "epoch": 24.640657084188913,
      "grad_norm": 0.12082810699939728,
      "learning_rate": 0.00048767891997397527,
      "loss": 0.4097,
      "step": 303000
    },
    {
      "epoch": 24.681318234492853,
      "grad_norm": 0.14994028210639954,
      "learning_rate": 0.0004876585881587508,
      "loss": 0.4098,
      "step": 303500
    },
    {
      "epoch": 24.721979384796796,
      "grad_norm": 0.12853692471981049,
      "learning_rate": 0.00048763825634352636,
      "loss": 0.41,
      "step": 304000
    },
    {
      "epoch": 24.762640535100736,
      "grad_norm": 0.1347937136888504,
      "learning_rate": 0.0004876179245283019,
      "loss": 0.4108,
      "step": 304500
    },
    {
      "epoch": 24.80330168540468,
      "grad_norm": 0.12395906448364258,
      "learning_rate": 0.0004875975927130774,
      "loss": 0.4098,
      "step": 305000
    },
    {
      "epoch": 24.843962835708624,
      "grad_norm": 0.11078482121229172,
      "learning_rate": 0.00048757726089785296,
      "loss": 0.4105,
      "step": 305500
    },
    {
      "epoch": 24.884623986012564,
      "grad_norm": 0.1447550356388092,
      "learning_rate": 0.0004875569290826285,
      "loss": 0.41,
      "step": 306000
    },
    {
      "epoch": 24.925285136316507,
      "grad_norm": 0.13520468771457672,
      "learning_rate": 0.00048753659726740405,
      "loss": 0.4104,
      "step": 306500
    },
    {
      "epoch": 24.965946286620447,
      "grad_norm": 0.12513834238052368,
      "learning_rate": 0.00048751626545217956,
      "loss": 0.4103,
      "step": 307000
    },
    {
      "epoch": 25.00660743692439,
      "grad_norm": 0.1263395994901657,
      "learning_rate": 0.0004874959336369551,
      "loss": 0.4091,
      "step": 307500
    },
    {
      "epoch": 25.047268587228334,
      "grad_norm": 0.14301292598247528,
      "learning_rate": 0.00048747560182173065,
      "loss": 0.4058,
      "step": 308000
    },
    {
      "epoch": 25.087929737532274,
      "grad_norm": 0.1376182585954666,
      "learning_rate": 0.00048745527000650617,
      "loss": 0.4063,
      "step": 308500
    },
    {
      "epoch": 25.128590887836218,
      "grad_norm": 0.13261233270168304,
      "learning_rate": 0.0004874349381912817,
      "loss": 0.4076,
      "step": 309000
    },
    {
      "epoch": 25.169252038140158,
      "grad_norm": 0.1315813958644867,
      "learning_rate": 0.00048741460637605726,
      "loss": 0.4078,
      "step": 309500
    },
    {
      "epoch": 25.2099131884441,
      "grad_norm": 0.15289250016212463,
      "learning_rate": 0.00048739427456083277,
      "loss": 0.4069,
      "step": 310000
    },
    {
      "epoch": 25.25057433874804,
      "grad_norm": 0.15928606688976288,
      "learning_rate": 0.0004873739427456083,
      "loss": 0.4077,
      "step": 310500
    },
    {
      "epoch": 25.291235489051985,
      "grad_norm": 0.14549694955348969,
      "learning_rate": 0.00048735361093038386,
      "loss": 0.408,
      "step": 311000
    },
    {
      "epoch": 25.33189663935593,
      "grad_norm": 0.1267339587211609,
      "learning_rate": 0.0004873332791151594,
      "loss": 0.4086,
      "step": 311500
    },
    {
      "epoch": 25.37255778965987,
      "grad_norm": 0.12141621857881546,
      "learning_rate": 0.00048731294729993495,
      "loss": 0.4081,
      "step": 312000
    },
    {
      "epoch": 25.413218939963812,
      "grad_norm": 0.12896515429019928,
      "learning_rate": 0.00048729261548471046,
      "loss": 0.4087,
      "step": 312500
    },
    {
      "epoch": 25.453880090267752,
      "grad_norm": 0.11394578218460083,
      "learning_rate": 0.000487272283669486,
      "loss": 0.409,
      "step": 313000
    },
    {
      "epoch": 25.494541240571696,
      "grad_norm": 0.12919564545154572,
      "learning_rate": 0.0004872519518542616,
      "loss": 0.4087,
      "step": 313500
    },
    {
      "epoch": 25.53520239087564,
      "grad_norm": 0.14291895925998688,
      "learning_rate": 0.0004872316200390371,
      "loss": 0.4087,
      "step": 314000
    },
    {
      "epoch": 25.57586354117958,
      "grad_norm": 0.12978293001651764,
      "learning_rate": 0.00048721128822381264,
      "loss": 0.4091,
      "step": 314500
    },
    {
      "epoch": 25.616524691483523,
      "grad_norm": 0.1411805897951126,
      "learning_rate": 0.0004871909564085882,
      "loss": 0.4086,
      "step": 315000
    },
    {
      "epoch": 25.657185841787463,
      "grad_norm": 0.1254616379737854,
      "learning_rate": 0.0004871706245933637,
      "loss": 0.4096,
      "step": 315500
    },
    {
      "epoch": 25.697846992091407,
      "grad_norm": 0.12744899094104767,
      "learning_rate": 0.0004871502927781393,
      "loss": 0.409,
      "step": 316000
    },
    {
      "epoch": 25.738508142395347,
      "grad_norm": 0.1298975646495819,
      "learning_rate": 0.0004871299609629148,
      "loss": 0.4092,
      "step": 316500
    },
    {
      "epoch": 25.77916929269929,
      "grad_norm": 0.12158895283937454,
      "learning_rate": 0.0004871096291476903,
      "loss": 0.4094,
      "step": 317000
    },
    {
      "epoch": 25.819830443003234,
      "grad_norm": 0.13262897729873657,
      "learning_rate": 0.0004870892973324659,
      "loss": 0.4091,
      "step": 317500
    },
    {
      "epoch": 25.860491593307174,
      "grad_norm": 0.11537867784500122,
      "learning_rate": 0.0004870689655172414,
      "loss": 0.4095,
      "step": 318000
    },
    {
      "epoch": 25.901152743611117,
      "grad_norm": 0.1155732199549675,
      "learning_rate": 0.00048704863370201693,
      "loss": 0.4093,
      "step": 318500
    },
    {
      "epoch": 25.941813893915057,
      "grad_norm": 0.12081123143434525,
      "learning_rate": 0.0004870283018867925,
      "loss": 0.4091,
      "step": 319000
    },
    {
      "epoch": 25.982475044219,
      "grad_norm": 0.1377449929714203,
      "learning_rate": 0.000487007970071568,
      "loss": 0.4097,
      "step": 319500
    },
    {
      "epoch": 26.023136194522944,
      "grad_norm": 0.126063272356987,
      "learning_rate": 0.00048698763825634353,
      "loss": 0.4073,
      "step": 320000
    },
    {
      "epoch": 26.063797344826884,
      "grad_norm": 0.12450295686721802,
      "learning_rate": 0.0004869673064411191,
      "loss": 0.4053,
      "step": 320500
    },
    {
      "epoch": 26.104458495130828,
      "grad_norm": 0.13992595672607422,
      "learning_rate": 0.0004869469746258946,
      "loss": 0.4059,
      "step": 321000
    },
    {
      "epoch": 26.145119645434768,
      "grad_norm": 0.13564535975456238,
      "learning_rate": 0.0004869266428106702,
      "loss": 0.4065,
      "step": 321500
    },
    {
      "epoch": 26.18578079573871,
      "grad_norm": 0.13599777221679688,
      "learning_rate": 0.0004869063109954457,
      "loss": 0.4066,
      "step": 322000
    },
    {
      "epoch": 26.226441946042655,
      "grad_norm": 0.11355685442686081,
      "learning_rate": 0.0004868859791802212,
      "loss": 0.4069,
      "step": 322500
    },
    {
      "epoch": 26.267103096346595,
      "grad_norm": 0.14812183380126953,
      "learning_rate": 0.0004868656473649968,
      "loss": 0.4065,
      "step": 323000
    },
    {
      "epoch": 26.30776424665054,
      "grad_norm": 0.12915955483913422,
      "learning_rate": 0.0004868453155497723,
      "loss": 0.4068,
      "step": 323500
    },
    {
      "epoch": 26.34842539695448,
      "grad_norm": 0.13497315347194672,
      "learning_rate": 0.00048682498373454783,
      "loss": 0.4073,
      "step": 324000
    },
    {
      "epoch": 26.389086547258422,
      "grad_norm": 0.131048783659935,
      "learning_rate": 0.0004868046519193234,
      "loss": 0.4079,
      "step": 324500
    },
    {
      "epoch": 26.429747697562362,
      "grad_norm": 0.14204232394695282,
      "learning_rate": 0.0004867843201040989,
      "loss": 0.4081,
      "step": 325000
    },
    {
      "epoch": 26.470408847866306,
      "grad_norm": 0.12911008298397064,
      "learning_rate": 0.00048676398828887443,
      "loss": 0.4076,
      "step": 325500
    },
    {
      "epoch": 26.51106999817025,
      "grad_norm": 0.12825800478458405,
      "learning_rate": 0.00048674365647365,
      "loss": 0.4082,
      "step": 326000
    },
    {
      "epoch": 26.55173114847419,
      "grad_norm": 0.11037720739841461,
      "learning_rate": 0.0004867233246584255,
      "loss": 0.4078,
      "step": 326500
    },
    {
      "epoch": 26.592392298778133,
      "grad_norm": 0.11829787492752075,
      "learning_rate": 0.0004867029928432011,
      "loss": 0.4083,
      "step": 327000
    },
    {
      "epoch": 26.633053449082073,
      "grad_norm": 0.12591437995433807,
      "learning_rate": 0.0004866826610279766,
      "loss": 0.4078,
      "step": 327500
    },
    {
      "epoch": 26.673714599386017,
      "grad_norm": 0.12241195142269135,
      "learning_rate": 0.0004866623292127521,
      "loss": 0.4087,
      "step": 328000
    },
    {
      "epoch": 26.71437574968996,
      "grad_norm": 0.12371105700731277,
      "learning_rate": 0.0004866419973975277,
      "loss": 0.4086,
      "step": 328500
    },
    {
      "epoch": 26.7550368999939,
      "grad_norm": 0.13034845888614655,
      "learning_rate": 0.0004866216655823032,
      "loss": 0.4084,
      "step": 329000
    },
    {
      "epoch": 26.795698050297844,
      "grad_norm": 0.15722329914569855,
      "learning_rate": 0.0004866013337670787,
      "loss": 0.409,
      "step": 329500
    },
    {
      "epoch": 26.836359200601784,
      "grad_norm": 0.13884834945201874,
      "learning_rate": 0.0004865810019518543,
      "loss": 0.4088,
      "step": 330000
    },
    {
      "epoch": 26.877020350905727,
      "grad_norm": 0.1252242922782898,
      "learning_rate": 0.0004865606701366298,
      "loss": 0.4087,
      "step": 330500
    },
    {
      "epoch": 26.917681501209668,
      "grad_norm": 0.12695558369159698,
      "learning_rate": 0.00048654033832140533,
      "loss": 0.4093,
      "step": 331000
    },
    {
      "epoch": 26.95834265151361,
      "grad_norm": 0.13068822026252747,
      "learning_rate": 0.0004865200065061809,
      "loss": 0.409,
      "step": 331500
    },
    {
      "epoch": 26.999003801817555,
      "grad_norm": 0.12005939334630966,
      "learning_rate": 0.0004864996746909564,
      "loss": 0.4089,
      "step": 332000
    },
    {
      "epoch": 27.039664952121495,
      "grad_norm": 0.13367563486099243,
      "learning_rate": 0.000486479342875732,
      "loss": 0.4046,
      "step": 332500
    },
    {
      "epoch": 27.08032610242544,
      "grad_norm": 0.12593218684196472,
      "learning_rate": 0.0004864590110605075,
      "loss": 0.4052,
      "step": 333000
    },
    {
      "epoch": 27.12098725272938,
      "grad_norm": 0.11958394944667816,
      "learning_rate": 0.000486438679245283,
      "loss": 0.406,
      "step": 333500
    },
    {
      "epoch": 27.161648403033322,
      "grad_norm": 0.1241171583533287,
      "learning_rate": 0.0004864183474300586,
      "loss": 0.4053,
      "step": 334000
    },
    {
      "epoch": 27.202309553337265,
      "grad_norm": 0.1427636444568634,
      "learning_rate": 0.0004863980156148341,
      "loss": 0.4065,
      "step": 334500
    },
    {
      "epoch": 27.242970703641205,
      "grad_norm": 0.12013760209083557,
      "learning_rate": 0.0004863776837996096,
      "loss": 0.4056,
      "step": 335000
    },
    {
      "epoch": 27.28363185394515,
      "grad_norm": 0.13104823231697083,
      "learning_rate": 0.0004863573519843852,
      "loss": 0.4064,
      "step": 335500
    },
    {
      "epoch": 27.32429300424909,
      "grad_norm": 0.14655464887619019,
      "learning_rate": 0.0004863370201691607,
      "loss": 0.4066,
      "step": 336000
    },
    {
      "epoch": 27.364954154553033,
      "grad_norm": 0.12470754981040955,
      "learning_rate": 0.0004863166883539363,
      "loss": 0.4071,
      "step": 336500
    },
    {
      "epoch": 27.405615304856973,
      "grad_norm": 0.1591389924287796,
      "learning_rate": 0.0004862963565387118,
      "loss": 0.4069,
      "step": 337000
    },
    {
      "epoch": 27.446276455160916,
      "grad_norm": 0.1331019401550293,
      "learning_rate": 0.0004862760247234873,
      "loss": 0.4074,
      "step": 337500
    },
    {
      "epoch": 27.48693760546486,
      "grad_norm": 0.12366823107004166,
      "learning_rate": 0.0004862556929082629,
      "loss": 0.4071,
      "step": 338000
    },
    {
      "epoch": 27.5275987557688,
      "grad_norm": 0.14387881755828857,
      "learning_rate": 0.0004862353610930384,
      "loss": 0.4068,
      "step": 338500
    },
    {
      "epoch": 27.568259906072743,
      "grad_norm": 0.13552875816822052,
      "learning_rate": 0.0004862150292778139,
      "loss": 0.4073,
      "step": 339000
    },
    {
      "epoch": 27.608921056376683,
      "grad_norm": 0.15294040739536285,
      "learning_rate": 0.0004861946974625895,
      "loss": 0.4072,
      "step": 339500
    },
    {
      "epoch": 27.649582206680627,
      "grad_norm": 0.1210726872086525,
      "learning_rate": 0.000486174365647365,
      "loss": 0.4076,
      "step": 340000
    },
    {
      "epoch": 27.69024335698457,
      "grad_norm": 0.16320721805095673,
      "learning_rate": 0.0004861540338321405,
      "loss": 0.4075,
      "step": 340500
    },
    {
      "epoch": 27.73090450728851,
      "grad_norm": 0.1379321813583374,
      "learning_rate": 0.0004861337020169161,
      "loss": 0.4074,
      "step": 341000
    },
    {
      "epoch": 27.771565657592454,
      "grad_norm": 0.11866503953933716,
      "learning_rate": 0.0004861133702016916,
      "loss": 0.4081,
      "step": 341500
    },
    {
      "epoch": 27.812226807896394,
      "grad_norm": 0.1406702846288681,
      "learning_rate": 0.0004860930383864672,
      "loss": 0.4083,
      "step": 342000
    },
    {
      "epoch": 27.852887958200338,
      "grad_norm": 0.13271167874336243,
      "learning_rate": 0.0004860727065712427,
      "loss": 0.4085,
      "step": 342500
    },
    {
      "epoch": 27.89354910850428,
      "grad_norm": 0.14501474797725677,
      "learning_rate": 0.0004860523747560182,
      "loss": 0.4082,
      "step": 343000
    },
    {
      "epoch": 27.93421025880822,
      "grad_norm": 0.13683347404003143,
      "learning_rate": 0.0004860320429407938,
      "loss": 0.4084,
      "step": 343500
    },
    {
      "epoch": 27.974871409112165,
      "grad_norm": 0.15920640528202057,
      "learning_rate": 0.0004860117111255693,
      "loss": 0.4082,
      "step": 344000
    },
    {
      "epoch": 28.015532559416105,
      "grad_norm": 0.1381518840789795,
      "learning_rate": 0.0004859913793103448,
      "loss": 0.4066,
      "step": 344500
    },
    {
      "epoch": 28.05619370972005,
      "grad_norm": 0.14507514238357544,
      "learning_rate": 0.0004859710474951204,
      "loss": 0.404,
      "step": 345000
    },
    {
      "epoch": 28.09685486002399,
      "grad_norm": 0.12861716747283936,
      "learning_rate": 0.0004859507156798959,
      "loss": 0.4047,
      "step": 345500
    },
    {
      "epoch": 28.137516010327932,
      "grad_norm": 0.14182494580745697,
      "learning_rate": 0.0004859303838646714,
      "loss": 0.405,
      "step": 346000
    },
    {
      "epoch": 28.178177160631876,
      "grad_norm": 0.12844951450824738,
      "learning_rate": 0.000485910052049447,
      "loss": 0.4051,
      "step": 346500
    },
    {
      "epoch": 28.218838310935816,
      "grad_norm": 0.12574473023414612,
      "learning_rate": 0.0004858897202342225,
      "loss": 0.4053,
      "step": 347000
    },
    {
      "epoch": 28.25949946123976,
      "grad_norm": 0.14158231019973755,
      "learning_rate": 0.0004858693884189981,
      "loss": 0.4056,
      "step": 347500
    },
    {
      "epoch": 28.3001606115437,
      "grad_norm": 0.14389477670192719,
      "learning_rate": 0.0004858490566037736,
      "loss": 0.4056,
      "step": 348000
    },
    {
      "epoch": 28.340821761847643,
      "grad_norm": 0.14064979553222656,
      "learning_rate": 0.0004858287247885491,
      "loss": 0.4058,
      "step": 348500
    },
    {
      "epoch": 28.381482912151586,
      "grad_norm": 0.12579767405986786,
      "learning_rate": 0.0004858083929733247,
      "loss": 0.4065,
      "step": 349000
    },
    {
      "epoch": 28.422144062455526,
      "grad_norm": 0.11901675164699554,
      "learning_rate": 0.0004857880611581002,
      "loss": 0.4063,
      "step": 349500
    },
    {
      "epoch": 28.46280521275947,
      "grad_norm": 0.14544592797756195,
      "learning_rate": 0.0004857677293428757,
      "loss": 0.4062,
      "step": 350000
    },
    {
      "epoch": 28.50346636306341,
      "grad_norm": 0.14058810472488403,
      "learning_rate": 0.0004857473975276513,
      "loss": 0.4067,
      "step": 350500
    },
    {
      "epoch": 28.544127513367354,
      "grad_norm": 0.1352531462907791,
      "learning_rate": 0.0004857270657124268,
      "loss": 0.4065,
      "step": 351000
    },
    {
      "epoch": 28.584788663671294,
      "grad_norm": 0.11905726790428162,
      "learning_rate": 0.00048570673389720237,
      "loss": 0.4064,
      "step": 351500
    },
    {
      "epoch": 28.625449813975237,
      "grad_norm": 0.1588539034128189,
      "learning_rate": 0.0004856864020819779,
      "loss": 0.4072,
      "step": 352000
    },
    {
      "epoch": 28.66611096427918,
      "grad_norm": 0.1342964619398117,
      "learning_rate": 0.0004856660702667534,
      "loss": 0.407,
      "step": 352500
    },
    {
      "epoch": 28.70677211458312,
      "grad_norm": 0.15788258612155914,
      "learning_rate": 0.000485645738451529,
      "loss": 0.4078,
      "step": 353000
    },
    {
      "epoch": 28.747433264887064,
      "grad_norm": 0.12601682543754578,
      "learning_rate": 0.0004856254066363045,
      "loss": 0.4074,
      "step": 353500
    },
    {
      "epoch": 28.788094415191004,
      "grad_norm": 0.12975192070007324,
      "learning_rate": 0.00048560507482108,
      "loss": 0.4068,
      "step": 354000
    },
    {
      "epoch": 28.828755565494948,
      "grad_norm": 0.11966285854578018,
      "learning_rate": 0.0004855847430058556,
      "loss": 0.4075,
      "step": 354500
    },
    {
      "epoch": 28.86941671579889,
      "grad_norm": 0.1594919115304947,
      "learning_rate": 0.0004855644111906311,
      "loss": 0.4075,
      "step": 355000
    },
    {
      "epoch": 28.91007786610283,
      "grad_norm": 0.12126564979553223,
      "learning_rate": 0.0004855440793754066,
      "loss": 0.4077,
      "step": 355500
    },
    {
      "epoch": 28.950739016406775,
      "grad_norm": 0.13400280475616455,
      "learning_rate": 0.0004855237475601822,
      "loss": 0.4083,
      "step": 356000
    },
    {
      "epoch": 28.991400166710715,
      "grad_norm": 0.11412263661623001,
      "learning_rate": 0.0004855034157449577,
      "loss": 0.4072,
      "step": 356500
    },
    {
      "epoch": 29.03206131701466,
      "grad_norm": 0.1270247846841812,
      "learning_rate": 0.00048548308392973327,
      "loss": 0.4044,
      "step": 357000
    },
    {
      "epoch": 29.0727224673186,
      "grad_norm": 0.12413392961025238,
      "learning_rate": 0.0004854627521145088,
      "loss": 0.4039,
      "step": 357500
    },
    {
      "epoch": 29.113383617622542,
      "grad_norm": 0.13880732655525208,
      "learning_rate": 0.0004854424202992843,
      "loss": 0.404,
      "step": 358000
    },
    {
      "epoch": 29.154044767926486,
      "grad_norm": 0.18098905682563782,
      "learning_rate": 0.00048542208848405987,
      "loss": 0.4046,
      "step": 358500
    },
    {
      "epoch": 29.194705918230426,
      "grad_norm": 0.14174306392669678,
      "learning_rate": 0.0004854017566688354,
      "loss": 0.4039,
      "step": 359000
    },
    {
      "epoch": 29.23536706853437,
      "grad_norm": 0.15444429218769073,
      "learning_rate": 0.0004853814248536109,
      "loss": 0.4049,
      "step": 359500
    },
    {
      "epoch": 29.27602821883831,
      "grad_norm": 0.12485960870981216,
      "learning_rate": 0.0004853610930383865,
      "loss": 0.4051,
      "step": 360000
    },
    {
      "epoch": 29.316689369142253,
      "grad_norm": 0.14495621621608734,
      "learning_rate": 0.000485340761223162,
      "loss": 0.4056,
      "step": 360500
    },
    {
      "epoch": 29.357350519446197,
      "grad_norm": 0.1580657809972763,
      "learning_rate": 0.0004853204294079375,
      "loss": 0.4052,
      "step": 361000
    },
    {
      "epoch": 29.398011669750137,
      "grad_norm": 0.1248234361410141,
      "learning_rate": 0.0004853000975927131,
      "loss": 0.4055,
      "step": 361500
    },
    {
      "epoch": 29.43867282005408,
      "grad_norm": 0.13082917034626007,
      "learning_rate": 0.0004852797657774886,
      "loss": 0.4056,
      "step": 362000
    },
    {
      "epoch": 29.47933397035802,
      "grad_norm": 0.12914760410785675,
      "learning_rate": 0.00048525943396226416,
      "loss": 0.4053,
      "step": 362500
    },
    {
      "epoch": 29.519995120661964,
      "grad_norm": 0.15855276584625244,
      "learning_rate": 0.0004852391021470397,
      "loss": 0.4066,
      "step": 363000
    },
    {
      "epoch": 29.560656270965907,
      "grad_norm": 0.14225353300571442,
      "learning_rate": 0.0004852187703318152,
      "loss": 0.4061,
      "step": 363500
    },
    {
      "epoch": 29.601317421269847,
      "grad_norm": 0.14659824967384338,
      "learning_rate": 0.00048519843851659077,
      "loss": 0.4064,
      "step": 364000
    },
    {
      "epoch": 29.64197857157379,
      "grad_norm": 0.13422247767448425,
      "learning_rate": 0.0004851781067013663,
      "loss": 0.406,
      "step": 364500
    },
    {
      "epoch": 29.68263972187773,
      "grad_norm": 0.12235002219676971,
      "learning_rate": 0.0004851577748861418,
      "loss": 0.4068,
      "step": 365000
    },
    {
      "epoch": 29.723300872181674,
      "grad_norm": 0.13909386098384857,
      "learning_rate": 0.00048513744307091737,
      "loss": 0.4067,
      "step": 365500
    },
    {
      "epoch": 29.763962022485615,
      "grad_norm": 0.12664565443992615,
      "learning_rate": 0.0004851171112556929,
      "loss": 0.4065,
      "step": 366000
    },
    {
      "epoch": 29.804623172789558,
      "grad_norm": 0.12890659272670746,
      "learning_rate": 0.0004850967794404684,
      "loss": 0.4067,
      "step": 366500
    },
    {
      "epoch": 29.8452843230935,
      "grad_norm": 0.12651623785495758,
      "learning_rate": 0.000485076447625244,
      "loss": 0.4063,
      "step": 367000
    },
    {
      "epoch": 29.88594547339744,
      "grad_norm": 0.13694941997528076,
      "learning_rate": 0.0004850561158100195,
      "loss": 0.4072,
      "step": 367500
    },
    {
      "epoch": 29.926606623701385,
      "grad_norm": 0.13476738333702087,
      "learning_rate": 0.00048503578399479506,
      "loss": 0.4068,
      "step": 368000
    },
    {
      "epoch": 29.967267774005325,
      "grad_norm": 0.13189494609832764,
      "learning_rate": 0.0004850154521795706,
      "loss": 0.4069,
      "step": 368500
    },
    {
      "epoch": 30.00792892430927,
      "grad_norm": 0.13955934345722198,
      "learning_rate": 0.0004849951203643461,
      "loss": 0.4065,
      "step": 369000
    },
    {
      "epoch": 30.048590074613212,
      "grad_norm": 0.15745119750499725,
      "learning_rate": 0.00048497478854912167,
      "loss": 0.4021,
      "step": 369500
    },
    {
      "epoch": 30.089251224917152,
      "grad_norm": 0.13093014061450958,
      "learning_rate": 0.0004849544567338972,
      "loss": 0.4037,
      "step": 370000
    },
    {
      "epoch": 30.129912375221096,
      "grad_norm": 0.12995246052742004,
      "learning_rate": 0.0004849341249186727,
      "loss": 0.4032,
      "step": 370500
    },
    {
      "epoch": 30.170573525525036,
      "grad_norm": 0.14302538335323334,
      "learning_rate": 0.0004849137931034483,
      "loss": 0.4038,
      "step": 371000
    },
    {
      "epoch": 30.21123467582898,
      "grad_norm": 0.13153532147407532,
      "learning_rate": 0.00048489346128822384,
      "loss": 0.404,
      "step": 371500
    },
    {
      "epoch": 30.25189582613292,
      "grad_norm": 0.1459781974554062,
      "learning_rate": 0.0004848731294729994,
      "loss": 0.4042,
      "step": 372000
    },
    {
      "epoch": 30.292556976436863,
      "grad_norm": 0.12271787226200104,
      "learning_rate": 0.0004848527976577749,
      "loss": 0.4051,
      "step": 372500
    },
    {
      "epoch": 30.333218126740807,
      "grad_norm": 0.1343958079814911,
      "learning_rate": 0.00048483246584255044,
      "loss": 0.4045,
      "step": 373000
    },
    {
      "epoch": 30.373879277044747,
      "grad_norm": 0.14254380762577057,
      "learning_rate": 0.000484812134027326,
      "loss": 0.4046,
      "step": 373500
    },
    {
      "epoch": 30.41454042734869,
      "grad_norm": 0.13978011906147003,
      "learning_rate": 0.00048479180221210153,
      "loss": 0.4053,
      "step": 374000
    },
    {
      "epoch": 30.45520157765263,
      "grad_norm": 0.13977259397506714,
      "learning_rate": 0.00048477147039687705,
      "loss": 0.4051,
      "step": 374500
    },
    {
      "epoch": 30.495862727956574,
      "grad_norm": 0.14416223764419556,
      "learning_rate": 0.0004847511385816526,
      "loss": 0.4053,
      "step": 375000
    },
    {
      "epoch": 30.536523878260518,
      "grad_norm": 0.1502687633037567,
      "learning_rate": 0.00048473080676642813,
      "loss": 0.4059,
      "step": 375500
    },
    {
      "epoch": 30.577185028564458,
      "grad_norm": 0.15297169983386993,
      "learning_rate": 0.00048471047495120365,
      "loss": 0.4064,
      "step": 376000
    },
    {
      "epoch": 30.6178461788684,
      "grad_norm": 0.1421121209859848,
      "learning_rate": 0.0004846901431359792,
      "loss": 0.4059,
      "step": 376500
    },
    {
      "epoch": 30.65850732917234,
      "grad_norm": 0.11920804530382156,
      "learning_rate": 0.00048466981132075474,
      "loss": 0.4057,
      "step": 377000
    },
    {
      "epoch": 30.699168479476285,
      "grad_norm": 0.13120263814926147,
      "learning_rate": 0.0004846494795055303,
      "loss": 0.4056,
      "step": 377500
    },
    {
      "epoch": 30.739829629780225,
      "grad_norm": 0.15853747725486755,
      "learning_rate": 0.0004846291476903058,
      "loss": 0.406,
      "step": 378000
    },
    {
      "epoch": 30.78049078008417,
      "grad_norm": 0.15438058972358704,
      "learning_rate": 0.00048460881587508134,
      "loss": 0.4059,
      "step": 378500
    },
    {
      "epoch": 30.821151930388112,
      "grad_norm": 0.13657227158546448,
      "learning_rate": 0.0004845884840598569,
      "loss": 0.4065,
      "step": 379000
    },
    {
      "epoch": 30.861813080692052,
      "grad_norm": 0.13352999091148376,
      "learning_rate": 0.00048456815224463243,
      "loss": 0.4062,
      "step": 379500
    },
    {
      "epoch": 30.902474230995995,
      "grad_norm": 0.13317805528640747,
      "learning_rate": 0.00048454782042940794,
      "loss": 0.4061,
      "step": 380000
    },
    {
      "epoch": 30.943135381299935,
      "grad_norm": 0.1290225088596344,
      "learning_rate": 0.0004845274886141835,
      "loss": 0.4063,
      "step": 380500
    },
    {
      "epoch": 30.98379653160388,
      "grad_norm": 0.13956715166568756,
      "learning_rate": 0.00048450715679895903,
      "loss": 0.4058,
      "step": 381000
    },
    {
      "epoch": 31.024457681907823,
      "grad_norm": 0.13281840085983276,
      "learning_rate": 0.00048448682498373455,
      "loss": 0.4042,
      "step": 381500
    },
    {
      "epoch": 31.065118832211763,
      "grad_norm": 0.14132924377918243,
      "learning_rate": 0.0004844664931685101,
      "loss": 0.4025,
      "step": 382000
    },
    {
      "epoch": 31.105779982515706,
      "grad_norm": 0.12580996751785278,
      "learning_rate": 0.00048444616135328563,
      "loss": 0.403,
      "step": 382500
    },
    {
      "epoch": 31.146441132819646,
      "grad_norm": 0.13814084231853485,
      "learning_rate": 0.0004844258295380612,
      "loss": 0.4028,
      "step": 383000
    },
    {
      "epoch": 31.18710228312359,
      "grad_norm": 0.15138885378837585,
      "learning_rate": 0.0004844054977228367,
      "loss": 0.404,
      "step": 383500
    },
    {
      "epoch": 31.227763433427533,
      "grad_norm": 0.16160689294338226,
      "learning_rate": 0.00048438516590761224,
      "loss": 0.4034,
      "step": 384000
    },
    {
      "epoch": 31.268424583731473,
      "grad_norm": 0.13600599765777588,
      "learning_rate": 0.0004843648340923878,
      "loss": 0.4041,
      "step": 384500
    },
    {
      "epoch": 31.309085734035417,
      "grad_norm": 0.14412985742092133,
      "learning_rate": 0.0004843445022771633,
      "loss": 0.4042,
      "step": 385000
    },
    {
      "epoch": 31.349746884339357,
      "grad_norm": 0.1533145308494568,
      "learning_rate": 0.00048432417046193884,
      "loss": 0.4039,
      "step": 385500
    },
    {
      "epoch": 31.3904080346433,
      "grad_norm": 0.1397009789943695,
      "learning_rate": 0.0004843038386467144,
      "loss": 0.4046,
      "step": 386000
    },
    {
      "epoch": 31.43106918494724,
      "grad_norm": 0.1477579027414322,
      "learning_rate": 0.00048428350683148993,
      "loss": 0.4046,
      "step": 386500
    },
    {
      "epoch": 31.471730335251184,
      "grad_norm": 0.16522163152694702,
      "learning_rate": 0.0004842631750162655,
      "loss": 0.4046,
      "step": 387000
    },
    {
      "epoch": 31.512391485555128,
      "grad_norm": 0.12776894867420197,
      "learning_rate": 0.000484242843201041,
      "loss": 0.405,
      "step": 387500
    },
    {
      "epoch": 31.553052635859068,
      "grad_norm": 0.1281966120004654,
      "learning_rate": 0.00048422251138581653,
      "loss": 0.405,
      "step": 388000
    },
    {
      "epoch": 31.59371378616301,
      "grad_norm": 0.12856164574623108,
      "learning_rate": 0.0004842021795705921,
      "loss": 0.4048,
      "step": 388500
    },
    {
      "epoch": 31.63437493646695,
      "grad_norm": 0.13828404247760773,
      "learning_rate": 0.0004841818477553676,
      "loss": 0.4047,
      "step": 389000
    },
    {
      "epoch": 31.675036086770895,
      "grad_norm": 0.1450287103652954,
      "learning_rate": 0.00048416151594014314,
      "loss": 0.4052,
      "step": 389500
    },
    {
      "epoch": 31.71569723707484,
      "grad_norm": 0.13459572196006775,
      "learning_rate": 0.0004841411841249187,
      "loss": 0.4051,
      "step": 390000
    },
    {
      "epoch": 31.75635838737878,
      "grad_norm": 0.1373964548110962,
      "learning_rate": 0.0004841208523096942,
      "loss": 0.4054,
      "step": 390500
    },
    {
      "epoch": 31.797019537682722,
      "grad_norm": 0.15110397338867188,
      "learning_rate": 0.00048410052049446974,
      "loss": 0.4049,
      "step": 391000
    },
    {
      "epoch": 31.837680687986662,
      "grad_norm": 0.1549144983291626,
      "learning_rate": 0.0004840801886792453,
      "loss": 0.4053,
      "step": 391500
    },
    {
      "epoch": 31.878341838290606,
      "grad_norm": 0.14857137203216553,
      "learning_rate": 0.0004840598568640208,
      "loss": 0.4056,
      "step": 392000
    },
    {
      "epoch": 31.919002988594546,
      "grad_norm": 0.13842712342739105,
      "learning_rate": 0.0004840395250487964,
      "loss": 0.406,
      "step": 392500
    },
    {
      "epoch": 31.95966413889849,
      "grad_norm": 0.1382686048746109,
      "learning_rate": 0.0004840191932335719,
      "loss": 0.4056,
      "step": 393000
    },
    {
      "epoch": 32.00032528920243,
      "grad_norm": 0.12838245928287506,
      "learning_rate": 0.00048399886141834743,
      "loss": 0.4056,
      "step": 393500
    },
    {
      "epoch": 32.040986439506376,
      "grad_norm": 0.14875763654708862,
      "learning_rate": 0.000483978529603123,
      "loss": 0.4011,
      "step": 394000
    },
    {
      "epoch": 32.081647589810316,
      "grad_norm": 0.14356975257396698,
      "learning_rate": 0.0004839581977878985,
      "loss": 0.4016,
      "step": 394500
    },
    {
      "epoch": 32.122308740114256,
      "grad_norm": 0.1423218995332718,
      "learning_rate": 0.00048393786597267403,
      "loss": 0.4029,
      "step": 395000
    },
    {
      "epoch": 32.162969890418196,
      "grad_norm": 0.12684251368045807,
      "learning_rate": 0.0004839175341574496,
      "loss": 0.4027,
      "step": 395500
    },
    {
      "epoch": 32.20363104072214,
      "grad_norm": 0.13958193361759186,
      "learning_rate": 0.0004838972023422251,
      "loss": 0.4032,
      "step": 396000
    },
    {
      "epoch": 32.244292191026084,
      "grad_norm": 0.1656496673822403,
      "learning_rate": 0.00048387687052700064,
      "loss": 0.4035,
      "step": 396500
    },
    {
      "epoch": 32.284953341330024,
      "grad_norm": 0.14971119165420532,
      "learning_rate": 0.0004838565387117762,
      "loss": 0.4031,
      "step": 397000
    },
    {
      "epoch": 32.32561449163397,
      "grad_norm": 0.1475823074579239,
      "learning_rate": 0.0004838362068965517,
      "loss": 0.4032,
      "step": 397500
    },
    {
      "epoch": 32.36627564193791,
      "grad_norm": 0.16589730978012085,
      "learning_rate": 0.0004838158750813273,
      "loss": 0.4037,
      "step": 398000
    },
    {
      "epoch": 32.40693679224185,
      "grad_norm": 0.1390368491411209,
      "learning_rate": 0.0004837955432661028,
      "loss": 0.4039,
      "step": 398500
    },
    {
      "epoch": 32.4475979425458,
      "grad_norm": 0.14695896208286285,
      "learning_rate": 0.00048377521145087833,
      "loss": 0.4045,
      "step": 399000
    },
    {
      "epoch": 32.48825909284974,
      "grad_norm": 0.15698951482772827,
      "learning_rate": 0.0004837548796356539,
      "loss": 0.4042,
      "step": 399500
    },
    {
      "epoch": 32.52892024315368,
      "grad_norm": 0.1560399979352951,
      "learning_rate": 0.0004837345478204294,
      "loss": 0.4043,
      "step": 400000
    },
    {
      "epoch": 32.56958139345762,
      "grad_norm": 0.13513827323913574,
      "learning_rate": 0.00048371421600520493,
      "loss": 0.4044,
      "step": 400500
    },
    {
      "epoch": 32.610242543761565,
      "grad_norm": 0.14673365652561188,
      "learning_rate": 0.0004836938841899805,
      "loss": 0.404,
      "step": 401000
    },
    {
      "epoch": 32.650903694065505,
      "grad_norm": 0.1533636599779129,
      "learning_rate": 0.000483673552374756,
      "loss": 0.4048,
      "step": 401500
    },
    {
      "epoch": 32.691564844369445,
      "grad_norm": 0.13530850410461426,
      "learning_rate": 0.0004836532205595316,
      "loss": 0.4047,
      "step": 402000
    },
    {
      "epoch": 32.73222599467339,
      "grad_norm": 0.16533474624156952,
      "learning_rate": 0.0004836328887443071,
      "loss": 0.4048,
      "step": 402500
    },
    {
      "epoch": 32.77288714497733,
      "grad_norm": 0.14103172719478607,
      "learning_rate": 0.0004836125569290826,
      "loss": 0.4052,
      "step": 403000
    },
    {
      "epoch": 32.81354829528127,
      "grad_norm": 0.18069298565387726,
      "learning_rate": 0.0004835922251138582,
      "loss": 0.4051,
      "step": 403500
    },
    {
      "epoch": 32.85420944558521,
      "grad_norm": 0.1322837769985199,
      "learning_rate": 0.0004835718932986337,
      "loss": 0.405,
      "step": 404000
    },
    {
      "epoch": 32.89487059588916,
      "grad_norm": 0.13585861027240753,
      "learning_rate": 0.0004835515614834092,
      "loss": 0.4048,
      "step": 404500
    },
    {
      "epoch": 32.9355317461931,
      "grad_norm": 0.13446275889873505,
      "learning_rate": 0.0004835312296681848,
      "loss": 0.4055,
      "step": 405000
    },
    {
      "epoch": 32.97619289649704,
      "grad_norm": 0.1274658888578415,
      "learning_rate": 0.0004835108978529603,
      "loss": 0.4055,
      "step": 405500
    },
    {
      "epoch": 33.01685404680099,
      "grad_norm": 0.15095573663711548,
      "learning_rate": 0.00048349056603773583,
      "loss": 0.4033,
      "step": 406000
    },
    {
      "epoch": 33.05751519710493,
      "grad_norm": 0.13622836768627167,
      "learning_rate": 0.0004834702342225114,
      "loss": 0.4014,
      "step": 406500
    },
    {
      "epoch": 33.09817634740887,
      "grad_norm": 0.15183520317077637,
      "learning_rate": 0.0004834499024072869,
      "loss": 0.4017,
      "step": 407000
    },
    {
      "epoch": 33.138837497712814,
      "grad_norm": 0.13723967969417572,
      "learning_rate": 0.0004834295705920625,
      "loss": 0.402,
      "step": 407500
    },
    {
      "epoch": 33.179498648016754,
      "grad_norm": 0.14102183282375336,
      "learning_rate": 0.000483409238776838,
      "loss": 0.4018,
      "step": 408000
    },
    {
      "epoch": 33.220159798320694,
      "grad_norm": 0.14770077168941498,
      "learning_rate": 0.0004833889069616135,
      "loss": 0.4023,
      "step": 408500
    },
    {
      "epoch": 33.260820948624634,
      "grad_norm": 0.15233676135540009,
      "learning_rate": 0.0004833685751463891,
      "loss": 0.4033,
      "step": 409000
    },
    {
      "epoch": 33.30148209892858,
      "grad_norm": 0.14130665361881256,
      "learning_rate": 0.0004833482433311646,
      "loss": 0.4025,
      "step": 409500
    },
    {
      "epoch": 33.34214324923252,
      "grad_norm": 0.1318250298500061,
      "learning_rate": 0.0004833279115159401,
      "loss": 0.4032,
      "step": 410000
    },
    {
      "epoch": 33.38280439953646,
      "grad_norm": 0.13520532846450806,
      "learning_rate": 0.0004833075797007157,
      "loss": 0.4034,
      "step": 410500
    },
    {
      "epoch": 33.42346554984041,
      "grad_norm": 0.13720126450061798,
      "learning_rate": 0.0004832872478854912,
      "loss": 0.4033,
      "step": 411000
    },
    {
      "epoch": 33.46412670014435,
      "grad_norm": 0.1420491337776184,
      "learning_rate": 0.0004832669160702667,
      "loss": 0.4034,
      "step": 411500
    },
    {
      "epoch": 33.50478785044829,
      "grad_norm": 0.14198991656303406,
      "learning_rate": 0.0004832465842550423,
      "loss": 0.404,
      "step": 412000
    },
    {
      "epoch": 33.54544900075223,
      "grad_norm": 0.13850520551204681,
      "learning_rate": 0.0004832262524398178,
      "loss": 0.4044,
      "step": 412500
    },
    {
      "epoch": 33.586110151056175,
      "grad_norm": 0.14225906133651733,
      "learning_rate": 0.0004832059206245934,
      "loss": 0.4041,
      "step": 413000
    },
    {
      "epoch": 33.626771301360115,
      "grad_norm": 0.17158666253089905,
      "learning_rate": 0.0004831855888093689,
      "loss": 0.4038,
      "step": 413500
    },
    {
      "epoch": 33.667432451664055,
      "grad_norm": 0.1535302996635437,
      "learning_rate": 0.0004831652569941444,
      "loss": 0.4042,
      "step": 414000
    },
    {
      "epoch": 33.708093601968,
      "grad_norm": 0.12946605682373047,
      "learning_rate": 0.00048314492517892,
      "loss": 0.404,
      "step": 414500
    },
    {
      "epoch": 33.74875475227194,
      "grad_norm": 0.14168325066566467,
      "learning_rate": 0.0004831245933636955,
      "loss": 0.4041,
      "step": 415000
    },
    {
      "epoch": 33.78941590257588,
      "grad_norm": 0.1582329124212265,
      "learning_rate": 0.000483104261548471,
      "loss": 0.4043,
      "step": 415500
    },
    {
      "epoch": 33.83007705287982,
      "grad_norm": 0.1632390320301056,
      "learning_rate": 0.0004830839297332466,
      "loss": 0.4043,
      "step": 416000
    },
    {
      "epoch": 33.87073820318377,
      "grad_norm": 0.1677221953868866,
      "learning_rate": 0.0004830635979180221,
      "loss": 0.405,
      "step": 416500
    },
    {
      "epoch": 33.91139935348771,
      "grad_norm": 0.15605534613132477,
      "learning_rate": 0.0004830432661027976,
      "loss": 0.4045,
      "step": 417000
    },
    {
      "epoch": 33.95206050379165,
      "grad_norm": 0.13732625544071198,
      "learning_rate": 0.0004830229342875732,
      "loss": 0.4045,
      "step": 417500
    },
    {
      "epoch": 33.9927216540956,
      "grad_norm": 0.1651080697774887,
      "learning_rate": 0.0004830026024723487,
      "loss": 0.4049,
      "step": 418000
    },
    {
      "epoch": 34.03338280439954,
      "grad_norm": 0.15212903916835785,
      "learning_rate": 0.0004829822706571243,
      "loss": 0.4009,
      "step": 418500
    },
    {
      "epoch": 34.07404395470348,
      "grad_norm": 0.13800105452537537,
      "learning_rate": 0.0004829619388418998,
      "loss": 0.401,
      "step": 419000
    },
    {
      "epoch": 34.114705105007424,
      "grad_norm": 0.14468775689601898,
      "learning_rate": 0.0004829416070266753,
      "loss": 0.4018,
      "step": 419500
    },
    {
      "epoch": 34.155366255311364,
      "grad_norm": 0.16103851795196533,
      "learning_rate": 0.0004829212752114509,
      "loss": 0.4016,
      "step": 420000
    },
    {
      "epoch": 34.196027405615304,
      "grad_norm": 0.13643458485603333,
      "learning_rate": 0.0004829009433962264,
      "loss": 0.4019,
      "step": 420500
    },
    {
      "epoch": 34.236688555919244,
      "grad_norm": 0.1724623143672943,
      "learning_rate": 0.0004828806115810019,
      "loss": 0.4018,
      "step": 421000
    },
    {
      "epoch": 34.27734970622319,
      "grad_norm": 0.14442086219787598,
      "learning_rate": 0.0004828602797657775,
      "loss": 0.4024,
      "step": 421500
    },
    {
      "epoch": 34.31801085652713,
      "grad_norm": 0.1360466182231903,
      "learning_rate": 0.000482839947950553,
      "loss": 0.4022,
      "step": 422000
    },
    {
      "epoch": 34.35867200683107,
      "grad_norm": 0.1241053119301796,
      "learning_rate": 0.0004828196161353286,
      "loss": 0.403,
      "step": 422500
    },
    {
      "epoch": 34.39933315713502,
      "grad_norm": 0.15271027386188507,
      "learning_rate": 0.0004827992843201041,
      "loss": 0.403,
      "step": 423000
    },
    {
      "epoch": 34.43999430743896,
      "grad_norm": 0.1392958015203476,
      "learning_rate": 0.0004827789525048796,
      "loss": 0.4036,
      "step": 423500
    },
    {
      "epoch": 34.4806554577429,
      "grad_norm": 0.1395377814769745,
      "learning_rate": 0.0004827586206896552,
      "loss": 0.4027,
      "step": 424000
    },
    {
      "epoch": 34.52131660804684,
      "grad_norm": 0.13674204051494598,
      "learning_rate": 0.0004827382888744307,
      "loss": 0.403,
      "step": 424500
    },
    {
      "epoch": 34.561977758350785,
      "grad_norm": 0.16828641295433044,
      "learning_rate": 0.0004827179570592062,
      "loss": 0.4029,
      "step": 425000
    },
    {
      "epoch": 34.602638908654725,
      "grad_norm": 0.1470976620912552,
      "learning_rate": 0.0004826976252439818,
      "loss": 0.4039,
      "step": 425500
    },
    {
      "epoch": 34.643300058958665,
      "grad_norm": 0.15435734391212463,
      "learning_rate": 0.0004826772934287573,
      "loss": 0.4038,
      "step": 426000
    },
    {
      "epoch": 34.68396120926261,
      "grad_norm": 0.15998785197734833,
      "learning_rate": 0.0004826569616135328,
      "loss": 0.4039,
      "step": 426500
    },
    {
      "epoch": 34.72462235956655,
      "grad_norm": 0.16378162801265717,
      "learning_rate": 0.00048263662979830844,
      "loss": 0.4036,
      "step": 427000
    },
    {
      "epoch": 34.76528350987049,
      "grad_norm": 0.16077382862567902,
      "learning_rate": 0.00048261629798308396,
      "loss": 0.4036,
      "step": 427500
    },
    {
      "epoch": 34.80594466017444,
      "grad_norm": 0.13580302894115448,
      "learning_rate": 0.0004825959661678595,
      "loss": 0.4039,
      "step": 428000
    },
    {
      "epoch": 34.84660581047838,
      "grad_norm": 0.13994517922401428,
      "learning_rate": 0.00048257563435263504,
      "loss": 0.4044,
      "step": 428500
    },
    {
      "epoch": 34.88726696078232,
      "grad_norm": 0.15564091503620148,
      "learning_rate": 0.00048255530253741056,
      "loss": 0.4044,
      "step": 429000
    },
    {
      "epoch": 34.92792811108626,
      "grad_norm": 0.14493627846240997,
      "learning_rate": 0.00048253497072218613,
      "loss": 0.4043,
      "step": 429500
    },
    {
      "epoch": 34.96858926139021,
      "grad_norm": 0.1379445344209671,
      "learning_rate": 0.00048251463890696165,
      "loss": 0.4045,
      "step": 430000
    },
    {
      "epoch": 35.00925041169415,
      "grad_norm": 0.12237323075532913,
      "learning_rate": 0.00048249430709173716,
      "loss": 0.403,
      "step": 430500
    },
    {
      "epoch": 35.04991156199809,
      "grad_norm": 0.1516321301460266,
      "learning_rate": 0.00048247397527651273,
      "loss": 0.4005,
      "step": 431000
    },
    {
      "epoch": 35.090572712302034,
      "grad_norm": 0.14926615357398987,
      "learning_rate": 0.00048245364346128825,
      "loss": 0.4006,
      "step": 431500
    },
    {
      "epoch": 35.131233862605974,
      "grad_norm": 0.15408846735954285,
      "learning_rate": 0.00048243331164606377,
      "loss": 0.4011,
      "step": 432000
    },
    {
      "epoch": 35.171895012909914,
      "grad_norm": 0.1329367309808731,
      "learning_rate": 0.00048241297983083934,
      "loss": 0.4012,
      "step": 432500
    },
    {
      "epoch": 35.212556163213854,
      "grad_norm": 0.1447990983724594,
      "learning_rate": 0.00048239264801561485,
      "loss": 0.4014,
      "step": 433000
    },
    {
      "epoch": 35.2532173135178,
      "grad_norm": 0.17390413582324982,
      "learning_rate": 0.0004823723162003904,
      "loss": 0.4014,
      "step": 433500
    },
    {
      "epoch": 35.29387846382174,
      "grad_norm": 0.14329206943511963,
      "learning_rate": 0.00048235198438516594,
      "loss": 0.4021,
      "step": 434000
    },
    {
      "epoch": 35.33453961412568,
      "grad_norm": 0.14518000185489655,
      "learning_rate": 0.00048233165256994146,
      "loss": 0.4021,
      "step": 434500
    },
    {
      "epoch": 35.37520076442963,
      "grad_norm": 0.13901221752166748,
      "learning_rate": 0.00048231132075471703,
      "loss": 0.4026,
      "step": 435000
    },
    {
      "epoch": 35.41586191473357,
      "grad_norm": 0.1540050506591797,
      "learning_rate": 0.00048229098893949254,
      "loss": 0.4021,
      "step": 435500
    },
    {
      "epoch": 35.45652306503751,
      "grad_norm": 0.13637757301330566,
      "learning_rate": 0.00048227065712426806,
      "loss": 0.4023,
      "step": 436000
    },
    {
      "epoch": 35.49718421534145,
      "grad_norm": 0.14576829969882965,
      "learning_rate": 0.00048225032530904363,
      "loss": 0.4027,
      "step": 436500
    },
    {
      "epoch": 35.537845365645396,
      "grad_norm": 0.1540760099887848,
      "learning_rate": 0.00048222999349381915,
      "loss": 0.4024,
      "step": 437000
    },
    {
      "epoch": 35.578506515949336,
      "grad_norm": 0.14440204203128815,
      "learning_rate": 0.0004822096616785947,
      "loss": 0.4036,
      "step": 437500
    },
    {
      "epoch": 35.619167666253276,
      "grad_norm": 0.16548827290534973,
      "learning_rate": 0.00048218932986337023,
      "loss": 0.4028,
      "step": 438000
    },
    {
      "epoch": 35.65982881655722,
      "grad_norm": 0.14239072799682617,
      "learning_rate": 0.00048216899804814575,
      "loss": 0.4031,
      "step": 438500
    },
    {
      "epoch": 35.70048996686116,
      "grad_norm": 0.14890331029891968,
      "learning_rate": 0.0004821486662329213,
      "loss": 0.403,
      "step": 439000
    },
    {
      "epoch": 35.7411511171651,
      "grad_norm": 0.14327427744865417,
      "learning_rate": 0.00048212833441769684,
      "loss": 0.4035,
      "step": 439500
    },
    {
      "epoch": 35.78181226746905,
      "grad_norm": 0.1281459778547287,
      "learning_rate": 0.00048210800260247235,
      "loss": 0.4034,
      "step": 440000
    },
    {
      "epoch": 35.82247341777299,
      "grad_norm": 0.14109240472316742,
      "learning_rate": 0.0004820876707872479,
      "loss": 0.4033,
      "step": 440500
    },
    {
      "epoch": 35.86313456807693,
      "grad_norm": 0.1396668255329132,
      "learning_rate": 0.00048206733897202344,
      "loss": 0.4035,
      "step": 441000
    },
    {
      "epoch": 35.90379571838087,
      "grad_norm": 0.15632328391075134,
      "learning_rate": 0.00048204700715679896,
      "loss": 0.4037,
      "step": 441500
    },
    {
      "epoch": 35.94445686868482,
      "grad_norm": 0.14331457018852234,
      "learning_rate": 0.00048202667534157453,
      "loss": 0.4036,
      "step": 442000
    },
    {
      "epoch": 35.98511801898876,
      "grad_norm": 0.14223474264144897,
      "learning_rate": 0.00048200634352635005,
      "loss": 0.4036,
      "step": 442500
    },
    {
      "epoch": 36.0257791692927,
      "grad_norm": 0.17274416983127594,
      "learning_rate": 0.0004819860117111256,
      "loss": 0.4008,
      "step": 443000
    },
    {
      "epoch": 36.066440319596644,
      "grad_norm": 0.15932823717594147,
      "learning_rate": 0.00048196567989590113,
      "loss": 0.3997,
      "step": 443500
    },
    {
      "epoch": 36.107101469900584,
      "grad_norm": 0.16272972524166107,
      "learning_rate": 0.00048194534808067665,
      "loss": 0.3998,
      "step": 444000
    },
    {
      "epoch": 36.147762620204524,
      "grad_norm": 0.1676626205444336,
      "learning_rate": 0.0004819250162654522,
      "loss": 0.4002,
      "step": 444500
    },
    {
      "epoch": 36.188423770508464,
      "grad_norm": 0.15463469922542572,
      "learning_rate": 0.00048190468445022774,
      "loss": 0.4007,
      "step": 445000
    },
    {
      "epoch": 36.22908492081241,
      "grad_norm": 0.1492297649383545,
      "learning_rate": 0.00048188435263500325,
      "loss": 0.4014,
      "step": 445500
    },
    {
      "epoch": 36.26974607111635,
      "grad_norm": 0.1456611305475235,
      "learning_rate": 0.0004818640208197788,
      "loss": 0.401,
      "step": 446000
    },
    {
      "epoch": 36.31040722142029,
      "grad_norm": 0.1689438670873642,
      "learning_rate": 0.00048184368900455434,
      "loss": 0.4022,
      "step": 446500
    },
    {
      "epoch": 36.35106837172424,
      "grad_norm": 0.1440918743610382,
      "learning_rate": 0.00048182335718932986,
      "loss": 0.4017,
      "step": 447000
    },
    {
      "epoch": 36.39172952202818,
      "grad_norm": 0.13964051008224487,
      "learning_rate": 0.0004818030253741054,
      "loss": 0.402,
      "step": 447500
    },
    {
      "epoch": 36.43239067233212,
      "grad_norm": 0.12707531452178955,
      "learning_rate": 0.00048178269355888094,
      "loss": 0.4018,
      "step": 448000
    },
    {
      "epoch": 36.473051822636066,
      "grad_norm": 0.1345931738615036,
      "learning_rate": 0.0004817623617436565,
      "loss": 0.4017,
      "step": 448500
    },
    {
      "epoch": 36.513712972940006,
      "grad_norm": 0.14952579140663147,
      "learning_rate": 0.00048174202992843203,
      "loss": 0.4021,
      "step": 449000
    },
    {
      "epoch": 36.554374123243946,
      "grad_norm": 0.14819499850273132,
      "learning_rate": 0.00048172169811320755,
      "loss": 0.4028,
      "step": 449500
    },
    {
      "epoch": 36.595035273547886,
      "grad_norm": 0.18141470849514008,
      "learning_rate": 0.0004817013662979831,
      "loss": 0.4028,
      "step": 450000
    },
    {
      "epoch": 36.63569642385183,
      "grad_norm": 0.14354340732097626,
      "learning_rate": 0.00048168103448275863,
      "loss": 0.4027,
      "step": 450500
    },
    {
      "epoch": 36.67635757415577,
      "grad_norm": 0.14310137927532196,
      "learning_rate": 0.00048166070266753415,
      "loss": 0.4027,
      "step": 451000
    },
    {
      "epoch": 36.71701872445971,
      "grad_norm": 0.150364488363266,
      "learning_rate": 0.0004816403708523097,
      "loss": 0.4027,
      "step": 451500
    },
    {
      "epoch": 36.75767987476366,
      "grad_norm": 0.14683029055595398,
      "learning_rate": 0.00048162003903708524,
      "loss": 0.4026,
      "step": 452000
    },
    {
      "epoch": 36.7983410250676,
      "grad_norm": 0.16056671738624573,
      "learning_rate": 0.00048159970722186075,
      "loss": 0.4025,
      "step": 452500
    },
    {
      "epoch": 36.83900217537154,
      "grad_norm": 0.1711999922990799,
      "learning_rate": 0.0004815793754066363,
      "loss": 0.4039,
      "step": 453000
    },
    {
      "epoch": 36.87966332567548,
      "grad_norm": 0.16859771311283112,
      "learning_rate": 0.00048155904359141184,
      "loss": 0.4031,
      "step": 453500
    },
    {
      "epoch": 36.92032447597943,
      "grad_norm": 0.14074324071407318,
      "learning_rate": 0.0004815387117761874,
      "loss": 0.4033,
      "step": 454000
    },
    {
      "epoch": 36.96098562628337,
      "grad_norm": 0.14201250672340393,
      "learning_rate": 0.00048151837996096293,
      "loss": 0.403,
      "step": 454500
    },
    {
      "epoch": 37.00164677658731,
      "grad_norm": 0.15059301257133484,
      "learning_rate": 0.00048149804814573844,
      "loss": 0.4034,
      "step": 455000
    },
    {
      "epoch": 37.042307926891255,
      "grad_norm": 0.15875808894634247,
      "learning_rate": 0.000481477716330514,
      "loss": 0.3989,
      "step": 455500
    },
    {
      "epoch": 37.082969077195195,
      "grad_norm": 0.16230271756649017,
      "learning_rate": 0.00048145738451528953,
      "loss": 0.3995,
      "step": 456000
    },
    {
      "epoch": 37.123630227499135,
      "grad_norm": 0.16100844740867615,
      "learning_rate": 0.00048143705270006505,
      "loss": 0.4,
      "step": 456500
    },
    {
      "epoch": 37.164291377803075,
      "grad_norm": 0.14246197044849396,
      "learning_rate": 0.0004814167208848406,
      "loss": 0.4003,
      "step": 457000
    },
    {
      "epoch": 37.20495252810702,
      "grad_norm": 0.14933674037456512,
      "learning_rate": 0.00048139638906961613,
      "loss": 0.4007,
      "step": 457500
    },
    {
      "epoch": 37.24561367841096,
      "grad_norm": 0.14507968723773956,
      "learning_rate": 0.0004813760572543917,
      "loss": 0.4009,
      "step": 458000
    },
    {
      "epoch": 37.2862748287149,
      "grad_norm": 0.1556648313999176,
      "learning_rate": 0.0004813557254391672,
      "loss": 0.4007,
      "step": 458500
    },
    {
      "epoch": 37.32693597901885,
      "grad_norm": 0.14373764395713806,
      "learning_rate": 0.00048133539362394274,
      "loss": 0.4013,
      "step": 459000
    },
    {
      "epoch": 37.36759712932279,
      "grad_norm": 0.17022182047367096,
      "learning_rate": 0.0004813150618087183,
      "loss": 0.4013,
      "step": 459500
    },
    {
      "epoch": 37.40825827962673,
      "grad_norm": 0.15939867496490479,
      "learning_rate": 0.0004812947299934938,
      "loss": 0.4012,
      "step": 460000
    },
    {
      "epoch": 37.448919429930676,
      "grad_norm": 0.15855933725833893,
      "learning_rate": 0.00048127439817826934,
      "loss": 0.4011,
      "step": 460500
    },
    {
      "epoch": 37.489580580234616,
      "grad_norm": 0.15366388857364655,
      "learning_rate": 0.0004812540663630449,
      "loss": 0.4021,
      "step": 461000
    },
    {
      "epoch": 37.530241730538556,
      "grad_norm": 0.14491060376167297,
      "learning_rate": 0.00048123373454782043,
      "loss": 0.402,
      "step": 461500
    },
    {
      "epoch": 37.570902880842496,
      "grad_norm": 0.14096787571907043,
      "learning_rate": 0.00048121340273259595,
      "loss": 0.4025,
      "step": 462000
    },
    {
      "epoch": 37.61156403114644,
      "grad_norm": 0.15758761763572693,
      "learning_rate": 0.0004811930709173715,
      "loss": 0.4021,
      "step": 462500
    },
    {
      "epoch": 37.65222518145038,
      "grad_norm": 0.15563231706619263,
      "learning_rate": 0.00048117273910214703,
      "loss": 0.4021,
      "step": 463000
    },
    {
      "epoch": 37.69288633175432,
      "grad_norm": 0.1752776950597763,
      "learning_rate": 0.0004811524072869226,
      "loss": 0.4021,
      "step": 463500
    },
    {
      "epoch": 37.73354748205827,
      "grad_norm": 0.1429465115070343,
      "learning_rate": 0.0004811320754716981,
      "loss": 0.4023,
      "step": 464000
    },
    {
      "epoch": 37.77420863236221,
      "grad_norm": 0.14833761751651764,
      "learning_rate": 0.00048111174365647364,
      "loss": 0.4026,
      "step": 464500
    },
    {
      "epoch": 37.81486978266615,
      "grad_norm": 0.17051415145397186,
      "learning_rate": 0.0004810914118412492,
      "loss": 0.4025,
      "step": 465000
    },
    {
      "epoch": 37.85553093297009,
      "grad_norm": 0.15063445270061493,
      "learning_rate": 0.0004810710800260247,
      "loss": 0.4029,
      "step": 465500
    },
    {
      "epoch": 37.89619208327404,
      "grad_norm": 0.13814298808574677,
      "learning_rate": 0.00048105074821080024,
      "loss": 0.4026,
      "step": 466000
    },
    {
      "epoch": 37.93685323357798,
      "grad_norm": 0.1406542807817459,
      "learning_rate": 0.0004810304163955758,
      "loss": 0.4025,
      "step": 466500
    },
    {
      "epoch": 37.97751438388192,
      "grad_norm": 0.12919296324253082,
      "learning_rate": 0.0004810100845803513,
      "loss": 0.4024,
      "step": 467000
    },
    {
      "epoch": 38.018175534185865,
      "grad_norm": 0.16553550958633423,
      "learning_rate": 0.00048098975276512684,
      "loss": 0.4009,
      "step": 467500
    },
    {
      "epoch": 38.058836684489805,
      "grad_norm": 0.1604119837284088,
      "learning_rate": 0.0004809694209499024,
      "loss": 0.3991,
      "step": 468000
    },
    {
      "epoch": 38.099497834793745,
      "grad_norm": 0.1410091519355774,
      "learning_rate": 0.00048094908913467793,
      "loss": 0.3991,
      "step": 468500
    },
    {
      "epoch": 38.14015898509769,
      "grad_norm": 0.13892707228660583,
      "learning_rate": 0.0004809287573194535,
      "loss": 0.3995,
      "step": 469000
    },
    {
      "epoch": 38.18082013540163,
      "grad_norm": 0.14418350160121918,
      "learning_rate": 0.000480908425504229,
      "loss": 0.3999,
      "step": 469500
    },
    {
      "epoch": 38.22148128570557,
      "grad_norm": 0.16553977131843567,
      "learning_rate": 0.00048088809368900453,
      "loss": 0.4001,
      "step": 470000
    },
    {
      "epoch": 38.26214243600951,
      "grad_norm": 0.17409668862819672,
      "learning_rate": 0.0004808677618737801,
      "loss": 0.4002,
      "step": 470500
    },
    {
      "epoch": 38.30280358631346,
      "grad_norm": 0.1400507688522339,
      "learning_rate": 0.0004808474300585556,
      "loss": 0.4008,
      "step": 471000
    },
    {
      "epoch": 38.3434647366174,
      "grad_norm": 0.14968276023864746,
      "learning_rate": 0.00048082709824333114,
      "loss": 0.401,
      "step": 471500
    },
    {
      "epoch": 38.38412588692134,
      "grad_norm": 0.14611056447029114,
      "learning_rate": 0.0004808067664281067,
      "loss": 0.4006,
      "step": 472000
    },
    {
      "epoch": 38.424787037225286,
      "grad_norm": 0.15460026264190674,
      "learning_rate": 0.0004807864346128822,
      "loss": 0.4012,
      "step": 472500
    },
    {
      "epoch": 38.465448187529226,
      "grad_norm": 0.14783650636672974,
      "learning_rate": 0.0004807661027976578,
      "loss": 0.4015,
      "step": 473000
    },
    {
      "epoch": 38.506109337833166,
      "grad_norm": 0.14525267481803894,
      "learning_rate": 0.0004807457709824333,
      "loss": 0.4012,
      "step": 473500
    },
    {
      "epoch": 38.546770488137106,
      "grad_norm": 0.1618422269821167,
      "learning_rate": 0.00048072543916720883,
      "loss": 0.4012,
      "step": 474000
    },
    {
      "epoch": 38.58743163844105,
      "grad_norm": 0.15415717661380768,
      "learning_rate": 0.0004807051073519844,
      "loss": 0.402,
      "step": 474500
    },
    {
      "epoch": 38.62809278874499,
      "grad_norm": 0.1601368635892868,
      "learning_rate": 0.0004806847755367599,
      "loss": 0.402,
      "step": 475000
    },
    {
      "epoch": 38.66875393904893,
      "grad_norm": 0.1571151465177536,
      "learning_rate": 0.00048066444372153543,
      "loss": 0.4018,
      "step": 475500
    },
    {
      "epoch": 38.70941508935288,
      "grad_norm": 0.24470005929470062,
      "learning_rate": 0.000480644111906311,
      "loss": 0.4017,
      "step": 476000
    },
    {
      "epoch": 38.75007623965682,
      "grad_norm": 0.1389589011669159,
      "learning_rate": 0.0004806237800910865,
      "loss": 0.402,
      "step": 476500
    },
    {
      "epoch": 38.79073738996076,
      "grad_norm": 0.14632689952850342,
      "learning_rate": 0.00048060344827586203,
      "loss": 0.402,
      "step": 477000
    },
    {
      "epoch": 38.8313985402647,
      "grad_norm": 0.1595216989517212,
      "learning_rate": 0.0004805831164606376,
      "loss": 0.402,
      "step": 477500
    },
    {
      "epoch": 38.87205969056865,
      "grad_norm": 0.1457119733095169,
      "learning_rate": 0.0004805627846454131,
      "loss": 0.4022,
      "step": 478000
    },
    {
      "epoch": 38.91272084087259,
      "grad_norm": 0.15827888250350952,
      "learning_rate": 0.0004805424528301887,
      "loss": 0.4023,
      "step": 478500
    },
    {
      "epoch": 38.95338199117653,
      "grad_norm": 0.16909442842006683,
      "learning_rate": 0.0004805221210149642,
      "loss": 0.4025,
      "step": 479000
    },
    {
      "epoch": 38.994043141480475,
      "grad_norm": 0.16268187761306763,
      "learning_rate": 0.0004805017891997397,
      "loss": 0.4029,
      "step": 479500
    },
    {
      "epoch": 39.034704291784415,
      "grad_norm": 0.14734601974487305,
      "learning_rate": 0.0004804814573845153,
      "loss": 0.3989,
      "step": 480000
    },
    {
      "epoch": 39.075365442088355,
      "grad_norm": 0.14324837923049927,
      "learning_rate": 0.0004804611255692908,
      "loss": 0.3985,
      "step": 480500
    },
    {
      "epoch": 39.1160265923923,
      "grad_norm": 0.16180512309074402,
      "learning_rate": 0.00048044079375406633,
      "loss": 0.3992,
      "step": 481000
    },
    {
      "epoch": 39.15668774269624,
      "grad_norm": 0.16228076815605164,
      "learning_rate": 0.0004804204619388419,
      "loss": 0.3994,
      "step": 481500
    },
    {
      "epoch": 39.19734889300018,
      "grad_norm": 0.15439985692501068,
      "learning_rate": 0.0004804001301236174,
      "loss": 0.3995,
      "step": 482000
    },
    {
      "epoch": 39.23801004330412,
      "grad_norm": 0.16771100461483002,
      "learning_rate": 0.00048037979830839293,
      "loss": 0.3999,
      "step": 482500
    },
    {
      "epoch": 39.27867119360807,
      "grad_norm": 0.14353136718273163,
      "learning_rate": 0.0004803594664931685,
      "loss": 0.4003,
      "step": 483000
    },
    {
      "epoch": 39.31933234391201,
      "grad_norm": 0.1644749790430069,
      "learning_rate": 0.000480339134677944,
      "loss": 0.4,
      "step": 483500
    },
    {
      "epoch": 39.35999349421595,
      "grad_norm": 0.14139346778392792,
      "learning_rate": 0.00048031880286271964,
      "loss": 0.4002,
      "step": 484000
    },
    {
      "epoch": 39.400654644519896,
      "grad_norm": 0.13876944780349731,
      "learning_rate": 0.00048029847104749516,
      "loss": 0.4006,
      "step": 484500
    },
    {
      "epoch": 39.441315794823836,
      "grad_norm": 0.16254112124443054,
      "learning_rate": 0.0004802781392322707,
      "loss": 0.4007,
      "step": 485000
    },
    {
      "epoch": 39.481976945127776,
      "grad_norm": 0.13869909942150116,
      "learning_rate": 0.00048025780741704625,
      "loss": 0.4012,
      "step": 485500
    },
    {
      "epoch": 39.52263809543172,
      "grad_norm": 0.1342579424381256,
      "learning_rate": 0.00048023747560182176,
      "loss": 0.4009,
      "step": 486000
    },
    {
      "epoch": 39.563299245735664,
      "grad_norm": 0.17321163415908813,
      "learning_rate": 0.0004802171437865973,
      "loss": 0.4012,
      "step": 486500
    },
    {
      "epoch": 39.603960396039604,
      "grad_norm": 0.14310137927532196,
      "learning_rate": 0.00048019681197137285,
      "loss": 0.4011,
      "step": 487000
    },
    {
      "epoch": 39.644621546343544,
      "grad_norm": 0.16430416703224182,
      "learning_rate": 0.00048017648015614837,
      "loss": 0.4009,
      "step": 487500
    },
    {
      "epoch": 39.68528269664749,
      "grad_norm": 0.14721466600894928,
      "learning_rate": 0.00048015614834092394,
      "loss": 0.401,
      "step": 488000
    },
    {
      "epoch": 39.72594384695143,
      "grad_norm": 0.15367570519447327,
      "learning_rate": 0.00048013581652569945,
      "loss": 0.401,
      "step": 488500
    },
    {
      "epoch": 39.76660499725537,
      "grad_norm": 0.17066442966461182,
      "learning_rate": 0.00048011548471047497,
      "loss": 0.402,
      "step": 489000
    },
    {
      "epoch": 39.80726614755932,
      "grad_norm": 0.15994681417942047,
      "learning_rate": 0.00048009515289525054,
      "loss": 0.4018,
      "step": 489500
    },
    {
      "epoch": 39.84792729786326,
      "grad_norm": 0.14985063672065735,
      "learning_rate": 0.00048007482108002606,
      "loss": 0.4018,
      "step": 490000
    },
    {
      "epoch": 39.8885884481672,
      "grad_norm": 0.16279715299606323,
      "learning_rate": 0.0004800544892648016,
      "loss": 0.4019,
      "step": 490500
    },
    {
      "epoch": 39.92924959847114,
      "grad_norm": 0.14399825036525726,
      "learning_rate": 0.00048003415744957714,
      "loss": 0.4021,
      "step": 491000
    },
    {
      "epoch": 39.969910748775085,
      "grad_norm": 0.1455225646495819,
      "learning_rate": 0.00048001382563435266,
      "loss": 0.402,
      "step": 491500
    },
    {
      "epoch": 40.010571899079025,
      "grad_norm": 0.15143036842346191,
      "learning_rate": 0.0004799934938191282,
      "loss": 0.4008,
      "step": 492000
    },
    {
      "epoch": 40.051233049382965,
      "grad_norm": 0.16114597022533417,
      "learning_rate": 0.00047997316200390375,
      "loss": 0.3981,
      "step": 492500
    },
    {
      "epoch": 40.09189419968691,
      "grad_norm": 0.14703278243541718,
      "learning_rate": 0.00047995283018867926,
      "loss": 0.3982,
      "step": 493000
    },
    {
      "epoch": 40.13255534999085,
      "grad_norm": 0.1701279729604721,
      "learning_rate": 0.00047993249837345484,
      "loss": 0.3991,
      "step": 493500
    },
    {
      "epoch": 40.17321650029479,
      "grad_norm": 0.15248528122901917,
      "learning_rate": 0.00047991216655823035,
      "loss": 0.3989,
      "step": 494000
    },
    {
      "epoch": 40.21387765059873,
      "grad_norm": 0.15619537234306335,
      "learning_rate": 0.00047989183474300587,
      "loss": 0.399,
      "step": 494500
    },
    {
      "epoch": 40.25453880090268,
      "grad_norm": 0.17481890320777893,
      "learning_rate": 0.00047987150292778144,
      "loss": 0.3998,
      "step": 495000
    },
    {
      "epoch": 40.29519995120662,
      "grad_norm": 0.13295486569404602,
      "learning_rate": 0.00047985117111255696,
      "loss": 0.3998,
      "step": 495500
    },
    {
      "epoch": 40.33586110151056,
      "grad_norm": 0.14073416590690613,
      "learning_rate": 0.00047983083929733247,
      "loss": 0.3999,
      "step": 496000
    },
    {
      "epoch": 40.37652225181451,
      "grad_norm": 0.16580763459205627,
      "learning_rate": 0.00047981050748210804,
      "loss": 0.4,
      "step": 496500
    },
    {
      "epoch": 40.41718340211845,
      "grad_norm": 0.16352114081382751,
      "learning_rate": 0.00047979017566688356,
      "loss": 0.4004,
      "step": 497000
    },
    {
      "epoch": 40.45784455242239,
      "grad_norm": 0.15287573635578156,
      "learning_rate": 0.0004797698438516591,
      "loss": 0.4001,
      "step": 497500
    },
    {
      "epoch": 40.49850570272633,
      "grad_norm": 0.15424887835979462,
      "learning_rate": 0.00047974951203643465,
      "loss": 0.4006,
      "step": 498000
    },
    {
      "epoch": 40.539166853030274,
      "grad_norm": 0.18455436825752258,
      "learning_rate": 0.00047972918022121016,
      "loss": 0.4006,
      "step": 498500
    },
    {
      "epoch": 40.579828003334214,
      "grad_norm": 0.15108880400657654,
      "learning_rate": 0.00047970884840598573,
      "loss": 0.4003,
      "step": 499000
    },
    {
      "epoch": 40.620489153638154,
      "grad_norm": 0.1533394455909729,
      "learning_rate": 0.00047968851659076125,
      "loss": 0.4004,
      "step": 499500
    },
    {
      "epoch": 40.6611503039421,
      "grad_norm": 0.1429533064365387,
      "learning_rate": 0.00047966818477553677,
      "loss": 0.4011,
      "step": 500000
    },
    {
      "epoch": 40.70181145424604,
      "grad_norm": 0.15455569326877594,
      "learning_rate": 0.00047964785296031234,
      "loss": 0.4008,
      "step": 500500
    },
    {
      "epoch": 40.74247260454998,
      "grad_norm": 0.16593435406684875,
      "learning_rate": 0.00047962752114508785,
      "loss": 0.4013,
      "step": 501000
    },
    {
      "epoch": 40.78313375485393,
      "grad_norm": 0.15883709490299225,
      "learning_rate": 0.00047960718932986337,
      "loss": 0.4015,
      "step": 501500
    },
    {
      "epoch": 40.82379490515787,
      "grad_norm": 0.15074458718299866,
      "learning_rate": 0.00047958685751463894,
      "loss": 0.4017,
      "step": 502000
    },
    {
      "epoch": 40.86445605546181,
      "grad_norm": 0.15560536086559296,
      "learning_rate": 0.00047956652569941446,
      "loss": 0.4012,
      "step": 502500
    },
    {
      "epoch": 40.90511720576575,
      "grad_norm": 0.17410625517368317,
      "learning_rate": 0.00047954619388418997,
      "loss": 0.4013,
      "step": 503000
    },
    {
      "epoch": 40.945778356069695,
      "grad_norm": 0.16419538855552673,
      "learning_rate": 0.00047952586206896554,
      "loss": 0.402,
      "step": 503500
    },
    {
      "epoch": 40.986439506373635,
      "grad_norm": 0.1367683857679367,
      "learning_rate": 0.00047950553025374106,
      "loss": 0.4018,
      "step": 504000
    },
    {
      "epoch": 41.027100656677575,
      "grad_norm": 0.19199052453041077,
      "learning_rate": 0.00047948519843851663,
      "loss": 0.3983,
      "step": 504500
    },
    {
      "epoch": 41.06776180698152,
      "grad_norm": 0.14633584022521973,
      "learning_rate": 0.00047946486662329215,
      "loss": 0.3978,
      "step": 505000
    },
    {
      "epoch": 41.10842295728546,
      "grad_norm": 0.16998711228370667,
      "learning_rate": 0.00047944453480806766,
      "loss": 0.3982,
      "step": 505500
    },
    {
      "epoch": 41.1490841075894,
      "grad_norm": 0.16977395117282867,
      "learning_rate": 0.00047942420299284323,
      "loss": 0.3986,
      "step": 506000
    },
    {
      "epoch": 41.18974525789334,
      "grad_norm": 0.17581066489219666,
      "learning_rate": 0.00047940387117761875,
      "loss": 0.399,
      "step": 506500
    },
    {
      "epoch": 41.23040640819729,
      "grad_norm": 0.152466282248497,
      "learning_rate": 0.00047938353936239427,
      "loss": 0.3993,
      "step": 507000
    },
    {
      "epoch": 41.27106755850123,
      "grad_norm": 0.17595063149929047,
      "learning_rate": 0.00047936320754716984,
      "loss": 0.3991,
      "step": 507500
    },
    {
      "epoch": 41.31172870880517,
      "grad_norm": 0.15736769139766693,
      "learning_rate": 0.00047934287573194535,
      "loss": 0.3989,
      "step": 508000
    },
    {
      "epoch": 41.35238985910912,
      "grad_norm": 0.15803833305835724,
      "learning_rate": 0.0004793225439167209,
      "loss": 0.3993,
      "step": 508500
    },
    {
      "epoch": 41.39305100941306,
      "grad_norm": 0.15472759306430817,
      "learning_rate": 0.00047930221210149644,
      "loss": 0.3996,
      "step": 509000
    },
    {
      "epoch": 41.433712159717,
      "grad_norm": 0.15124082565307617,
      "learning_rate": 0.00047928188028627196,
      "loss": 0.3996,
      "step": 509500
    },
    {
      "epoch": 41.474373310020944,
      "grad_norm": 0.14679864048957825,
      "learning_rate": 0.00047926154847104753,
      "loss": 0.4,
      "step": 510000
    },
    {
      "epoch": 41.515034460324884,
      "grad_norm": 0.16743285953998566,
      "learning_rate": 0.00047924121665582304,
      "loss": 0.4003,
      "step": 510500
    },
    {
      "epoch": 41.555695610628824,
      "grad_norm": 0.16174781322479248,
      "learning_rate": 0.00047922088484059856,
      "loss": 0.4004,
      "step": 511000
    },
    {
      "epoch": 41.596356760932764,
      "grad_norm": 0.1597832292318344,
      "learning_rate": 0.00047920055302537413,
      "loss": 0.4008,
      "step": 511500
    },
    {
      "epoch": 41.63701791123671,
      "grad_norm": 0.14827027916908264,
      "learning_rate": 0.00047918022121014965,
      "loss": 0.4003,
      "step": 512000
    },
    {
      "epoch": 41.67767906154065,
      "grad_norm": 0.15134181082248688,
      "learning_rate": 0.00047915988939492516,
      "loss": 0.4001,
      "step": 512500
    },
    {
      "epoch": 41.71834021184459,
      "grad_norm": 0.1594550609588623,
      "learning_rate": 0.00047913955757970073,
      "loss": 0.4005,
      "step": 513000
    },
    {
      "epoch": 41.75900136214854,
      "grad_norm": 0.15821409225463867,
      "learning_rate": 0.00047911922576447625,
      "loss": 0.4005,
      "step": 513500
    },
    {
      "epoch": 41.79966251245248,
      "grad_norm": 0.1478213667869568,
      "learning_rate": 0.0004790988939492518,
      "loss": 0.4009,
      "step": 514000
    },
    {
      "epoch": 41.84032366275642,
      "grad_norm": 0.16608662903308868,
      "learning_rate": 0.00047907856213402734,
      "loss": 0.4008,
      "step": 514500
    },
    {
      "epoch": 41.88098481306036,
      "grad_norm": 0.15488341450691223,
      "learning_rate": 0.00047905823031880285,
      "loss": 0.4013,
      "step": 515000
    },
    {
      "epoch": 41.921645963364305,
      "grad_norm": 0.19250985980033875,
      "learning_rate": 0.0004790378985035784,
      "loss": 0.4008,
      "step": 515500
    },
    {
      "epoch": 41.962307113668246,
      "grad_norm": 0.1655876636505127,
      "learning_rate": 0.00047901756668835394,
      "loss": 0.4009,
      "step": 516000
    },
    {
      "epoch": 42.002968263972186,
      "grad_norm": 0.1635684370994568,
      "learning_rate": 0.00047899723487312946,
      "loss": 0.4008,
      "step": 516500
    },
    {
      "epoch": 42.04362941427613,
      "grad_norm": 0.16110998392105103,
      "learning_rate": 0.00047897690305790503,
      "loss": 0.3967,
      "step": 517000
    },
    {
      "epoch": 42.08429056458007,
      "grad_norm": 0.16354039311408997,
      "learning_rate": 0.00047895657124268055,
      "loss": 0.3975,
      "step": 517500
    },
    {
      "epoch": 42.12495171488401,
      "grad_norm": 0.1572505235671997,
      "learning_rate": 0.00047893623942745606,
      "loss": 0.3975,
      "step": 518000
    },
    {
      "epoch": 42.16561286518795,
      "grad_norm": 0.15412147343158722,
      "learning_rate": 0.00047891590761223163,
      "loss": 0.3981,
      "step": 518500
    },
    {
      "epoch": 42.2062740154919,
      "grad_norm": 0.15742428600788116,
      "learning_rate": 0.00047889557579700715,
      "loss": 0.3984,
      "step": 519000
    },
    {
      "epoch": 42.24693516579584,
      "grad_norm": 0.14004679024219513,
      "learning_rate": 0.0004788752439817827,
      "loss": 0.3987,
      "step": 519500
    },
    {
      "epoch": 42.28759631609978,
      "grad_norm": 0.17538772523403168,
      "learning_rate": 0.00047885491216655824,
      "loss": 0.3992,
      "step": 520000
    },
    {
      "epoch": 42.32825746640373,
      "grad_norm": 0.1409841626882553,
      "learning_rate": 0.00047883458035133375,
      "loss": 0.3994,
      "step": 520500
    },
    {
      "epoch": 42.36891861670767,
      "grad_norm": 0.1419941782951355,
      "learning_rate": 0.0004788142485361093,
      "loss": 0.3988,
      "step": 521000
    },
    {
      "epoch": 42.40957976701161,
      "grad_norm": 0.14712348580360413,
      "learning_rate": 0.00047879391672088484,
      "loss": 0.3992,
      "step": 521500
    },
    {
      "epoch": 42.450240917315554,
      "grad_norm": 0.18311192095279694,
      "learning_rate": 0.00047877358490566036,
      "loss": 0.3988,
      "step": 522000
    },
    {
      "epoch": 42.490902067619494,
      "grad_norm": 0.16383770108222961,
      "learning_rate": 0.0004787532530904359,
      "loss": 0.3999,
      "step": 522500
    },
    {
      "epoch": 42.531563217923434,
      "grad_norm": 0.15116822719573975,
      "learning_rate": 0.00047873292127521144,
      "loss": 0.3996,
      "step": 523000
    },
    {
      "epoch": 42.572224368227374,
      "grad_norm": 0.16000054776668549,
      "learning_rate": 0.000478712589459987,
      "loss": 0.4001,
      "step": 523500
    },
    {
      "epoch": 42.61288551853132,
      "grad_norm": 0.14738884568214417,
      "learning_rate": 0.00047869225764476253,
      "loss": 0.4002,
      "step": 524000
    },
    {
      "epoch": 42.65354666883526,
      "grad_norm": 0.2080269455909729,
      "learning_rate": 0.00047867192582953805,
      "loss": 0.3998,
      "step": 524500
    },
    {
      "epoch": 42.6942078191392,
      "grad_norm": 0.15145885944366455,
      "learning_rate": 0.0004786515940143136,
      "loss": 0.4006,
      "step": 525000
    },
    {
      "epoch": 42.73486896944315,
      "grad_norm": 0.1420368105173111,
      "learning_rate": 0.00047863126219908913,
      "loss": 0.4002,
      "step": 525500
    },
    {
      "epoch": 42.77553011974709,
      "grad_norm": 0.1760886162519455,
      "learning_rate": 0.00047861093038386465,
      "loss": 0.4,
      "step": 526000
    },
    {
      "epoch": 42.81619127005103,
      "grad_norm": 0.14564549922943115,
      "learning_rate": 0.0004785905985686402,
      "loss": 0.4009,
      "step": 526500
    },
    {
      "epoch": 42.85685242035497,
      "grad_norm": 0.17692486941814423,
      "learning_rate": 0.00047857026675341574,
      "loss": 0.4006,
      "step": 527000
    },
    {
      "epoch": 42.897513570658916,
      "grad_norm": 0.16879943013191223,
      "learning_rate": 0.00047854993493819125,
      "loss": 0.401,
      "step": 527500
    },
    {
      "epoch": 42.938174720962856,
      "grad_norm": 0.16282875835895538,
      "learning_rate": 0.0004785296031229668,
      "loss": 0.4008,
      "step": 528000
    },
    {
      "epoch": 42.978835871266796,
      "grad_norm": 0.16799405217170715,
      "learning_rate": 0.00047850927130774234,
      "loss": 0.401,
      "step": 528500
    },
    {
      "epoch": 43.01949702157074,
      "grad_norm": 0.15308481454849243,
      "learning_rate": 0.0004784889394925179,
      "loss": 0.3989,
      "step": 529000
    },
    {
      "epoch": 43.06015817187468,
      "grad_norm": 0.1522359400987625,
      "learning_rate": 0.00047846860767729343,
      "loss": 0.3973,
      "step": 529500
    },
    {
      "epoch": 43.10081932217862,
      "grad_norm": 0.16113533079624176,
      "learning_rate": 0.00047844827586206894,
      "loss": 0.3973,
      "step": 530000
    },
    {
      "epoch": 43.14148047248257,
      "grad_norm": 0.15705792605876923,
      "learning_rate": 0.0004784279440468445,
      "loss": 0.3979,
      "step": 530500
    },
    {
      "epoch": 43.18214162278651,
      "grad_norm": 0.13528673350811005,
      "learning_rate": 0.00047840761223162003,
      "loss": 0.3977,
      "step": 531000
    },
    {
      "epoch": 43.22280277309045,
      "grad_norm": 0.16686378419399261,
      "learning_rate": 0.00047838728041639555,
      "loss": 0.3977,
      "step": 531500
    },
    {
      "epoch": 43.26346392339439,
      "grad_norm": 0.15288735926151276,
      "learning_rate": 0.0004783669486011711,
      "loss": 0.3985,
      "step": 532000
    },
    {
      "epoch": 43.30412507369834,
      "grad_norm": 0.17504926025867462,
      "learning_rate": 0.00047834661678594663,
      "loss": 0.3983,
      "step": 532500
    },
    {
      "epoch": 43.34478622400228,
      "grad_norm": 0.16622602939605713,
      "learning_rate": 0.00047832628497072215,
      "loss": 0.3986,
      "step": 533000
    },
    {
      "epoch": 43.38544737430622,
      "grad_norm": 0.16905398666858673,
      "learning_rate": 0.0004783059531554977,
      "loss": 0.3994,
      "step": 533500
    },
    {
      "epoch": 43.426108524610164,
      "grad_norm": 0.18038973212242126,
      "learning_rate": 0.00047828562134027324,
      "loss": 0.3991,
      "step": 534000
    },
    {
      "epoch": 43.466769674914104,
      "grad_norm": 0.1750672161579132,
      "learning_rate": 0.0004782652895250488,
      "loss": 0.3989,
      "step": 534500
    },
    {
      "epoch": 43.507430825218044,
      "grad_norm": 0.14902131259441376,
      "learning_rate": 0.0004782449577098243,
      "loss": 0.3997,
      "step": 535000
    },
    {
      "epoch": 43.548091975521984,
      "grad_norm": 0.1390736848115921,
      "learning_rate": 0.00047822462589459984,
      "loss": 0.3996,
      "step": 535500
    },
    {
      "epoch": 43.58875312582593,
      "grad_norm": 0.14265745878219604,
      "learning_rate": 0.0004782042940793754,
      "loss": 0.3999,
      "step": 536000
    },
    {
      "epoch": 43.62941427612987,
      "grad_norm": 0.15094538033008575,
      "learning_rate": 0.00047818396226415093,
      "loss": 0.3997,
      "step": 536500
    },
    {
      "epoch": 43.67007542643381,
      "grad_norm": 0.15548451244831085,
      "learning_rate": 0.00047816363044892644,
      "loss": 0.3997,
      "step": 537000
    },
    {
      "epoch": 43.71073657673776,
      "grad_norm": 0.14494870603084564,
      "learning_rate": 0.000478143298633702,
      "loss": 0.3998,
      "step": 537500
    },
    {
      "epoch": 43.7513977270417,
      "grad_norm": 0.19053715467453003,
      "learning_rate": 0.00047812296681847753,
      "loss": 0.4005,
      "step": 538000
    },
    {
      "epoch": 43.79205887734564,
      "grad_norm": 0.16733130812644958,
      "learning_rate": 0.0004781026350032531,
      "loss": 0.3998,
      "step": 538500
    },
    {
      "epoch": 43.83272002764958,
      "grad_norm": 0.1395043134689331,
      "learning_rate": 0.0004780823031880286,
      "loss": 0.3999,
      "step": 539000
    },
    {
      "epoch": 43.873381177953526,
      "grad_norm": 0.1592216193675995,
      "learning_rate": 0.00047806197137280414,
      "loss": 0.4004,
      "step": 539500
    },
    {
      "epoch": 43.914042328257466,
      "grad_norm": 0.1650109738111496,
      "learning_rate": 0.0004780416395575797,
      "loss": 0.4003,
      "step": 540000
    },
    {
      "epoch": 43.954703478561406,
      "grad_norm": 0.17151525616645813,
      "learning_rate": 0.0004780213077423552,
      "loss": 0.4008,
      "step": 540500
    },
    {
      "epoch": 43.99536462886535,
      "grad_norm": 0.15553076565265656,
      "learning_rate": 0.00047800097592713074,
      "loss": 0.4004,
      "step": 541000
    },
    {
      "epoch": 44.03602577916929,
      "grad_norm": 0.1543935388326645,
      "learning_rate": 0.00047798064411190636,
      "loss": 0.3965,
      "step": 541500
    },
    {
      "epoch": 44.07668692947323,
      "grad_norm": 0.16442804038524628,
      "learning_rate": 0.0004779603122966819,
      "loss": 0.3969,
      "step": 542000
    },
    {
      "epoch": 44.11734807977718,
      "grad_norm": 0.15286237001419067,
      "learning_rate": 0.0004779399804814574,
      "loss": 0.3969,
      "step": 542500
    },
    {
      "epoch": 44.15800923008112,
      "grad_norm": 0.17681856453418732,
      "learning_rate": 0.00047791964866623297,
      "loss": 0.3971,
      "step": 543000
    },
    {
      "epoch": 44.19867038038506,
      "grad_norm": 0.16797424852848053,
      "learning_rate": 0.0004778993168510085,
      "loss": 0.398,
      "step": 543500
    },
    {
      "epoch": 44.239331530689,
      "grad_norm": 0.17310428619384766,
      "learning_rate": 0.00047787898503578405,
      "loss": 0.3974,
      "step": 544000
    },
    {
      "epoch": 44.27999268099295,
      "grad_norm": 0.16479429602622986,
      "learning_rate": 0.00047785865322055957,
      "loss": 0.3983,
      "step": 544500
    },
    {
      "epoch": 44.32065383129689,
      "grad_norm": 0.16659235954284668,
      "learning_rate": 0.0004778383214053351,
      "loss": 0.3983,
      "step": 545000
    },
    {
      "epoch": 44.36131498160083,
      "grad_norm": 0.17533808946609497,
      "learning_rate": 0.00047781798959011066,
      "loss": 0.3985,
      "step": 545500
    },
    {
      "epoch": 44.401976131904775,
      "grad_norm": 0.14293649792671204,
      "learning_rate": 0.0004777976577748862,
      "loss": 0.3991,
      "step": 546000
    },
    {
      "epoch": 44.442637282208715,
      "grad_norm": 0.20218545198440552,
      "learning_rate": 0.0004777773259596617,
      "loss": 0.3987,
      "step": 546500
    },
    {
      "epoch": 44.483298432512655,
      "grad_norm": 0.21512256562709808,
      "learning_rate": 0.00047775699414443726,
      "loss": 0.3987,
      "step": 547000
    },
    {
      "epoch": 44.523959582816595,
      "grad_norm": 0.15273705124855042,
      "learning_rate": 0.0004777366623292128,
      "loss": 0.3989,
      "step": 547500
    },
    {
      "epoch": 44.56462073312054,
      "grad_norm": 0.19062720239162445,
      "learning_rate": 0.0004777163305139883,
      "loss": 0.3994,
      "step": 548000
    },
    {
      "epoch": 44.60528188342448,
      "grad_norm": 0.1694113165140152,
      "learning_rate": 0.00047769599869876386,
      "loss": 0.3996,
      "step": 548500
    },
    {
      "epoch": 44.64594303372842,
      "grad_norm": 0.15056568384170532,
      "learning_rate": 0.0004776756668835394,
      "loss": 0.3994,
      "step": 549000
    },
    {
      "epoch": 44.68660418403237,
      "grad_norm": 0.16845571994781494,
      "learning_rate": 0.00047765533506831495,
      "loss": 0.3994,
      "step": 549500
    },
    {
      "epoch": 44.72726533433631,
      "grad_norm": 0.152666836977005,
      "learning_rate": 0.00047763500325309047,
      "loss": 0.3995,
      "step": 550000
    },
    {
      "epoch": 44.76792648464025,
      "grad_norm": 0.165587916970253,
      "learning_rate": 0.000477614671437866,
      "loss": 0.3996,
      "step": 550500
    },
    {
      "epoch": 44.808587634944196,
      "grad_norm": 0.13384436070919037,
      "learning_rate": 0.00047759433962264156,
      "loss": 0.3995,
      "step": 551000
    },
    {
      "epoch": 44.849248785248136,
      "grad_norm": 0.17869621515274048,
      "learning_rate": 0.00047757400780741707,
      "loss": 0.4001,
      "step": 551500
    },
    {
      "epoch": 44.889909935552076,
      "grad_norm": 0.15509562194347382,
      "learning_rate": 0.0004775536759921926,
      "loss": 0.4001,
      "step": 552000
    },
    {
      "epoch": 44.930571085856016,
      "grad_norm": 0.15268784761428833,
      "learning_rate": 0.00047753334417696816,
      "loss": 0.4,
      "step": 552500
    },
    {
      "epoch": 44.97123223615996,
      "grad_norm": 0.15890231728553772,
      "learning_rate": 0.0004775130123617437,
      "loss": 0.3995,
      "step": 553000
    },
    {
      "epoch": 45.0118933864639,
      "grad_norm": 0.15906403958797455,
      "learning_rate": 0.0004774926805465192,
      "loss": 0.3991,
      "step": 553500
    },
    {
      "epoch": 45.05255453676784,
      "grad_norm": 0.16554364562034607,
      "learning_rate": 0.00047747234873129476,
      "loss": 0.3965,
      "step": 554000
    },
    {
      "epoch": 45.09321568707179,
      "grad_norm": 0.14970187842845917,
      "learning_rate": 0.0004774520169160703,
      "loss": 0.3968,
      "step": 554500
    },
    {
      "epoch": 45.13387683737573,
      "grad_norm": 0.16772159934043884,
      "learning_rate": 0.00047743168510084585,
      "loss": 0.3967,
      "step": 555000
    },
    {
      "epoch": 45.17453798767967,
      "grad_norm": 0.16984328627586365,
      "learning_rate": 0.00047741135328562137,
      "loss": 0.3974,
      "step": 555500
    },
    {
      "epoch": 45.21519913798361,
      "grad_norm": 0.153694286942482,
      "learning_rate": 0.0004773910214703969,
      "loss": 0.3978,
      "step": 556000
    },
    {
      "epoch": 45.25586028828756,
      "grad_norm": 0.14556802809238434,
      "learning_rate": 0.00047737068965517245,
      "loss": 0.3975,
      "step": 556500
    },
    {
      "epoch": 45.2965214385915,
      "grad_norm": 0.18421238660812378,
      "learning_rate": 0.00047735035783994797,
      "loss": 0.3976,
      "step": 557000
    },
    {
      "epoch": 45.33718258889544,
      "grad_norm": 0.15994249284267426,
      "learning_rate": 0.0004773300260247235,
      "loss": 0.3982,
      "step": 557500
    },
    {
      "epoch": 45.377843739199385,
      "grad_norm": 0.1694306582212448,
      "learning_rate": 0.00047730969420949906,
      "loss": 0.398,
      "step": 558000
    },
    {
      "epoch": 45.418504889503325,
      "grad_norm": 0.18692727386951447,
      "learning_rate": 0.00047728936239427457,
      "loss": 0.3982,
      "step": 558500
    },
    {
      "epoch": 45.459166039807265,
      "grad_norm": 0.14766480028629303,
      "learning_rate": 0.00047726903057905014,
      "loss": 0.3981,
      "step": 559000
    },
    {
      "epoch": 45.499827190111205,
      "grad_norm": 0.1802978664636612,
      "learning_rate": 0.00047724869876382566,
      "loss": 0.3988,
      "step": 559500
    },
    {
      "epoch": 45.54048834041515,
      "grad_norm": 0.1370786428451538,
      "learning_rate": 0.0004772283669486012,
      "loss": 0.3991,
      "step": 560000
    },
    {
      "epoch": 45.58114949071909,
      "grad_norm": 0.16248829662799835,
      "learning_rate": 0.00047720803513337675,
      "loss": 0.3988,
      "step": 560500
    },
    {
      "epoch": 45.62181064102303,
      "grad_norm": 0.16299009323120117,
      "learning_rate": 0.00047718770331815226,
      "loss": 0.3982,
      "step": 561000
    },
    {
      "epoch": 45.66247179132698,
      "grad_norm": 0.16726277768611908,
      "learning_rate": 0.0004771673715029278,
      "loss": 0.3992,
      "step": 561500
    },
    {
      "epoch": 45.70313294163092,
      "grad_norm": 0.16001155972480774,
      "learning_rate": 0.00047714703968770335,
      "loss": 0.399,
      "step": 562000
    },
    {
      "epoch": 45.74379409193486,
      "grad_norm": 0.14523978531360626,
      "learning_rate": 0.00047712670787247887,
      "loss": 0.3996,
      "step": 562500
    },
    {
      "epoch": 45.784455242238806,
      "grad_norm": 0.14941012859344482,
      "learning_rate": 0.0004771063760572544,
      "loss": 0.3992,
      "step": 563000
    },
    {
      "epoch": 45.825116392542746,
      "grad_norm": 0.15464238822460175,
      "learning_rate": 0.00047708604424202995,
      "loss": 0.3992,
      "step": 563500
    },
    {
      "epoch": 45.865777542846686,
      "grad_norm": 0.16553637385368347,
      "learning_rate": 0.00047706571242680547,
      "loss": 0.3999,
      "step": 564000
    },
    {
      "epoch": 45.906438693150626,
      "grad_norm": 0.13806000351905823,
      "learning_rate": 0.00047704538061158104,
      "loss": 0.3994,
      "step": 564500
    },
    {
      "epoch": 45.94709984345457,
      "grad_norm": 0.1667519509792328,
      "learning_rate": 0.00047702504879635656,
      "loss": 0.3998,
      "step": 565000
    },
    {
      "epoch": 45.98776099375851,
      "grad_norm": 0.1521938294172287,
      "learning_rate": 0.0004770047169811321,
      "loss": 0.3994,
      "step": 565500
    },
    {
      "epoch": 46.02842214406245,
      "grad_norm": 0.17139869928359985,
      "learning_rate": 0.00047698438516590764,
      "loss": 0.3966,
      "step": 566000
    },
    {
      "epoch": 46.0690832943664,
      "grad_norm": 0.1364576667547226,
      "learning_rate": 0.00047696405335068316,
      "loss": 0.3961,
      "step": 566500
    },
    {
      "epoch": 46.10974444467034,
      "grad_norm": 0.17127440869808197,
      "learning_rate": 0.0004769437215354587,
      "loss": 0.396,
      "step": 567000
    },
    {
      "epoch": 46.15040559497428,
      "grad_norm": 0.16850021481513977,
      "learning_rate": 0.00047692338972023425,
      "loss": 0.3968,
      "step": 567500
    },
    {
      "epoch": 46.19106674527822,
      "grad_norm": 0.17565816640853882,
      "learning_rate": 0.00047690305790500976,
      "loss": 0.3974,
      "step": 568000
    },
    {
      "epoch": 46.23172789558217,
      "grad_norm": 0.15100906789302826,
      "learning_rate": 0.0004768827260897853,
      "loss": 0.3969,
      "step": 568500
    },
    {
      "epoch": 46.27238904588611,
      "grad_norm": 0.1642887443304062,
      "learning_rate": 0.00047686239427456085,
      "loss": 0.3973,
      "step": 569000
    },
    {
      "epoch": 46.31305019619005,
      "grad_norm": 0.14462195336818695,
      "learning_rate": 0.00047684206245933637,
      "loss": 0.3977,
      "step": 569500
    },
    {
      "epoch": 46.353711346493995,
      "grad_norm": 0.18041110038757324,
      "learning_rate": 0.00047682173064411194,
      "loss": 0.3977,
      "step": 570000
    },
    {
      "epoch": 46.394372496797935,
      "grad_norm": 0.18226759135723114,
      "learning_rate": 0.00047680139882888745,
      "loss": 0.3979,
      "step": 570500
    },
    {
      "epoch": 46.435033647101875,
      "grad_norm": 0.1537735015153885,
      "learning_rate": 0.00047678106701366297,
      "loss": 0.3982,
      "step": 571000
    },
    {
      "epoch": 46.47569479740582,
      "grad_norm": 0.14810124039649963,
      "learning_rate": 0.00047676073519843854,
      "loss": 0.3983,
      "step": 571500
    },
    {
      "epoch": 46.51635594770976,
      "grad_norm": 0.19472743570804596,
      "learning_rate": 0.00047674040338321406,
      "loss": 0.3985,
      "step": 572000
    },
    {
      "epoch": 46.5570170980137,
      "grad_norm": 0.18960769474506378,
      "learning_rate": 0.0004767200715679896,
      "loss": 0.3984,
      "step": 572500
    },
    {
      "epoch": 46.59767824831764,
      "grad_norm": 0.17114566266536713,
      "learning_rate": 0.00047669973975276515,
      "loss": 0.3985,
      "step": 573000
    },
    {
      "epoch": 46.63833939862159,
      "grad_norm": 0.15535184741020203,
      "learning_rate": 0.00047667940793754066,
      "loss": 0.3988,
      "step": 573500
    },
    {
      "epoch": 46.67900054892553,
      "grad_norm": 0.1688118278980255,
      "learning_rate": 0.00047665907612231623,
      "loss": 0.3991,
      "step": 574000
    },
    {
      "epoch": 46.71966169922947,
      "grad_norm": 0.13930386304855347,
      "learning_rate": 0.00047663874430709175,
      "loss": 0.3991,
      "step": 574500
    },
    {
      "epoch": 46.760322849533416,
      "grad_norm": 0.1661957949399948,
      "learning_rate": 0.00047661841249186727,
      "loss": 0.3989,
      "step": 575000
    },
    {
      "epoch": 46.80098399983736,
      "grad_norm": 0.17109225690364838,
      "learning_rate": 0.00047659808067664284,
      "loss": 0.3989,
      "step": 575500
    },
    {
      "epoch": 46.8416451501413,
      "grad_norm": 0.1706792414188385,
      "learning_rate": 0.00047657774886141835,
      "loss": 0.3993,
      "step": 576000
    },
    {
      "epoch": 46.88230630044524,
      "grad_norm": 0.15810975432395935,
      "learning_rate": 0.00047655741704619387,
      "loss": 0.3991,
      "step": 576500
    },
    {
      "epoch": 46.922967450749184,
      "grad_norm": 0.17160215973854065,
      "learning_rate": 0.00047653708523096944,
      "loss": 0.3993,
      "step": 577000
    },
    {
      "epoch": 46.963628601053124,
      "grad_norm": 0.16858437657356262,
      "learning_rate": 0.00047651675341574496,
      "loss": 0.3995,
      "step": 577500
    },
    {
      "epoch": 47.004289751357064,
      "grad_norm": 0.1688716560602188,
      "learning_rate": 0.00047649642160052047,
      "loss": 0.3988,
      "step": 578000
    },
    {
      "epoch": 47.04495090166101,
      "grad_norm": 0.16334249079227448,
      "learning_rate": 0.00047647608978529604,
      "loss": 0.3954,
      "step": 578500
    },
    {
      "epoch": 47.08561205196495,
      "grad_norm": 0.179725781083107,
      "learning_rate": 0.00047645575797007156,
      "loss": 0.3956,
      "step": 579000
    },
    {
      "epoch": 47.12627320226889,
      "grad_norm": 0.13875891268253326,
      "learning_rate": 0.00047643542615484713,
      "loss": 0.3959,
      "step": 579500
    },
    {
      "epoch": 47.16693435257283,
      "grad_norm": 0.16184666752815247,
      "learning_rate": 0.00047641509433962265,
      "loss": 0.3974,
      "step": 580000
    },
    {
      "epoch": 47.20759550287678,
      "grad_norm": 0.16712874174118042,
      "learning_rate": 0.00047639476252439816,
      "loss": 0.396,
      "step": 580500
    },
    {
      "epoch": 47.24825665318072,
      "grad_norm": 0.13622891902923584,
      "learning_rate": 0.00047637443070917373,
      "loss": 0.397,
      "step": 581000
    },
    {
      "epoch": 47.28891780348466,
      "grad_norm": 0.18377946317195892,
      "learning_rate": 0.00047635409889394925,
      "loss": 0.3973,
      "step": 581500
    },
    {
      "epoch": 47.329578953788605,
      "grad_norm": 0.179933562874794,
      "learning_rate": 0.00047633376707872477,
      "loss": 0.3978,
      "step": 582000
    },
    {
      "epoch": 47.370240104092545,
      "grad_norm": 0.1649143248796463,
      "learning_rate": 0.00047631343526350034,
      "loss": 0.3974,
      "step": 582500
    },
    {
      "epoch": 47.410901254396485,
      "grad_norm": 0.14982250332832336,
      "learning_rate": 0.00047629310344827585,
      "loss": 0.3976,
      "step": 583000
    },
    {
      "epoch": 47.45156240470043,
      "grad_norm": 0.17194750905036926,
      "learning_rate": 0.00047627277163305137,
      "loss": 0.3977,
      "step": 583500
    },
    {
      "epoch": 47.49222355500437,
      "grad_norm": 0.17460200190544128,
      "learning_rate": 0.00047625243981782694,
      "loss": 0.398,
      "step": 584000
    },
    {
      "epoch": 47.53288470530831,
      "grad_norm": 0.15837347507476807,
      "learning_rate": 0.00047623210800260246,
      "loss": 0.3979,
      "step": 584500
    },
    {
      "epoch": 47.57354585561225,
      "grad_norm": 0.1667797565460205,
      "learning_rate": 0.00047621177618737803,
      "loss": 0.3982,
      "step": 585000
    },
    {
      "epoch": 47.6142070059162,
      "grad_norm": 0.18308816850185394,
      "learning_rate": 0.00047619144437215354,
      "loss": 0.3981,
      "step": 585500
    },
    {
      "epoch": 47.65486815622014,
      "grad_norm": 0.15064138174057007,
      "learning_rate": 0.00047617111255692906,
      "loss": 0.3985,
      "step": 586000
    },
    {
      "epoch": 47.69552930652408,
      "grad_norm": 0.16693320870399475,
      "learning_rate": 0.00047615078074170463,
      "loss": 0.3985,
      "step": 586500
    },
    {
      "epoch": 47.73619045682803,
      "grad_norm": 0.16806632280349731,
      "learning_rate": 0.00047613044892648015,
      "loss": 0.3984,
      "step": 587000
    },
    {
      "epoch": 47.77685160713197,
      "grad_norm": 0.17942197620868683,
      "learning_rate": 0.00047611011711125566,
      "loss": 0.398,
      "step": 587500
    },
    {
      "epoch": 47.81751275743591,
      "grad_norm": 0.16760492324829102,
      "learning_rate": 0.00047608978529603123,
      "loss": 0.3988,
      "step": 588000
    },
    {
      "epoch": 47.85817390773985,
      "grad_norm": 0.1630944013595581,
      "learning_rate": 0.00047606945348080675,
      "loss": 0.3993,
      "step": 588500
    },
    {
      "epoch": 47.898835058043794,
      "grad_norm": 0.16664086282253265,
      "learning_rate": 0.00047604912166558227,
      "loss": 0.3991,
      "step": 589000
    },
    {
      "epoch": 47.939496208347734,
      "grad_norm": 0.154347762465477,
      "learning_rate": 0.00047602878985035784,
      "loss": 0.3989,
      "step": 589500
    },
    {
      "epoch": 47.980157358651674,
      "grad_norm": 0.1697000116109848,
      "learning_rate": 0.00047600845803513335,
      "loss": 0.3993,
      "step": 590000
    },
    {
      "epoch": 48.02081850895562,
      "grad_norm": 0.14837732911109924,
      "learning_rate": 0.0004759881262199089,
      "loss": 0.3971,
      "step": 590500
    },
    {
      "epoch": 48.06147965925956,
      "grad_norm": 0.17333629727363586,
      "learning_rate": 0.00047596779440468444,
      "loss": 0.3953,
      "step": 591000
    },
    {
      "epoch": 48.1021408095635,
      "grad_norm": 0.17532703280448914,
      "learning_rate": 0.00047594746258945996,
      "loss": 0.3955,
      "step": 591500
    },
    {
      "epoch": 48.14280195986745,
      "grad_norm": 0.16405396163463593,
      "learning_rate": 0.00047592713077423553,
      "loss": 0.3958,
      "step": 592000
    },
    {
      "epoch": 48.18346311017139,
      "grad_norm": 0.16444717347621918,
      "learning_rate": 0.00047590679895901104,
      "loss": 0.3966,
      "step": 592500
    },
    {
      "epoch": 48.22412426047533,
      "grad_norm": 0.17597080767154694,
      "learning_rate": 0.00047588646714378656,
      "loss": 0.3965,
      "step": 593000
    },
    {
      "epoch": 48.26478541077927,
      "grad_norm": 0.16505935788154602,
      "learning_rate": 0.00047586613532856213,
      "loss": 0.3966,
      "step": 593500
    },
    {
      "epoch": 48.305446561083215,
      "grad_norm": 0.1523519605398178,
      "learning_rate": 0.00047584580351333765,
      "loss": 0.3963,
      "step": 594000
    },
    {
      "epoch": 48.346107711387155,
      "grad_norm": 0.15972183644771576,
      "learning_rate": 0.0004758254716981132,
      "loss": 0.397,
      "step": 594500
    },
    {
      "epoch": 48.386768861691095,
      "grad_norm": 0.1741960197687149,
      "learning_rate": 0.00047580513988288874,
      "loss": 0.3976,
      "step": 595000
    },
    {
      "epoch": 48.42743001199504,
      "grad_norm": 0.15404008328914642,
      "learning_rate": 0.00047578480806766425,
      "loss": 0.3973,
      "step": 595500
    },
    {
      "epoch": 48.46809116229898,
      "grad_norm": 0.16099412739276886,
      "learning_rate": 0.0004757644762524398,
      "loss": 0.397,
      "step": 596000
    },
    {
      "epoch": 48.50875231260292,
      "grad_norm": 0.15974204242229462,
      "learning_rate": 0.00047574414443721534,
      "loss": 0.3974,
      "step": 596500
    },
    {
      "epoch": 48.54941346290686,
      "grad_norm": 0.15753859281539917,
      "learning_rate": 0.00047572381262199086,
      "loss": 0.3976,
      "step": 597000
    },
    {
      "epoch": 48.59007461321081,
      "grad_norm": 0.19260664284229279,
      "learning_rate": 0.0004757034808067665,
      "loss": 0.3978,
      "step": 597500
    },
    {
      "epoch": 48.63073576351475,
      "grad_norm": 0.14799049496650696,
      "learning_rate": 0.000475683148991542,
      "loss": 0.3982,
      "step": 598000
    },
    {
      "epoch": 48.67139691381869,
      "grad_norm": 0.14054840803146362,
      "learning_rate": 0.0004756628171763175,
      "loss": 0.398,
      "step": 598500
    },
    {
      "epoch": 48.71205806412264,
      "grad_norm": 0.17102983593940735,
      "learning_rate": 0.0004756424853610931,
      "loss": 0.3985,
      "step": 599000
    },
    {
      "epoch": 48.75271921442658,
      "grad_norm": 0.16190780699253082,
      "learning_rate": 0.0004756221535458686,
      "loss": 0.3989,
      "step": 599500
    },
    {
      "epoch": 48.79338036473052,
      "grad_norm": 0.19618330895900726,
      "learning_rate": 0.00047560182173064417,
      "loss": 0.3985,
      "step": 600000
    },
    {
      "epoch": 48.83404151503446,
      "grad_norm": 0.16670101881027222,
      "learning_rate": 0.0004755814899154197,
      "loss": 0.3985,
      "step": 600500
    },
    {
      "epoch": 48.874702665338404,
      "grad_norm": 0.17506203055381775,
      "learning_rate": 0.0004755611581001952,
      "loss": 0.3981,
      "step": 601000
    },
    {
      "epoch": 48.915363815642344,
      "grad_norm": 0.18379619717597961,
      "learning_rate": 0.0004755408262849708,
      "loss": 0.3988,
      "step": 601500
    },
    {
      "epoch": 48.956024965946284,
      "grad_norm": 0.16454944014549255,
      "learning_rate": 0.0004755204944697463,
      "loss": 0.3988,
      "step": 602000
    },
    {
      "epoch": 48.99668611625023,
      "grad_norm": 0.15938550233840942,
      "learning_rate": 0.0004755001626545218,
      "loss": 0.3989,
      "step": 602500
    },
    {
      "epoch": 49.03734726655417,
      "grad_norm": 0.17215490341186523,
      "learning_rate": 0.0004754798308392974,
      "loss": 0.3952,
      "step": 603000
    },
    {
      "epoch": 49.07800841685811,
      "grad_norm": 0.15815840661525726,
      "learning_rate": 0.0004754594990240729,
      "loss": 0.3949,
      "step": 603500
    },
    {
      "epoch": 49.11866956716206,
      "grad_norm": 0.1575680673122406,
      "learning_rate": 0.0004754391672088484,
      "loss": 0.3955,
      "step": 604000
    },
    {
      "epoch": 49.159330717466,
      "grad_norm": 0.1609722077846527,
      "learning_rate": 0.000475418835393624,
      "loss": 0.3953,
      "step": 604500
    },
    {
      "epoch": 49.19999186776994,
      "grad_norm": 0.1761251538991928,
      "learning_rate": 0.0004753985035783995,
      "loss": 0.3957,
      "step": 605000
    },
    {
      "epoch": 49.24065301807388,
      "grad_norm": 0.18302665650844574,
      "learning_rate": 0.00047537817176317507,
      "loss": 0.396,
      "step": 605500
    },
    {
      "epoch": 49.281314168377826,
      "grad_norm": 0.16500380635261536,
      "learning_rate": 0.0004753578399479506,
      "loss": 0.3965,
      "step": 606000
    },
    {
      "epoch": 49.321975318681766,
      "grad_norm": 0.17278002202510834,
      "learning_rate": 0.0004753375081327261,
      "loss": 0.3969,
      "step": 606500
    },
    {
      "epoch": 49.362636468985706,
      "grad_norm": 0.17022190988063812,
      "learning_rate": 0.00047531717631750167,
      "loss": 0.397,
      "step": 607000
    },
    {
      "epoch": 49.40329761928965,
      "grad_norm": 0.14109821617603302,
      "learning_rate": 0.0004752968445022772,
      "loss": 0.3966,
      "step": 607500
    },
    {
      "epoch": 49.44395876959359,
      "grad_norm": 0.16139112412929535,
      "learning_rate": 0.0004752765126870527,
      "loss": 0.3977,
      "step": 608000
    },
    {
      "epoch": 49.48461991989753,
      "grad_norm": 0.16446252167224884,
      "learning_rate": 0.0004752561808718283,
      "loss": 0.397,
      "step": 608500
    },
    {
      "epoch": 49.52528107020147,
      "grad_norm": 0.16344033181667328,
      "learning_rate": 0.0004752358490566038,
      "loss": 0.3975,
      "step": 609000
    },
    {
      "epoch": 49.56594222050542,
      "grad_norm": 0.14789243042469025,
      "learning_rate": 0.00047521551724137936,
      "loss": 0.3976,
      "step": 609500
    },
    {
      "epoch": 49.60660337080936,
      "grad_norm": 0.1876799464225769,
      "learning_rate": 0.0004751951854261549,
      "loss": 0.3978,
      "step": 610000
    },
    {
      "epoch": 49.6472645211133,
      "grad_norm": 0.1697746068239212,
      "learning_rate": 0.0004751748536109304,
      "loss": 0.3973,
      "step": 610500
    },
    {
      "epoch": 49.68792567141725,
      "grad_norm": 0.18254536390304565,
      "learning_rate": 0.00047515452179570597,
      "loss": 0.3988,
      "step": 611000
    },
    {
      "epoch": 49.72858682172119,
      "grad_norm": 0.16585855185985565,
      "learning_rate": 0.0004751341899804815,
      "loss": 0.3978,
      "step": 611500
    },
    {
      "epoch": 49.76924797202513,
      "grad_norm": 0.1434343308210373,
      "learning_rate": 0.000475113858165257,
      "loss": 0.398,
      "step": 612000
    },
    {
      "epoch": 49.809909122329074,
      "grad_norm": 0.16969889402389526,
      "learning_rate": 0.00047509352635003257,
      "loss": 0.398,
      "step": 612500
    },
    {
      "epoch": 49.850570272633014,
      "grad_norm": 0.17587976157665253,
      "learning_rate": 0.0004750731945348081,
      "loss": 0.398,
      "step": 613000
    },
    {
      "epoch": 49.891231422936954,
      "grad_norm": 0.18654818832874298,
      "learning_rate": 0.0004750528627195836,
      "loss": 0.3982,
      "step": 613500
    },
    {
      "epoch": 49.931892573240894,
      "grad_norm": 0.18681985139846802,
      "learning_rate": 0.00047503253090435917,
      "loss": 0.3986,
      "step": 614000
    },
    {
      "epoch": 49.97255372354484,
      "grad_norm": 0.1699286550283432,
      "learning_rate": 0.0004750121990891347,
      "loss": 0.3982,
      "step": 614500
    },
    {
      "epoch": 50.01321487384878,
      "grad_norm": 0.16427603363990784,
      "learning_rate": 0.00047499186727391026,
      "loss": 0.3969,
      "step": 615000
    },
    {
      "epoch": 50.05387602415272,
      "grad_norm": 0.15980803966522217,
      "learning_rate": 0.0004749715354586858,
      "loss": 0.3949,
      "step": 615500
    },
    {
      "epoch": 50.09453717445667,
      "grad_norm": 0.16607476770877838,
      "learning_rate": 0.0004749512036434613,
      "loss": 0.395,
      "step": 616000
    },
    {
      "epoch": 50.13519832476061,
      "grad_norm": 0.15743416547775269,
      "learning_rate": 0.00047493087182823686,
      "loss": 0.3951,
      "step": 616500
    },
    {
      "epoch": 50.17585947506455,
      "grad_norm": 0.1627485305070877,
      "learning_rate": 0.0004749105400130124,
      "loss": 0.3953,
      "step": 617000
    },
    {
      "epoch": 50.21652062536849,
      "grad_norm": 0.1683044731616974,
      "learning_rate": 0.0004748902081977879,
      "loss": 0.3964,
      "step": 617500
    },
    {
      "epoch": 50.257181775672436,
      "grad_norm": 0.17566809058189392,
      "learning_rate": 0.00047486987638256347,
      "loss": 0.396,
      "step": 618000
    },
    {
      "epoch": 50.297842925976376,
      "grad_norm": 0.14739210903644562,
      "learning_rate": 0.000474849544567339,
      "loss": 0.3964,
      "step": 618500
    },
    {
      "epoch": 50.338504076280316,
      "grad_norm": 0.16755153238773346,
      "learning_rate": 0.0004748292127521145,
      "loss": 0.3968,
      "step": 619000
    },
    {
      "epoch": 50.37916522658426,
      "grad_norm": 0.16740202903747559,
      "learning_rate": 0.00047480888093689007,
      "loss": 0.3968,
      "step": 619500
    },
    {
      "epoch": 50.4198263768882,
      "grad_norm": 0.16528497636318207,
      "learning_rate": 0.0004747885491216656,
      "loss": 0.3966,
      "step": 620000
    },
    {
      "epoch": 50.46048752719214,
      "grad_norm": 0.19205130636692047,
      "learning_rate": 0.00047476821730644116,
      "loss": 0.3966,
      "step": 620500
    },
    {
      "epoch": 50.50114867749608,
      "grad_norm": 0.16739779710769653,
      "learning_rate": 0.0004747478854912167,
      "loss": 0.397,
      "step": 621000
    },
    {
      "epoch": 50.54180982780003,
      "grad_norm": 0.17929688096046448,
      "learning_rate": 0.0004747275536759922,
      "loss": 0.3971,
      "step": 621500
    },
    {
      "epoch": 50.58247097810397,
      "grad_norm": 0.18921828269958496,
      "learning_rate": 0.00047470722186076776,
      "loss": 0.3969,
      "step": 622000
    },
    {
      "epoch": 50.62313212840791,
      "grad_norm": 0.17592279613018036,
      "learning_rate": 0.0004746868900455433,
      "loss": 0.3972,
      "step": 622500
    },
    {
      "epoch": 50.66379327871186,
      "grad_norm": 0.16317446529865265,
      "learning_rate": 0.0004746665582303188,
      "loss": 0.3979,
      "step": 623000
    },
    {
      "epoch": 50.7044544290158,
      "grad_norm": 0.15671369433403015,
      "learning_rate": 0.00047464622641509436,
      "loss": 0.3972,
      "step": 623500
    },
    {
      "epoch": 50.74511557931974,
      "grad_norm": 0.1667100340127945,
      "learning_rate": 0.0004746258945998699,
      "loss": 0.3981,
      "step": 624000
    },
    {
      "epoch": 50.785776729623684,
      "grad_norm": 0.17669084668159485,
      "learning_rate": 0.00047460556278464545,
      "loss": 0.3976,
      "step": 624500
    },
    {
      "epoch": 50.826437879927624,
      "grad_norm": 0.16467934846878052,
      "learning_rate": 0.00047458523096942097,
      "loss": 0.3975,
      "step": 625000
    },
    {
      "epoch": 50.867099030231564,
      "grad_norm": 0.17444546520709991,
      "learning_rate": 0.0004745648991541965,
      "loss": 0.3983,
      "step": 625500
    },
    {
      "epoch": 50.907760180535504,
      "grad_norm": 0.17206701636314392,
      "learning_rate": 0.00047454456733897205,
      "loss": 0.3976,
      "step": 626000
    },
    {
      "epoch": 50.94842133083945,
      "grad_norm": 0.16413280367851257,
      "learning_rate": 0.00047452423552374757,
      "loss": 0.3981,
      "step": 626500
    },
    {
      "epoch": 50.98908248114339,
      "grad_norm": 0.15932078659534454,
      "learning_rate": 0.0004745039037085231,
      "loss": 0.3983,
      "step": 627000
    },
    {
      "epoch": 51.02974363144733,
      "grad_norm": 0.16891014575958252,
      "learning_rate": 0.00047448357189329866,
      "loss": 0.3947,
      "step": 627500
    },
    {
      "epoch": 51.07040478175128,
      "grad_norm": 0.17428304255008698,
      "learning_rate": 0.0004744632400780742,
      "loss": 0.3944,
      "step": 628000
    },
    {
      "epoch": 51.11106593205522,
      "grad_norm": 0.20583972334861755,
      "learning_rate": 0.0004744429082628497,
      "loss": 0.3947,
      "step": 628500
    },
    {
      "epoch": 51.15172708235916,
      "grad_norm": 0.17628052830696106,
      "learning_rate": 0.00047442257644762526,
      "loss": 0.3945,
      "step": 629000
    },
    {
      "epoch": 51.1923882326631,
      "grad_norm": 0.15233232080936432,
      "learning_rate": 0.0004744022446324008,
      "loss": 0.3953,
      "step": 629500
    },
    {
      "epoch": 51.233049382967046,
      "grad_norm": 0.16488340497016907,
      "learning_rate": 0.00047438191281717635,
      "loss": 0.3953,
      "step": 630000
    },
    {
      "epoch": 51.273710533270986,
      "grad_norm": 0.20673197507858276,
      "learning_rate": 0.00047436158100195187,
      "loss": 0.3952,
      "step": 630500
    },
    {
      "epoch": 51.314371683574926,
      "grad_norm": 0.1465233415365219,
      "learning_rate": 0.0004743412491867274,
      "loss": 0.3958,
      "step": 631000
    },
    {
      "epoch": 51.35503283387887,
      "grad_norm": 0.17371542751789093,
      "learning_rate": 0.00047432091737150295,
      "loss": 0.396,
      "step": 631500
    },
    {
      "epoch": 51.39569398418281,
      "grad_norm": 0.15748946368694305,
      "learning_rate": 0.00047430058555627847,
      "loss": 0.3963,
      "step": 632000
    },
    {
      "epoch": 51.43635513448675,
      "grad_norm": 0.20146620273590088,
      "learning_rate": 0.000474280253741054,
      "loss": 0.3969,
      "step": 632500
    },
    {
      "epoch": 51.4770162847907,
      "grad_norm": 0.16140244901180267,
      "learning_rate": 0.00047425992192582956,
      "loss": 0.3965,
      "step": 633000
    },
    {
      "epoch": 51.51767743509464,
      "grad_norm": 0.1710335612297058,
      "learning_rate": 0.00047423959011060507,
      "loss": 0.3969,
      "step": 633500
    },
    {
      "epoch": 51.55833858539858,
      "grad_norm": 0.16420994699001312,
      "learning_rate": 0.0004742192582953806,
      "loss": 0.3972,
      "step": 634000
    },
    {
      "epoch": 51.59899973570252,
      "grad_norm": 0.1678844392299652,
      "learning_rate": 0.00047419892648015616,
      "loss": 0.3973,
      "step": 634500
    },
    {
      "epoch": 51.63966088600647,
      "grad_norm": 0.183233842253685,
      "learning_rate": 0.0004741785946649317,
      "loss": 0.3973,
      "step": 635000
    },
    {
      "epoch": 51.68032203631041,
      "grad_norm": 0.17457881569862366,
      "learning_rate": 0.00047415826284970725,
      "loss": 0.3969,
      "step": 635500
    },
    {
      "epoch": 51.72098318661435,
      "grad_norm": 0.17072376608848572,
      "learning_rate": 0.00047413793103448276,
      "loss": 0.3975,
      "step": 636000
    },
    {
      "epoch": 51.761644336918295,
      "grad_norm": 0.1685270071029663,
      "learning_rate": 0.0004741175992192583,
      "loss": 0.3976,
      "step": 636500
    },
    {
      "epoch": 51.802305487222235,
      "grad_norm": 0.1726234406232834,
      "learning_rate": 0.00047409726740403385,
      "loss": 0.3977,
      "step": 637000
    },
    {
      "epoch": 51.842966637526175,
      "grad_norm": 0.14788520336151123,
      "learning_rate": 0.00047407693558880937,
      "loss": 0.3978,
      "step": 637500
    },
    {
      "epoch": 51.883627787830115,
      "grad_norm": 0.16757328808307648,
      "learning_rate": 0.0004740566037735849,
      "loss": 0.3976,
      "step": 638000
    },
    {
      "epoch": 51.92428893813406,
      "grad_norm": 0.1851847916841507,
      "learning_rate": 0.00047403627195836045,
      "loss": 0.3977,
      "step": 638500
    },
    {
      "epoch": 51.964950088438,
      "grad_norm": 0.16345278918743134,
      "learning_rate": 0.00047401594014313597,
      "loss": 0.3982,
      "step": 639000
    },
    {
      "epoch": 52.00561123874194,
      "grad_norm": 0.15665262937545776,
      "learning_rate": 0.0004739956083279115,
      "loss": 0.3973,
      "step": 639500
    },
    {
      "epoch": 52.04627238904589,
      "grad_norm": 0.17069430649280548,
      "learning_rate": 0.00047397527651268706,
      "loss": 0.3935,
      "step": 640000
    },
    {
      "epoch": 52.08693353934983,
      "grad_norm": 0.1821199208498001,
      "learning_rate": 0.0004739549446974626,
      "loss": 0.3941,
      "step": 640500
    },
    {
      "epoch": 52.12759468965377,
      "grad_norm": 0.16649071872234344,
      "learning_rate": 0.00047393461288223814,
      "loss": 0.3948,
      "step": 641000
    },
    {
      "epoch": 52.16825583995771,
      "grad_norm": 0.2062229961156845,
      "learning_rate": 0.00047391428106701366,
      "loss": 0.3951,
      "step": 641500
    },
    {
      "epoch": 52.208916990261656,
      "grad_norm": 0.18024125695228577,
      "learning_rate": 0.0004738939492517892,
      "loss": 0.3953,
      "step": 642000
    },
    {
      "epoch": 52.249578140565596,
      "grad_norm": 0.154142826795578,
      "learning_rate": 0.00047387361743656475,
      "loss": 0.3952,
      "step": 642500
    },
    {
      "epoch": 52.290239290869536,
      "grad_norm": 0.1522083431482315,
      "learning_rate": 0.00047385328562134026,
      "loss": 0.3952,
      "step": 643000
    },
    {
      "epoch": 52.33090044117348,
      "grad_norm": 0.17104202508926392,
      "learning_rate": 0.0004738329538061158,
      "loss": 0.3958,
      "step": 643500
    },
    {
      "epoch": 52.37156159147742,
      "grad_norm": 0.19164925813674927,
      "learning_rate": 0.00047381262199089135,
      "loss": 0.396,
      "step": 644000
    },
    {
      "epoch": 52.41222274178136,
      "grad_norm": 0.15280930697917938,
      "learning_rate": 0.00047379229017566687,
      "loss": 0.396,
      "step": 644500
    },
    {
      "epoch": 52.45288389208531,
      "grad_norm": 0.17080791294574738,
      "learning_rate": 0.00047377195836044244,
      "loss": 0.3963,
      "step": 645000
    },
    {
      "epoch": 52.49354504238925,
      "grad_norm": 0.16032572090625763,
      "learning_rate": 0.00047375162654521795,
      "loss": 0.396,
      "step": 645500
    },
    {
      "epoch": 52.53420619269319,
      "grad_norm": 0.16954129934310913,
      "learning_rate": 0.00047373129472999347,
      "loss": 0.3967,
      "step": 646000
    },
    {
      "epoch": 52.57486734299713,
      "grad_norm": 0.1723168045282364,
      "learning_rate": 0.00047371096291476904,
      "loss": 0.3972,
      "step": 646500
    },
    {
      "epoch": 52.61552849330108,
      "grad_norm": 0.17867396771907806,
      "learning_rate": 0.00047369063109954456,
      "loss": 0.3964,
      "step": 647000
    },
    {
      "epoch": 52.65618964360502,
      "grad_norm": 0.18600256741046906,
      "learning_rate": 0.0004736702992843201,
      "loss": 0.397,
      "step": 647500
    },
    {
      "epoch": 52.69685079390896,
      "grad_norm": 0.15524934232234955,
      "learning_rate": 0.00047364996746909565,
      "loss": 0.3968,
      "step": 648000
    },
    {
      "epoch": 52.737511944212905,
      "grad_norm": 0.17453967034816742,
      "learning_rate": 0.00047362963565387116,
      "loss": 0.397,
      "step": 648500
    },
    {
      "epoch": 52.778173094516845,
      "grad_norm": 0.17972250282764435,
      "learning_rate": 0.0004736093038386467,
      "loss": 0.3973,
      "step": 649000
    },
    {
      "epoch": 52.818834244820785,
      "grad_norm": 0.1689978390932083,
      "learning_rate": 0.00047358897202342225,
      "loss": 0.3979,
      "step": 649500
    },
    {
      "epoch": 52.859495395124725,
      "grad_norm": 0.14252103865146637,
      "learning_rate": 0.00047356864020819776,
      "loss": 0.3974,
      "step": 650000
    },
    {
      "epoch": 52.90015654542867,
      "grad_norm": 0.1684567779302597,
      "learning_rate": 0.00047354830839297334,
      "loss": 0.3977,
      "step": 650500
    },
    {
      "epoch": 52.94081769573261,
      "grad_norm": 0.1589447557926178,
      "learning_rate": 0.00047352797657774885,
      "loss": 0.3975,
      "step": 651000
    },
    {
      "epoch": 52.98147884603655,
      "grad_norm": 0.16898486018180847,
      "learning_rate": 0.00047350764476252437,
      "loss": 0.3981,
      "step": 651500
    },
    {
      "epoch": 53.0221399963405,
      "grad_norm": 0.17135286331176758,
      "learning_rate": 0.00047348731294729994,
      "loss": 0.3959,
      "step": 652000
    },
    {
      "epoch": 53.06280114664444,
      "grad_norm": 0.1599072962999344,
      "learning_rate": 0.00047346698113207546,
      "loss": 0.3938,
      "step": 652500
    },
    {
      "epoch": 53.10346229694838,
      "grad_norm": 0.17973260581493378,
      "learning_rate": 0.00047344664931685097,
      "loss": 0.3942,
      "step": 653000
    },
    {
      "epoch": 53.144123447252326,
      "grad_norm": 0.16648191213607788,
      "learning_rate": 0.00047342631750162654,
      "loss": 0.3946,
      "step": 653500
    },
    {
      "epoch": 53.184784597556266,
      "grad_norm": 0.15581464767456055,
      "learning_rate": 0.00047340598568640206,
      "loss": 0.3944,
      "step": 654000
    },
    {
      "epoch": 53.225445747860206,
      "grad_norm": 0.16874480247497559,
      "learning_rate": 0.0004733856538711776,
      "loss": 0.3952,
      "step": 654500
    },
    {
      "epoch": 53.266106898164146,
      "grad_norm": 0.16232112050056458,
      "learning_rate": 0.0004733653220559532,
      "loss": 0.3948,
      "step": 655000
    },
    {
      "epoch": 53.30676804846809,
      "grad_norm": 0.18492425978183746,
      "learning_rate": 0.0004733449902407287,
      "loss": 0.3955,
      "step": 655500
    },
    {
      "epoch": 53.34742919877203,
      "grad_norm": 0.19052714109420776,
      "learning_rate": 0.0004733246584255043,
      "loss": 0.3958,
      "step": 656000
    },
    {
      "epoch": 53.38809034907597,
      "grad_norm": 0.16066202521324158,
      "learning_rate": 0.0004733043266102798,
      "loss": 0.3958,
      "step": 656500
    },
    {
      "epoch": 53.42875149937992,
      "grad_norm": 0.16913443803787231,
      "learning_rate": 0.0004732839947950553,
      "loss": 0.3955,
      "step": 657000
    },
    {
      "epoch": 53.46941264968386,
      "grad_norm": 0.16196678578853607,
      "learning_rate": 0.0004732636629798309,
      "loss": 0.3959,
      "step": 657500
    },
    {
      "epoch": 53.5100737999878,
      "grad_norm": 0.1908242404460907,
      "learning_rate": 0.0004732433311646064,
      "loss": 0.3957,
      "step": 658000
    },
    {
      "epoch": 53.55073495029174,
      "grad_norm": 0.1463027149438858,
      "learning_rate": 0.0004732229993493819,
      "loss": 0.3967,
      "step": 658500
    },
    {
      "epoch": 53.59139610059569,
      "grad_norm": 0.1771537959575653,
      "learning_rate": 0.0004732026675341575,
      "loss": 0.3954,
      "step": 659000
    },
    {
      "epoch": 53.63205725089963,
      "grad_norm": 0.16113866865634918,
      "learning_rate": 0.000473182335718933,
      "loss": 0.3966,
      "step": 659500
    },
    {
      "epoch": 53.67271840120357,
      "grad_norm": 0.17773520946502686,
      "learning_rate": 0.0004731620039037086,
      "loss": 0.3967,
      "step": 660000
    },
    {
      "epoch": 53.713379551507515,
      "grad_norm": 0.18714193999767303,
      "learning_rate": 0.0004731416720884841,
      "loss": 0.3966,
      "step": 660500
    },
    {
      "epoch": 53.754040701811455,
      "grad_norm": 0.1815999448299408,
      "learning_rate": 0.0004731213402732596,
      "loss": 0.3969,
      "step": 661000
    },
    {
      "epoch": 53.794701852115395,
      "grad_norm": 0.17465047538280487,
      "learning_rate": 0.0004731010084580352,
      "loss": 0.3975,
      "step": 661500
    },
    {
      "epoch": 53.835363002419335,
      "grad_norm": 0.18039660155773163,
      "learning_rate": 0.0004730806766428107,
      "loss": 0.3969,
      "step": 662000
    },
    {
      "epoch": 53.87602415272328,
      "grad_norm": 0.1882065385580063,
      "learning_rate": 0.0004730603448275862,
      "loss": 0.3976,
      "step": 662500
    },
    {
      "epoch": 53.91668530302722,
      "grad_norm": 0.1624329537153244,
      "learning_rate": 0.0004730400130123618,
      "loss": 0.3971,
      "step": 663000
    },
    {
      "epoch": 53.95734645333116,
      "grad_norm": 0.18160130083560944,
      "learning_rate": 0.0004730196811971373,
      "loss": 0.3977,
      "step": 663500
    },
    {
      "epoch": 53.99800760363511,
      "grad_norm": 0.1831078678369522,
      "learning_rate": 0.0004729993493819128,
      "loss": 0.3973,
      "step": 664000
    },
    {
      "epoch": 54.03866875393905,
      "grad_norm": 0.2037939876317978,
      "learning_rate": 0.0004729790175666884,
      "loss": 0.3931,
      "step": 664500
    },
    {
      "epoch": 54.07932990424299,
      "grad_norm": 0.16866900026798248,
      "learning_rate": 0.0004729586857514639,
      "loss": 0.3934,
      "step": 665000
    },
    {
      "epoch": 54.11999105454694,
      "grad_norm": 0.18615032732486725,
      "learning_rate": 0.0004729383539362395,
      "loss": 0.3938,
      "step": 665500
    },
    {
      "epoch": 54.16065220485088,
      "grad_norm": 0.15179546177387238,
      "learning_rate": 0.000472918022121015,
      "loss": 0.3942,
      "step": 666000
    },
    {
      "epoch": 54.20131335515482,
      "grad_norm": 0.1742977648973465,
      "learning_rate": 0.0004728976903057905,
      "loss": 0.3947,
      "step": 666500
    },
    {
      "epoch": 54.24197450545876,
      "grad_norm": 0.16205663979053497,
      "learning_rate": 0.0004728773584905661,
      "loss": 0.395,
      "step": 667000
    },
    {
      "epoch": 54.282635655762704,
      "grad_norm": 0.16812539100646973,
      "learning_rate": 0.0004728570266753416,
      "loss": 0.3948,
      "step": 667500
    },
    {
      "epoch": 54.323296806066644,
      "grad_norm": 0.15865299105644226,
      "learning_rate": 0.0004728366948601171,
      "loss": 0.3951,
      "step": 668000
    },
    {
      "epoch": 54.363957956370584,
      "grad_norm": 0.15569239854812622,
      "learning_rate": 0.0004728163630448927,
      "loss": 0.3952,
      "step": 668500
    },
    {
      "epoch": 54.40461910667453,
      "grad_norm": 0.1740916669368744,
      "learning_rate": 0.0004727960312296682,
      "loss": 0.3956,
      "step": 669000
    },
    {
      "epoch": 54.44528025697847,
      "grad_norm": 0.1524277776479721,
      "learning_rate": 0.0004727756994144437,
      "loss": 0.3959,
      "step": 669500
    },
    {
      "epoch": 54.48594140728241,
      "grad_norm": 0.2007215917110443,
      "learning_rate": 0.0004727553675992193,
      "loss": 0.3956,
      "step": 670000
    },
    {
      "epoch": 54.52660255758635,
      "grad_norm": 0.167485773563385,
      "learning_rate": 0.0004727350357839948,
      "loss": 0.3961,
      "step": 670500
    },
    {
      "epoch": 54.5672637078903,
      "grad_norm": 0.2109692394733429,
      "learning_rate": 0.0004727147039687704,
      "loss": 0.3963,
      "step": 671000
    },
    {
      "epoch": 54.60792485819424,
      "grad_norm": 0.18178503215312958,
      "learning_rate": 0.0004726943721535459,
      "loss": 0.3961,
      "step": 671500
    },
    {
      "epoch": 54.64858600849818,
      "grad_norm": 0.15944184362888336,
      "learning_rate": 0.0004726740403383214,
      "loss": 0.3965,
      "step": 672000
    },
    {
      "epoch": 54.689247158802125,
      "grad_norm": 0.17847956717014313,
      "learning_rate": 0.000472653708523097,
      "loss": 0.3966,
      "step": 672500
    },
    {
      "epoch": 54.729908309106065,
      "grad_norm": 0.16424092650413513,
      "learning_rate": 0.0004726333767078725,
      "loss": 0.3972,
      "step": 673000
    },
    {
      "epoch": 54.770569459410005,
      "grad_norm": 0.19134221971035004,
      "learning_rate": 0.000472613044892648,
      "loss": 0.3964,
      "step": 673500
    },
    {
      "epoch": 54.811230609713945,
      "grad_norm": 0.15077807009220123,
      "learning_rate": 0.0004725927130774236,
      "loss": 0.3964,
      "step": 674000
    },
    {
      "epoch": 54.85189176001789,
      "grad_norm": 0.18224744498729706,
      "learning_rate": 0.0004725723812621991,
      "loss": 0.3968,
      "step": 674500
    },
    {
      "epoch": 54.89255291032183,
      "grad_norm": 0.1783817708492279,
      "learning_rate": 0.00047255204944697467,
      "loss": 0.3965,
      "step": 675000
    },
    {
      "epoch": 54.93321406062577,
      "grad_norm": 0.1598782241344452,
      "learning_rate": 0.0004725317176317502,
      "loss": 0.3966,
      "step": 675500
    },
    {
      "epoch": 54.97387521092972,
      "grad_norm": 0.19044412672519684,
      "learning_rate": 0.0004725113858165257,
      "loss": 0.3971,
      "step": 676000
    },
    {
      "epoch": 55.01453636123366,
      "grad_norm": 0.19827114045619965,
      "learning_rate": 0.0004724910540013013,
      "loss": 0.395,
      "step": 676500
    },
    {
      "epoch": 55.0551975115376,
      "grad_norm": 0.1782962828874588,
      "learning_rate": 0.0004724707221860768,
      "loss": 0.3928,
      "step": 677000
    },
    {
      "epoch": 55.09585866184155,
      "grad_norm": 0.19714687764644623,
      "learning_rate": 0.0004724503903708523,
      "loss": 0.3937,
      "step": 677500
    },
    {
      "epoch": 55.13651981214549,
      "grad_norm": 0.17633156478405,
      "learning_rate": 0.0004724300585556279,
      "loss": 0.3939,
      "step": 678000
    },
    {
      "epoch": 55.17718096244943,
      "grad_norm": 0.1682259887456894,
      "learning_rate": 0.0004724097267404034,
      "loss": 0.394,
      "step": 678500
    },
    {
      "epoch": 55.21784211275337,
      "grad_norm": 0.19098658859729767,
      "learning_rate": 0.0004723893949251789,
      "loss": 0.3948,
      "step": 679000
    },
    {
      "epoch": 55.258503263057314,
      "grad_norm": 0.16172535717487335,
      "learning_rate": 0.0004723690631099545,
      "loss": 0.3947,
      "step": 679500
    },
    {
      "epoch": 55.299164413361254,
      "grad_norm": 0.1787610501050949,
      "learning_rate": 0.00047234873129473,
      "loss": 0.3953,
      "step": 680000
    },
    {
      "epoch": 55.339825563665194,
      "grad_norm": 0.17459607124328613,
      "learning_rate": 0.00047232839947950557,
      "loss": 0.3953,
      "step": 680500
    },
    {
      "epoch": 55.38048671396914,
      "grad_norm": 0.16558866202831268,
      "learning_rate": 0.0004723080676642811,
      "loss": 0.3954,
      "step": 681000
    },
    {
      "epoch": 55.42114786427308,
      "grad_norm": 0.18166959285736084,
      "learning_rate": 0.0004722877358490566,
      "loss": 0.3957,
      "step": 681500
    },
    {
      "epoch": 55.46180901457702,
      "grad_norm": 0.15925075113773346,
      "learning_rate": 0.00047226740403383217,
      "loss": 0.3956,
      "step": 682000
    },
    {
      "epoch": 55.50247016488096,
      "grad_norm": 0.17917345464229584,
      "learning_rate": 0.0004722470722186077,
      "loss": 0.3958,
      "step": 682500
    },
    {
      "epoch": 55.54313131518491,
      "grad_norm": 0.1888001710176468,
      "learning_rate": 0.0004722267404033832,
      "loss": 0.3951,
      "step": 683000
    },
    {
      "epoch": 55.58379246548885,
      "grad_norm": 0.18851077556610107,
      "learning_rate": 0.0004722064085881588,
      "loss": 0.3953,
      "step": 683500
    },
    {
      "epoch": 55.62445361579279,
      "grad_norm": 0.18290448188781738,
      "learning_rate": 0.0004721860767729343,
      "loss": 0.3959,
      "step": 684000
    },
    {
      "epoch": 55.665114766096735,
      "grad_norm": 0.167607843875885,
      "learning_rate": 0.0004721657449577098,
      "loss": 0.3959,
      "step": 684500
    },
    {
      "epoch": 55.705775916400675,
      "grad_norm": 0.17594406008720398,
      "learning_rate": 0.0004721454131424854,
      "loss": 0.3965,
      "step": 685000
    },
    {
      "epoch": 55.746437066704615,
      "grad_norm": 0.1729678511619568,
      "learning_rate": 0.0004721250813272609,
      "loss": 0.3962,
      "step": 685500
    },
    {
      "epoch": 55.78709821700856,
      "grad_norm": 0.16673514246940613,
      "learning_rate": 0.00047210474951203647,
      "loss": 0.3962,
      "step": 686000
    },
    {
      "epoch": 55.8277593673125,
      "grad_norm": 0.16097517311573029,
      "learning_rate": 0.000472084417696812,
      "loss": 0.3964,
      "step": 686500
    },
    {
      "epoch": 55.86842051761644,
      "grad_norm": 0.18526574969291687,
      "learning_rate": 0.0004720640858815875,
      "loss": 0.3965,
      "step": 687000
    },
    {
      "epoch": 55.90908166792038,
      "grad_norm": 0.16050118207931519,
      "learning_rate": 0.00047204375406636307,
      "loss": 0.3965,
      "step": 687500
    },
    {
      "epoch": 55.94974281822433,
      "grad_norm": 0.1934334933757782,
      "learning_rate": 0.0004720234222511386,
      "loss": 0.3966,
      "step": 688000
    },
    {
      "epoch": 55.99040396852827,
      "grad_norm": 0.19085215032100677,
      "learning_rate": 0.0004720030904359141,
      "loss": 0.3969,
      "step": 688500
    },
    {
      "epoch": 56.03106511883221,
      "grad_norm": 0.1961069107055664,
      "learning_rate": 0.00047198275862068967,
      "loss": 0.3936,
      "step": 689000
    },
    {
      "epoch": 56.07172626913616,
      "grad_norm": 0.1930280178785324,
      "learning_rate": 0.0004719624268054652,
      "loss": 0.393,
      "step": 689500
    },
    {
      "epoch": 56.1123874194401,
      "grad_norm": 0.1542844921350479,
      "learning_rate": 0.0004719420949902407,
      "loss": 0.393,
      "step": 690000
    },
    {
      "epoch": 56.15304856974404,
      "grad_norm": 0.16530324518680573,
      "learning_rate": 0.0004719217631750163,
      "loss": 0.3938,
      "step": 690500
    },
    {
      "epoch": 56.19370972004798,
      "grad_norm": 0.17602738738059998,
      "learning_rate": 0.0004719014313597918,
      "loss": 0.3945,
      "step": 691000
    },
    {
      "epoch": 56.234370870351924,
      "grad_norm": 0.17662197351455688,
      "learning_rate": 0.00047188109954456736,
      "loss": 0.3943,
      "step": 691500
    },
    {
      "epoch": 56.275032020655864,
      "grad_norm": 0.179376021027565,
      "learning_rate": 0.0004718607677293429,
      "loss": 0.3943,
      "step": 692000
    },
    {
      "epoch": 56.315693170959804,
      "grad_norm": 0.16020424664020538,
      "learning_rate": 0.0004718404359141184,
      "loss": 0.3943,
      "step": 692500
    },
    {
      "epoch": 56.35635432126375,
      "grad_norm": 0.16545338928699493,
      "learning_rate": 0.00047182010409889397,
      "loss": 0.3953,
      "step": 693000
    },
    {
      "epoch": 56.39701547156769,
      "grad_norm": 0.1811225712299347,
      "learning_rate": 0.0004717997722836695,
      "loss": 0.395,
      "step": 693500
    },
    {
      "epoch": 56.43767662187163,
      "grad_norm": 0.16574393212795258,
      "learning_rate": 0.000471779440468445,
      "loss": 0.3953,
      "step": 694000
    },
    {
      "epoch": 56.47833777217558,
      "grad_norm": 0.18360085785388947,
      "learning_rate": 0.00047175910865322057,
      "loss": 0.3949,
      "step": 694500
    },
    {
      "epoch": 56.51899892247952,
      "grad_norm": 0.14317691326141357,
      "learning_rate": 0.0004717387768379961,
      "loss": 0.3956,
      "step": 695000
    },
    {
      "epoch": 56.55966007278346,
      "grad_norm": 0.1703428477048874,
      "learning_rate": 0.00047171844502277166,
      "loss": 0.3958,
      "step": 695500
    },
    {
      "epoch": 56.6003212230874,
      "grad_norm": 0.17703777551651,
      "learning_rate": 0.0004716981132075472,
      "loss": 0.3958,
      "step": 696000
    },
    {
      "epoch": 56.640982373391346,
      "grad_norm": 0.16773299872875214,
      "learning_rate": 0.0004716777813923227,
      "loss": 0.396,
      "step": 696500
    },
    {
      "epoch": 56.681643523695286,
      "grad_norm": 0.18559595942497253,
      "learning_rate": 0.00047165744957709826,
      "loss": 0.3958,
      "step": 697000
    },
    {
      "epoch": 56.722304673999226,
      "grad_norm": 0.17623348534107208,
      "learning_rate": 0.0004716371177618738,
      "loss": 0.396,
      "step": 697500
    },
    {
      "epoch": 56.76296582430317,
      "grad_norm": 0.18715600669384003,
      "learning_rate": 0.0004716167859466493,
      "loss": 0.3959,
      "step": 698000
    },
    {
      "epoch": 56.80362697460711,
      "grad_norm": 0.18096743524074554,
      "learning_rate": 0.00047159645413142486,
      "loss": 0.3965,
      "step": 698500
    },
    {
      "epoch": 56.84428812491105,
      "grad_norm": 0.15816298127174377,
      "learning_rate": 0.0004715761223162004,
      "loss": 0.396,
      "step": 699000
    },
    {
      "epoch": 56.88494927521499,
      "grad_norm": 0.16305609047412872,
      "learning_rate": 0.0004715557905009759,
      "loss": 0.3967,
      "step": 699500
    },
    {
      "epoch": 56.92561042551894,
      "grad_norm": 0.20054323971271515,
      "learning_rate": 0.00047153545868575147,
      "loss": 0.3968,
      "step": 700000
    },
    {
      "epoch": 56.96627157582288,
      "grad_norm": 0.16847048699855804,
      "learning_rate": 0.000471515126870527,
      "loss": 0.3961,
      "step": 700500
    },
    {
      "epoch": 57.00693272612682,
      "grad_norm": 0.1889699250459671,
      "learning_rate": 0.00047149479505530255,
      "loss": 0.3955,
      "step": 701000
    },
    {
      "epoch": 57.04759387643077,
      "grad_norm": 0.16817706823349,
      "learning_rate": 0.00047147446324007807,
      "loss": 0.392,
      "step": 701500
    },
    {
      "epoch": 57.08825502673471,
      "grad_norm": 0.1875544637441635,
      "learning_rate": 0.0004714541314248536,
      "loss": 0.3927,
      "step": 702000
    },
    {
      "epoch": 57.12891617703865,
      "grad_norm": 0.18275362253189087,
      "learning_rate": 0.00047143379960962916,
      "loss": 0.3928,
      "step": 702500
    },
    {
      "epoch": 57.16957732734259,
      "grad_norm": 0.17581962049007416,
      "learning_rate": 0.0004714134677944047,
      "loss": 0.3934,
      "step": 703000
    },
    {
      "epoch": 57.210238477646534,
      "grad_norm": 0.20524446666240692,
      "learning_rate": 0.0004713931359791802,
      "loss": 0.3938,
      "step": 703500
    },
    {
      "epoch": 57.250899627950474,
      "grad_norm": 0.16576656699180603,
      "learning_rate": 0.00047137280416395576,
      "loss": 0.3938,
      "step": 704000
    },
    {
      "epoch": 57.291560778254414,
      "grad_norm": 0.16260716319084167,
      "learning_rate": 0.0004713524723487313,
      "loss": 0.394,
      "step": 704500
    },
    {
      "epoch": 57.33222192855836,
      "grad_norm": 0.19947032630443573,
      "learning_rate": 0.0004713321405335068,
      "loss": 0.395,
      "step": 705000
    },
    {
      "epoch": 57.3728830788623,
      "grad_norm": 0.183344766497612,
      "learning_rate": 0.00047131180871828237,
      "loss": 0.3947,
      "step": 705500
    },
    {
      "epoch": 57.41354422916624,
      "grad_norm": 0.19236277043819427,
      "learning_rate": 0.0004712914769030579,
      "loss": 0.3951,
      "step": 706000
    },
    {
      "epoch": 57.45420537947019,
      "grad_norm": 0.16884370148181915,
      "learning_rate": 0.00047127114508783345,
      "loss": 0.3951,
      "step": 706500
    },
    {
      "epoch": 57.49486652977413,
      "grad_norm": 0.17871929705142975,
      "learning_rate": 0.00047125081327260897,
      "loss": 0.3956,
      "step": 707000
    },
    {
      "epoch": 57.53552768007807,
      "grad_norm": 0.15185289084911346,
      "learning_rate": 0.0004712304814573845,
      "loss": 0.3952,
      "step": 707500
    },
    {
      "epoch": 57.57618883038201,
      "grad_norm": 0.17248523235321045,
      "learning_rate": 0.00047121014964216006,
      "loss": 0.3952,
      "step": 708000
    },
    {
      "epoch": 57.616849980685956,
      "grad_norm": 0.16765755414962769,
      "learning_rate": 0.00047118981782693557,
      "loss": 0.3954,
      "step": 708500
    },
    {
      "epoch": 57.657511130989896,
      "grad_norm": 0.15403439104557037,
      "learning_rate": 0.0004711694860117111,
      "loss": 0.3953,
      "step": 709000
    },
    {
      "epoch": 57.698172281293836,
      "grad_norm": 0.172822967171669,
      "learning_rate": 0.00047114915419648666,
      "loss": 0.3954,
      "step": 709500
    },
    {
      "epoch": 57.73883343159778,
      "grad_norm": 0.17479318380355835,
      "learning_rate": 0.0004711288223812622,
      "loss": 0.3958,
      "step": 710000
    },
    {
      "epoch": 57.77949458190172,
      "grad_norm": 0.16754546761512756,
      "learning_rate": 0.00047110849056603775,
      "loss": 0.3959,
      "step": 710500
    },
    {
      "epoch": 57.82015573220566,
      "grad_norm": 0.1604895293712616,
      "learning_rate": 0.00047108815875081326,
      "loss": 0.3962,
      "step": 711000
    },
    {
      "epoch": 57.8608168825096,
      "grad_norm": 0.1647128313779831,
      "learning_rate": 0.0004710678269355888,
      "loss": 0.3958,
      "step": 711500
    },
    {
      "epoch": 57.90147803281355,
      "grad_norm": 0.17223094403743744,
      "learning_rate": 0.0004710474951203644,
      "loss": 0.3964,
      "step": 712000
    },
    {
      "epoch": 57.94213918311749,
      "grad_norm": 0.18987871706485748,
      "learning_rate": 0.0004710271633051399,
      "loss": 0.3967,
      "step": 712500
    },
    {
      "epoch": 57.98280033342143,
      "grad_norm": 0.19017121195793152,
      "learning_rate": 0.00047100683148991544,
      "loss": 0.396,
      "step": 713000
    },
    {
      "epoch": 58.02346148372538,
      "grad_norm": 0.1736362874507904,
      "learning_rate": 0.000470986499674691,
      "loss": 0.3936,
      "step": 713500
    },
    {
      "epoch": 58.06412263402932,
      "grad_norm": 0.17878946661949158,
      "learning_rate": 0.0004709661678594665,
      "loss": 0.3922,
      "step": 714000
    },
    {
      "epoch": 58.10478378433326,
      "grad_norm": 0.15702500939369202,
      "learning_rate": 0.00047094583604424204,
      "loss": 0.3936,
      "step": 714500
    },
    {
      "epoch": 58.1454449346372,
      "grad_norm": 0.19202803075313568,
      "learning_rate": 0.0004709255042290176,
      "loss": 0.3928,
      "step": 715000
    },
    {
      "epoch": 58.186106084941144,
      "grad_norm": 0.17783328890800476,
      "learning_rate": 0.00047090517241379313,
      "loss": 0.3936,
      "step": 715500
    },
    {
      "epoch": 58.226767235245084,
      "grad_norm": 0.16544964909553528,
      "learning_rate": 0.0004708848405985687,
      "loss": 0.3939,
      "step": 716000
    },
    {
      "epoch": 58.267428385549024,
      "grad_norm": 0.1715000569820404,
      "learning_rate": 0.0004708645087833442,
      "loss": 0.3933,
      "step": 716500
    },
    {
      "epoch": 58.30808953585297,
      "grad_norm": 0.1840270608663559,
      "learning_rate": 0.00047084417696811973,
      "loss": 0.394,
      "step": 717000
    },
    {
      "epoch": 58.34875068615691,
      "grad_norm": 0.16730870306491852,
      "learning_rate": 0.0004708238451528953,
      "loss": 0.3942,
      "step": 717500
    },
    {
      "epoch": 58.38941183646085,
      "grad_norm": 0.18604743480682373,
      "learning_rate": 0.0004708035133376708,
      "loss": 0.3944,
      "step": 718000
    },
    {
      "epoch": 58.4300729867648,
      "grad_norm": 0.1912909597158432,
      "learning_rate": 0.00047078318152244633,
      "loss": 0.3951,
      "step": 718500
    },
    {
      "epoch": 58.47073413706874,
      "grad_norm": 0.1762319654226303,
      "learning_rate": 0.0004707628497072219,
      "loss": 0.3952,
      "step": 719000
    },
    {
      "epoch": 58.51139528737268,
      "grad_norm": 0.16262657940387726,
      "learning_rate": 0.0004707425178919974,
      "loss": 0.3952,
      "step": 719500
    },
    {
      "epoch": 58.55205643767662,
      "grad_norm": 0.17261452972888947,
      "learning_rate": 0.00047072218607677294,
      "loss": 0.3952,
      "step": 720000
    },
    {
      "epoch": 58.592717587980566,
      "grad_norm": 0.19190625846385956,
      "learning_rate": 0.0004707018542615485,
      "loss": 0.3954,
      "step": 720500
    },
    {
      "epoch": 58.633378738284506,
      "grad_norm": 0.16094107925891876,
      "learning_rate": 0.000470681522446324,
      "loss": 0.3952,
      "step": 721000
    },
    {
      "epoch": 58.674039888588446,
      "grad_norm": 0.18269287049770355,
      "learning_rate": 0.0004706611906310996,
      "loss": 0.3949,
      "step": 721500
    },
    {
      "epoch": 58.71470103889239,
      "grad_norm": 0.1810981184244156,
      "learning_rate": 0.0004706408588158751,
      "loss": 0.396,
      "step": 722000
    },
    {
      "epoch": 58.75536218919633,
      "grad_norm": 0.1720345914363861,
      "learning_rate": 0.00047062052700065063,
      "loss": 0.3956,
      "step": 722500
    },
    {
      "epoch": 58.79602333950027,
      "grad_norm": 0.18035757541656494,
      "learning_rate": 0.0004706001951854262,
      "loss": 0.3956,
      "step": 723000
    },
    {
      "epoch": 58.83668448980421,
      "grad_norm": 0.16924145817756653,
      "learning_rate": 0.0004705798633702017,
      "loss": 0.3951,
      "step": 723500
    },
    {
      "epoch": 58.87734564010816,
      "grad_norm": 0.16258381307125092,
      "learning_rate": 0.00047055953155497723,
      "loss": 0.3959,
      "step": 724000
    },
    {
      "epoch": 58.9180067904121,
      "grad_norm": 0.20117464661598206,
      "learning_rate": 0.0004705391997397528,
      "loss": 0.396,
      "step": 724500
    },
    {
      "epoch": 58.95866794071604,
      "grad_norm": 0.171391561627388,
      "learning_rate": 0.0004705188679245283,
      "loss": 0.3955,
      "step": 725000
    },
    {
      "epoch": 58.99932909101999,
      "grad_norm": 0.1642782986164093,
      "learning_rate": 0.00047049853610930384,
      "loss": 0.3963,
      "step": 725500
    },
    {
      "epoch": 59.03999024132393,
      "grad_norm": 0.17039503157138824,
      "learning_rate": 0.0004704782042940794,
      "loss": 0.3917,
      "step": 726000
    },
    {
      "epoch": 59.08065139162787,
      "grad_norm": 0.17556129395961761,
      "learning_rate": 0.0004704578724788549,
      "loss": 0.3922,
      "step": 726500
    },
    {
      "epoch": 59.121312541931815,
      "grad_norm": 0.18825119733810425,
      "learning_rate": 0.0004704375406636305,
      "loss": 0.3929,
      "step": 727000
    },
    {
      "epoch": 59.161973692235755,
      "grad_norm": 0.19159060716629028,
      "learning_rate": 0.000470417208848406,
      "loss": 0.3928,
      "step": 727500
    },
    {
      "epoch": 59.202634842539695,
      "grad_norm": 0.2006300687789917,
      "learning_rate": 0.0004703968770331815,
      "loss": 0.3933,
      "step": 728000
    },
    {
      "epoch": 59.243295992843635,
      "grad_norm": 0.17627385258674622,
      "learning_rate": 0.0004703765452179571,
      "loss": 0.394,
      "step": 728500
    },
    {
      "epoch": 59.28395714314758,
      "grad_norm": 0.19520831108093262,
      "learning_rate": 0.0004703562134027326,
      "loss": 0.394,
      "step": 729000
    },
    {
      "epoch": 59.32461829345152,
      "grad_norm": 0.17671898007392883,
      "learning_rate": 0.00047033588158750813,
      "loss": 0.395,
      "step": 729500
    },
    {
      "epoch": 59.36527944375546,
      "grad_norm": 0.1989666074514389,
      "learning_rate": 0.0004703155497722837,
      "loss": 0.3939,
      "step": 730000
    },
    {
      "epoch": 59.40594059405941,
      "grad_norm": 0.1709173172712326,
      "learning_rate": 0.0004702952179570592,
      "loss": 0.3944,
      "step": 730500
    },
    {
      "epoch": 59.44660174436335,
      "grad_norm": 0.15851344168186188,
      "learning_rate": 0.0004702748861418348,
      "loss": 0.3948,
      "step": 731000
    },
    {
      "epoch": 59.48726289466729,
      "grad_norm": 0.18135540187358856,
      "learning_rate": 0.0004702545543266103,
      "loss": 0.3944,
      "step": 731500
    },
    {
      "epoch": 59.52792404497123,
      "grad_norm": 0.2018241137266159,
      "learning_rate": 0.0004702342225113858,
      "loss": 0.395,
      "step": 732000
    },
    {
      "epoch": 59.568585195275176,
      "grad_norm": 0.1913461983203888,
      "learning_rate": 0.0004702138906961614,
      "loss": 0.3943,
      "step": 732500
    },
    {
      "epoch": 59.609246345579116,
      "grad_norm": 0.16981454193592072,
      "learning_rate": 0.0004701935588809369,
      "loss": 0.3946,
      "step": 733000
    },
    {
      "epoch": 59.649907495883056,
      "grad_norm": 0.19667160511016846,
      "learning_rate": 0.0004701732270657124,
      "loss": 0.3948,
      "step": 733500
    },
    {
      "epoch": 59.690568646187,
      "grad_norm": 0.17533133924007416,
      "learning_rate": 0.000470152895250488,
      "loss": 0.395,
      "step": 734000
    },
    {
      "epoch": 59.73122979649094,
      "grad_norm": 0.21492573618888855,
      "learning_rate": 0.0004701325634352635,
      "loss": 0.3952,
      "step": 734500
    },
    {
      "epoch": 59.77189094679488,
      "grad_norm": 0.16428321599960327,
      "learning_rate": 0.000470112231620039,
      "loss": 0.3951,
      "step": 735000
    },
    {
      "epoch": 59.81255209709883,
      "grad_norm": 0.17989715933799744,
      "learning_rate": 0.0004700918998048146,
      "loss": 0.3954,
      "step": 735500
    },
    {
      "epoch": 59.85321324740277,
      "grad_norm": 0.19075675308704376,
      "learning_rate": 0.0004700715679895901,
      "loss": 0.395,
      "step": 736000
    },
    {
      "epoch": 59.89387439770671,
      "grad_norm": 0.16778720915317535,
      "learning_rate": 0.0004700512361743657,
      "loss": 0.3957,
      "step": 736500
    },
    {
      "epoch": 59.93453554801065,
      "grad_norm": 0.1793753206729889,
      "learning_rate": 0.0004700309043591412,
      "loss": 0.3952,
      "step": 737000
    },
    {
      "epoch": 59.9751966983146,
      "grad_norm": 0.16828076541423798,
      "learning_rate": 0.0004700105725439167,
      "loss": 0.396,
      "step": 737500
    },
    {
      "epoch": 60.01585784861854,
      "grad_norm": 0.16191646456718445,
      "learning_rate": 0.0004699902407286923,
      "loss": 0.3941,
      "step": 738000
    },
    {
      "epoch": 60.05651899892248,
      "grad_norm": 0.19009998440742493,
      "learning_rate": 0.0004699699089134678,
      "loss": 0.392,
      "step": 738500
    },
    {
      "epoch": 60.097180149226425,
      "grad_norm": 0.19421157240867615,
      "learning_rate": 0.0004699495770982433,
      "loss": 0.3923,
      "step": 739000
    },
    {
      "epoch": 60.137841299530365,
      "grad_norm": 0.16505898535251617,
      "learning_rate": 0.0004699292452830189,
      "loss": 0.3927,
      "step": 739500
    },
    {
      "epoch": 60.178502449834305,
      "grad_norm": 0.1887836754322052,
      "learning_rate": 0.0004699089134677944,
      "loss": 0.393,
      "step": 740000
    },
    {
      "epoch": 60.219163600138245,
      "grad_norm": 0.18696129322052002,
      "learning_rate": 0.0004698885816525699,
      "loss": 0.3926,
      "step": 740500
    },
    {
      "epoch": 60.25982475044219,
      "grad_norm": 0.18537920713424683,
      "learning_rate": 0.0004698682498373455,
      "loss": 0.3939,
      "step": 741000
    },
    {
      "epoch": 60.30048590074613,
      "grad_norm": 0.18392783403396606,
      "learning_rate": 0.000469847918022121,
      "loss": 0.3932,
      "step": 741500
    },
    {
      "epoch": 60.34114705105007,
      "grad_norm": 0.19081924855709076,
      "learning_rate": 0.0004698275862068966,
      "loss": 0.394,
      "step": 742000
    },
    {
      "epoch": 60.38180820135402,
      "grad_norm": 0.16951674222946167,
      "learning_rate": 0.0004698072543916721,
      "loss": 0.3938,
      "step": 742500
    },
    {
      "epoch": 60.42246935165796,
      "grad_norm": 0.2044149488210678,
      "learning_rate": 0.0004697869225764476,
      "loss": 0.3939,
      "step": 743000
    },
    {
      "epoch": 60.4631305019619,
      "grad_norm": 0.22173984348773956,
      "learning_rate": 0.0004697665907612232,
      "loss": 0.3941,
      "step": 743500
    },
    {
      "epoch": 60.50379165226584,
      "grad_norm": 0.16753241419792175,
      "learning_rate": 0.0004697462589459987,
      "loss": 0.3937,
      "step": 744000
    },
    {
      "epoch": 60.544452802569786,
      "grad_norm": 0.1633194088935852,
      "learning_rate": 0.0004697259271307742,
      "loss": 0.394,
      "step": 744500
    },
    {
      "epoch": 60.585113952873726,
      "grad_norm": 0.19570720195770264,
      "learning_rate": 0.0004697055953155498,
      "loss": 0.3946,
      "step": 745000
    },
    {
      "epoch": 60.625775103177666,
      "grad_norm": 0.17340832948684692,
      "learning_rate": 0.0004696852635003253,
      "loss": 0.3947,
      "step": 745500
    },
    {
      "epoch": 60.66643625348161,
      "grad_norm": 0.18732428550720215,
      "learning_rate": 0.0004696649316851009,
      "loss": 0.3949,
      "step": 746000
    },
    {
      "epoch": 60.70709740378555,
      "grad_norm": 0.19210657477378845,
      "learning_rate": 0.0004696445998698764,
      "loss": 0.3954,
      "step": 746500
    },
    {
      "epoch": 60.74775855408949,
      "grad_norm": 0.2085929811000824,
      "learning_rate": 0.0004696242680546519,
      "loss": 0.3951,
      "step": 747000
    },
    {
      "epoch": 60.78841970439344,
      "grad_norm": 0.22255979478359222,
      "learning_rate": 0.0004696039362394275,
      "loss": 0.3955,
      "step": 747500
    },
    {
      "epoch": 60.82908085469738,
      "grad_norm": 0.2006300836801529,
      "learning_rate": 0.000469583604424203,
      "loss": 0.3954,
      "step": 748000
    },
    {
      "epoch": 60.86974200500132,
      "grad_norm": 0.1882113367319107,
      "learning_rate": 0.0004695632726089785,
      "loss": 0.3953,
      "step": 748500
    },
    {
      "epoch": 60.91040315530526,
      "grad_norm": 0.16086013615131378,
      "learning_rate": 0.0004695429407937541,
      "loss": 0.3955,
      "step": 749000
    },
    {
      "epoch": 60.95106430560921,
      "grad_norm": 0.17197774350643158,
      "learning_rate": 0.0004695226089785296,
      "loss": 0.3953,
      "step": 749500
    },
    {
      "epoch": 60.99172545591315,
      "grad_norm": 0.17281413078308105,
      "learning_rate": 0.0004695022771633051,
      "loss": 0.396,
      "step": 750000
    },
    {
      "epoch": 61.03238660621709,
      "grad_norm": 0.20306286215782166,
      "learning_rate": 0.0004694819453480807,
      "loss": 0.3921,
      "step": 750500
    },
    {
      "epoch": 61.073047756521035,
      "grad_norm": 0.1698741763830185,
      "learning_rate": 0.0004694616135328562,
      "loss": 0.3916,
      "step": 751000
    },
    {
      "epoch": 61.113708906824975,
      "grad_norm": 0.17815572023391724,
      "learning_rate": 0.0004694412817176318,
      "loss": 0.3924,
      "step": 751500
    },
    {
      "epoch": 61.154370057128915,
      "grad_norm": 0.1776980608701706,
      "learning_rate": 0.0004694209499024073,
      "loss": 0.3924,
      "step": 752000
    },
    {
      "epoch": 61.195031207432855,
      "grad_norm": 0.17624659836292267,
      "learning_rate": 0.0004694006180871828,
      "loss": 0.393,
      "step": 752500
    },
    {
      "epoch": 61.2356923577368,
      "grad_norm": 0.1524081528186798,
      "learning_rate": 0.0004693802862719584,
      "loss": 0.3935,
      "step": 753000
    },
    {
      "epoch": 61.27635350804074,
      "grad_norm": 0.1976078301668167,
      "learning_rate": 0.0004693599544567339,
      "loss": 0.3932,
      "step": 753500
    },
    {
      "epoch": 61.31701465834468,
      "grad_norm": 0.18542331457138062,
      "learning_rate": 0.0004693396226415094,
      "loss": 0.3938,
      "step": 754000
    },
    {
      "epoch": 61.35767580864863,
      "grad_norm": 0.1728917360305786,
      "learning_rate": 0.000469319290826285,
      "loss": 0.3933,
      "step": 754500
    },
    {
      "epoch": 61.39833695895257,
      "grad_norm": 0.19748590886592865,
      "learning_rate": 0.0004692989590110605,
      "loss": 0.3941,
      "step": 755000
    },
    {
      "epoch": 61.43899810925651,
      "grad_norm": 0.19733071327209473,
      "learning_rate": 0.000469278627195836,
      "loss": 0.3935,
      "step": 755500
    },
    {
      "epoch": 61.47965925956045,
      "grad_norm": 0.17401808500289917,
      "learning_rate": 0.0004692582953806116,
      "loss": 0.3941,
      "step": 756000
    },
    {
      "epoch": 61.5203204098644,
      "grad_norm": 0.18605802953243256,
      "learning_rate": 0.0004692379635653871,
      "loss": 0.3936,
      "step": 756500
    },
    {
      "epoch": 61.56098156016834,
      "grad_norm": 0.17469312250614166,
      "learning_rate": 0.00046921763175016267,
      "loss": 0.3944,
      "step": 757000
    },
    {
      "epoch": 61.60164271047228,
      "grad_norm": 0.1789178103208542,
      "learning_rate": 0.0004691972999349382,
      "loss": 0.3947,
      "step": 757500
    },
    {
      "epoch": 61.642303860776224,
      "grad_norm": 0.16765433549880981,
      "learning_rate": 0.0004691769681197137,
      "loss": 0.3946,
      "step": 758000
    },
    {
      "epoch": 61.682965011080164,
      "grad_norm": 0.1768801510334015,
      "learning_rate": 0.0004691566363044893,
      "loss": 0.3949,
      "step": 758500
    },
    {
      "epoch": 61.723626161384104,
      "grad_norm": 0.17617350816726685,
      "learning_rate": 0.0004691363044892648,
      "loss": 0.3945,
      "step": 759000
    },
    {
      "epoch": 61.76428731168805,
      "grad_norm": 0.176768496632576,
      "learning_rate": 0.0004691159726740403,
      "loss": 0.3953,
      "step": 759500
    },
    {
      "epoch": 61.80494846199199,
      "grad_norm": 0.18414732813835144,
      "learning_rate": 0.0004690956408588159,
      "loss": 0.3946,
      "step": 760000
    },
    {
      "epoch": 61.84560961229593,
      "grad_norm": 0.17689184844493866,
      "learning_rate": 0.0004690753090435914,
      "loss": 0.3947,
      "step": 760500
    },
    {
      "epoch": 61.88627076259987,
      "grad_norm": 0.16549503803253174,
      "learning_rate": 0.00046905497722836697,
      "loss": 0.3951,
      "step": 761000
    },
    {
      "epoch": 61.92693191290382,
      "grad_norm": 0.18294283747673035,
      "learning_rate": 0.0004690346454131425,
      "loss": 0.3949,
      "step": 761500
    },
    {
      "epoch": 61.96759306320776,
      "grad_norm": 0.17042696475982666,
      "learning_rate": 0.000469014313597918,
      "loss": 0.3954,
      "step": 762000
    },
    {
      "epoch": 62.0082542135117,
      "grad_norm": 0.17859452962875366,
      "learning_rate": 0.00046899398178269357,
      "loss": 0.3944,
      "step": 762500
    },
    {
      "epoch": 62.048915363815645,
      "grad_norm": 0.18982332944869995,
      "learning_rate": 0.0004689736499674691,
      "loss": 0.3914,
      "step": 763000
    },
    {
      "epoch": 62.089576514119585,
      "grad_norm": 0.1755688339471817,
      "learning_rate": 0.0004689533181522446,
      "loss": 0.3921,
      "step": 763500
    },
    {
      "epoch": 62.130237664423525,
      "grad_norm": 0.19525733590126038,
      "learning_rate": 0.00046893298633702017,
      "loss": 0.3925,
      "step": 764000
    },
    {
      "epoch": 62.170898814727465,
      "grad_norm": 0.18932978808879852,
      "learning_rate": 0.0004689126545217957,
      "loss": 0.3922,
      "step": 764500
    },
    {
      "epoch": 62.21155996503141,
      "grad_norm": 0.17569267749786377,
      "learning_rate": 0.0004688923227065712,
      "loss": 0.3925,
      "step": 765000
    },
    {
      "epoch": 62.25222111533535,
      "grad_norm": 0.17663107812404633,
      "learning_rate": 0.0004688719908913468,
      "loss": 0.3928,
      "step": 765500
    },
    {
      "epoch": 62.29288226563929,
      "grad_norm": 0.18257375061511993,
      "learning_rate": 0.0004688516590761223,
      "loss": 0.3932,
      "step": 766000
    },
    {
      "epoch": 62.33354341594324,
      "grad_norm": 0.16398885846138,
      "learning_rate": 0.00046883132726089786,
      "loss": 0.3929,
      "step": 766500
    },
    {
      "epoch": 62.37420456624718,
      "grad_norm": 0.1869649440050125,
      "learning_rate": 0.0004688109954456734,
      "loss": 0.3932,
      "step": 767000
    },
    {
      "epoch": 62.41486571655112,
      "grad_norm": 0.17966338992118835,
      "learning_rate": 0.0004687906636304489,
      "loss": 0.3936,
      "step": 767500
    },
    {
      "epoch": 62.45552686685507,
      "grad_norm": 0.1743009239435196,
      "learning_rate": 0.0004687703318152245,
      "loss": 0.3936,
      "step": 768000
    },
    {
      "epoch": 62.49618801715901,
      "grad_norm": 0.1597026288509369,
      "learning_rate": 0.00046875,
      "loss": 0.3933,
      "step": 768500
    },
    {
      "epoch": 62.53684916746295,
      "grad_norm": 0.19478902220726013,
      "learning_rate": 0.0004687296681847755,
      "loss": 0.394,
      "step": 769000
    },
    {
      "epoch": 62.57751031776689,
      "grad_norm": 0.17063407599925995,
      "learning_rate": 0.0004687093363695511,
      "loss": 0.3942,
      "step": 769500
    },
    {
      "epoch": 62.618171468070834,
      "grad_norm": 0.17457081377506256,
      "learning_rate": 0.00046868900455432664,
      "loss": 0.3949,
      "step": 770000
    },
    {
      "epoch": 62.658832618374774,
      "grad_norm": 0.16685886681079865,
      "learning_rate": 0.00046866867273910216,
      "loss": 0.3947,
      "step": 770500
    },
    {
      "epoch": 62.699493768678714,
      "grad_norm": 0.17479307949543,
      "learning_rate": 0.00046864834092387773,
      "loss": 0.3947,
      "step": 771000
    },
    {
      "epoch": 62.74015491898266,
      "grad_norm": 0.17948277294635773,
      "learning_rate": 0.00046862800910865324,
      "loss": 0.3948,
      "step": 771500
    },
    {
      "epoch": 62.7808160692866,
      "grad_norm": 0.16188010573387146,
      "learning_rate": 0.0004686076772934288,
      "loss": 0.3951,
      "step": 772000
    },
    {
      "epoch": 62.82147721959054,
      "grad_norm": 0.19858811795711517,
      "learning_rate": 0.00046858734547820433,
      "loss": 0.3945,
      "step": 772500
    },
    {
      "epoch": 62.86213836989448,
      "grad_norm": 0.18931707739830017,
      "learning_rate": 0.00046856701366297985,
      "loss": 0.3946,
      "step": 773000
    },
    {
      "epoch": 62.90279952019843,
      "grad_norm": 0.18146249651908875,
      "learning_rate": 0.0004685466818477554,
      "loss": 0.3951,
      "step": 773500
    },
    {
      "epoch": 62.94346067050237,
      "grad_norm": 0.20506209135055542,
      "learning_rate": 0.00046852635003253093,
      "loss": 0.3951,
      "step": 774000
    },
    {
      "epoch": 62.98412182080631,
      "grad_norm": 0.16150283813476562,
      "learning_rate": 0.00046850601821730645,
      "loss": 0.395,
      "step": 774500
    },
    {
      "epoch": 63.024782971110255,
      "grad_norm": 0.20049874484539032,
      "learning_rate": 0.000468485686402082,
      "loss": 0.3924,
      "step": 775000
    },
    {
      "epoch": 63.065444121414195,
      "grad_norm": 0.17318254709243774,
      "learning_rate": 0.00046846535458685754,
      "loss": 0.3917,
      "step": 775500
    },
    {
      "epoch": 63.106105271718135,
      "grad_norm": 0.23021194338798523,
      "learning_rate": 0.00046844502277163305,
      "loss": 0.3918,
      "step": 776000
    },
    {
      "epoch": 63.14676642202208,
      "grad_norm": 0.1545991450548172,
      "learning_rate": 0.0004684246909564086,
      "loss": 0.3921,
      "step": 776500
    },
    {
      "epoch": 63.18742757232602,
      "grad_norm": 0.19925478100776672,
      "learning_rate": 0.00046840435914118414,
      "loss": 0.3925,
      "step": 777000
    },
    {
      "epoch": 63.22808872262996,
      "grad_norm": 0.17721623182296753,
      "learning_rate": 0.0004683840273259597,
      "loss": 0.3926,
      "step": 777500
    },
    {
      "epoch": 63.2687498729339,
      "grad_norm": 0.17320898175239563,
      "learning_rate": 0.00046836369551073523,
      "loss": 0.3931,
      "step": 778000
    },
    {
      "epoch": 63.30941102323785,
      "grad_norm": 0.21234118938446045,
      "learning_rate": 0.00046834336369551074,
      "loss": 0.3931,
      "step": 778500
    },
    {
      "epoch": 63.35007217354179,
      "grad_norm": 0.18746022880077362,
      "learning_rate": 0.0004683230318802863,
      "loss": 0.3929,
      "step": 779000
    },
    {
      "epoch": 63.39073332384573,
      "grad_norm": 0.17744790017604828,
      "learning_rate": 0.00046830270006506183,
      "loss": 0.3936,
      "step": 779500
    },
    {
      "epoch": 63.43139447414968,
      "grad_norm": 0.19169658422470093,
      "learning_rate": 0.00046828236824983735,
      "loss": 0.3934,
      "step": 780000
    },
    {
      "epoch": 63.47205562445362,
      "grad_norm": 0.17063862085342407,
      "learning_rate": 0.0004682620364346129,
      "loss": 0.3935,
      "step": 780500
    },
    {
      "epoch": 63.51271677475756,
      "grad_norm": 0.17164164781570435,
      "learning_rate": 0.00046824170461938844,
      "loss": 0.3939,
      "step": 781000
    },
    {
      "epoch": 63.5533779250615,
      "grad_norm": 0.17743974924087524,
      "learning_rate": 0.000468221372804164,
      "loss": 0.3939,
      "step": 781500
    },
    {
      "epoch": 63.594039075365444,
      "grad_norm": 0.17687688767910004,
      "learning_rate": 0.0004682010409889395,
      "loss": 0.3936,
      "step": 782000
    },
    {
      "epoch": 63.634700225669384,
      "grad_norm": 0.19629719853401184,
      "learning_rate": 0.00046818070917371504,
      "loss": 0.3941,
      "step": 782500
    },
    {
      "epoch": 63.675361375973324,
      "grad_norm": 0.19674549996852875,
      "learning_rate": 0.0004681603773584906,
      "loss": 0.394,
      "step": 783000
    },
    {
      "epoch": 63.71602252627727,
      "grad_norm": 0.18701723217964172,
      "learning_rate": 0.0004681400455432661,
      "loss": 0.3938,
      "step": 783500
    },
    {
      "epoch": 63.75668367658121,
      "grad_norm": 0.17563951015472412,
      "learning_rate": 0.00046811971372804164,
      "loss": 0.3945,
      "step": 784000
    },
    {
      "epoch": 63.79734482688515,
      "grad_norm": 0.1804009974002838,
      "learning_rate": 0.0004680993819128172,
      "loss": 0.3941,
      "step": 784500
    },
    {
      "epoch": 63.83800597718909,
      "grad_norm": 0.16487182676792145,
      "learning_rate": 0.00046807905009759273,
      "loss": 0.3945,
      "step": 785000
    },
    {
      "epoch": 63.87866712749304,
      "grad_norm": 0.1660010814666748,
      "learning_rate": 0.00046805871828236825,
      "loss": 0.3946,
      "step": 785500
    },
    {
      "epoch": 63.91932827779698,
      "grad_norm": 0.1989440768957138,
      "learning_rate": 0.0004680383864671438,
      "loss": 0.3941,
      "step": 786000
    },
    {
      "epoch": 63.95998942810092,
      "grad_norm": 0.18928027153015137,
      "learning_rate": 0.00046801805465191933,
      "loss": 0.3944,
      "step": 786500
    },
    {
      "epoch": 64.00065057840486,
      "grad_norm": 0.1757902204990387,
      "learning_rate": 0.0004679977228366949,
      "loss": 0.3952,
      "step": 787000
    },
    {
      "epoch": 64.0413117287088,
      "grad_norm": 0.18312625586986542,
      "learning_rate": 0.0004679773910214704,
      "loss": 0.3907,
      "step": 787500
    },
    {
      "epoch": 64.08197287901275,
      "grad_norm": 0.1742546558380127,
      "learning_rate": 0.00046795705920624594,
      "loss": 0.3909,
      "step": 788000
    },
    {
      "epoch": 64.12263402931669,
      "grad_norm": 0.16100206971168518,
      "learning_rate": 0.0004679367273910215,
      "loss": 0.3917,
      "step": 788500
    },
    {
      "epoch": 64.16329517962063,
      "grad_norm": 0.16543881595134735,
      "learning_rate": 0.000467916395575797,
      "loss": 0.3923,
      "step": 789000
    },
    {
      "epoch": 64.20395632992458,
      "grad_norm": 0.18112538754940033,
      "learning_rate": 0.00046789606376057254,
      "loss": 0.3925,
      "step": 789500
    },
    {
      "epoch": 64.24461748022851,
      "grad_norm": 0.16183419525623322,
      "learning_rate": 0.0004678757319453481,
      "loss": 0.3928,
      "step": 790000
    },
    {
      "epoch": 64.28527863053246,
      "grad_norm": 0.1725655347108841,
      "learning_rate": 0.0004678554001301236,
      "loss": 0.3919,
      "step": 790500
    },
    {
      "epoch": 64.32593978083639,
      "grad_norm": 0.1739397644996643,
      "learning_rate": 0.00046783506831489914,
      "loss": 0.3927,
      "step": 791000
    },
    {
      "epoch": 64.36660093114034,
      "grad_norm": 0.17678123712539673,
      "learning_rate": 0.0004678147364996747,
      "loss": 0.393,
      "step": 791500
    },
    {
      "epoch": 64.40726208144429,
      "grad_norm": 0.183333158493042,
      "learning_rate": 0.00046779440468445023,
      "loss": 0.3931,
      "step": 792000
    },
    {
      "epoch": 64.44792323174822,
      "grad_norm": 0.18767815828323364,
      "learning_rate": 0.0004677740728692258,
      "loss": 0.3933,
      "step": 792500
    },
    {
      "epoch": 64.48858438205217,
      "grad_norm": 0.19771425426006317,
      "learning_rate": 0.0004677537410540013,
      "loss": 0.3934,
      "step": 793000
    },
    {
      "epoch": 64.52924553235611,
      "grad_norm": 0.19952380657196045,
      "learning_rate": 0.00046773340923877683,
      "loss": 0.3939,
      "step": 793500
    },
    {
      "epoch": 64.56990668266005,
      "grad_norm": 0.1914149522781372,
      "learning_rate": 0.0004677130774235524,
      "loss": 0.3939,
      "step": 794000
    },
    {
      "epoch": 64.610567832964,
      "grad_norm": 0.17539632320404053,
      "learning_rate": 0.0004676927456083279,
      "loss": 0.3936,
      "step": 794500
    },
    {
      "epoch": 64.65122898326794,
      "grad_norm": 0.20884409546852112,
      "learning_rate": 0.00046767241379310344,
      "loss": 0.3941,
      "step": 795000
    },
    {
      "epoch": 64.69189013357187,
      "grad_norm": 0.18973234295845032,
      "learning_rate": 0.000467652081977879,
      "loss": 0.3938,
      "step": 795500
    },
    {
      "epoch": 64.73255128387582,
      "grad_norm": 0.17449121177196503,
      "learning_rate": 0.0004676317501626545,
      "loss": 0.3943,
      "step": 796000
    },
    {
      "epoch": 64.77321243417977,
      "grad_norm": 0.17821946740150452,
      "learning_rate": 0.0004676114183474301,
      "loss": 0.3942,
      "step": 796500
    },
    {
      "epoch": 64.8138735844837,
      "grad_norm": 0.2177712768316269,
      "learning_rate": 0.0004675910865322056,
      "loss": 0.3946,
      "step": 797000
    },
    {
      "epoch": 64.85453473478765,
      "grad_norm": 0.1750391721725464,
      "learning_rate": 0.00046757075471698113,
      "loss": 0.3946,
      "step": 797500
    },
    {
      "epoch": 64.8951958850916,
      "grad_norm": 0.18598073720932007,
      "learning_rate": 0.0004675504229017567,
      "loss": 0.3946,
      "step": 798000
    },
    {
      "epoch": 64.93585703539553,
      "grad_norm": 0.18816401064395905,
      "learning_rate": 0.0004675300910865322,
      "loss": 0.394,
      "step": 798500
    },
    {
      "epoch": 64.97651818569948,
      "grad_norm": 0.1852036416530609,
      "learning_rate": 0.00046750975927130773,
      "loss": 0.3947,
      "step": 799000
    },
    {
      "epoch": 65.01717933600341,
      "grad_norm": 0.18132926523685455,
      "learning_rate": 0.0004674894274560833,
      "loss": 0.3931,
      "step": 799500
    },
    {
      "epoch": 65.05784048630736,
      "grad_norm": 0.19877591729164124,
      "learning_rate": 0.0004674690956408588,
      "loss": 0.3905,
      "step": 800000
    },
    {
      "epoch": 65.0985016366113,
      "grad_norm": 0.19145451486110687,
      "learning_rate": 0.00046744876382563433,
      "loss": 0.3914,
      "step": 800500
    },
    {
      "epoch": 65.13916278691524,
      "grad_norm": 0.1919640153646469,
      "learning_rate": 0.0004674284320104099,
      "loss": 0.3914,
      "step": 801000
    },
    {
      "epoch": 65.17982393721918,
      "grad_norm": 0.195469468832016,
      "learning_rate": 0.0004674081001951854,
      "loss": 0.3922,
      "step": 801500
    },
    {
      "epoch": 65.22048508752313,
      "grad_norm": 0.1846589297056198,
      "learning_rate": 0.000467387768379961,
      "loss": 0.3919,
      "step": 802000
    },
    {
      "epoch": 65.26114623782706,
      "grad_norm": 0.18889249861240387,
      "learning_rate": 0.0004673674365647365,
      "loss": 0.3919,
      "step": 802500
    },
    {
      "epoch": 65.30180738813101,
      "grad_norm": 0.2071991115808487,
      "learning_rate": 0.000467347104749512,
      "loss": 0.3921,
      "step": 803000
    },
    {
      "epoch": 65.34246853843496,
      "grad_norm": 0.19046999514102936,
      "learning_rate": 0.0004673267729342876,
      "loss": 0.3926,
      "step": 803500
    },
    {
      "epoch": 65.38312968873889,
      "grad_norm": 0.1972649097442627,
      "learning_rate": 0.0004673064411190631,
      "loss": 0.3926,
      "step": 804000
    },
    {
      "epoch": 65.42379083904284,
      "grad_norm": 0.18425701558589935,
      "learning_rate": 0.00046728610930383863,
      "loss": 0.3931,
      "step": 804500
    },
    {
      "epoch": 65.46445198934678,
      "grad_norm": 0.19103091955184937,
      "learning_rate": 0.0004672657774886142,
      "loss": 0.3931,
      "step": 805000
    },
    {
      "epoch": 65.50511313965072,
      "grad_norm": 0.19257640838623047,
      "learning_rate": 0.0004672454456733897,
      "loss": 0.3936,
      "step": 805500
    },
    {
      "epoch": 65.54577428995466,
      "grad_norm": 0.1642826348543167,
      "learning_rate": 0.00046722511385816523,
      "loss": 0.3932,
      "step": 806000
    },
    {
      "epoch": 65.58643544025861,
      "grad_norm": 0.17748697102069855,
      "learning_rate": 0.0004672047820429408,
      "loss": 0.3935,
      "step": 806500
    },
    {
      "epoch": 65.62709659056254,
      "grad_norm": 0.18033072352409363,
      "learning_rate": 0.0004671844502277163,
      "loss": 0.3936,
      "step": 807000
    },
    {
      "epoch": 65.66775774086649,
      "grad_norm": 0.174502894282341,
      "learning_rate": 0.0004671641184124919,
      "loss": 0.3938,
      "step": 807500
    },
    {
      "epoch": 65.70841889117042,
      "grad_norm": 0.18822486698627472,
      "learning_rate": 0.0004671437865972674,
      "loss": 0.3939,
      "step": 808000
    },
    {
      "epoch": 65.74908004147437,
      "grad_norm": 0.18459071218967438,
      "learning_rate": 0.0004671234547820429,
      "loss": 0.3941,
      "step": 808500
    },
    {
      "epoch": 65.78974119177832,
      "grad_norm": 0.17424491047859192,
      "learning_rate": 0.0004671031229668185,
      "loss": 0.3942,
      "step": 809000
    },
    {
      "epoch": 65.83040234208225,
      "grad_norm": 0.16203615069389343,
      "learning_rate": 0.000467082791151594,
      "loss": 0.3938,
      "step": 809500
    },
    {
      "epoch": 65.8710634923862,
      "grad_norm": 0.176950141787529,
      "learning_rate": 0.0004670624593363695,
      "loss": 0.3947,
      "step": 810000
    },
    {
      "epoch": 65.91172464269015,
      "grad_norm": 0.17017368972301483,
      "learning_rate": 0.0004670421275211451,
      "loss": 0.3943,
      "step": 810500
    },
    {
      "epoch": 65.95238579299408,
      "grad_norm": 0.1902364045381546,
      "learning_rate": 0.0004670217957059206,
      "loss": 0.3939,
      "step": 811000
    },
    {
      "epoch": 65.99304694329803,
      "grad_norm": 0.17663000524044037,
      "learning_rate": 0.0004670014638906962,
      "loss": 0.3944,
      "step": 811500
    },
    {
      "epoch": 66.03370809360197,
      "grad_norm": 0.17751500010490417,
      "learning_rate": 0.0004669811320754717,
      "loss": 0.391,
      "step": 812000
    },
    {
      "epoch": 66.0743692439059,
      "grad_norm": 0.18959186971187592,
      "learning_rate": 0.0004669608002602472,
      "loss": 0.3902,
      "step": 812500
    },
    {
      "epoch": 66.11503039420985,
      "grad_norm": 0.17045551538467407,
      "learning_rate": 0.0004669404684450228,
      "loss": 0.3915,
      "step": 813000
    },
    {
      "epoch": 66.1556915445138,
      "grad_norm": 0.188978374004364,
      "learning_rate": 0.0004669201366297983,
      "loss": 0.391,
      "step": 813500
    },
    {
      "epoch": 66.19635269481773,
      "grad_norm": 0.19963635504245758,
      "learning_rate": 0.0004668998048145738,
      "loss": 0.3919,
      "step": 814000
    },
    {
      "epoch": 66.23701384512168,
      "grad_norm": 0.1817827969789505,
      "learning_rate": 0.0004668794729993494,
      "loss": 0.3918,
      "step": 814500
    },
    {
      "epoch": 66.27767499542563,
      "grad_norm": 0.19421793520450592,
      "learning_rate": 0.0004668591411841249,
      "loss": 0.3923,
      "step": 815000
    },
    {
      "epoch": 66.31833614572956,
      "grad_norm": 0.18936128914356232,
      "learning_rate": 0.0004668388093689004,
      "loss": 0.3926,
      "step": 815500
    },
    {
      "epoch": 66.35899729603351,
      "grad_norm": 0.17001338303089142,
      "learning_rate": 0.000466818477553676,
      "loss": 0.3922,
      "step": 816000
    },
    {
      "epoch": 66.39965844633744,
      "grad_norm": 0.1857357621192932,
      "learning_rate": 0.0004667981457384515,
      "loss": 0.3929,
      "step": 816500
    },
    {
      "epoch": 66.44031959664139,
      "grad_norm": 0.16593748331069946,
      "learning_rate": 0.0004667778139232271,
      "loss": 0.3935,
      "step": 817000
    },
    {
      "epoch": 66.48098074694533,
      "grad_norm": 0.19517144560813904,
      "learning_rate": 0.0004667574821080026,
      "loss": 0.3933,
      "step": 817500
    },
    {
      "epoch": 66.52164189724927,
      "grad_norm": 0.2013401985168457,
      "learning_rate": 0.0004667371502927781,
      "loss": 0.3932,
      "step": 818000
    },
    {
      "epoch": 66.56230304755321,
      "grad_norm": 0.17471350729465485,
      "learning_rate": 0.0004667168184775537,
      "loss": 0.3935,
      "step": 818500
    },
    {
      "epoch": 66.60296419785716,
      "grad_norm": 0.18855412304401398,
      "learning_rate": 0.0004666964866623292,
      "loss": 0.3933,
      "step": 819000
    },
    {
      "epoch": 66.6436253481611,
      "grad_norm": 0.18764539062976837,
      "learning_rate": 0.0004666761548471047,
      "loss": 0.3935,
      "step": 819500
    },
    {
      "epoch": 66.68428649846504,
      "grad_norm": 0.18661324679851532,
      "learning_rate": 0.0004666558230318803,
      "loss": 0.3934,
      "step": 820000
    },
    {
      "epoch": 66.72494764876899,
      "grad_norm": 0.2198534905910492,
      "learning_rate": 0.0004666354912166558,
      "loss": 0.394,
      "step": 820500
    },
    {
      "epoch": 66.76560879907292,
      "grad_norm": 0.22321659326553345,
      "learning_rate": 0.0004666151594014313,
      "loss": 0.3933,
      "step": 821000
    },
    {
      "epoch": 66.80626994937687,
      "grad_norm": 0.17913328111171722,
      "learning_rate": 0.0004665948275862069,
      "loss": 0.3938,
      "step": 821500
    },
    {
      "epoch": 66.84693109968082,
      "grad_norm": 0.19685332477092743,
      "learning_rate": 0.0004665744957709824,
      "loss": 0.3945,
      "step": 822000
    },
    {
      "epoch": 66.88759224998475,
      "grad_norm": 0.18554775416851044,
      "learning_rate": 0.000466554163955758,
      "loss": 0.3939,
      "step": 822500
    },
    {
      "epoch": 66.9282534002887,
      "grad_norm": 0.20589691400527954,
      "learning_rate": 0.0004665338321405335,
      "loss": 0.3939,
      "step": 823000
    },
    {
      "epoch": 66.96891455059264,
      "grad_norm": 0.18275649845600128,
      "learning_rate": 0.000466513500325309,
      "loss": 0.3942,
      "step": 823500
    },
    {
      "epoch": 67.00957570089658,
      "grad_norm": 0.1889979988336563,
      "learning_rate": 0.0004664931685100846,
      "loss": 0.3937,
      "step": 824000
    },
    {
      "epoch": 67.05023685120052,
      "grad_norm": 0.2174578756093979,
      "learning_rate": 0.0004664728366948601,
      "loss": 0.3904,
      "step": 824500
    },
    {
      "epoch": 67.09089800150446,
      "grad_norm": 0.16857796907424927,
      "learning_rate": 0.0004664525048796356,
      "loss": 0.3908,
      "step": 825000
    },
    {
      "epoch": 67.1315591518084,
      "grad_norm": 0.1770809292793274,
      "learning_rate": 0.00046643217306441124,
      "loss": 0.3904,
      "step": 825500
    },
    {
      "epoch": 67.17222030211235,
      "grad_norm": 0.17157097160816193,
      "learning_rate": 0.00046641184124918676,
      "loss": 0.3914,
      "step": 826000
    },
    {
      "epoch": 67.21288145241628,
      "grad_norm": 0.1802077442407608,
      "learning_rate": 0.0004663915094339623,
      "loss": 0.3918,
      "step": 826500
    },
    {
      "epoch": 67.25354260272023,
      "grad_norm": 0.19469590485095978,
      "learning_rate": 0.00046637117761873784,
      "loss": 0.392,
      "step": 827000
    },
    {
      "epoch": 67.29420375302418,
      "grad_norm": 0.18156272172927856,
      "learning_rate": 0.00046635084580351336,
      "loss": 0.3922,
      "step": 827500
    },
    {
      "epoch": 67.33486490332811,
      "grad_norm": 0.18576422333717346,
      "learning_rate": 0.00046633051398828893,
      "loss": 0.3927,
      "step": 828000
    },
    {
      "epoch": 67.37552605363206,
      "grad_norm": 0.17712187767028809,
      "learning_rate": 0.00046631018217306445,
      "loss": 0.3922,
      "step": 828500
    },
    {
      "epoch": 67.416187203936,
      "grad_norm": 0.20125161111354828,
      "learning_rate": 0.00046628985035783996,
      "loss": 0.3926,
      "step": 829000
    },
    {
      "epoch": 67.45684835423994,
      "grad_norm": 0.1666412353515625,
      "learning_rate": 0.00046626951854261553,
      "loss": 0.3927,
      "step": 829500
    },
    {
      "epoch": 67.49750950454388,
      "grad_norm": 0.1881190687417984,
      "learning_rate": 0.00046624918672739105,
      "loss": 0.3926,
      "step": 830000
    },
    {
      "epoch": 67.53817065484783,
      "grad_norm": 0.22209227085113525,
      "learning_rate": 0.00046622885491216657,
      "loss": 0.3927,
      "step": 830500
    },
    {
      "epoch": 67.57883180515176,
      "grad_norm": 0.1841595619916916,
      "learning_rate": 0.00046620852309694214,
      "loss": 0.3929,
      "step": 831000
    },
    {
      "epoch": 67.61949295545571,
      "grad_norm": 0.17910154163837433,
      "learning_rate": 0.00046618819128171765,
      "loss": 0.394,
      "step": 831500
    },
    {
      "epoch": 67.66015410575964,
      "grad_norm": 0.1906566172838211,
      "learning_rate": 0.0004661678594664932,
      "loss": 0.393,
      "step": 832000
    },
    {
      "epoch": 67.70081525606359,
      "grad_norm": 0.2059386521577835,
      "learning_rate": 0.00046614752765126874,
      "loss": 0.3933,
      "step": 832500
    },
    {
      "epoch": 67.74147640636754,
      "grad_norm": 0.1956755816936493,
      "learning_rate": 0.00046612719583604426,
      "loss": 0.3933,
      "step": 833000
    },
    {
      "epoch": 67.78213755667147,
      "grad_norm": 0.17766258120536804,
      "learning_rate": 0.00046610686402081983,
      "loss": 0.3937,
      "step": 833500
    },
    {
      "epoch": 67.82279870697542,
      "grad_norm": 0.1891072690486908,
      "learning_rate": 0.00046608653220559535,
      "loss": 0.3938,
      "step": 834000
    },
    {
      "epoch": 67.86345985727937,
      "grad_norm": 0.1794295758008957,
      "learning_rate": 0.00046606620039037086,
      "loss": 0.3941,
      "step": 834500
    },
    {
      "epoch": 67.9041210075833,
      "grad_norm": 0.1752341389656067,
      "learning_rate": 0.00046604586857514643,
      "loss": 0.3939,
      "step": 835000
    },
    {
      "epoch": 67.94478215788725,
      "grad_norm": 0.19306905567646027,
      "learning_rate": 0.00046602553675992195,
      "loss": 0.3937,
      "step": 835500
    },
    {
      "epoch": 67.9854433081912,
      "grad_norm": 0.19081707298755646,
      "learning_rate": 0.00046600520494469746,
      "loss": 0.394,
      "step": 836000
    },
    {
      "epoch": 68.02610445849513,
      "grad_norm": 0.19387218356132507,
      "learning_rate": 0.00046598487312947304,
      "loss": 0.3912,
      "step": 836500
    },
    {
      "epoch": 68.06676560879907,
      "grad_norm": 0.20650982856750488,
      "learning_rate": 0.00046596454131424855,
      "loss": 0.3903,
      "step": 837000
    },
    {
      "epoch": 68.10742675910302,
      "grad_norm": 0.1736215502023697,
      "learning_rate": 0.0004659442094990241,
      "loss": 0.3908,
      "step": 837500
    },
    {
      "epoch": 68.14808790940695,
      "grad_norm": 0.19625714421272278,
      "learning_rate": 0.00046592387768379964,
      "loss": 0.3908,
      "step": 838000
    },
    {
      "epoch": 68.1887490597109,
      "grad_norm": 0.16420750319957733,
      "learning_rate": 0.00046590354586857516,
      "loss": 0.3916,
      "step": 838500
    },
    {
      "epoch": 68.22941021001485,
      "grad_norm": 0.17862622439861298,
      "learning_rate": 0.0004658832140533507,
      "loss": 0.3916,
      "step": 839000
    },
    {
      "epoch": 68.27007136031878,
      "grad_norm": 0.21424539387226105,
      "learning_rate": 0.00046586288223812624,
      "loss": 0.392,
      "step": 839500
    },
    {
      "epoch": 68.31073251062273,
      "grad_norm": 0.19317775964736938,
      "learning_rate": 0.00046584255042290176,
      "loss": 0.3921,
      "step": 840000
    },
    {
      "epoch": 68.35139366092666,
      "grad_norm": 0.18642112612724304,
      "learning_rate": 0.00046582221860767733,
      "loss": 0.392,
      "step": 840500
    },
    {
      "epoch": 68.39205481123061,
      "grad_norm": 0.19521383941173553,
      "learning_rate": 0.00046580188679245285,
      "loss": 0.3925,
      "step": 841000
    },
    {
      "epoch": 68.43271596153456,
      "grad_norm": 0.16068412363529205,
      "learning_rate": 0.00046578155497722836,
      "loss": 0.3921,
      "step": 841500
    },
    {
      "epoch": 68.47337711183849,
      "grad_norm": 0.2048194855451584,
      "learning_rate": 0.00046576122316200393,
      "loss": 0.3923,
      "step": 842000
    },
    {
      "epoch": 68.51403826214244,
      "grad_norm": 0.18848450481891632,
      "learning_rate": 0.00046574089134677945,
      "loss": 0.3925,
      "step": 842500
    },
    {
      "epoch": 68.55469941244638,
      "grad_norm": 0.17725329101085663,
      "learning_rate": 0.000465720559531555,
      "loss": 0.3932,
      "step": 843000
    },
    {
      "epoch": 68.59536056275032,
      "grad_norm": 0.18816937506198883,
      "learning_rate": 0.00046570022771633054,
      "loss": 0.3931,
      "step": 843500
    },
    {
      "epoch": 68.63602171305426,
      "grad_norm": 0.19004109501838684,
      "learning_rate": 0.00046567989590110605,
      "loss": 0.3932,
      "step": 844000
    },
    {
      "epoch": 68.67668286335821,
      "grad_norm": 0.18572427332401276,
      "learning_rate": 0.0004656595640858816,
      "loss": 0.3935,
      "step": 844500
    },
    {
      "epoch": 68.71734401366214,
      "grad_norm": 0.17314445972442627,
      "learning_rate": 0.00046563923227065714,
      "loss": 0.393,
      "step": 845000
    },
    {
      "epoch": 68.75800516396609,
      "grad_norm": 0.1675446629524231,
      "learning_rate": 0.00046561890045543266,
      "loss": 0.3931,
      "step": 845500
    },
    {
      "epoch": 68.79866631427004,
      "grad_norm": 0.19392065703868866,
      "learning_rate": 0.00046559856864020823,
      "loss": 0.3927,
      "step": 846000
    },
    {
      "epoch": 68.83932746457397,
      "grad_norm": 0.17351794242858887,
      "learning_rate": 0.00046557823682498374,
      "loss": 0.3938,
      "step": 846500
    },
    {
      "epoch": 68.87998861487792,
      "grad_norm": 0.21080191433429718,
      "learning_rate": 0.0004655579050097593,
      "loss": 0.3936,
      "step": 847000
    },
    {
      "epoch": 68.92064976518186,
      "grad_norm": 0.20626603066921234,
      "learning_rate": 0.00046553757319453483,
      "loss": 0.3941,
      "step": 847500
    },
    {
      "epoch": 68.9613109154858,
      "grad_norm": 0.22777344286441803,
      "learning_rate": 0.00046551724137931035,
      "loss": 0.3936,
      "step": 848000
    },
    {
      "epoch": 69.00197206578974,
      "grad_norm": 0.1899498850107193,
      "learning_rate": 0.0004654969095640859,
      "loss": 0.3935,
      "step": 848500
    },
    {
      "epoch": 69.04263321609368,
      "grad_norm": 0.19031532108783722,
      "learning_rate": 0.00046547657774886143,
      "loss": 0.39,
      "step": 849000
    },
    {
      "epoch": 69.08329436639762,
      "grad_norm": 0.1910412609577179,
      "learning_rate": 0.00046545624593363695,
      "loss": 0.3905,
      "step": 849500
    },
    {
      "epoch": 69.12395551670157,
      "grad_norm": 0.16422665119171143,
      "learning_rate": 0.0004654359141184125,
      "loss": 0.3903,
      "step": 850000
    },
    {
      "epoch": 69.1646166670055,
      "grad_norm": 0.19043923914432526,
      "learning_rate": 0.00046541558230318804,
      "loss": 0.3911,
      "step": 850500
    },
    {
      "epoch": 69.20527781730945,
      "grad_norm": 0.2197311669588089,
      "learning_rate": 0.00046539525048796355,
      "loss": 0.3913,
      "step": 851000
    },
    {
      "epoch": 69.2459389676134,
      "grad_norm": 0.20129944384098053,
      "learning_rate": 0.0004653749186727391,
      "loss": 0.3907,
      "step": 851500
    },
    {
      "epoch": 69.28660011791733,
      "grad_norm": 0.1975177377462387,
      "learning_rate": 0.00046535458685751464,
      "loss": 0.3919,
      "step": 852000
    },
    {
      "epoch": 69.32726126822128,
      "grad_norm": 0.19747509062290192,
      "learning_rate": 0.0004653342550422902,
      "loss": 0.3917,
      "step": 852500
    },
    {
      "epoch": 69.36792241852523,
      "grad_norm": 0.1797896921634674,
      "learning_rate": 0.00046531392322706573,
      "loss": 0.3917,
      "step": 853000
    },
    {
      "epoch": 69.40858356882916,
      "grad_norm": 0.1961900144815445,
      "learning_rate": 0.00046529359141184124,
      "loss": 0.3922,
      "step": 853500
    },
    {
      "epoch": 69.4492447191331,
      "grad_norm": 0.1977694034576416,
      "learning_rate": 0.0004652732595966168,
      "loss": 0.3927,
      "step": 854000
    },
    {
      "epoch": 69.48990586943705,
      "grad_norm": 0.19230765104293823,
      "learning_rate": 0.00046525292778139233,
      "loss": 0.3924,
      "step": 854500
    },
    {
      "epoch": 69.53056701974099,
      "grad_norm": 0.17342880368232727,
      "learning_rate": 0.00046523259596616785,
      "loss": 0.3928,
      "step": 855000
    },
    {
      "epoch": 69.57122817004493,
      "grad_norm": 0.20322713255882263,
      "learning_rate": 0.0004652122641509434,
      "loss": 0.3926,
      "step": 855500
    },
    {
      "epoch": 69.61188932034888,
      "grad_norm": 0.1940443217754364,
      "learning_rate": 0.00046519193233571894,
      "loss": 0.3931,
      "step": 856000
    },
    {
      "epoch": 69.65255047065281,
      "grad_norm": 0.20651960372924805,
      "learning_rate": 0.00046517160052049445,
      "loss": 0.3929,
      "step": 856500
    },
    {
      "epoch": 69.69321162095676,
      "grad_norm": 0.18100838363170624,
      "learning_rate": 0.00046515126870527,
      "loss": 0.3933,
      "step": 857000
    },
    {
      "epoch": 69.73387277126069,
      "grad_norm": 0.19498087465763092,
      "learning_rate": 0.00046513093689004554,
      "loss": 0.3931,
      "step": 857500
    },
    {
      "epoch": 69.77453392156464,
      "grad_norm": 0.20976635813713074,
      "learning_rate": 0.0004651106050748211,
      "loss": 0.3934,
      "step": 858000
    },
    {
      "epoch": 69.81519507186859,
      "grad_norm": 0.1737499088048935,
      "learning_rate": 0.0004650902732595966,
      "loss": 0.3933,
      "step": 858500
    },
    {
      "epoch": 69.85585622217252,
      "grad_norm": 0.17665915191173553,
      "learning_rate": 0.00046506994144437214,
      "loss": 0.3928,
      "step": 859000
    },
    {
      "epoch": 69.89651737247647,
      "grad_norm": 0.18168078362941742,
      "learning_rate": 0.0004650496096291477,
      "loss": 0.3938,
      "step": 859500
    },
    {
      "epoch": 69.93717852278041,
      "grad_norm": 0.20462968945503235,
      "learning_rate": 0.00046502927781392323,
      "loss": 0.3935,
      "step": 860000
    },
    {
      "epoch": 69.97783967308435,
      "grad_norm": 0.17615918815135956,
      "learning_rate": 0.00046500894599869875,
      "loss": 0.3937,
      "step": 860500
    },
    {
      "epoch": 70.0185008233883,
      "grad_norm": 0.16869784891605377,
      "learning_rate": 0.0004649886141834743,
      "loss": 0.3917,
      "step": 861000
    },
    {
      "epoch": 70.05916197369224,
      "grad_norm": 0.19208112359046936,
      "learning_rate": 0.00046496828236824983,
      "loss": 0.3898,
      "step": 861500
    },
    {
      "epoch": 70.09982312399617,
      "grad_norm": 0.18559406697750092,
      "learning_rate": 0.00046494795055302535,
      "loss": 0.39,
      "step": 862000
    },
    {
      "epoch": 70.14048427430012,
      "grad_norm": 0.22606399655342102,
      "learning_rate": 0.0004649276187378009,
      "loss": 0.3909,
      "step": 862500
    },
    {
      "epoch": 70.18114542460407,
      "grad_norm": 0.1906377077102661,
      "learning_rate": 0.00046490728692257644,
      "loss": 0.3905,
      "step": 863000
    },
    {
      "epoch": 70.221806574908,
      "grad_norm": 0.19160877168178558,
      "learning_rate": 0.000464886955107352,
      "loss": 0.391,
      "step": 863500
    },
    {
      "epoch": 70.26246772521195,
      "grad_norm": 0.19285070896148682,
      "learning_rate": 0.0004648666232921275,
      "loss": 0.3916,
      "step": 864000
    },
    {
      "epoch": 70.3031288755159,
      "grad_norm": 0.20087836682796478,
      "learning_rate": 0.00046484629147690304,
      "loss": 0.3914,
      "step": 864500
    },
    {
      "epoch": 70.34379002581983,
      "grad_norm": 0.20736245810985565,
      "learning_rate": 0.0004648259596616786,
      "loss": 0.3914,
      "step": 865000
    },
    {
      "epoch": 70.38445117612378,
      "grad_norm": 0.20871154963970184,
      "learning_rate": 0.0004648056278464541,
      "loss": 0.3916,
      "step": 865500
    },
    {
      "epoch": 70.42511232642771,
      "grad_norm": 0.2022191286087036,
      "learning_rate": 0.00046478529603122964,
      "loss": 0.392,
      "step": 866000
    },
    {
      "epoch": 70.46577347673166,
      "grad_norm": 0.18754543364048004,
      "learning_rate": 0.0004647649642160052,
      "loss": 0.3922,
      "step": 866500
    },
    {
      "epoch": 70.5064346270356,
      "grad_norm": 0.18427816033363342,
      "learning_rate": 0.00046474463240078073,
      "loss": 0.3922,
      "step": 867000
    },
    {
      "epoch": 70.54709577733954,
      "grad_norm": 0.17198623716831207,
      "learning_rate": 0.0004647243005855563,
      "loss": 0.3922,
      "step": 867500
    },
    {
      "epoch": 70.58775692764348,
      "grad_norm": 0.17906221747398376,
      "learning_rate": 0.0004647039687703318,
      "loss": 0.3922,
      "step": 868000
    },
    {
      "epoch": 70.62841807794743,
      "grad_norm": 0.2013607919216156,
      "learning_rate": 0.00046468363695510733,
      "loss": 0.3928,
      "step": 868500
    },
    {
      "epoch": 70.66907922825136,
      "grad_norm": 0.18879075348377228,
      "learning_rate": 0.0004646633051398829,
      "loss": 0.3932,
      "step": 869000
    },
    {
      "epoch": 70.70974037855531,
      "grad_norm": 0.1733228862285614,
      "learning_rate": 0.0004646429733246584,
      "loss": 0.3935,
      "step": 869500
    },
    {
      "epoch": 70.75040152885926,
      "grad_norm": 0.1904359608888626,
      "learning_rate": 0.00046462264150943394,
      "loss": 0.3928,
      "step": 870000
    },
    {
      "epoch": 70.79106267916319,
      "grad_norm": 0.2148178517818451,
      "learning_rate": 0.0004646023096942095,
      "loss": 0.3929,
      "step": 870500
    },
    {
      "epoch": 70.83172382946714,
      "grad_norm": 0.17940428853034973,
      "learning_rate": 0.000464581977878985,
      "loss": 0.393,
      "step": 871000
    },
    {
      "epoch": 70.87238497977108,
      "grad_norm": 0.19095949828624725,
      "learning_rate": 0.00046456164606376054,
      "loss": 0.393,
      "step": 871500
    },
    {
      "epoch": 70.91304613007502,
      "grad_norm": 0.1904725283384323,
      "learning_rate": 0.0004645413142485361,
      "loss": 0.3934,
      "step": 872000
    },
    {
      "epoch": 70.95370728037896,
      "grad_norm": 0.18390098214149475,
      "learning_rate": 0.00046452098243331163,
      "loss": 0.3936,
      "step": 872500
    },
    {
      "epoch": 70.9943684306829,
      "grad_norm": 0.1924033761024475,
      "learning_rate": 0.0004645006506180872,
      "loss": 0.3936,
      "step": 873000
    },
    {
      "epoch": 71.03502958098684,
      "grad_norm": 0.196153923869133,
      "learning_rate": 0.0004644803188028627,
      "loss": 0.39,
      "step": 873500
    },
    {
      "epoch": 71.07569073129079,
      "grad_norm": 0.19528737664222717,
      "learning_rate": 0.00046445998698763823,
      "loss": 0.3902,
      "step": 874000
    },
    {
      "epoch": 71.11635188159472,
      "grad_norm": 0.18691109120845795,
      "learning_rate": 0.0004644396551724138,
      "loss": 0.3901,
      "step": 874500
    },
    {
      "epoch": 71.15701303189867,
      "grad_norm": 0.17418020963668823,
      "learning_rate": 0.0004644193233571893,
      "loss": 0.3903,
      "step": 875000
    },
    {
      "epoch": 71.19767418220262,
      "grad_norm": 0.18506398797035217,
      "learning_rate": 0.00046439899154196483,
      "loss": 0.3907,
      "step": 875500
    },
    {
      "epoch": 71.23833533250655,
      "grad_norm": 0.19973601400852203,
      "learning_rate": 0.0004643786597267404,
      "loss": 0.3912,
      "step": 876000
    },
    {
      "epoch": 71.2789964828105,
      "grad_norm": 0.1825960874557495,
      "learning_rate": 0.0004643583279115159,
      "loss": 0.3911,
      "step": 876500
    },
    {
      "epoch": 71.31965763311445,
      "grad_norm": 0.19060440361499786,
      "learning_rate": 0.00046433799609629144,
      "loss": 0.391,
      "step": 877000
    },
    {
      "epoch": 71.36031878341838,
      "grad_norm": 0.19725070893764496,
      "learning_rate": 0.000464317664281067,
      "loss": 0.3911,
      "step": 877500
    },
    {
      "epoch": 71.40097993372233,
      "grad_norm": 0.17799288034439087,
      "learning_rate": 0.0004642973324658425,
      "loss": 0.3916,
      "step": 878000
    },
    {
      "epoch": 71.44164108402627,
      "grad_norm": 0.18228335678577423,
      "learning_rate": 0.0004642770006506181,
      "loss": 0.3923,
      "step": 878500
    },
    {
      "epoch": 71.4823022343302,
      "grad_norm": 0.18641725182533264,
      "learning_rate": 0.0004642566688353936,
      "loss": 0.392,
      "step": 879000
    },
    {
      "epoch": 71.52296338463415,
      "grad_norm": 0.17726293206214905,
      "learning_rate": 0.00046423633702016913,
      "loss": 0.392,
      "step": 879500
    },
    {
      "epoch": 71.5636245349381,
      "grad_norm": 0.20058082044124603,
      "learning_rate": 0.0004642160052049447,
      "loss": 0.3921,
      "step": 880000
    },
    {
      "epoch": 71.60428568524203,
      "grad_norm": 0.19375047087669373,
      "learning_rate": 0.0004641956733897202,
      "loss": 0.3923,
      "step": 880500
    },
    {
      "epoch": 71.64494683554598,
      "grad_norm": 0.17096151411533356,
      "learning_rate": 0.00046417534157449573,
      "loss": 0.3924,
      "step": 881000
    },
    {
      "epoch": 71.68560798584991,
      "grad_norm": 0.19937050342559814,
      "learning_rate": 0.0004641550097592713,
      "loss": 0.3928,
      "step": 881500
    },
    {
      "epoch": 71.72626913615386,
      "grad_norm": 0.23878085613250732,
      "learning_rate": 0.0004641346779440468,
      "loss": 0.393,
      "step": 882000
    },
    {
      "epoch": 71.76693028645781,
      "grad_norm": 0.20987369120121002,
      "learning_rate": 0.00046411434612882244,
      "loss": 0.393,
      "step": 882500
    },
    {
      "epoch": 71.80759143676174,
      "grad_norm": 0.1984683871269226,
      "learning_rate": 0.00046409401431359796,
      "loss": 0.3928,
      "step": 883000
    },
    {
      "epoch": 71.84825258706569,
      "grad_norm": 0.18168935179710388,
      "learning_rate": 0.0004640736824983735,
      "loss": 0.393,
      "step": 883500
    },
    {
      "epoch": 71.88891373736963,
      "grad_norm": 0.18424732983112335,
      "learning_rate": 0.00046405335068314905,
      "loss": 0.3934,
      "step": 884000
    },
    {
      "epoch": 71.92957488767357,
      "grad_norm": 0.18607684969902039,
      "learning_rate": 0.00046403301886792456,
      "loss": 0.3932,
      "step": 884500
    },
    {
      "epoch": 71.97023603797751,
      "grad_norm": 0.19484953582286835,
      "learning_rate": 0.0004640126870527001,
      "loss": 0.3933,
      "step": 885000
    },
    {
      "epoch": 72.01089718828146,
      "grad_norm": 0.1903611421585083,
      "learning_rate": 0.00046399235523747565,
      "loss": 0.3917,
      "step": 885500
    },
    {
      "epoch": 72.0515583385854,
      "grad_norm": 0.19422321021556854,
      "learning_rate": 0.00046397202342225117,
      "loss": 0.389,
      "step": 886000
    },
    {
      "epoch": 72.09221948888934,
      "grad_norm": 0.1908373236656189,
      "learning_rate": 0.0004639516916070267,
      "loss": 0.3893,
      "step": 886500
    },
    {
      "epoch": 72.13288063919329,
      "grad_norm": 0.18391874432563782,
      "learning_rate": 0.00046393135979180225,
      "loss": 0.3902,
      "step": 887000
    },
    {
      "epoch": 72.17354178949722,
      "grad_norm": 0.18367436528205872,
      "learning_rate": 0.00046391102797657777,
      "loss": 0.3906,
      "step": 887500
    },
    {
      "epoch": 72.21420293980117,
      "grad_norm": 0.18568763136863708,
      "learning_rate": 0.00046389069616135334,
      "loss": 0.3908,
      "step": 888000
    },
    {
      "epoch": 72.25486409010512,
      "grad_norm": 0.17326536774635315,
      "learning_rate": 0.00046387036434612886,
      "loss": 0.3913,
      "step": 888500
    },
    {
      "epoch": 72.29552524040905,
      "grad_norm": 0.1983731985092163,
      "learning_rate": 0.0004638500325309044,
      "loss": 0.3913,
      "step": 889000
    },
    {
      "epoch": 72.336186390713,
      "grad_norm": 0.20228074491024017,
      "learning_rate": 0.00046382970071567995,
      "loss": 0.391,
      "step": 889500
    },
    {
      "epoch": 72.37684754101693,
      "grad_norm": 0.18029442429542542,
      "learning_rate": 0.00046380936890045546,
      "loss": 0.3915,
      "step": 890000
    },
    {
      "epoch": 72.41750869132088,
      "grad_norm": 0.23920711874961853,
      "learning_rate": 0.000463789037085231,
      "loss": 0.3917,
      "step": 890500
    },
    {
      "epoch": 72.45816984162482,
      "grad_norm": 0.2147374302148819,
      "learning_rate": 0.00046376870527000655,
      "loss": 0.3917,
      "step": 891000
    },
    {
      "epoch": 72.49883099192876,
      "grad_norm": 0.19146370887756348,
      "learning_rate": 0.00046374837345478207,
      "loss": 0.3922,
      "step": 891500
    },
    {
      "epoch": 72.5394921422327,
      "grad_norm": 0.20551955699920654,
      "learning_rate": 0.0004637280416395576,
      "loss": 0.3922,
      "step": 892000
    },
    {
      "epoch": 72.58015329253665,
      "grad_norm": 0.17667587101459503,
      "learning_rate": 0.00046370770982433315,
      "loss": 0.392,
      "step": 892500
    },
    {
      "epoch": 72.62081444284058,
      "grad_norm": 0.16029661893844604,
      "learning_rate": 0.00046368737800910867,
      "loss": 0.3922,
      "step": 893000
    },
    {
      "epoch": 72.66147559314453,
      "grad_norm": 0.19626843929290771,
      "learning_rate": 0.00046366704619388424,
      "loss": 0.3923,
      "step": 893500
    },
    {
      "epoch": 72.70213674344848,
      "grad_norm": 0.19361037015914917,
      "learning_rate": 0.00046364671437865976,
      "loss": 0.3925,
      "step": 894000
    },
    {
      "epoch": 72.74279789375241,
      "grad_norm": 0.179525688290596,
      "learning_rate": 0.00046362638256343527,
      "loss": 0.3926,
      "step": 894500
    },
    {
      "epoch": 72.78345904405636,
      "grad_norm": 0.18069037795066833,
      "learning_rate": 0.00046360605074821084,
      "loss": 0.3932,
      "step": 895000
    },
    {
      "epoch": 72.8241201943603,
      "grad_norm": 0.1905675232410431,
      "learning_rate": 0.00046358571893298636,
      "loss": 0.3925,
      "step": 895500
    },
    {
      "epoch": 72.86478134466424,
      "grad_norm": 0.1848871111869812,
      "learning_rate": 0.0004635653871177619,
      "loss": 0.3932,
      "step": 896000
    },
    {
      "epoch": 72.90544249496818,
      "grad_norm": 0.19314545392990112,
      "learning_rate": 0.00046354505530253745,
      "loss": 0.3931,
      "step": 896500
    },
    {
      "epoch": 72.94610364527213,
      "grad_norm": 0.18601900339126587,
      "learning_rate": 0.00046352472348731296,
      "loss": 0.3928,
      "step": 897000
    },
    {
      "epoch": 72.98676479557606,
      "grad_norm": 0.19722895324230194,
      "learning_rate": 0.00046350439167208853,
      "loss": 0.3929,
      "step": 897500
    },
    {
      "epoch": 73.02742594588001,
      "grad_norm": 0.19768406450748444,
      "learning_rate": 0.00046348405985686405,
      "loss": 0.39,
      "step": 898000
    },
    {
      "epoch": 73.06808709618394,
      "grad_norm": 0.1717609316110611,
      "learning_rate": 0.00046346372804163957,
      "loss": 0.3896,
      "step": 898500
    },
    {
      "epoch": 73.10874824648789,
      "grad_norm": 0.18548016250133514,
      "learning_rate": 0.00046344339622641514,
      "loss": 0.39,
      "step": 899000
    },
    {
      "epoch": 73.14940939679184,
      "grad_norm": 0.1947314739227295,
      "learning_rate": 0.00046342306441119065,
      "loss": 0.39,
      "step": 899500
    },
    {
      "epoch": 73.19007054709577,
      "grad_norm": 0.1922609508037567,
      "learning_rate": 0.00046340273259596617,
      "loss": 0.3906,
      "step": 900000
    },
    {
      "epoch": 73.23073169739972,
      "grad_norm": 0.18808144330978394,
      "learning_rate": 0.00046338240078074174,
      "loss": 0.3903,
      "step": 900500
    },
    {
      "epoch": 73.27139284770367,
      "grad_norm": 0.1881619095802307,
      "learning_rate": 0.00046336206896551726,
      "loss": 0.3911,
      "step": 901000
    },
    {
      "epoch": 73.3120539980076,
      "grad_norm": 0.2144635021686554,
      "learning_rate": 0.0004633417371502928,
      "loss": 0.3909,
      "step": 901500
    },
    {
      "epoch": 73.35271514831155,
      "grad_norm": 0.20607644319534302,
      "learning_rate": 0.00046332140533506834,
      "loss": 0.391,
      "step": 902000
    },
    {
      "epoch": 73.3933762986155,
      "grad_norm": 0.2067636251449585,
      "learning_rate": 0.00046330107351984386,
      "loss": 0.3912,
      "step": 902500
    },
    {
      "epoch": 73.43403744891943,
      "grad_norm": 0.1796068549156189,
      "learning_rate": 0.00046328074170461943,
      "loss": 0.3919,
      "step": 903000
    },
    {
      "epoch": 73.47469859922337,
      "grad_norm": 0.18192502856254578,
      "learning_rate": 0.00046326040988939495,
      "loss": 0.3914,
      "step": 903500
    },
    {
      "epoch": 73.51535974952732,
      "grad_norm": 0.19076448678970337,
      "learning_rate": 0.00046324007807417046,
      "loss": 0.3917,
      "step": 904000
    },
    {
      "epoch": 73.55602089983125,
      "grad_norm": 0.193254753947258,
      "learning_rate": 0.00046321974625894603,
      "loss": 0.3923,
      "step": 904500
    },
    {
      "epoch": 73.5966820501352,
      "grad_norm": 0.1913173645734787,
      "learning_rate": 0.00046319941444372155,
      "loss": 0.3917,
      "step": 905000
    },
    {
      "epoch": 73.63734320043915,
      "grad_norm": 0.1853686273097992,
      "learning_rate": 0.00046317908262849707,
      "loss": 0.3916,
      "step": 905500
    },
    {
      "epoch": 73.67800435074308,
      "grad_norm": 0.24050095677375793,
      "learning_rate": 0.00046315875081327264,
      "loss": 0.3922,
      "step": 906000
    },
    {
      "epoch": 73.71866550104703,
      "grad_norm": 0.17153088748455048,
      "learning_rate": 0.00046313841899804815,
      "loss": 0.3925,
      "step": 906500
    },
    {
      "epoch": 73.75932665135096,
      "grad_norm": 0.1861722767353058,
      "learning_rate": 0.00046311808718282367,
      "loss": 0.3924,
      "step": 907000
    },
    {
      "epoch": 73.79998780165491,
      "grad_norm": 0.2065347135066986,
      "learning_rate": 0.00046309775536759924,
      "loss": 0.3931,
      "step": 907500
    },
    {
      "epoch": 73.84064895195885,
      "grad_norm": 0.1885353922843933,
      "learning_rate": 0.00046307742355237476,
      "loss": 0.3933,
      "step": 908000
    },
    {
      "epoch": 73.88131010226279,
      "grad_norm": 0.19155694544315338,
      "learning_rate": 0.00046305709173715033,
      "loss": 0.3923,
      "step": 908500
    },
    {
      "epoch": 73.92197125256673,
      "grad_norm": 0.17134088277816772,
      "learning_rate": 0.00046303675992192584,
      "loss": 0.3928,
      "step": 909000
    },
    {
      "epoch": 73.96263240287068,
      "grad_norm": 0.19080179929733276,
      "learning_rate": 0.00046301642810670136,
      "loss": 0.3925,
      "step": 909500
    },
    {
      "epoch": 74.00329355317461,
      "grad_norm": 0.17054130136966705,
      "learning_rate": 0.00046299609629147693,
      "loss": 0.392,
      "step": 910000
    },
    {
      "epoch": 74.04395470347856,
      "grad_norm": 0.1838667243719101,
      "learning_rate": 0.00046297576447625245,
      "loss": 0.3887,
      "step": 910500
    },
    {
      "epoch": 74.08461585378251,
      "grad_norm": 0.22191791236400604,
      "learning_rate": 0.00046295543266102796,
      "loss": 0.3897,
      "step": 911000
    },
    {
      "epoch": 74.12527700408644,
      "grad_norm": 0.22449401021003723,
      "learning_rate": 0.00046293510084580354,
      "loss": 0.3897,
      "step": 911500
    },
    {
      "epoch": 74.16593815439039,
      "grad_norm": 0.19919148087501526,
      "learning_rate": 0.00046291476903057905,
      "loss": 0.3898,
      "step": 912000
    },
    {
      "epoch": 74.20659930469434,
      "grad_norm": 0.21862828731536865,
      "learning_rate": 0.00046289443721535457,
      "loss": 0.3905,
      "step": 912500
    },
    {
      "epoch": 74.24726045499827,
      "grad_norm": 0.1743863970041275,
      "learning_rate": 0.00046287410540013014,
      "loss": 0.3905,
      "step": 913000
    },
    {
      "epoch": 74.28792160530222,
      "grad_norm": 0.21404308080673218,
      "learning_rate": 0.00046285377358490566,
      "loss": 0.3909,
      "step": 913500
    },
    {
      "epoch": 74.32858275560615,
      "grad_norm": 0.20077422261238098,
      "learning_rate": 0.0004628334417696812,
      "loss": 0.3908,
      "step": 914000
    },
    {
      "epoch": 74.3692439059101,
      "grad_norm": 0.1779322326183319,
      "learning_rate": 0.00046281310995445674,
      "loss": 0.3917,
      "step": 914500
    },
    {
      "epoch": 74.40990505621404,
      "grad_norm": 0.20550592243671417,
      "learning_rate": 0.00046279277813923226,
      "loss": 0.3912,
      "step": 915000
    },
    {
      "epoch": 74.45056620651798,
      "grad_norm": 0.18145321309566498,
      "learning_rate": 0.00046277244632400783,
      "loss": 0.3916,
      "step": 915500
    },
    {
      "epoch": 74.49122735682192,
      "grad_norm": 0.19625410437583923,
      "learning_rate": 0.00046275211450878335,
      "loss": 0.3917,
      "step": 916000
    },
    {
      "epoch": 74.53188850712587,
      "grad_norm": 0.17567090690135956,
      "learning_rate": 0.00046273178269355886,
      "loss": 0.3914,
      "step": 916500
    },
    {
      "epoch": 74.5725496574298,
      "grad_norm": 0.17968253791332245,
      "learning_rate": 0.00046271145087833443,
      "loss": 0.3918,
      "step": 917000
    },
    {
      "epoch": 74.61321080773375,
      "grad_norm": 0.19544973969459534,
      "learning_rate": 0.00046269111906310995,
      "loss": 0.3921,
      "step": 917500
    },
    {
      "epoch": 74.6538719580377,
      "grad_norm": 0.1765003353357315,
      "learning_rate": 0.0004626707872478855,
      "loss": 0.3919,
      "step": 918000
    },
    {
      "epoch": 74.69453310834163,
      "grad_norm": 0.18947915732860565,
      "learning_rate": 0.00046265045543266104,
      "loss": 0.3921,
      "step": 918500
    },
    {
      "epoch": 74.73519425864558,
      "grad_norm": 0.19180828332901,
      "learning_rate": 0.00046263012361743655,
      "loss": 0.392,
      "step": 919000
    },
    {
      "epoch": 74.77585540894952,
      "grad_norm": 0.2179030179977417,
      "learning_rate": 0.0004626097918022121,
      "loss": 0.3924,
      "step": 919500
    },
    {
      "epoch": 74.81651655925346,
      "grad_norm": 0.19567832350730896,
      "learning_rate": 0.00046258945998698764,
      "loss": 0.3922,
      "step": 920000
    },
    {
      "epoch": 74.8571777095574,
      "grad_norm": 0.22380761802196503,
      "learning_rate": 0.00046256912817176316,
      "loss": 0.3926,
      "step": 920500
    },
    {
      "epoch": 74.89783885986135,
      "grad_norm": 0.1903536170721054,
      "learning_rate": 0.0004625487963565387,
      "loss": 0.3929,
      "step": 921000
    },
    {
      "epoch": 74.93850001016528,
      "grad_norm": 0.18914787471294403,
      "learning_rate": 0.00046252846454131424,
      "loss": 0.3923,
      "step": 921500
    },
    {
      "epoch": 74.97916116046923,
      "grad_norm": 0.18859760463237762,
      "learning_rate": 0.00046250813272608976,
      "loss": 0.3924,
      "step": 922000
    },
    {
      "epoch": 75.01982231077316,
      "grad_norm": 0.22405852377414703,
      "learning_rate": 0.00046248780091086533,
      "loss": 0.3904,
      "step": 922500
    },
    {
      "epoch": 75.06048346107711,
      "grad_norm": 0.1704079657793045,
      "learning_rate": 0.00046246746909564085,
      "loss": 0.3895,
      "step": 923000
    },
    {
      "epoch": 75.10114461138106,
      "grad_norm": 0.19849498569965363,
      "learning_rate": 0.0004624471372804164,
      "loss": 0.3892,
      "step": 923500
    },
    {
      "epoch": 75.14180576168499,
      "grad_norm": 0.18563073873519897,
      "learning_rate": 0.00046242680546519193,
      "loss": 0.3898,
      "step": 924000
    },
    {
      "epoch": 75.18246691198894,
      "grad_norm": 0.1949358880519867,
      "learning_rate": 0.00046240647364996745,
      "loss": 0.3901,
      "step": 924500
    },
    {
      "epoch": 75.22312806229289,
      "grad_norm": 0.19678084552288055,
      "learning_rate": 0.000462386141834743,
      "loss": 0.3902,
      "step": 925000
    },
    {
      "epoch": 75.26378921259682,
      "grad_norm": 0.20366953313350677,
      "learning_rate": 0.00046236581001951854,
      "loss": 0.3906,
      "step": 925500
    },
    {
      "epoch": 75.30445036290077,
      "grad_norm": 0.1853518784046173,
      "learning_rate": 0.00046234547820429405,
      "loss": 0.3902,
      "step": 926000
    },
    {
      "epoch": 75.34511151320471,
      "grad_norm": 0.19878657162189484,
      "learning_rate": 0.0004623251463890696,
      "loss": 0.3907,
      "step": 926500
    },
    {
      "epoch": 75.38577266350865,
      "grad_norm": 0.21550185978412628,
      "learning_rate": 0.00046230481457384514,
      "loss": 0.3909,
      "step": 927000
    },
    {
      "epoch": 75.4264338138126,
      "grad_norm": 0.19344080984592438,
      "learning_rate": 0.00046228448275862066,
      "loss": 0.3907,
      "step": 927500
    },
    {
      "epoch": 75.46709496411654,
      "grad_norm": 0.184627965092659,
      "learning_rate": 0.00046226415094339623,
      "loss": 0.3912,
      "step": 928000
    },
    {
      "epoch": 75.50775611442047,
      "grad_norm": 0.18918438255786896,
      "learning_rate": 0.00046224381912817174,
      "loss": 0.3914,
      "step": 928500
    },
    {
      "epoch": 75.54841726472442,
      "grad_norm": 0.17843332886695862,
      "learning_rate": 0.0004622234873129473,
      "loss": 0.3916,
      "step": 929000
    },
    {
      "epoch": 75.58907841502837,
      "grad_norm": 0.20602937042713165,
      "learning_rate": 0.00046220315549772283,
      "loss": 0.3913,
      "step": 929500
    },
    {
      "epoch": 75.6297395653323,
      "grad_norm": 0.21090421080589294,
      "learning_rate": 0.00046218282368249835,
      "loss": 0.3918,
      "step": 930000
    },
    {
      "epoch": 75.67040071563625,
      "grad_norm": 0.17850062251091003,
      "learning_rate": 0.0004621624918672739,
      "loss": 0.3915,
      "step": 930500
    },
    {
      "epoch": 75.71106186594018,
      "grad_norm": 0.20041987299919128,
      "learning_rate": 0.00046214216005204943,
      "loss": 0.3921,
      "step": 931000
    },
    {
      "epoch": 75.75172301624413,
      "grad_norm": 0.1963111013174057,
      "learning_rate": 0.00046212182823682495,
      "loss": 0.3922,
      "step": 931500
    },
    {
      "epoch": 75.79238416654808,
      "grad_norm": 0.1861133575439453,
      "learning_rate": 0.0004621014964216005,
      "loss": 0.392,
      "step": 932000
    },
    {
      "epoch": 75.83304531685201,
      "grad_norm": 0.20614831149578094,
      "learning_rate": 0.00046208116460637604,
      "loss": 0.3923,
      "step": 932500
    },
    {
      "epoch": 75.87370646715596,
      "grad_norm": 0.18256434798240662,
      "learning_rate": 0.0004620608327911516,
      "loss": 0.3924,
      "step": 933000
    },
    {
      "epoch": 75.9143676174599,
      "grad_norm": 0.19294776022434235,
      "learning_rate": 0.0004620405009759271,
      "loss": 0.3923,
      "step": 933500
    },
    {
      "epoch": 75.95502876776384,
      "grad_norm": 0.17798282206058502,
      "learning_rate": 0.00046202016916070264,
      "loss": 0.3929,
      "step": 934000
    },
    {
      "epoch": 75.99568991806778,
      "grad_norm": 0.2058316171169281,
      "learning_rate": 0.0004619998373454782,
      "loss": 0.3929,
      "step": 934500
    },
    {
      "epoch": 76.03635106837173,
      "grad_norm": 0.19459927082061768,
      "learning_rate": 0.00046197950553025373,
      "loss": 0.3889,
      "step": 935000
    },
    {
      "epoch": 76.07701221867566,
      "grad_norm": 0.18758733570575714,
      "learning_rate": 0.00046195917371502925,
      "loss": 0.3889,
      "step": 935500
    },
    {
      "epoch": 76.11767336897961,
      "grad_norm": 0.18345659971237183,
      "learning_rate": 0.0004619388418998048,
      "loss": 0.389,
      "step": 936000
    },
    {
      "epoch": 76.15833451928356,
      "grad_norm": 0.1958160549402237,
      "learning_rate": 0.00046191851008458033,
      "loss": 0.39,
      "step": 936500
    },
    {
      "epoch": 76.19899566958749,
      "grad_norm": 0.17248791456222534,
      "learning_rate": 0.00046189817826935585,
      "loss": 0.3896,
      "step": 937000
    },
    {
      "epoch": 76.23965681989144,
      "grad_norm": 0.1933048963546753,
      "learning_rate": 0.0004618778464541314,
      "loss": 0.3904,
      "step": 937500
    },
    {
      "epoch": 76.28031797019538,
      "grad_norm": 0.17344851791858673,
      "learning_rate": 0.00046185751463890694,
      "loss": 0.3905,
      "step": 938000
    },
    {
      "epoch": 76.32097912049932,
      "grad_norm": 0.19855321943759918,
      "learning_rate": 0.0004618371828236825,
      "loss": 0.3911,
      "step": 938500
    },
    {
      "epoch": 76.36164027080326,
      "grad_norm": 0.19092851877212524,
      "learning_rate": 0.000461816851008458,
      "loss": 0.3912,
      "step": 939000
    },
    {
      "epoch": 76.4023014211072,
      "grad_norm": 0.1908886730670929,
      "learning_rate": 0.00046179651919323354,
      "loss": 0.3904,
      "step": 939500
    },
    {
      "epoch": 76.44296257141114,
      "grad_norm": 0.17960405349731445,
      "learning_rate": 0.00046177618737800916,
      "loss": 0.3916,
      "step": 940000
    },
    {
      "epoch": 76.48362372171509,
      "grad_norm": 0.20141416788101196,
      "learning_rate": 0.0004617558555627847,
      "loss": 0.3908,
      "step": 940500
    },
    {
      "epoch": 76.52428487201902,
      "grad_norm": 0.18464882671833038,
      "learning_rate": 0.0004617355237475602,
      "loss": 0.3914,
      "step": 941000
    },
    {
      "epoch": 76.56494602232297,
      "grad_norm": 0.21674808859825134,
      "learning_rate": 0.00046171519193233577,
      "loss": 0.3911,
      "step": 941500
    },
    {
      "epoch": 76.60560717262692,
      "grad_norm": 0.19834615290164948,
      "learning_rate": 0.0004616948601171113,
      "loss": 0.3911,
      "step": 942000
    },
    {
      "epoch": 76.64626832293085,
      "grad_norm": 0.2031368613243103,
      "learning_rate": 0.0004616745283018868,
      "loss": 0.3916,
      "step": 942500
    },
    {
      "epoch": 76.6869294732348,
      "grad_norm": 0.19807365536689758,
      "learning_rate": 0.00046165419648666237,
      "loss": 0.392,
      "step": 943000
    },
    {
      "epoch": 76.72759062353875,
      "grad_norm": 0.19411912560462952,
      "learning_rate": 0.0004616338646714379,
      "loss": 0.3918,
      "step": 943500
    },
    {
      "epoch": 76.76825177384268,
      "grad_norm": 0.21107237040996552,
      "learning_rate": 0.00046161353285621346,
      "loss": 0.3919,
      "step": 944000
    },
    {
      "epoch": 76.80891292414663,
      "grad_norm": 0.16696198284626007,
      "learning_rate": 0.000461593201040989,
      "loss": 0.3919,
      "step": 944500
    },
    {
      "epoch": 76.84957407445057,
      "grad_norm": 0.19076204299926758,
      "learning_rate": 0.0004615728692257645,
      "loss": 0.3924,
      "step": 945000
    },
    {
      "epoch": 76.8902352247545,
      "grad_norm": 0.19698892533779144,
      "learning_rate": 0.00046155253741054006,
      "loss": 0.3922,
      "step": 945500
    },
    {
      "epoch": 76.93089637505845,
      "grad_norm": 0.19044670462608337,
      "learning_rate": 0.0004615322055953156,
      "loss": 0.3919,
      "step": 946000
    },
    {
      "epoch": 76.97155752536239,
      "grad_norm": 0.1889352947473526,
      "learning_rate": 0.0004615118737800911,
      "loss": 0.3919,
      "step": 946500
    },
    {
      "epoch": 77.01221867566633,
      "grad_norm": 0.1921306997537613,
      "learning_rate": 0.00046149154196486667,
      "loss": 0.391,
      "step": 947000
    },
    {
      "epoch": 77.05287982597028,
      "grad_norm": 0.19678762555122375,
      "learning_rate": 0.0004614712101496422,
      "loss": 0.3886,
      "step": 947500
    },
    {
      "epoch": 77.09354097627421,
      "grad_norm": 0.1971888393163681,
      "learning_rate": 0.00046145087833441775,
      "loss": 0.3889,
      "step": 948000
    },
    {
      "epoch": 77.13420212657816,
      "grad_norm": 0.20777325332164764,
      "learning_rate": 0.00046143054651919327,
      "loss": 0.3897,
      "step": 948500
    },
    {
      "epoch": 77.1748632768821,
      "grad_norm": 0.20282824337482452,
      "learning_rate": 0.0004614102147039688,
      "loss": 0.3897,
      "step": 949000
    },
    {
      "epoch": 77.21552442718604,
      "grad_norm": 0.2279021143913269,
      "learning_rate": 0.00046138988288874436,
      "loss": 0.39,
      "step": 949500
    },
    {
      "epoch": 77.25618557748999,
      "grad_norm": 0.19370535016059875,
      "learning_rate": 0.00046136955107351987,
      "loss": 0.39,
      "step": 950000
    },
    {
      "epoch": 77.29684672779393,
      "grad_norm": 0.19684693217277527,
      "learning_rate": 0.0004613492192582954,
      "loss": 0.3908,
      "step": 950500
    },
    {
      "epoch": 77.33750787809787,
      "grad_norm": 0.2164705991744995,
      "learning_rate": 0.00046132888744307096,
      "loss": 0.3902,
      "step": 951000
    },
    {
      "epoch": 77.37816902840181,
      "grad_norm": 0.22258101403713226,
      "learning_rate": 0.0004613085556278465,
      "loss": 0.3905,
      "step": 951500
    },
    {
      "epoch": 77.41883017870576,
      "grad_norm": 0.21525134146213531,
      "learning_rate": 0.000461288223812622,
      "loss": 0.3912,
      "step": 952000
    },
    {
      "epoch": 77.4594913290097,
      "grad_norm": 0.20495694875717163,
      "learning_rate": 0.00046126789199739756,
      "loss": 0.3904,
      "step": 952500
    },
    {
      "epoch": 77.50015247931364,
      "grad_norm": 0.18565888702869415,
      "learning_rate": 0.0004612475601821731,
      "loss": 0.3913,
      "step": 953000
    },
    {
      "epoch": 77.54081362961759,
      "grad_norm": 0.17100657522678375,
      "learning_rate": 0.00046122722836694865,
      "loss": 0.3915,
      "step": 953500
    },
    {
      "epoch": 77.58147477992152,
      "grad_norm": 0.19490166008472443,
      "learning_rate": 0.00046120689655172417,
      "loss": 0.3913,
      "step": 954000
    },
    {
      "epoch": 77.62213593022547,
      "grad_norm": 0.20273277163505554,
      "learning_rate": 0.0004611865647364997,
      "loss": 0.3911,
      "step": 954500
    },
    {
      "epoch": 77.6627970805294,
      "grad_norm": 0.20811617374420166,
      "learning_rate": 0.00046116623292127525,
      "loss": 0.3909,
      "step": 955000
    },
    {
      "epoch": 77.70345823083335,
      "grad_norm": 0.1857665777206421,
      "learning_rate": 0.00046114590110605077,
      "loss": 0.3919,
      "step": 955500
    },
    {
      "epoch": 77.7441193811373,
      "grad_norm": 0.18858078122138977,
      "learning_rate": 0.0004611255692908263,
      "loss": 0.3917,
      "step": 956000
    },
    {
      "epoch": 77.78478053144123,
      "grad_norm": 0.200203076004982,
      "learning_rate": 0.00046110523747560186,
      "loss": 0.3917,
      "step": 956500
    },
    {
      "epoch": 77.82544168174518,
      "grad_norm": 0.19893760979175568,
      "learning_rate": 0.0004610849056603774,
      "loss": 0.3921,
      "step": 957000
    },
    {
      "epoch": 77.86610283204912,
      "grad_norm": 0.19681257009506226,
      "learning_rate": 0.0004610645738451529,
      "loss": 0.3918,
      "step": 957500
    },
    {
      "epoch": 77.90676398235306,
      "grad_norm": 0.1942768096923828,
      "learning_rate": 0.00046104424202992846,
      "loss": 0.3916,
      "step": 958000
    },
    {
      "epoch": 77.947425132657,
      "grad_norm": 0.17840279638767242,
      "learning_rate": 0.000461023910214704,
      "loss": 0.3916,
      "step": 958500
    },
    {
      "epoch": 77.98808628296095,
      "grad_norm": 0.2054382562637329,
      "learning_rate": 0.00046100357839947955,
      "loss": 0.3925,
      "step": 959000
    },
    {
      "epoch": 78.02874743326488,
      "grad_norm": 0.18237780034542084,
      "learning_rate": 0.00046098324658425506,
      "loss": 0.3894,
      "step": 959500
    },
    {
      "epoch": 78.06940858356883,
      "grad_norm": 0.16729597747325897,
      "learning_rate": 0.0004609629147690306,
      "loss": 0.3884,
      "step": 960000
    },
    {
      "epoch": 78.11006973387278,
      "grad_norm": 0.17679092288017273,
      "learning_rate": 0.00046094258295380615,
      "loss": 0.3889,
      "step": 960500
    },
    {
      "epoch": 78.15073088417671,
      "grad_norm": 0.18223147094249725,
      "learning_rate": 0.00046092225113858167,
      "loss": 0.3895,
      "step": 961000
    },
    {
      "epoch": 78.19139203448066,
      "grad_norm": 0.1885954886674881,
      "learning_rate": 0.0004609019193233572,
      "loss": 0.3895,
      "step": 961500
    },
    {
      "epoch": 78.2320531847846,
      "grad_norm": 0.19853930175304413,
      "learning_rate": 0.00046088158750813275,
      "loss": 0.3899,
      "step": 962000
    },
    {
      "epoch": 78.27271433508854,
      "grad_norm": 0.21501241624355316,
      "learning_rate": 0.00046086125569290827,
      "loss": 0.3905,
      "step": 962500
    },
    {
      "epoch": 78.31337548539248,
      "grad_norm": 0.2003127485513687,
      "learning_rate": 0.0004608409238776838,
      "loss": 0.3905,
      "step": 963000
    },
    {
      "epoch": 78.35403663569642,
      "grad_norm": 0.191128671169281,
      "learning_rate": 0.00046082059206245936,
      "loss": 0.3905,
      "step": 963500
    },
    {
      "epoch": 78.39469778600036,
      "grad_norm": 0.1812872439622879,
      "learning_rate": 0.0004608002602472349,
      "loss": 0.3907,
      "step": 964000
    },
    {
      "epoch": 78.43535893630431,
      "grad_norm": 0.16413050889968872,
      "learning_rate": 0.00046077992843201044,
      "loss": 0.3903,
      "step": 964500
    },
    {
      "epoch": 78.47602008660824,
      "grad_norm": 0.20153284072875977,
      "learning_rate": 0.00046075959661678596,
      "loss": 0.3911,
      "step": 965000
    },
    {
      "epoch": 78.51668123691219,
      "grad_norm": 0.2190389782190323,
      "learning_rate": 0.0004607392648015615,
      "loss": 0.3913,
      "step": 965500
    },
    {
      "epoch": 78.55734238721614,
      "grad_norm": 0.19005994498729706,
      "learning_rate": 0.00046071893298633705,
      "loss": 0.3904,
      "step": 966000
    },
    {
      "epoch": 78.59800353752007,
      "grad_norm": 0.18477293848991394,
      "learning_rate": 0.00046069860117111256,
      "loss": 0.3912,
      "step": 966500
    },
    {
      "epoch": 78.63866468782402,
      "grad_norm": 0.21185803413391113,
      "learning_rate": 0.0004606782693558881,
      "loss": 0.3909,
      "step": 967000
    },
    {
      "epoch": 78.67932583812797,
      "grad_norm": 0.20108407735824585,
      "learning_rate": 0.00046065793754066365,
      "loss": 0.3912,
      "step": 967500
    },
    {
      "epoch": 78.7199869884319,
      "grad_norm": 0.1939150094985962,
      "learning_rate": 0.00046063760572543917,
      "loss": 0.3913,
      "step": 968000
    },
    {
      "epoch": 78.76064813873585,
      "grad_norm": 0.211168110370636,
      "learning_rate": 0.00046061727391021474,
      "loss": 0.3914,
      "step": 968500
    },
    {
      "epoch": 78.80130928903979,
      "grad_norm": 0.19300806522369385,
      "learning_rate": 0.00046059694209499026,
      "loss": 0.3915,
      "step": 969000
    },
    {
      "epoch": 78.84197043934373,
      "grad_norm": 0.20006173849105835,
      "learning_rate": 0.00046057661027976577,
      "loss": 0.3922,
      "step": 969500
    },
    {
      "epoch": 78.88263158964767,
      "grad_norm": 0.16151203215122223,
      "learning_rate": 0.00046055627846454134,
      "loss": 0.3916,
      "step": 970000
    },
    {
      "epoch": 78.92329273995162,
      "grad_norm": 0.21871545910835266,
      "learning_rate": 0.00046053594664931686,
      "loss": 0.392,
      "step": 970500
    },
    {
      "epoch": 78.96395389025555,
      "grad_norm": 0.19067925214767456,
      "learning_rate": 0.0004605156148340924,
      "loss": 0.3919,
      "step": 971000
    },
    {
      "epoch": 79.0046150405595,
      "grad_norm": 0.18550360202789307,
      "learning_rate": 0.00046049528301886795,
      "loss": 0.3915,
      "step": 971500
    },
    {
      "epoch": 79.04527619086343,
      "grad_norm": 0.2093173861503601,
      "learning_rate": 0.00046047495120364346,
      "loss": 0.388,
      "step": 972000
    },
    {
      "epoch": 79.08593734116738,
      "grad_norm": 0.1866835504770279,
      "learning_rate": 0.000460454619388419,
      "loss": 0.3886,
      "step": 972500
    },
    {
      "epoch": 79.12659849147133,
      "grad_norm": 0.18427512049674988,
      "learning_rate": 0.00046043428757319455,
      "loss": 0.3888,
      "step": 973000
    },
    {
      "epoch": 79.16725964177526,
      "grad_norm": 0.1753382831811905,
      "learning_rate": 0.00046041395575797007,
      "loss": 0.3888,
      "step": 973500
    },
    {
      "epoch": 79.20792079207921,
      "grad_norm": 0.20043380558490753,
      "learning_rate": 0.00046039362394274564,
      "loss": 0.3892,
      "step": 974000
    },
    {
      "epoch": 79.24858194238315,
      "grad_norm": 0.20161758363246918,
      "learning_rate": 0.00046037329212752115,
      "loss": 0.3895,
      "step": 974500
    },
    {
      "epoch": 79.28924309268709,
      "grad_norm": 0.2075931578874588,
      "learning_rate": 0.00046035296031229667,
      "loss": 0.3897,
      "step": 975000
    },
    {
      "epoch": 79.32990424299103,
      "grad_norm": 0.18883371353149414,
      "learning_rate": 0.00046033262849707224,
      "loss": 0.3904,
      "step": 975500
    },
    {
      "epoch": 79.37056539329498,
      "grad_norm": 0.23192419111728668,
      "learning_rate": 0.00046031229668184776,
      "loss": 0.39,
      "step": 976000
    },
    {
      "epoch": 79.41122654359891,
      "grad_norm": 0.22010451555252075,
      "learning_rate": 0.00046029196486662327,
      "loss": 0.3906,
      "step": 976500
    },
    {
      "epoch": 79.45188769390286,
      "grad_norm": 0.1721472442150116,
      "learning_rate": 0.00046027163305139884,
      "loss": 0.3912,
      "step": 977000
    },
    {
      "epoch": 79.49254884420681,
      "grad_norm": 0.22012855112552643,
      "learning_rate": 0.00046025130123617436,
      "loss": 0.3906,
      "step": 977500
    },
    {
      "epoch": 79.53320999451074,
      "grad_norm": 0.20985698699951172,
      "learning_rate": 0.0004602309694209499,
      "loss": 0.3912,
      "step": 978000
    },
    {
      "epoch": 79.57387114481469,
      "grad_norm": 0.18208593130111694,
      "learning_rate": 0.00046021063760572545,
      "loss": 0.3908,
      "step": 978500
    },
    {
      "epoch": 79.61453229511864,
      "grad_norm": 0.19664700329303741,
      "learning_rate": 0.00046019030579050096,
      "loss": 0.3912,
      "step": 979000
    },
    {
      "epoch": 79.65519344542257,
      "grad_norm": 0.21159085631370544,
      "learning_rate": 0.00046016997397527653,
      "loss": 0.391,
      "step": 979500
    },
    {
      "epoch": 79.69585459572652,
      "grad_norm": 0.18345201015472412,
      "learning_rate": 0.00046014964216005205,
      "loss": 0.3913,
      "step": 980000
    },
    {
      "epoch": 79.73651574603045,
      "grad_norm": 0.2180362492799759,
      "learning_rate": 0.00046012931034482757,
      "loss": 0.3912,
      "step": 980500
    },
    {
      "epoch": 79.7771768963344,
      "grad_norm": 0.2105833888053894,
      "learning_rate": 0.00046010897852960314,
      "loss": 0.3914,
      "step": 981000
    },
    {
      "epoch": 79.81783804663834,
      "grad_norm": 0.2072674036026001,
      "learning_rate": 0.00046008864671437865,
      "loss": 0.3916,
      "step": 981500
    },
    {
      "epoch": 79.85849919694228,
      "grad_norm": 0.18704628944396973,
      "learning_rate": 0.00046006831489915417,
      "loss": 0.392,
      "step": 982000
    },
    {
      "epoch": 79.89916034724622,
      "grad_norm": 0.20115961134433746,
      "learning_rate": 0.00046004798308392974,
      "loss": 0.3918,
      "step": 982500
    },
    {
      "epoch": 79.93982149755017,
      "grad_norm": 0.20197078585624695,
      "learning_rate": 0.00046002765126870526,
      "loss": 0.3915,
      "step": 983000
    },
    {
      "epoch": 79.9804826478541,
      "grad_norm": 0.18745888769626617,
      "learning_rate": 0.00046000731945348083,
      "loss": 0.3918,
      "step": 983500
    },
    {
      "epoch": 80.02114379815805,
      "grad_norm": 0.20647870004177094,
      "learning_rate": 0.00045998698763825634,
      "loss": 0.3898,
      "step": 984000
    },
    {
      "epoch": 80.061804948462,
      "grad_norm": 0.2037224918603897,
      "learning_rate": 0.00045996665582303186,
      "loss": 0.3879,
      "step": 984500
    },
    {
      "epoch": 80.10246609876593,
      "grad_norm": 0.19507350027561188,
      "learning_rate": 0.00045994632400780743,
      "loss": 0.3882,
      "step": 985000
    },
    {
      "epoch": 80.14312724906988,
      "grad_norm": 0.19357816874980927,
      "learning_rate": 0.00045992599219258295,
      "loss": 0.3887,
      "step": 985500
    },
    {
      "epoch": 80.18378839937382,
      "grad_norm": 0.20631636679172516,
      "learning_rate": 0.00045990566037735846,
      "loss": 0.3891,
      "step": 986000
    },
    {
      "epoch": 80.22444954967776,
      "grad_norm": 0.19787423312664032,
      "learning_rate": 0.00045988532856213403,
      "loss": 0.3895,
      "step": 986500
    },
    {
      "epoch": 80.2651106999817,
      "grad_norm": 0.19737644493579865,
      "learning_rate": 0.00045986499674690955,
      "loss": 0.3892,
      "step": 987000
    },
    {
      "epoch": 80.30577185028565,
      "grad_norm": 0.2008829116821289,
      "learning_rate": 0.00045984466493168507,
      "loss": 0.3899,
      "step": 987500
    },
    {
      "epoch": 80.34643300058958,
      "grad_norm": 0.19767342507839203,
      "learning_rate": 0.00045982433311646064,
      "loss": 0.39,
      "step": 988000
    },
    {
      "epoch": 80.38709415089353,
      "grad_norm": 0.18473729491233826,
      "learning_rate": 0.00045980400130123615,
      "loss": 0.3903,
      "step": 988500
    },
    {
      "epoch": 80.42775530119746,
      "grad_norm": 0.19924603402614594,
      "learning_rate": 0.0004597836694860117,
      "loss": 0.3904,
      "step": 989000
    },
    {
      "epoch": 80.46841645150141,
      "grad_norm": 0.2333020567893982,
      "learning_rate": 0.00045976333767078724,
      "loss": 0.3902,
      "step": 989500
    },
    {
      "epoch": 80.50907760180536,
      "grad_norm": 0.18895649909973145,
      "learning_rate": 0.00045974300585556276,
      "loss": 0.3901,
      "step": 990000
    },
    {
      "epoch": 80.54973875210929,
      "grad_norm": 0.21688364446163177,
      "learning_rate": 0.00045972267404033833,
      "loss": 0.3906,
      "step": 990500
    },
    {
      "epoch": 80.59039990241324,
      "grad_norm": 0.19966517388820648,
      "learning_rate": 0.00045970234222511385,
      "loss": 0.3911,
      "step": 991000
    },
    {
      "epoch": 80.63106105271719,
      "grad_norm": 0.20770275592803955,
      "learning_rate": 0.00045968201040988936,
      "loss": 0.3911,
      "step": 991500
    },
    {
      "epoch": 80.67172220302112,
      "grad_norm": 0.20398271083831787,
      "learning_rate": 0.00045966167859466493,
      "loss": 0.3909,
      "step": 992000
    },
    {
      "epoch": 80.71238335332507,
      "grad_norm": 0.17910127341747284,
      "learning_rate": 0.00045964134677944045,
      "loss": 0.3909,
      "step": 992500
    },
    {
      "epoch": 80.75304450362901,
      "grad_norm": 0.20373211801052094,
      "learning_rate": 0.00045962101496421597,
      "loss": 0.3915,
      "step": 993000
    },
    {
      "epoch": 80.79370565393295,
      "grad_norm": 0.18623359501361847,
      "learning_rate": 0.00045960068314899154,
      "loss": 0.3914,
      "step": 993500
    },
    {
      "epoch": 80.8343668042369,
      "grad_norm": 0.19703245162963867,
      "learning_rate": 0.00045958035133376705,
      "loss": 0.3912,
      "step": 994000
    },
    {
      "epoch": 80.87502795454084,
      "grad_norm": 0.19124509394168854,
      "learning_rate": 0.0004595600195185426,
      "loss": 0.3917,
      "step": 994500
    },
    {
      "epoch": 80.91568910484477,
      "grad_norm": 0.2043214589357376,
      "learning_rate": 0.00045953968770331814,
      "loss": 0.3921,
      "step": 995000
    },
    {
      "epoch": 80.95635025514872,
      "grad_norm": 0.2010951042175293,
      "learning_rate": 0.00045951935588809366,
      "loss": 0.392,
      "step": 995500
    },
    {
      "epoch": 80.99701140545265,
      "grad_norm": 0.18801183998584747,
      "learning_rate": 0.0004594990240728693,
      "loss": 0.3917,
      "step": 996000
    },
    {
      "epoch": 81.0376725557566,
      "grad_norm": 0.19889949262142181,
      "learning_rate": 0.0004594786922576448,
      "loss": 0.3882,
      "step": 996500
    },
    {
      "epoch": 81.07833370606055,
      "grad_norm": 0.19879868626594543,
      "learning_rate": 0.0004594583604424203,
      "loss": 0.3883,
      "step": 997000
    },
    {
      "epoch": 81.11899485636448,
      "grad_norm": 0.1953217089176178,
      "learning_rate": 0.0004594380286271959,
      "loss": 0.3883,
      "step": 997500
    },
    {
      "epoch": 81.15965600666843,
      "grad_norm": 0.19583475589752197,
      "learning_rate": 0.0004594176968119714,
      "loss": 0.389,
      "step": 998000
    },
    {
      "epoch": 81.20031715697237,
      "grad_norm": 0.16550059616565704,
      "learning_rate": 0.0004593973649967469,
      "loss": 0.3889,
      "step": 998500
    },
    {
      "epoch": 81.24097830727631,
      "grad_norm": 0.22455015778541565,
      "learning_rate": 0.0004593770331815225,
      "loss": 0.3899,
      "step": 999000
    },
    {
      "epoch": 81.28163945758025,
      "grad_norm": 0.20647035539150238,
      "learning_rate": 0.000459356701366298,
      "loss": 0.3892,
      "step": 999500
    },
    {
      "epoch": 81.3223006078842,
      "grad_norm": 0.18955166637897491,
      "learning_rate": 0.0004593363695510736,
      "loss": 0.3897,
      "step": 1000000
    },
    {
      "epoch": 81.36296175818813,
      "grad_norm": 0.18024921417236328,
      "learning_rate": 0.0004593160377358491,
      "loss": 0.3897,
      "step": 1000500
    },
    {
      "epoch": 81.40362290849208,
      "grad_norm": 0.185531347990036,
      "learning_rate": 0.0004592957059206246,
      "loss": 0.3899,
      "step": 1001000
    },
    {
      "epoch": 81.44428405879603,
      "grad_norm": 0.17648868262767792,
      "learning_rate": 0.0004592753741054002,
      "loss": 0.39,
      "step": 1001500
    },
    {
      "epoch": 81.48494520909996,
      "grad_norm": 0.20202863216400146,
      "learning_rate": 0.0004592550422901757,
      "loss": 0.3899,
      "step": 1002000
    },
    {
      "epoch": 81.52560635940391,
      "grad_norm": 0.191402867436409,
      "learning_rate": 0.0004592347104749512,
      "loss": 0.3908,
      "step": 1002500
    },
    {
      "epoch": 81.56626750970786,
      "grad_norm": 0.19493368268013,
      "learning_rate": 0.0004592143786597268,
      "loss": 0.3904,
      "step": 1003000
    },
    {
      "epoch": 81.60692866001179,
      "grad_norm": 0.19881671667099,
      "learning_rate": 0.0004591940468445023,
      "loss": 0.3903,
      "step": 1003500
    },
    {
      "epoch": 81.64758981031574,
      "grad_norm": 0.20517674088478088,
      "learning_rate": 0.00045917371502927787,
      "loss": 0.3908,
      "step": 1004000
    },
    {
      "epoch": 81.68825096061967,
      "grad_norm": 0.205314040184021,
      "learning_rate": 0.0004591533832140534,
      "loss": 0.3911,
      "step": 1004500
    },
    {
      "epoch": 81.72891211092362,
      "grad_norm": 0.1890479177236557,
      "learning_rate": 0.0004591330513988289,
      "loss": 0.3907,
      "step": 1005000
    },
    {
      "epoch": 81.76957326122756,
      "grad_norm": 0.22210703790187836,
      "learning_rate": 0.00045911271958360447,
      "loss": 0.3914,
      "step": 1005500
    },
    {
      "epoch": 81.8102344115315,
      "grad_norm": 0.18922306597232819,
      "learning_rate": 0.00045909238776838,
      "loss": 0.391,
      "step": 1006000
    },
    {
      "epoch": 81.85089556183544,
      "grad_norm": 0.17604190111160278,
      "learning_rate": 0.0004590720559531555,
      "loss": 0.3911,
      "step": 1006500
    },
    {
      "epoch": 81.89155671213939,
      "grad_norm": 0.2089167833328247,
      "learning_rate": 0.0004590517241379311,
      "loss": 0.3921,
      "step": 1007000
    },
    {
      "epoch": 81.93221786244332,
      "grad_norm": 0.17925797402858734,
      "learning_rate": 0.0004590313923227066,
      "loss": 0.3915,
      "step": 1007500
    },
    {
      "epoch": 81.97287901274727,
      "grad_norm": 0.20307482779026031,
      "learning_rate": 0.0004590110605074821,
      "loss": 0.3914,
      "step": 1008000
    },
    {
      "epoch": 82.01354016305122,
      "grad_norm": 0.20772071182727814,
      "learning_rate": 0.0004589907286922577,
      "loss": 0.3903,
      "step": 1008500
    },
    {
      "epoch": 82.05420131335515,
      "grad_norm": 0.19936738908290863,
      "learning_rate": 0.0004589703968770332,
      "loss": 0.3876,
      "step": 1009000
    },
    {
      "epoch": 82.0948624636591,
      "grad_norm": 0.18882372975349426,
      "learning_rate": 0.00045895006506180877,
      "loss": 0.3886,
      "step": 1009500
    },
    {
      "epoch": 82.13552361396304,
      "grad_norm": 0.20275597274303436,
      "learning_rate": 0.0004589297332465843,
      "loss": 0.3885,
      "step": 1010000
    },
    {
      "epoch": 82.17618476426698,
      "grad_norm": 0.18843665719032288,
      "learning_rate": 0.0004589094014313598,
      "loss": 0.3883,
      "step": 1010500
    },
    {
      "epoch": 82.21684591457092,
      "grad_norm": 0.22006715834140778,
      "learning_rate": 0.00045888906961613537,
      "loss": 0.3892,
      "step": 1011000
    },
    {
      "epoch": 82.25750706487487,
      "grad_norm": 0.20006594061851501,
      "learning_rate": 0.0004588687378009109,
      "loss": 0.389,
      "step": 1011500
    },
    {
      "epoch": 82.2981682151788,
      "grad_norm": 0.19216714799404144,
      "learning_rate": 0.0004588484059856864,
      "loss": 0.3895,
      "step": 1012000
    },
    {
      "epoch": 82.33882936548275,
      "grad_norm": 0.19192516803741455,
      "learning_rate": 0.000458828074170462,
      "loss": 0.3899,
      "step": 1012500
    },
    {
      "epoch": 82.37949051578669,
      "grad_norm": 0.21891319751739502,
      "learning_rate": 0.0004588077423552375,
      "loss": 0.3901,
      "step": 1013000
    },
    {
      "epoch": 82.42015166609063,
      "grad_norm": 0.18473847210407257,
      "learning_rate": 0.000458787410540013,
      "loss": 0.3903,
      "step": 1013500
    },
    {
      "epoch": 82.46081281639458,
      "grad_norm": 0.2129293829202652,
      "learning_rate": 0.0004587670787247886,
      "loss": 0.3897,
      "step": 1014000
    },
    {
      "epoch": 82.50147396669851,
      "grad_norm": 0.18362882733345032,
      "learning_rate": 0.0004587467469095641,
      "loss": 0.3901,
      "step": 1014500
    },
    {
      "epoch": 82.54213511700246,
      "grad_norm": 0.19428791105747223,
      "learning_rate": 0.00045872641509433966,
      "loss": 0.3906,
      "step": 1015000
    },
    {
      "epoch": 82.5827962673064,
      "grad_norm": 0.22824443876743317,
      "learning_rate": 0.0004587060832791152,
      "loss": 0.3911,
      "step": 1015500
    },
    {
      "epoch": 82.62345741761034,
      "grad_norm": 0.1947769671678543,
      "learning_rate": 0.0004586857514638907,
      "loss": 0.3905,
      "step": 1016000
    },
    {
      "epoch": 82.66411856791429,
      "grad_norm": 0.19326327741146088,
      "learning_rate": 0.00045866541964866627,
      "loss": 0.3905,
      "step": 1016500
    },
    {
      "epoch": 82.70477971821823,
      "grad_norm": 0.20286725461483002,
      "learning_rate": 0.0004586450878334418,
      "loss": 0.3905,
      "step": 1017000
    },
    {
      "epoch": 82.74544086852217,
      "grad_norm": 0.1894541233778,
      "learning_rate": 0.0004586247560182173,
      "loss": 0.3908,
      "step": 1017500
    },
    {
      "epoch": 82.78610201882611,
      "grad_norm": 0.19324590265750885,
      "learning_rate": 0.00045860442420299287,
      "loss": 0.3905,
      "step": 1018000
    },
    {
      "epoch": 82.82676316913006,
      "grad_norm": 0.21383918821811676,
      "learning_rate": 0.0004585840923877684,
      "loss": 0.3915,
      "step": 1018500
    },
    {
      "epoch": 82.867424319434,
      "grad_norm": 0.18217161297798157,
      "learning_rate": 0.00045856376057254396,
      "loss": 0.3913,
      "step": 1019000
    },
    {
      "epoch": 82.90808546973794,
      "grad_norm": 0.1938677430152893,
      "learning_rate": 0.0004585434287573195,
      "loss": 0.3912,
      "step": 1019500
    },
    {
      "epoch": 82.94874662004189,
      "grad_norm": 0.2176482379436493,
      "learning_rate": 0.000458523096942095,
      "loss": 0.3912,
      "step": 1020000
    },
    {
      "epoch": 82.98940777034582,
      "grad_norm": 0.19420774281024933,
      "learning_rate": 0.00045850276512687056,
      "loss": 0.3916,
      "step": 1020500
    },
    {
      "epoch": 83.03006892064977,
      "grad_norm": 0.174057275056839,
      "learning_rate": 0.0004584824333116461,
      "loss": 0.3885,
      "step": 1021000
    },
    {
      "epoch": 83.0707300709537,
      "grad_norm": 0.2206350415945053,
      "learning_rate": 0.0004584621014964216,
      "loss": 0.3877,
      "step": 1021500
    },
    {
      "epoch": 83.11139122125765,
      "grad_norm": 0.2091519683599472,
      "learning_rate": 0.00045844176968119716,
      "loss": 0.3883,
      "step": 1022000
    },
    {
      "epoch": 83.1520523715616,
      "grad_norm": 0.20200985670089722,
      "learning_rate": 0.0004584214378659727,
      "loss": 0.3881,
      "step": 1022500
    },
    {
      "epoch": 83.19271352186553,
      "grad_norm": 0.20036466419696808,
      "learning_rate": 0.0004584011060507482,
      "loss": 0.3888,
      "step": 1023000
    },
    {
      "epoch": 83.23337467216948,
      "grad_norm": 0.2020096778869629,
      "learning_rate": 0.00045838077423552377,
      "loss": 0.389,
      "step": 1023500
    },
    {
      "epoch": 83.27403582247342,
      "grad_norm": 0.18272940814495087,
      "learning_rate": 0.0004583604424202993,
      "loss": 0.3898,
      "step": 1024000
    },
    {
      "epoch": 83.31469697277736,
      "grad_norm": 0.209825798869133,
      "learning_rate": 0.00045834011060507486,
      "loss": 0.3892,
      "step": 1024500
    },
    {
      "epoch": 83.3553581230813,
      "grad_norm": 0.18934769928455353,
      "learning_rate": 0.00045831977878985037,
      "loss": 0.3894,
      "step": 1025000
    },
    {
      "epoch": 83.39601927338525,
      "grad_norm": 0.20629267394542694,
      "learning_rate": 0.0004582994469746259,
      "loss": 0.3893,
      "step": 1025500
    },
    {
      "epoch": 83.43668042368918,
      "grad_norm": 0.19637112319469452,
      "learning_rate": 0.00045827911515940146,
      "loss": 0.3898,
      "step": 1026000
    },
    {
      "epoch": 83.47734157399313,
      "grad_norm": 0.2078200727701187,
      "learning_rate": 0.000458258783344177,
      "loss": 0.39,
      "step": 1026500
    },
    {
      "epoch": 83.51800272429708,
      "grad_norm": 0.19502468407154083,
      "learning_rate": 0.0004582384515289525,
      "loss": 0.3904,
      "step": 1027000
    },
    {
      "epoch": 83.55866387460101,
      "grad_norm": 0.20771224796772003,
      "learning_rate": 0.00045821811971372806,
      "loss": 0.39,
      "step": 1027500
    },
    {
      "epoch": 83.59932502490496,
      "grad_norm": 0.19757473468780518,
      "learning_rate": 0.0004581977878985036,
      "loss": 0.3905,
      "step": 1028000
    },
    {
      "epoch": 83.63998617520889,
      "grad_norm": 0.1930444836616516,
      "learning_rate": 0.0004581774560832791,
      "loss": 0.39,
      "step": 1028500
    },
    {
      "epoch": 83.68064732551284,
      "grad_norm": 0.20731347799301147,
      "learning_rate": 0.00045815712426805467,
      "loss": 0.3905,
      "step": 1029000
    },
    {
      "epoch": 83.72130847581678,
      "grad_norm": 0.21966928243637085,
      "learning_rate": 0.0004581367924528302,
      "loss": 0.3906,
      "step": 1029500
    },
    {
      "epoch": 83.76196962612072,
      "grad_norm": 0.16855907440185547,
      "learning_rate": 0.00045811646063760575,
      "loss": 0.3911,
      "step": 1030000
    },
    {
      "epoch": 83.80263077642466,
      "grad_norm": 0.22294895350933075,
      "learning_rate": 0.00045809612882238127,
      "loss": 0.3908,
      "step": 1030500
    },
    {
      "epoch": 83.84329192672861,
      "grad_norm": 0.22963717579841614,
      "learning_rate": 0.0004580757970071568,
      "loss": 0.3907,
      "step": 1031000
    },
    {
      "epoch": 83.88395307703254,
      "grad_norm": 0.20334789156913757,
      "learning_rate": 0.00045805546519193236,
      "loss": 0.3907,
      "step": 1031500
    },
    {
      "epoch": 83.92461422733649,
      "grad_norm": 0.23243378102779388,
      "learning_rate": 0.0004580351333767079,
      "loss": 0.3909,
      "step": 1032000
    },
    {
      "epoch": 83.96527537764044,
      "grad_norm": 0.23316840827465057,
      "learning_rate": 0.0004580148015614834,
      "loss": 0.3912,
      "step": 1032500
    },
    {
      "epoch": 84.00593652794437,
      "grad_norm": 0.2171206921339035,
      "learning_rate": 0.00045799446974625896,
      "loss": 0.3908,
      "step": 1033000
    },
    {
      "epoch": 84.04659767824832,
      "grad_norm": 0.22218719124794006,
      "learning_rate": 0.0004579741379310345,
      "loss": 0.3875,
      "step": 1033500
    },
    {
      "epoch": 84.08725882855227,
      "grad_norm": 0.2056897133588791,
      "learning_rate": 0.00045795380611581005,
      "loss": 0.3882,
      "step": 1034000
    },
    {
      "epoch": 84.1279199788562,
      "grad_norm": 0.24293366074562073,
      "learning_rate": 0.00045793347430058556,
      "loss": 0.388,
      "step": 1034500
    },
    {
      "epoch": 84.16858112916015,
      "grad_norm": 0.1969453990459442,
      "learning_rate": 0.0004579131424853611,
      "loss": 0.388,
      "step": 1035000
    },
    {
      "epoch": 84.20924227946409,
      "grad_norm": 0.1936178058385849,
      "learning_rate": 0.00045789281067013665,
      "loss": 0.3884,
      "step": 1035500
    },
    {
      "epoch": 84.24990342976803,
      "grad_norm": 0.21168489754199982,
      "learning_rate": 0.00045787247885491217,
      "loss": 0.3896,
      "step": 1036000
    },
    {
      "epoch": 84.29056458007197,
      "grad_norm": 0.20591826736927032,
      "learning_rate": 0.0004578521470396877,
      "loss": 0.389,
      "step": 1036500
    },
    {
      "epoch": 84.3312257303759,
      "grad_norm": 0.18046751618385315,
      "learning_rate": 0.00045783181522446325,
      "loss": 0.3893,
      "step": 1037000
    },
    {
      "epoch": 84.37188688067985,
      "grad_norm": 0.2308882176876068,
      "learning_rate": 0.00045781148340923877,
      "loss": 0.3891,
      "step": 1037500
    },
    {
      "epoch": 84.4125480309838,
      "grad_norm": 0.22859486937522888,
      "learning_rate": 0.0004577911515940143,
      "loss": 0.3896,
      "step": 1038000
    },
    {
      "epoch": 84.45320918128773,
      "grad_norm": 0.19495783746242523,
      "learning_rate": 0.00045777081977878986,
      "loss": 0.3895,
      "step": 1038500
    },
    {
      "epoch": 84.49387033159168,
      "grad_norm": 0.18478527665138245,
      "learning_rate": 0.0004577504879635654,
      "loss": 0.3894,
      "step": 1039000
    },
    {
      "epoch": 84.53453148189563,
      "grad_norm": 0.1913750171661377,
      "learning_rate": 0.00045773015614834094,
      "loss": 0.3901,
      "step": 1039500
    },
    {
      "epoch": 84.57519263219956,
      "grad_norm": 0.20239755511283875,
      "learning_rate": 0.00045770982433311646,
      "loss": 0.3902,
      "step": 1040000
    },
    {
      "epoch": 84.6158537825035,
      "grad_norm": 0.20037078857421875,
      "learning_rate": 0.000457689492517892,
      "loss": 0.3906,
      "step": 1040500
    },
    {
      "epoch": 84.65651493280745,
      "grad_norm": 0.2051226645708084,
      "learning_rate": 0.00045766916070266755,
      "loss": 0.3907,
      "step": 1041000
    },
    {
      "epoch": 84.69717608311139,
      "grad_norm": 0.20311309397220612,
      "learning_rate": 0.00045764882888744306,
      "loss": 0.3908,
      "step": 1041500
    },
    {
      "epoch": 84.73783723341533,
      "grad_norm": 0.19449497759342194,
      "learning_rate": 0.0004576284970722186,
      "loss": 0.3906,
      "step": 1042000
    },
    {
      "epoch": 84.77849838371928,
      "grad_norm": 0.23391176760196686,
      "learning_rate": 0.00045760816525699415,
      "loss": 0.3907,
      "step": 1042500
    },
    {
      "epoch": 84.81915953402321,
      "grad_norm": 0.2036307007074356,
      "learning_rate": 0.00045758783344176967,
      "loss": 0.3906,
      "step": 1043000
    },
    {
      "epoch": 84.85982068432716,
      "grad_norm": 0.19860953092575073,
      "learning_rate": 0.0004575675016265452,
      "loss": 0.391,
      "step": 1043500
    },
    {
      "epoch": 84.90048183463111,
      "grad_norm": 0.2103516012430191,
      "learning_rate": 0.00045754716981132076,
      "loss": 0.3908,
      "step": 1044000
    },
    {
      "epoch": 84.94114298493504,
      "grad_norm": 0.23231393098831177,
      "learning_rate": 0.00045752683799609627,
      "loss": 0.3913,
      "step": 1044500
    },
    {
      "epoch": 84.98180413523899,
      "grad_norm": 0.2130698263645172,
      "learning_rate": 0.00045750650618087184,
      "loss": 0.3906,
      "step": 1045000
    },
    {
      "epoch": 85.02246528554292,
      "grad_norm": 0.2349518984556198,
      "learning_rate": 0.00045748617436564736,
      "loss": 0.3887,
      "step": 1045500
    },
    {
      "epoch": 85.06312643584687,
      "grad_norm": 0.1851228028535843,
      "learning_rate": 0.0004574658425504229,
      "loss": 0.3872,
      "step": 1046000
    },
    {
      "epoch": 85.10378758615082,
      "grad_norm": 0.2001693993806839,
      "learning_rate": 0.00045744551073519845,
      "loss": 0.3877,
      "step": 1046500
    },
    {
      "epoch": 85.14444873645475,
      "grad_norm": 0.21528559923171997,
      "learning_rate": 0.00045742517891997396,
      "loss": 0.388,
      "step": 1047000
    },
    {
      "epoch": 85.1851098867587,
      "grad_norm": 0.20450909435749054,
      "learning_rate": 0.0004574048471047495,
      "loss": 0.3887,
      "step": 1047500
    },
    {
      "epoch": 85.22577103706264,
      "grad_norm": 0.2192928045988083,
      "learning_rate": 0.00045738451528952505,
      "loss": 0.389,
      "step": 1048000
    },
    {
      "epoch": 85.26643218736658,
      "grad_norm": 0.18458004295825958,
      "learning_rate": 0.00045736418347430057,
      "loss": 0.3886,
      "step": 1048500
    },
    {
      "epoch": 85.30709333767052,
      "grad_norm": 0.2114214450120926,
      "learning_rate": 0.0004573438516590761,
      "loss": 0.3897,
      "step": 1049000
    },
    {
      "epoch": 85.34775448797447,
      "grad_norm": 0.19714710116386414,
      "learning_rate": 0.00045732351984385165,
      "loss": 0.3893,
      "step": 1049500
    },
    {
      "epoch": 85.3884156382784,
      "grad_norm": 0.18761783838272095,
      "learning_rate": 0.00045730318802862717,
      "loss": 0.3894,
      "step": 1050000
    },
    {
      "epoch": 85.42907678858235,
      "grad_norm": 0.20725108683109283,
      "learning_rate": 0.00045728285621340274,
      "loss": 0.3892,
      "step": 1050500
    },
    {
      "epoch": 85.4697379388863,
      "grad_norm": 0.214020237326622,
      "learning_rate": 0.00045726252439817826,
      "loss": 0.3897,
      "step": 1051000
    },
    {
      "epoch": 85.51039908919023,
      "grad_norm": 0.18489468097686768,
      "learning_rate": 0.00045724219258295377,
      "loss": 0.3893,
      "step": 1051500
    },
    {
      "epoch": 85.55106023949418,
      "grad_norm": 0.19850903749465942,
      "learning_rate": 0.00045722186076772934,
      "loss": 0.39,
      "step": 1052000
    },
    {
      "epoch": 85.59172138979812,
      "grad_norm": 0.20065660774707794,
      "learning_rate": 0.00045720152895250486,
      "loss": 0.3897,
      "step": 1052500
    },
    {
      "epoch": 85.63238254010206,
      "grad_norm": 0.20768345892429352,
      "learning_rate": 0.0004571811971372804,
      "loss": 0.3901,
      "step": 1053000
    },
    {
      "epoch": 85.673043690406,
      "grad_norm": 0.2015393227338791,
      "learning_rate": 0.000457160865322056,
      "loss": 0.3902,
      "step": 1053500
    },
    {
      "epoch": 85.71370484070994,
      "grad_norm": 0.19833050668239594,
      "learning_rate": 0.0004571405335068315,
      "loss": 0.3907,
      "step": 1054000
    },
    {
      "epoch": 85.75436599101388,
      "grad_norm": 0.19124735891819,
      "learning_rate": 0.0004571202016916071,
      "loss": 0.3907,
      "step": 1054500
    },
    {
      "epoch": 85.79502714131783,
      "grad_norm": 0.1846432387828827,
      "learning_rate": 0.0004570998698763826,
      "loss": 0.3904,
      "step": 1055000
    },
    {
      "epoch": 85.83568829162176,
      "grad_norm": 0.21623533964157104,
      "learning_rate": 0.0004570795380611581,
      "loss": 0.3908,
      "step": 1055500
    },
    {
      "epoch": 85.87634944192571,
      "grad_norm": 0.2113954722881317,
      "learning_rate": 0.0004570592062459337,
      "loss": 0.3905,
      "step": 1056000
    },
    {
      "epoch": 85.91701059222966,
      "grad_norm": 0.1904599815607071,
      "learning_rate": 0.0004570388744307092,
      "loss": 0.3908,
      "step": 1056500
    },
    {
      "epoch": 85.95767174253359,
      "grad_norm": 0.18452301621437073,
      "learning_rate": 0.0004570185426154847,
      "loss": 0.3911,
      "step": 1057000
    },
    {
      "epoch": 85.99833289283754,
      "grad_norm": 0.21385014057159424,
      "learning_rate": 0.0004569982108002603,
      "loss": 0.391,
      "step": 1057500
    },
    {
      "epoch": 86.03899404314149,
      "grad_norm": 0.21248719096183777,
      "learning_rate": 0.0004569778789850358,
      "loss": 0.3872,
      "step": 1058000
    },
    {
      "epoch": 86.07965519344542,
      "grad_norm": 0.218710258603096,
      "learning_rate": 0.00045695754716981133,
      "loss": 0.3878,
      "step": 1058500
    },
    {
      "epoch": 86.12031634374937,
      "grad_norm": 0.1968526542186737,
      "learning_rate": 0.0004569372153545869,
      "loss": 0.3877,
      "step": 1059000
    },
    {
      "epoch": 86.16097749405331,
      "grad_norm": 0.20205943286418915,
      "learning_rate": 0.0004569168835393624,
      "loss": 0.388,
      "step": 1059500
    },
    {
      "epoch": 86.20163864435725,
      "grad_norm": 0.2224961519241333,
      "learning_rate": 0.000456896551724138,
      "loss": 0.3884,
      "step": 1060000
    },
    {
      "epoch": 86.24229979466119,
      "grad_norm": 0.1798703819513321,
      "learning_rate": 0.0004568762199089135,
      "loss": 0.3886,
      "step": 1060500
    },
    {
      "epoch": 86.28296094496514,
      "grad_norm": 0.1872689425945282,
      "learning_rate": 0.000456855888093689,
      "loss": 0.3889,
      "step": 1061000
    },
    {
      "epoch": 86.32362209526907,
      "grad_norm": 0.18548111617565155,
      "learning_rate": 0.0004568355562784646,
      "loss": 0.3887,
      "step": 1061500
    },
    {
      "epoch": 86.36428324557302,
      "grad_norm": 0.2247982919216156,
      "learning_rate": 0.0004568152244632401,
      "loss": 0.3893,
      "step": 1062000
    },
    {
      "epoch": 86.40494439587695,
      "grad_norm": 0.20716597139835358,
      "learning_rate": 0.0004567948926480156,
      "loss": 0.3895,
      "step": 1062500
    },
    {
      "epoch": 86.4456055461809,
      "grad_norm": 0.2160443514585495,
      "learning_rate": 0.0004567745608327912,
      "loss": 0.3888,
      "step": 1063000
    },
    {
      "epoch": 86.48626669648485,
      "grad_norm": 0.19488367438316345,
      "learning_rate": 0.0004567542290175667,
      "loss": 0.3897,
      "step": 1063500
    },
    {
      "epoch": 86.52692784678878,
      "grad_norm": 0.19700077176094055,
      "learning_rate": 0.0004567338972023422,
      "loss": 0.3899,
      "step": 1064000
    },
    {
      "epoch": 86.56758899709273,
      "grad_norm": 0.1958979070186615,
      "learning_rate": 0.0004567135653871178,
      "loss": 0.3901,
      "step": 1064500
    },
    {
      "epoch": 86.60825014739667,
      "grad_norm": 0.21018940210342407,
      "learning_rate": 0.0004566932335718933,
      "loss": 0.3898,
      "step": 1065000
    },
    {
      "epoch": 86.64891129770061,
      "grad_norm": 0.21924488246440887,
      "learning_rate": 0.0004566729017566689,
      "loss": 0.3902,
      "step": 1065500
    },
    {
      "epoch": 86.68957244800455,
      "grad_norm": 0.18654777109622955,
      "learning_rate": 0.0004566525699414444,
      "loss": 0.3901,
      "step": 1066000
    },
    {
      "epoch": 86.7302335983085,
      "grad_norm": 0.20216839015483856,
      "learning_rate": 0.0004566322381262199,
      "loss": 0.3904,
      "step": 1066500
    },
    {
      "epoch": 86.77089474861243,
      "grad_norm": 0.18917013704776764,
      "learning_rate": 0.0004566119063109955,
      "loss": 0.3905,
      "step": 1067000
    },
    {
      "epoch": 86.81155589891638,
      "grad_norm": 0.19535109400749207,
      "learning_rate": 0.000456591574495771,
      "loss": 0.3907,
      "step": 1067500
    },
    {
      "epoch": 86.85221704922033,
      "grad_norm": 0.18646858632564545,
      "learning_rate": 0.0004565712426805465,
      "loss": 0.3902,
      "step": 1068000
    },
    {
      "epoch": 86.89287819952426,
      "grad_norm": 0.1938890665769577,
      "learning_rate": 0.0004565509108653221,
      "loss": 0.3907,
      "step": 1068500
    },
    {
      "epoch": 86.93353934982821,
      "grad_norm": 0.2020072638988495,
      "learning_rate": 0.0004565305790500976,
      "loss": 0.3904,
      "step": 1069000
    },
    {
      "epoch": 86.97420050013216,
      "grad_norm": 0.2035122662782669,
      "learning_rate": 0.0004565102472348732,
      "loss": 0.3912,
      "step": 1069500
    },
    {
      "epoch": 87.01486165043609,
      "grad_norm": 0.20108617842197418,
      "learning_rate": 0.0004564899154196487,
      "loss": 0.3891,
      "step": 1070000
    },
    {
      "epoch": 87.05552280074004,
      "grad_norm": 0.20668615400791168,
      "learning_rate": 0.0004564695836044242,
      "loss": 0.3873,
      "step": 1070500
    },
    {
      "epoch": 87.09618395104397,
      "grad_norm": 0.20360307395458221,
      "learning_rate": 0.0004564492517891998,
      "loss": 0.3874,
      "step": 1071000
    },
    {
      "epoch": 87.13684510134792,
      "grad_norm": 0.2166324257850647,
      "learning_rate": 0.0004564289199739753,
      "loss": 0.3877,
      "step": 1071500
    },
    {
      "epoch": 87.17750625165186,
      "grad_norm": 0.2112375646829605,
      "learning_rate": 0.0004564085881587508,
      "loss": 0.3885,
      "step": 1072000
    },
    {
      "epoch": 87.2181674019558,
      "grad_norm": 0.21023380756378174,
      "learning_rate": 0.0004563882563435264,
      "loss": 0.3877,
      "step": 1072500
    },
    {
      "epoch": 87.25882855225974,
      "grad_norm": 0.1937503218650818,
      "learning_rate": 0.0004563679245283019,
      "loss": 0.3886,
      "step": 1073000
    },
    {
      "epoch": 87.29948970256369,
      "grad_norm": 0.20816271007061005,
      "learning_rate": 0.0004563475927130774,
      "loss": 0.3886,
      "step": 1073500
    },
    {
      "epoch": 87.34015085286762,
      "grad_norm": 0.2111869752407074,
      "learning_rate": 0.000456327260897853,
      "loss": 0.3883,
      "step": 1074000
    },
    {
      "epoch": 87.38081200317157,
      "grad_norm": 0.19887970387935638,
      "learning_rate": 0.0004563069290826285,
      "loss": 0.3888,
      "step": 1074500
    },
    {
      "epoch": 87.42147315347552,
      "grad_norm": 0.20600086450576782,
      "learning_rate": 0.0004562865972674041,
      "loss": 0.3893,
      "step": 1075000
    },
    {
      "epoch": 87.46213430377945,
      "grad_norm": 0.18007084727287292,
      "learning_rate": 0.0004562662654521796,
      "loss": 0.3899,
      "step": 1075500
    },
    {
      "epoch": 87.5027954540834,
      "grad_norm": 0.1838570535182953,
      "learning_rate": 0.0004562459336369551,
      "loss": 0.3896,
      "step": 1076000
    },
    {
      "epoch": 87.54345660438734,
      "grad_norm": 0.18568728864192963,
      "learning_rate": 0.0004562256018217307,
      "loss": 0.3896,
      "step": 1076500
    },
    {
      "epoch": 87.58411775469128,
      "grad_norm": 0.20559677481651306,
      "learning_rate": 0.0004562052700065062,
      "loss": 0.39,
      "step": 1077000
    },
    {
      "epoch": 87.62477890499522,
      "grad_norm": 0.19074484705924988,
      "learning_rate": 0.0004561849381912817,
      "loss": 0.3895,
      "step": 1077500
    },
    {
      "epoch": 87.66544005529916,
      "grad_norm": 0.19566795229911804,
      "learning_rate": 0.0004561646063760573,
      "loss": 0.3901,
      "step": 1078000
    },
    {
      "epoch": 87.7061012056031,
      "grad_norm": 0.21349558234214783,
      "learning_rate": 0.0004561442745608328,
      "loss": 0.3899,
      "step": 1078500
    },
    {
      "epoch": 87.74676235590705,
      "grad_norm": 0.18781495094299316,
      "learning_rate": 0.0004561239427456083,
      "loss": 0.39,
      "step": 1079000
    },
    {
      "epoch": 87.78742350621098,
      "grad_norm": 0.21754337847232819,
      "learning_rate": 0.0004561036109303839,
      "loss": 0.3904,
      "step": 1079500
    },
    {
      "epoch": 87.82808465651493,
      "grad_norm": 0.21287119388580322,
      "learning_rate": 0.0004560832791151594,
      "loss": 0.3899,
      "step": 1080000
    },
    {
      "epoch": 87.86874580681888,
      "grad_norm": 0.20989827811717987,
      "learning_rate": 0.00045606294729993497,
      "loss": 0.3904,
      "step": 1080500
    },
    {
      "epoch": 87.90940695712281,
      "grad_norm": 0.22718845307826996,
      "learning_rate": 0.0004560426154847105,
      "loss": 0.3905,
      "step": 1081000
    },
    {
      "epoch": 87.95006810742676,
      "grad_norm": 0.2113368660211563,
      "learning_rate": 0.000456022283669486,
      "loss": 0.3911,
      "step": 1081500
    },
    {
      "epoch": 87.9907292577307,
      "grad_norm": 0.23147106170654297,
      "learning_rate": 0.0004560019518542616,
      "loss": 0.3908,
      "step": 1082000
    },
    {
      "epoch": 88.03139040803464,
      "grad_norm": 0.20947310328483582,
      "learning_rate": 0.0004559816200390371,
      "loss": 0.3877,
      "step": 1082500
    },
    {
      "epoch": 88.07205155833859,
      "grad_norm": 0.21049058437347412,
      "learning_rate": 0.0004559612882238126,
      "loss": 0.3867,
      "step": 1083000
    },
    {
      "epoch": 88.11271270864253,
      "grad_norm": 0.20938128232955933,
      "learning_rate": 0.0004559409564085882,
      "loss": 0.3872,
      "step": 1083500
    },
    {
      "epoch": 88.15337385894647,
      "grad_norm": 0.23178322613239288,
      "learning_rate": 0.0004559206245933637,
      "loss": 0.3875,
      "step": 1084000
    },
    {
      "epoch": 88.19403500925041,
      "grad_norm": 0.19896790385246277,
      "learning_rate": 0.00045590029277813927,
      "loss": 0.3878,
      "step": 1084500
    },
    {
      "epoch": 88.23469615955436,
      "grad_norm": 0.235967218875885,
      "learning_rate": 0.0004558799609629148,
      "loss": 0.3888,
      "step": 1085000
    },
    {
      "epoch": 88.2753573098583,
      "grad_norm": 0.21002277731895447,
      "learning_rate": 0.0004558596291476903,
      "loss": 0.3882,
      "step": 1085500
    },
    {
      "epoch": 88.31601846016224,
      "grad_norm": 0.20448483526706696,
      "learning_rate": 0.00045583929733246587,
      "loss": 0.3886,
      "step": 1086000
    },
    {
      "epoch": 88.35667961046617,
      "grad_norm": 0.2070748507976532,
      "learning_rate": 0.0004558189655172414,
      "loss": 0.389,
      "step": 1086500
    },
    {
      "epoch": 88.39734076077012,
      "grad_norm": 0.2144104242324829,
      "learning_rate": 0.0004557986337020169,
      "loss": 0.3888,
      "step": 1087000
    },
    {
      "epoch": 88.43800191107407,
      "grad_norm": 0.19302374124526978,
      "learning_rate": 0.0004557783018867925,
      "loss": 0.3889,
      "step": 1087500
    },
    {
      "epoch": 88.478663061378,
      "grad_norm": 0.2159658819437027,
      "learning_rate": 0.000455757970071568,
      "loss": 0.3898,
      "step": 1088000
    },
    {
      "epoch": 88.51932421168195,
      "grad_norm": 0.20972135663032532,
      "learning_rate": 0.0004557376382563435,
      "loss": 0.3898,
      "step": 1088500
    },
    {
      "epoch": 88.5599853619859,
      "grad_norm": 0.212836354970932,
      "learning_rate": 0.0004557173064411191,
      "loss": 0.3896,
      "step": 1089000
    },
    {
      "epoch": 88.60064651228983,
      "grad_norm": 0.19972987473011017,
      "learning_rate": 0.0004556969746258946,
      "loss": 0.39,
      "step": 1089500
    },
    {
      "epoch": 88.64130766259377,
      "grad_norm": 0.191362664103508,
      "learning_rate": 0.00045567664281067016,
      "loss": 0.3896,
      "step": 1090000
    },
    {
      "epoch": 88.68196881289772,
      "grad_norm": 0.1952841728925705,
      "learning_rate": 0.0004556563109954457,
      "loss": 0.3893,
      "step": 1090500
    },
    {
      "epoch": 88.72262996320165,
      "grad_norm": 0.23530593514442444,
      "learning_rate": 0.0004556359791802212,
      "loss": 0.3902,
      "step": 1091000
    },
    {
      "epoch": 88.7632911135056,
      "grad_norm": 0.20013728737831116,
      "learning_rate": 0.00045561564736499677,
      "loss": 0.3906,
      "step": 1091500
    },
    {
      "epoch": 88.80395226380955,
      "grad_norm": 0.2329656481742859,
      "learning_rate": 0.0004555953155497723,
      "loss": 0.39,
      "step": 1092000
    },
    {
      "epoch": 88.84461341411348,
      "grad_norm": 0.1866869479417801,
      "learning_rate": 0.0004555749837345478,
      "loss": 0.3902,
      "step": 1092500
    },
    {
      "epoch": 88.88527456441743,
      "grad_norm": 0.19236227869987488,
      "learning_rate": 0.00045555465191932337,
      "loss": 0.3905,
      "step": 1093000
    },
    {
      "epoch": 88.92593571472138,
      "grad_norm": 0.18207770586013794,
      "learning_rate": 0.0004555343201040989,
      "loss": 0.3904,
      "step": 1093500
    },
    {
      "epoch": 88.96659686502531,
      "grad_norm": 0.18975241482257843,
      "learning_rate": 0.0004555139882888744,
      "loss": 0.3903,
      "step": 1094000
    },
    {
      "epoch": 89.00725801532926,
      "grad_norm": 0.22567160427570343,
      "learning_rate": 0.00045549365647365,
      "loss": 0.3899,
      "step": 1094500
    },
    {
      "epoch": 89.04791916563319,
      "grad_norm": 0.19626422226428986,
      "learning_rate": 0.0004554733246584255,
      "loss": 0.3869,
      "step": 1095000
    },
    {
      "epoch": 89.08858031593714,
      "grad_norm": 0.20869794487953186,
      "learning_rate": 0.00045545299284320106,
      "loss": 0.3871,
      "step": 1095500
    },
    {
      "epoch": 89.12924146624108,
      "grad_norm": 0.23731078207492828,
      "learning_rate": 0.0004554326610279766,
      "loss": 0.3872,
      "step": 1096000
    },
    {
      "epoch": 89.16990261654502,
      "grad_norm": 0.20159567892551422,
      "learning_rate": 0.0004554123292127521,
      "loss": 0.3876,
      "step": 1096500
    },
    {
      "epoch": 89.21056376684896,
      "grad_norm": 0.20527704060077667,
      "learning_rate": 0.00045539199739752766,
      "loss": 0.3876,
      "step": 1097000
    },
    {
      "epoch": 89.25122491715291,
      "grad_norm": 0.190118208527565,
      "learning_rate": 0.0004553716655823032,
      "loss": 0.3883,
      "step": 1097500
    },
    {
      "epoch": 89.29188606745684,
      "grad_norm": 0.20828430354595184,
      "learning_rate": 0.0004553513337670787,
      "loss": 0.3885,
      "step": 1098000
    },
    {
      "epoch": 89.33254721776079,
      "grad_norm": 0.20912158489227295,
      "learning_rate": 0.00045533100195185427,
      "loss": 0.3891,
      "step": 1098500
    },
    {
      "epoch": 89.37320836806474,
      "grad_norm": 0.21420679986476898,
      "learning_rate": 0.0004553106701366298,
      "loss": 0.3889,
      "step": 1099000
    },
    {
      "epoch": 89.41386951836867,
      "grad_norm": 0.22233796119689941,
      "learning_rate": 0.0004552903383214053,
      "loss": 0.3887,
      "step": 1099500
    },
    {
      "epoch": 89.45453066867262,
      "grad_norm": 0.22592216730117798,
      "learning_rate": 0.00045527000650618087,
      "loss": 0.389,
      "step": 1100000
    },
    {
      "epoch": 89.49519181897656,
      "grad_norm": 0.225801482796669,
      "learning_rate": 0.0004552496746909564,
      "loss": 0.3889,
      "step": 1100500
    },
    {
      "epoch": 89.5358529692805,
      "grad_norm": 0.2019796222448349,
      "learning_rate": 0.00045522934287573196,
      "loss": 0.3894,
      "step": 1101000
    },
    {
      "epoch": 89.57651411958444,
      "grad_norm": 0.2161550521850586,
      "learning_rate": 0.0004552090110605075,
      "loss": 0.3894,
      "step": 1101500
    },
    {
      "epoch": 89.61717526988839,
      "grad_norm": 0.19582776725292206,
      "learning_rate": 0.000455188679245283,
      "loss": 0.3897,
      "step": 1102000
    },
    {
      "epoch": 89.65783642019233,
      "grad_norm": 0.21714584529399872,
      "learning_rate": 0.00045516834743005856,
      "loss": 0.3893,
      "step": 1102500
    },
    {
      "epoch": 89.69849757049627,
      "grad_norm": 0.2035658210515976,
      "learning_rate": 0.0004551480156148341,
      "loss": 0.3899,
      "step": 1103000
    },
    {
      "epoch": 89.7391587208002,
      "grad_norm": 0.22050310671329498,
      "learning_rate": 0.0004551276837996096,
      "loss": 0.39,
      "step": 1103500
    },
    {
      "epoch": 89.77981987110415,
      "grad_norm": 0.18586531281471252,
      "learning_rate": 0.00045510735198438517,
      "loss": 0.3894,
      "step": 1104000
    },
    {
      "epoch": 89.8204810214081,
      "grad_norm": 0.20510700345039368,
      "learning_rate": 0.0004550870201691607,
      "loss": 0.3901,
      "step": 1104500
    },
    {
      "epoch": 89.86114217171203,
      "grad_norm": 0.18298524618148804,
      "learning_rate": 0.00045506668835393625,
      "loss": 0.3899,
      "step": 1105000
    },
    {
      "epoch": 89.90180332201598,
      "grad_norm": 0.20049285888671875,
      "learning_rate": 0.00045504635653871177,
      "loss": 0.3898,
      "step": 1105500
    },
    {
      "epoch": 89.94246447231993,
      "grad_norm": 0.2062157690525055,
      "learning_rate": 0.0004550260247234873,
      "loss": 0.3902,
      "step": 1106000
    },
    {
      "epoch": 89.98312562262386,
      "grad_norm": 0.210388645529747,
      "learning_rate": 0.00045500569290826286,
      "loss": 0.3903,
      "step": 1106500
    },
    {
      "epoch": 90.0237867729278,
      "grad_norm": 0.21224290132522583,
      "learning_rate": 0.00045498536109303837,
      "loss": 0.3878,
      "step": 1107000
    },
    {
      "epoch": 90.06444792323175,
      "grad_norm": 0.22148743271827698,
      "learning_rate": 0.0004549650292778139,
      "loss": 0.3866,
      "step": 1107500
    },
    {
      "epoch": 90.10510907353569,
      "grad_norm": 0.21628551185131073,
      "learning_rate": 0.00045494469746258946,
      "loss": 0.3866,
      "step": 1108000
    },
    {
      "epoch": 90.14577022383963,
      "grad_norm": 0.19807587563991547,
      "learning_rate": 0.000454924365647365,
      "loss": 0.3875,
      "step": 1108500
    },
    {
      "epoch": 90.18643137414358,
      "grad_norm": 0.20135606825351715,
      "learning_rate": 0.0004549040338321405,
      "loss": 0.3874,
      "step": 1109000
    },
    {
      "epoch": 90.22709252444751,
      "grad_norm": 0.20754112303256989,
      "learning_rate": 0.00045488370201691606,
      "loss": 0.3877,
      "step": 1109500
    },
    {
      "epoch": 90.26775367475146,
      "grad_norm": 0.19943173229694366,
      "learning_rate": 0.0004548633702016916,
      "loss": 0.3883,
      "step": 1110000
    },
    {
      "epoch": 90.3084148250554,
      "grad_norm": 0.19586235284805298,
      "learning_rate": 0.0004548430383864672,
      "loss": 0.3881,
      "step": 1110500
    },
    {
      "epoch": 90.34907597535934,
      "grad_norm": 0.2368125319480896,
      "learning_rate": 0.0004548227065712427,
      "loss": 0.3885,
      "step": 1111000
    },
    {
      "epoch": 90.38973712566329,
      "grad_norm": 0.20493917167186737,
      "learning_rate": 0.00045480237475601824,
      "loss": 0.3889,
      "step": 1111500
    },
    {
      "epoch": 90.43039827596722,
      "grad_norm": 0.22116553783416748,
      "learning_rate": 0.0004547820429407938,
      "loss": 0.3888,
      "step": 1112000
    },
    {
      "epoch": 90.47105942627117,
      "grad_norm": 0.19228386878967285,
      "learning_rate": 0.0004547617111255693,
      "loss": 0.3887,
      "step": 1112500
    },
    {
      "epoch": 90.51172057657512,
      "grad_norm": 0.235969677567482,
      "learning_rate": 0.00045474137931034484,
      "loss": 0.3891,
      "step": 1113000
    },
    {
      "epoch": 90.55238172687905,
      "grad_norm": 0.21181608736515045,
      "learning_rate": 0.0004547210474951204,
      "loss": 0.3893,
      "step": 1113500
    },
    {
      "epoch": 90.593042877183,
      "grad_norm": 0.20464369654655457,
      "learning_rate": 0.00045470071567989593,
      "loss": 0.3891,
      "step": 1114000
    },
    {
      "epoch": 90.63370402748694,
      "grad_norm": 0.20881426334381104,
      "learning_rate": 0.00045468038386467144,
      "loss": 0.3893,
      "step": 1114500
    },
    {
      "epoch": 90.67436517779088,
      "grad_norm": 0.22316358983516693,
      "learning_rate": 0.000454660052049447,
      "loss": 0.3892,
      "step": 1115000
    },
    {
      "epoch": 90.71502632809482,
      "grad_norm": 0.23250438272953033,
      "learning_rate": 0.00045463972023422253,
      "loss": 0.39,
      "step": 1115500
    },
    {
      "epoch": 90.75568747839877,
      "grad_norm": 0.18985529243946075,
      "learning_rate": 0.0004546193884189981,
      "loss": 0.3898,
      "step": 1116000
    },
    {
      "epoch": 90.7963486287027,
      "grad_norm": 0.18439054489135742,
      "learning_rate": 0.0004545990566037736,
      "loss": 0.3902,
      "step": 1116500
    },
    {
      "epoch": 90.83700977900665,
      "grad_norm": 0.20328304171562195,
      "learning_rate": 0.00045457872478854913,
      "loss": 0.3898,
      "step": 1117000
    },
    {
      "epoch": 90.8776709293106,
      "grad_norm": 0.21404513716697693,
      "learning_rate": 0.0004545583929733247,
      "loss": 0.3902,
      "step": 1117500
    },
    {
      "epoch": 90.91833207961453,
      "grad_norm": 0.21649117767810822,
      "learning_rate": 0.0004545380611581002,
      "loss": 0.3903,
      "step": 1118000
    },
    {
      "epoch": 90.95899322991848,
      "grad_norm": 0.22153708338737488,
      "learning_rate": 0.00045451772934287574,
      "loss": 0.3904,
      "step": 1118500
    },
    {
      "epoch": 90.99965438022241,
      "grad_norm": 0.20465117692947388,
      "learning_rate": 0.0004544973975276513,
      "loss": 0.3904,
      "step": 1119000
    },
    {
      "epoch": 91.04031553052636,
      "grad_norm": 0.18795782327651978,
      "learning_rate": 0.0004544770657124268,
      "loss": 0.3859,
      "step": 1119500
    },
    {
      "epoch": 91.0809766808303,
      "grad_norm": 0.22350311279296875,
      "learning_rate": 0.0004544567338972024,
      "loss": 0.3869,
      "step": 1120000
    },
    {
      "epoch": 91.12163783113424,
      "grad_norm": 0.23156338930130005,
      "learning_rate": 0.0004544364020819779,
      "loss": 0.387,
      "step": 1120500
    },
    {
      "epoch": 91.16229898143818,
      "grad_norm": 0.18292376399040222,
      "learning_rate": 0.00045441607026675343,
      "loss": 0.3874,
      "step": 1121000
    },
    {
      "epoch": 91.20296013174213,
      "grad_norm": 0.2313946783542633,
      "learning_rate": 0.000454395738451529,
      "loss": 0.3877,
      "step": 1121500
    },
    {
      "epoch": 91.24362128204606,
      "grad_norm": 0.18347914516925812,
      "learning_rate": 0.0004543754066363045,
      "loss": 0.3881,
      "step": 1122000
    },
    {
      "epoch": 91.28428243235001,
      "grad_norm": 0.20518963038921356,
      "learning_rate": 0.00045435507482108003,
      "loss": 0.3878,
      "step": 1122500
    },
    {
      "epoch": 91.32494358265396,
      "grad_norm": 0.20672915875911713,
      "learning_rate": 0.0004543347430058556,
      "loss": 0.3883,
      "step": 1123000
    },
    {
      "epoch": 91.36560473295789,
      "grad_norm": 0.20023438334465027,
      "learning_rate": 0.0004543144111906311,
      "loss": 0.3886,
      "step": 1123500
    },
    {
      "epoch": 91.40626588326184,
      "grad_norm": 0.21033382415771484,
      "learning_rate": 0.00045429407937540664,
      "loss": 0.3883,
      "step": 1124000
    },
    {
      "epoch": 91.44692703356579,
      "grad_norm": 0.2184869796037674,
      "learning_rate": 0.0004542737475601822,
      "loss": 0.3891,
      "step": 1124500
    },
    {
      "epoch": 91.48758818386972,
      "grad_norm": 0.20220640301704407,
      "learning_rate": 0.0004542534157449577,
      "loss": 0.3887,
      "step": 1125000
    },
    {
      "epoch": 91.52824933417367,
      "grad_norm": 0.18911203742027283,
      "learning_rate": 0.0004542330839297333,
      "loss": 0.3889,
      "step": 1125500
    },
    {
      "epoch": 91.56891048447761,
      "grad_norm": 0.1970866173505783,
      "learning_rate": 0.0004542127521145088,
      "loss": 0.3888,
      "step": 1126000
    },
    {
      "epoch": 91.60957163478155,
      "grad_norm": 0.2718905210494995,
      "learning_rate": 0.0004541924202992843,
      "loss": 0.3893,
      "step": 1126500
    },
    {
      "epoch": 91.65023278508549,
      "grad_norm": 0.20589469373226166,
      "learning_rate": 0.0004541720884840599,
      "loss": 0.3892,
      "step": 1127000
    },
    {
      "epoch": 91.69089393538943,
      "grad_norm": 0.21917322278022766,
      "learning_rate": 0.0004541517566688354,
      "loss": 0.3895,
      "step": 1127500
    },
    {
      "epoch": 91.73155508569337,
      "grad_norm": 0.21161776781082153,
      "learning_rate": 0.00045413142485361093,
      "loss": 0.3897,
      "step": 1128000
    },
    {
      "epoch": 91.77221623599732,
      "grad_norm": 0.2139706164598465,
      "learning_rate": 0.0004541110930383865,
      "loss": 0.3896,
      "step": 1128500
    },
    {
      "epoch": 91.81287738630125,
      "grad_norm": 0.21546663343906403,
      "learning_rate": 0.000454090761223162,
      "loss": 0.3893,
      "step": 1129000
    },
    {
      "epoch": 91.8535385366052,
      "grad_norm": 0.2091130167245865,
      "learning_rate": 0.00045407042940793753,
      "loss": 0.3898,
      "step": 1129500
    },
    {
      "epoch": 91.89419968690915,
      "grad_norm": 0.19487199187278748,
      "learning_rate": 0.0004540500975927131,
      "loss": 0.3902,
      "step": 1130000
    },
    {
      "epoch": 91.93486083721308,
      "grad_norm": 0.17767280340194702,
      "learning_rate": 0.0004540297657774886,
      "loss": 0.3899,
      "step": 1130500
    },
    {
      "epoch": 91.97552198751703,
      "grad_norm": 0.22572061419487,
      "learning_rate": 0.0004540094339622642,
      "loss": 0.3906,
      "step": 1131000
    },
    {
      "epoch": 92.01618313782097,
      "grad_norm": 0.2153782695531845,
      "learning_rate": 0.0004539891021470397,
      "loss": 0.3881,
      "step": 1131500
    },
    {
      "epoch": 92.0568442881249,
      "grad_norm": 0.19567438960075378,
      "learning_rate": 0.0004539687703318152,
      "loss": 0.3868,
      "step": 1132000
    },
    {
      "epoch": 92.09750543842885,
      "grad_norm": 0.21819931268692017,
      "learning_rate": 0.0004539484385165908,
      "loss": 0.3871,
      "step": 1132500
    },
    {
      "epoch": 92.1381665887328,
      "grad_norm": 0.19668246805667877,
      "learning_rate": 0.0004539281067013663,
      "loss": 0.3869,
      "step": 1133000
    },
    {
      "epoch": 92.17882773903673,
      "grad_norm": 0.19526611268520355,
      "learning_rate": 0.00045390777488614183,
      "loss": 0.387,
      "step": 1133500
    },
    {
      "epoch": 92.21948888934068,
      "grad_norm": 0.1944480985403061,
      "learning_rate": 0.0004538874430709174,
      "loss": 0.3875,
      "step": 1134000
    },
    {
      "epoch": 92.26015003964463,
      "grad_norm": 0.21733298897743225,
      "learning_rate": 0.0004538671112556929,
      "loss": 0.3881,
      "step": 1134500
    },
    {
      "epoch": 92.30081118994856,
      "grad_norm": 0.19819746911525726,
      "learning_rate": 0.00045384677944046843,
      "loss": 0.3883,
      "step": 1135000
    },
    {
      "epoch": 92.34147234025251,
      "grad_norm": 0.22067709267139435,
      "learning_rate": 0.000453826447625244,
      "loss": 0.3883,
      "step": 1135500
    },
    {
      "epoch": 92.38213349055644,
      "grad_norm": 0.21967460215091705,
      "learning_rate": 0.0004538061158100195,
      "loss": 0.3882,
      "step": 1136000
    },
    {
      "epoch": 92.42279464086039,
      "grad_norm": 0.21631819009780884,
      "learning_rate": 0.0004537857839947951,
      "loss": 0.3883,
      "step": 1136500
    },
    {
      "epoch": 92.46345579116434,
      "grad_norm": 0.21963579952716827,
      "learning_rate": 0.0004537654521795706,
      "loss": 0.3885,
      "step": 1137000
    },
    {
      "epoch": 92.50411694146827,
      "grad_norm": 0.1993287056684494,
      "learning_rate": 0.0004537451203643461,
      "loss": 0.3889,
      "step": 1137500
    },
    {
      "epoch": 92.54477809177222,
      "grad_norm": 0.21236178278923035,
      "learning_rate": 0.0004537247885491217,
      "loss": 0.3891,
      "step": 1138000
    },
    {
      "epoch": 92.58543924207616,
      "grad_norm": 0.17993266880512238,
      "learning_rate": 0.0004537044567338972,
      "loss": 0.3894,
      "step": 1138500
    },
    {
      "epoch": 92.6261003923801,
      "grad_norm": 0.2180674821138382,
      "learning_rate": 0.0004536841249186727,
      "loss": 0.3891,
      "step": 1139000
    },
    {
      "epoch": 92.66676154268404,
      "grad_norm": 0.19483089447021484,
      "learning_rate": 0.0004536637931034483,
      "loss": 0.3894,
      "step": 1139500
    },
    {
      "epoch": 92.70742269298799,
      "grad_norm": 0.21189890801906586,
      "learning_rate": 0.0004536434612882238,
      "loss": 0.389,
      "step": 1140000
    },
    {
      "epoch": 92.74808384329192,
      "grad_norm": 0.20198743045330048,
      "learning_rate": 0.0004536231294729994,
      "loss": 0.3893,
      "step": 1140500
    },
    {
      "epoch": 92.78874499359587,
      "grad_norm": 0.2684365510940552,
      "learning_rate": 0.0004536027976577749,
      "loss": 0.3896,
      "step": 1141000
    },
    {
      "epoch": 92.82940614389982,
      "grad_norm": 0.18099190294742584,
      "learning_rate": 0.0004535824658425504,
      "loss": 0.3898,
      "step": 1141500
    },
    {
      "epoch": 92.87006729420375,
      "grad_norm": 0.2059362828731537,
      "learning_rate": 0.000453562134027326,
      "loss": 0.3893,
      "step": 1142000
    },
    {
      "epoch": 92.9107284445077,
      "grad_norm": 0.22032149136066437,
      "learning_rate": 0.0004535418022121015,
      "loss": 0.39,
      "step": 1142500
    },
    {
      "epoch": 92.95138959481164,
      "grad_norm": 0.22141288220882416,
      "learning_rate": 0.000453521470396877,
      "loss": 0.3898,
      "step": 1143000
    },
    {
      "epoch": 92.99205074511558,
      "grad_norm": 0.19999101758003235,
      "learning_rate": 0.0004535011385816526,
      "loss": 0.3904,
      "step": 1143500
    },
    {
      "epoch": 93.03271189541952,
      "grad_norm": 0.1952519565820694,
      "learning_rate": 0.0004534808067664281,
      "loss": 0.3867,
      "step": 1144000
    },
    {
      "epoch": 93.07337304572346,
      "grad_norm": 0.21287041902542114,
      "learning_rate": 0.0004534604749512036,
      "loss": 0.3862,
      "step": 1144500
    },
    {
      "epoch": 93.1140341960274,
      "grad_norm": 0.210208460688591,
      "learning_rate": 0.0004534401431359792,
      "loss": 0.3871,
      "step": 1145000
    },
    {
      "epoch": 93.15469534633135,
      "grad_norm": 0.20910115540027618,
      "learning_rate": 0.0004534198113207547,
      "loss": 0.3868,
      "step": 1145500
    },
    {
      "epoch": 93.19535649663528,
      "grad_norm": 0.2150738686323166,
      "learning_rate": 0.0004533994795055303,
      "loss": 0.3873,
      "step": 1146000
    },
    {
      "epoch": 93.23601764693923,
      "grad_norm": 0.22741903364658356,
      "learning_rate": 0.0004533791476903058,
      "loss": 0.3875,
      "step": 1146500
    },
    {
      "epoch": 93.27667879724318,
      "grad_norm": 0.21121670305728912,
      "learning_rate": 0.0004533588158750813,
      "loss": 0.3876,
      "step": 1147000
    },
    {
      "epoch": 93.31733994754711,
      "grad_norm": 0.2001211941242218,
      "learning_rate": 0.0004533384840598569,
      "loss": 0.388,
      "step": 1147500
    },
    {
      "epoch": 93.35800109785106,
      "grad_norm": 0.22084729373455048,
      "learning_rate": 0.0004533181522446324,
      "loss": 0.388,
      "step": 1148000
    },
    {
      "epoch": 93.398662248155,
      "grad_norm": 0.2193928062915802,
      "learning_rate": 0.0004532978204294079,
      "loss": 0.3884,
      "step": 1148500
    },
    {
      "epoch": 93.43932339845894,
      "grad_norm": 0.21703919768333435,
      "learning_rate": 0.0004532774886141835,
      "loss": 0.3884,
      "step": 1149000
    },
    {
      "epoch": 93.47998454876289,
      "grad_norm": 0.2018854320049286,
      "learning_rate": 0.000453257156798959,
      "loss": 0.3885,
      "step": 1149500
    },
    {
      "epoch": 93.52064569906683,
      "grad_norm": 0.20479723811149597,
      "learning_rate": 0.0004532368249837345,
      "loss": 0.3887,
      "step": 1150000
    },
    {
      "epoch": 93.56130684937077,
      "grad_norm": 0.1865132600069046,
      "learning_rate": 0.0004532164931685101,
      "loss": 0.3886,
      "step": 1150500
    },
    {
      "epoch": 93.60196799967471,
      "grad_norm": 0.19279392063617706,
      "learning_rate": 0.0004531961613532856,
      "loss": 0.3893,
      "step": 1151000
    },
    {
      "epoch": 93.64262914997866,
      "grad_norm": 0.2301200032234192,
      "learning_rate": 0.0004531758295380612,
      "loss": 0.389,
      "step": 1151500
    },
    {
      "epoch": 93.6832903002826,
      "grad_norm": 0.1933797001838684,
      "learning_rate": 0.0004531554977228367,
      "loss": 0.3889,
      "step": 1152000
    },
    {
      "epoch": 93.72395145058654,
      "grad_norm": 0.19534870982170105,
      "learning_rate": 0.0004531351659076122,
      "loss": 0.3889,
      "step": 1152500
    },
    {
      "epoch": 93.76461260089047,
      "grad_norm": 0.2033138871192932,
      "learning_rate": 0.0004531148340923878,
      "loss": 0.3895,
      "step": 1153000
    },
    {
      "epoch": 93.80527375119442,
      "grad_norm": 0.1906893402338028,
      "learning_rate": 0.0004530945022771633,
      "loss": 0.3898,
      "step": 1153500
    },
    {
      "epoch": 93.84593490149837,
      "grad_norm": 0.19470098614692688,
      "learning_rate": 0.0004530741704619388,
      "loss": 0.3895,
      "step": 1154000
    },
    {
      "epoch": 93.8865960518023,
      "grad_norm": 0.2125370055437088,
      "learning_rate": 0.0004530538386467144,
      "loss": 0.3898,
      "step": 1154500
    },
    {
      "epoch": 93.92725720210625,
      "grad_norm": 0.2260052114725113,
      "learning_rate": 0.0004530335068314899,
      "loss": 0.39,
      "step": 1155000
    },
    {
      "epoch": 93.9679183524102,
      "grad_norm": 0.19531823694705963,
      "learning_rate": 0.00045301317501626547,
      "loss": 0.3899,
      "step": 1155500
    },
    {
      "epoch": 94.00857950271413,
      "grad_norm": 0.21693438291549683,
      "learning_rate": 0.000452992843201041,
      "loss": 0.3887,
      "step": 1156000
    },
    {
      "epoch": 94.04924065301807,
      "grad_norm": 0.21083517372608185,
      "learning_rate": 0.0004529725113858165,
      "loss": 0.3855,
      "step": 1156500
    },
    {
      "epoch": 94.08990180332202,
      "grad_norm": 0.22973349690437317,
      "learning_rate": 0.0004529521795705921,
      "loss": 0.3873,
      "step": 1157000
    },
    {
      "epoch": 94.13056295362595,
      "grad_norm": 0.20170503854751587,
      "learning_rate": 0.0004529318477553676,
      "loss": 0.3865,
      "step": 1157500
    },
    {
      "epoch": 94.1712241039299,
      "grad_norm": 0.24757634103298187,
      "learning_rate": 0.0004529115159401431,
      "loss": 0.387,
      "step": 1158000
    },
    {
      "epoch": 94.21188525423385,
      "grad_norm": 0.19983379542827606,
      "learning_rate": 0.0004528911841249187,
      "loss": 0.3875,
      "step": 1158500
    },
    {
      "epoch": 94.25254640453778,
      "grad_norm": 0.22724300622940063,
      "learning_rate": 0.0004528708523096942,
      "loss": 0.3876,
      "step": 1159000
    },
    {
      "epoch": 94.29320755484173,
      "grad_norm": 0.19263862073421478,
      "learning_rate": 0.0004528505204944697,
      "loss": 0.3878,
      "step": 1159500
    },
    {
      "epoch": 94.33386870514566,
      "grad_norm": 0.19494009017944336,
      "learning_rate": 0.0004528301886792453,
      "loss": 0.3881,
      "step": 1160000
    },
    {
      "epoch": 94.37452985544961,
      "grad_norm": 0.22879023849964142,
      "learning_rate": 0.0004528098568640208,
      "loss": 0.3878,
      "step": 1160500
    },
    {
      "epoch": 94.41519100575356,
      "grad_norm": 0.19734451174736023,
      "learning_rate": 0.00045278952504879637,
      "loss": 0.388,
      "step": 1161000
    },
    {
      "epoch": 94.45585215605749,
      "grad_norm": 0.19991113245487213,
      "learning_rate": 0.0004527691932335719,
      "loss": 0.3882,
      "step": 1161500
    },
    {
      "epoch": 94.49651330636144,
      "grad_norm": 0.195773646235466,
      "learning_rate": 0.0004527488614183474,
      "loss": 0.3883,
      "step": 1162000
    },
    {
      "epoch": 94.53717445666538,
      "grad_norm": 0.22089651226997375,
      "learning_rate": 0.00045272852960312297,
      "loss": 0.3887,
      "step": 1162500
    },
    {
      "epoch": 94.57783560696932,
      "grad_norm": 0.21044978499412537,
      "learning_rate": 0.0004527081977878985,
      "loss": 0.3889,
      "step": 1163000
    },
    {
      "epoch": 94.61849675727326,
      "grad_norm": 0.20657691359519958,
      "learning_rate": 0.000452687865972674,
      "loss": 0.3886,
      "step": 1163500
    },
    {
      "epoch": 94.65915790757721,
      "grad_norm": 0.18524350225925446,
      "learning_rate": 0.0004526675341574496,
      "loss": 0.3886,
      "step": 1164000
    },
    {
      "epoch": 94.69981905788114,
      "grad_norm": 0.23795151710510254,
      "learning_rate": 0.0004526472023422251,
      "loss": 0.3893,
      "step": 1164500
    },
    {
      "epoch": 94.74048020818509,
      "grad_norm": 0.2186475247144699,
      "learning_rate": 0.0004526268705270006,
      "loss": 0.389,
      "step": 1165000
    },
    {
      "epoch": 94.78114135848904,
      "grad_norm": 0.20203857123851776,
      "learning_rate": 0.0004526065387117762,
      "loss": 0.3895,
      "step": 1165500
    },
    {
      "epoch": 94.82180250879297,
      "grad_norm": 0.23536317050457,
      "learning_rate": 0.0004525862068965517,
      "loss": 0.3892,
      "step": 1166000
    },
    {
      "epoch": 94.86246365909692,
      "grad_norm": 0.19129040837287903,
      "learning_rate": 0.0004525658750813273,
      "loss": 0.3898,
      "step": 1166500
    },
    {
      "epoch": 94.90312480940086,
      "grad_norm": 0.19302035868167877,
      "learning_rate": 0.00045254554326610284,
      "loss": 0.3892,
      "step": 1167000
    },
    {
      "epoch": 94.9437859597048,
      "grad_norm": 0.21909646689891815,
      "learning_rate": 0.00045252521145087835,
      "loss": 0.39,
      "step": 1167500
    },
    {
      "epoch": 94.98444711000874,
      "grad_norm": 0.21773432195186615,
      "learning_rate": 0.0004525048796356539,
      "loss": 0.3897,
      "step": 1168000
    },
    {
      "epoch": 95.02510826031268,
      "grad_norm": 0.212196484208107,
      "learning_rate": 0.00045248454782042944,
      "loss": 0.3869,
      "step": 1168500
    },
    {
      "epoch": 95.06576941061662,
      "grad_norm": 0.21042707562446594,
      "learning_rate": 0.00045246421600520496,
      "loss": 0.3862,
      "step": 1169000
    },
    {
      "epoch": 95.10643056092057,
      "grad_norm": 0.2289777249097824,
      "learning_rate": 0.00045244388418998053,
      "loss": 0.3866,
      "step": 1169500
    },
    {
      "epoch": 95.1470917112245,
      "grad_norm": 0.20792502164840698,
      "learning_rate": 0.00045242355237475604,
      "loss": 0.3864,
      "step": 1170000
    },
    {
      "epoch": 95.18775286152845,
      "grad_norm": 0.21458232402801514,
      "learning_rate": 0.0004524032205595316,
      "loss": 0.3873,
      "step": 1170500
    },
    {
      "epoch": 95.2284140118324,
      "grad_norm": 0.20249035954475403,
      "learning_rate": 0.00045238288874430713,
      "loss": 0.3869,
      "step": 1171000
    },
    {
      "epoch": 95.26907516213633,
      "grad_norm": 0.2193405032157898,
      "learning_rate": 0.00045236255692908265,
      "loss": 0.3877,
      "step": 1171500
    },
    {
      "epoch": 95.30973631244028,
      "grad_norm": 0.20648139715194702,
      "learning_rate": 0.0004523422251138582,
      "loss": 0.3875,
      "step": 1172000
    },
    {
      "epoch": 95.35039746274423,
      "grad_norm": 0.2123468816280365,
      "learning_rate": 0.00045232189329863373,
      "loss": 0.3879,
      "step": 1172500
    },
    {
      "epoch": 95.39105861304816,
      "grad_norm": 0.20738279819488525,
      "learning_rate": 0.00045230156148340925,
      "loss": 0.3882,
      "step": 1173000
    },
    {
      "epoch": 95.4317197633521,
      "grad_norm": 0.22407007217407227,
      "learning_rate": 0.0004522812296681848,
      "loss": 0.3877,
      "step": 1173500
    },
    {
      "epoch": 95.47238091365605,
      "grad_norm": 0.2026185542345047,
      "learning_rate": 0.00045226089785296034,
      "loss": 0.388,
      "step": 1174000
    },
    {
      "epoch": 95.51304206395999,
      "grad_norm": 0.21224279701709747,
      "learning_rate": 0.00045224056603773585,
      "loss": 0.3882,
      "step": 1174500
    },
    {
      "epoch": 95.55370321426393,
      "grad_norm": 0.20436330139636993,
      "learning_rate": 0.0004522202342225114,
      "loss": 0.389,
      "step": 1175000
    },
    {
      "epoch": 95.59436436456788,
      "grad_norm": 0.18998855352401733,
      "learning_rate": 0.00045219990240728694,
      "loss": 0.3888,
      "step": 1175500
    },
    {
      "epoch": 95.63502551487181,
      "grad_norm": 0.2230731099843979,
      "learning_rate": 0.0004521795705920625,
      "loss": 0.3887,
      "step": 1176000
    },
    {
      "epoch": 95.67568666517576,
      "grad_norm": 0.2190319448709488,
      "learning_rate": 0.00045215923877683803,
      "loss": 0.389,
      "step": 1176500
    },
    {
      "epoch": 95.7163478154797,
      "grad_norm": 0.2117956280708313,
      "learning_rate": 0.00045213890696161355,
      "loss": 0.389,
      "step": 1177000
    },
    {
      "epoch": 95.75700896578364,
      "grad_norm": 0.20307323336601257,
      "learning_rate": 0.0004521185751463891,
      "loss": 0.3891,
      "step": 1177500
    },
    {
      "epoch": 95.79767011608759,
      "grad_norm": 0.2083893120288849,
      "learning_rate": 0.00045209824333116463,
      "loss": 0.3888,
      "step": 1178000
    },
    {
      "epoch": 95.83833126639152,
      "grad_norm": 0.2298412173986435,
      "learning_rate": 0.00045207791151594015,
      "loss": 0.3894,
      "step": 1178500
    },
    {
      "epoch": 95.87899241669547,
      "grad_norm": 0.20840218663215637,
      "learning_rate": 0.0004520575797007157,
      "loss": 0.3896,
      "step": 1179000
    },
    {
      "epoch": 95.91965356699941,
      "grad_norm": 0.2152729481458664,
      "learning_rate": 0.00045203724788549124,
      "loss": 0.3896,
      "step": 1179500
    },
    {
      "epoch": 95.96031471730335,
      "grad_norm": 0.19771361351013184,
      "learning_rate": 0.00045201691607026675,
      "loss": 0.3899,
      "step": 1180000
    },
    {
      "epoch": 96.0009758676073,
      "grad_norm": 0.20659852027893066,
      "learning_rate": 0.0004519965842550423,
      "loss": 0.3897,
      "step": 1180500
    },
    {
      "epoch": 96.04163701791124,
      "grad_norm": 0.2014084756374359,
      "learning_rate": 0.00045197625243981784,
      "loss": 0.3858,
      "step": 1181000
    },
    {
      "epoch": 96.08229816821517,
      "grad_norm": 0.22552312910556793,
      "learning_rate": 0.0004519559206245934,
      "loss": 0.3863,
      "step": 1181500
    },
    {
      "epoch": 96.12295931851912,
      "grad_norm": 0.19453155994415283,
      "learning_rate": 0.0004519355888093689,
      "loss": 0.3869,
      "step": 1182000
    },
    {
      "epoch": 96.16362046882307,
      "grad_norm": 0.23007777333259583,
      "learning_rate": 0.00045191525699414444,
      "loss": 0.3869,
      "step": 1182500
    },
    {
      "epoch": 96.204281619127,
      "grad_norm": 0.21381554007530212,
      "learning_rate": 0.00045189492517892,
      "loss": 0.3872,
      "step": 1183000
    },
    {
      "epoch": 96.24494276943095,
      "grad_norm": 0.20013582706451416,
      "learning_rate": 0.00045187459336369553,
      "loss": 0.3874,
      "step": 1183500
    },
    {
      "epoch": 96.2856039197349,
      "grad_norm": 0.22127996385097504,
      "learning_rate": 0.00045185426154847105,
      "loss": 0.3876,
      "step": 1184000
    },
    {
      "epoch": 96.32626507003883,
      "grad_norm": 0.19789476692676544,
      "learning_rate": 0.0004518339297332466,
      "loss": 0.3881,
      "step": 1184500
    },
    {
      "epoch": 96.36692622034278,
      "grad_norm": 0.19655907154083252,
      "learning_rate": 0.00045181359791802213,
      "loss": 0.3883,
      "step": 1185000
    },
    {
      "epoch": 96.40758737064671,
      "grad_norm": 0.18993937969207764,
      "learning_rate": 0.00045179326610279765,
      "loss": 0.3876,
      "step": 1185500
    },
    {
      "epoch": 96.44824852095066,
      "grad_norm": 0.21504747867584229,
      "learning_rate": 0.0004517729342875732,
      "loss": 0.3876,
      "step": 1186000
    },
    {
      "epoch": 96.4889096712546,
      "grad_norm": 0.2242441326379776,
      "learning_rate": 0.00045175260247234874,
      "loss": 0.3879,
      "step": 1186500
    },
    {
      "epoch": 96.52957082155854,
      "grad_norm": 0.23974038660526276,
      "learning_rate": 0.0004517322706571243,
      "loss": 0.3879,
      "step": 1187000
    },
    {
      "epoch": 96.57023197186248,
      "grad_norm": 0.21534880995750427,
      "learning_rate": 0.0004517119388418998,
      "loss": 0.388,
      "step": 1187500
    },
    {
      "epoch": 96.61089312216643,
      "grad_norm": 0.20412801206111908,
      "learning_rate": 0.00045169160702667534,
      "loss": 0.3884,
      "step": 1188000
    },
    {
      "epoch": 96.65155427247036,
      "grad_norm": 0.19929936528205872,
      "learning_rate": 0.0004516712752114509,
      "loss": 0.3889,
      "step": 1188500
    },
    {
      "epoch": 96.69221542277431,
      "grad_norm": 0.20515821874141693,
      "learning_rate": 0.00045165094339622643,
      "loss": 0.3886,
      "step": 1189000
    },
    {
      "epoch": 96.73287657307826,
      "grad_norm": 0.2014974057674408,
      "learning_rate": 0.00045163061158100194,
      "loss": 0.3889,
      "step": 1189500
    },
    {
      "epoch": 96.77353772338219,
      "grad_norm": 0.22908875346183777,
      "learning_rate": 0.0004516102797657775,
      "loss": 0.3894,
      "step": 1190000
    },
    {
      "epoch": 96.81419887368614,
      "grad_norm": 0.2040553241968155,
      "learning_rate": 0.00045158994795055303,
      "loss": 0.3894,
      "step": 1190500
    },
    {
      "epoch": 96.85486002399008,
      "grad_norm": 0.18459849059581757,
      "learning_rate": 0.0004515696161353286,
      "loss": 0.3892,
      "step": 1191000
    },
    {
      "epoch": 96.89552117429402,
      "grad_norm": 0.22178666293621063,
      "learning_rate": 0.0004515492843201041,
      "loss": 0.389,
      "step": 1191500
    },
    {
      "epoch": 96.93618232459797,
      "grad_norm": 0.20038826763629913,
      "learning_rate": 0.00045152895250487963,
      "loss": 0.3891,
      "step": 1192000
    },
    {
      "epoch": 96.9768434749019,
      "grad_norm": 0.23344406485557556,
      "learning_rate": 0.0004515086206896552,
      "loss": 0.3895,
      "step": 1192500
    },
    {
      "epoch": 97.01750462520585,
      "grad_norm": 0.20037433505058289,
      "learning_rate": 0.0004514882888744307,
      "loss": 0.3876,
      "step": 1193000
    },
    {
      "epoch": 97.05816577550979,
      "grad_norm": 0.19848299026489258,
      "learning_rate": 0.00045146795705920624,
      "loss": 0.3852,
      "step": 1193500
    },
    {
      "epoch": 97.09882692581373,
      "grad_norm": 0.1963479220867157,
      "learning_rate": 0.0004514476252439818,
      "loss": 0.3861,
      "step": 1194000
    },
    {
      "epoch": 97.13948807611767,
      "grad_norm": 0.21546363830566406,
      "learning_rate": 0.0004514272934287573,
      "loss": 0.3865,
      "step": 1194500
    },
    {
      "epoch": 97.18014922642162,
      "grad_norm": 0.21335706114768982,
      "learning_rate": 0.00045140696161353284,
      "loss": 0.3868,
      "step": 1195000
    },
    {
      "epoch": 97.22081037672555,
      "grad_norm": 0.19291354715824127,
      "learning_rate": 0.0004513866297983084,
      "loss": 0.3867,
      "step": 1195500
    },
    {
      "epoch": 97.2614715270295,
      "grad_norm": 0.2035420536994934,
      "learning_rate": 0.00045136629798308393,
      "loss": 0.3874,
      "step": 1196000
    },
    {
      "epoch": 97.30213267733345,
      "grad_norm": 0.22660565376281738,
      "learning_rate": 0.0004513459661678595,
      "loss": 0.387,
      "step": 1196500
    },
    {
      "epoch": 97.34279382763738,
      "grad_norm": 0.23553304374217987,
      "learning_rate": 0.000451325634352635,
      "loss": 0.3873,
      "step": 1197000
    },
    {
      "epoch": 97.38345497794133,
      "grad_norm": 0.22367380559444427,
      "learning_rate": 0.00045130530253741053,
      "loss": 0.3877,
      "step": 1197500
    },
    {
      "epoch": 97.42411612824527,
      "grad_norm": 0.20942850410938263,
      "learning_rate": 0.0004512849707221861,
      "loss": 0.3876,
      "step": 1198000
    },
    {
      "epoch": 97.4647772785492,
      "grad_norm": 0.20751671493053436,
      "learning_rate": 0.0004512646389069616,
      "loss": 0.3883,
      "step": 1198500
    },
    {
      "epoch": 97.50543842885315,
      "grad_norm": 0.1929078996181488,
      "learning_rate": 0.00045124430709173714,
      "loss": 0.388,
      "step": 1199000
    },
    {
      "epoch": 97.5460995791571,
      "grad_norm": 0.2534818947315216,
      "learning_rate": 0.0004512239752765127,
      "loss": 0.3885,
      "step": 1199500
    },
    {
      "epoch": 97.58676072946103,
      "grad_norm": 0.22863873839378357,
      "learning_rate": 0.0004512036434612882,
      "loss": 0.3889,
      "step": 1200000
    },
    {
      "epoch": 97.62742187976498,
      "grad_norm": 0.20164607465267181,
      "learning_rate": 0.00045118331164606374,
      "loss": 0.3885,
      "step": 1200500
    },
    {
      "epoch": 97.66808303006891,
      "grad_norm": 0.2331182062625885,
      "learning_rate": 0.0004511629798308393,
      "loss": 0.3887,
      "step": 1201000
    },
    {
      "epoch": 97.70874418037286,
      "grad_norm": 0.21600277721881866,
      "learning_rate": 0.0004511426480156148,
      "loss": 0.3893,
      "step": 1201500
    },
    {
      "epoch": 97.74940533067681,
      "grad_norm": 0.20725023746490479,
      "learning_rate": 0.0004511223162003904,
      "loss": 0.3889,
      "step": 1202000
    },
    {
      "epoch": 97.79006648098074,
      "grad_norm": 0.18015757203102112,
      "learning_rate": 0.0004511019843851659,
      "loss": 0.3889,
      "step": 1202500
    },
    {
      "epoch": 97.83072763128469,
      "grad_norm": 0.18882954120635986,
      "learning_rate": 0.00045108165256994143,
      "loss": 0.3893,
      "step": 1203000
    },
    {
      "epoch": 97.87138878158864,
      "grad_norm": 0.20114167034626007,
      "learning_rate": 0.000451061320754717,
      "loss": 0.3888,
      "step": 1203500
    },
    {
      "epoch": 97.91204993189257,
      "grad_norm": 0.21714557707309723,
      "learning_rate": 0.0004510409889394925,
      "loss": 0.3898,
      "step": 1204000
    },
    {
      "epoch": 97.95271108219652,
      "grad_norm": 0.20157283544540405,
      "learning_rate": 0.00045102065712426803,
      "loss": 0.3894,
      "step": 1204500
    },
    {
      "epoch": 97.99337223250046,
      "grad_norm": 0.2154029756784439,
      "learning_rate": 0.0004510003253090436,
      "loss": 0.3894,
      "step": 1205000
    },
    {
      "epoch": 98.0340333828044,
      "grad_norm": 0.2136034518480301,
      "learning_rate": 0.0004509799934938191,
      "loss": 0.386,
      "step": 1205500
    },
    {
      "epoch": 98.07469453310834,
      "grad_norm": 0.1825443059206009,
      "learning_rate": 0.0004509596616785947,
      "loss": 0.3857,
      "step": 1206000
    },
    {
      "epoch": 98.11535568341229,
      "grad_norm": 0.22316594421863556,
      "learning_rate": 0.0004509393298633702,
      "loss": 0.3857,
      "step": 1206500
    },
    {
      "epoch": 98.15601683371622,
      "grad_norm": 0.21042780578136444,
      "learning_rate": 0.0004509189980481457,
      "loss": 0.3866,
      "step": 1207000
    },
    {
      "epoch": 98.19667798402017,
      "grad_norm": 0.20444652438163757,
      "learning_rate": 0.0004508986662329213,
      "loss": 0.3865,
      "step": 1207500
    },
    {
      "epoch": 98.23733913432412,
      "grad_norm": 0.22028225660324097,
      "learning_rate": 0.0004508783344176968,
      "loss": 0.3868,
      "step": 1208000
    },
    {
      "epoch": 98.27800028462805,
      "grad_norm": 0.21942122280597687,
      "learning_rate": 0.00045085800260247233,
      "loss": 0.3873,
      "step": 1208500
    },
    {
      "epoch": 98.318661434932,
      "grad_norm": 0.2181394249200821,
      "learning_rate": 0.0004508376707872479,
      "loss": 0.3873,
      "step": 1209000
    },
    {
      "epoch": 98.35932258523593,
      "grad_norm": 0.27207979559898376,
      "learning_rate": 0.0004508173389720234,
      "loss": 0.3877,
      "step": 1209500
    },
    {
      "epoch": 98.39998373553988,
      "grad_norm": 0.22226928174495697,
      "learning_rate": 0.00045079700715679893,
      "loss": 0.3877,
      "step": 1210000
    },
    {
      "epoch": 98.44064488584382,
      "grad_norm": 0.2176445573568344,
      "learning_rate": 0.0004507766753415745,
      "loss": 0.3877,
      "step": 1210500
    },
    {
      "epoch": 98.48130603614776,
      "grad_norm": 0.18367111682891846,
      "learning_rate": 0.00045075634352635,
      "loss": 0.3877,
      "step": 1211000
    },
    {
      "epoch": 98.5219671864517,
      "grad_norm": 0.19296012818813324,
      "learning_rate": 0.0004507360117111256,
      "loss": 0.3881,
      "step": 1211500
    },
    {
      "epoch": 98.56262833675565,
      "grad_norm": 0.19737622141838074,
      "learning_rate": 0.0004507156798959011,
      "loss": 0.3877,
      "step": 1212000
    },
    {
      "epoch": 98.60328948705958,
      "grad_norm": 0.191232830286026,
      "learning_rate": 0.0004506953480806766,
      "loss": 0.3886,
      "step": 1212500
    },
    {
      "epoch": 98.64395063736353,
      "grad_norm": 0.21785256266593933,
      "learning_rate": 0.0004506750162654522,
      "loss": 0.3882,
      "step": 1213000
    },
    {
      "epoch": 98.68461178766748,
      "grad_norm": 0.22139379382133484,
      "learning_rate": 0.0004506546844502277,
      "loss": 0.388,
      "step": 1213500
    },
    {
      "epoch": 98.72527293797141,
      "grad_norm": 0.1936326026916504,
      "learning_rate": 0.0004506343526350032,
      "loss": 0.389,
      "step": 1214000
    },
    {
      "epoch": 98.76593408827536,
      "grad_norm": 0.2318316251039505,
      "learning_rate": 0.0004506140208197788,
      "loss": 0.3893,
      "step": 1214500
    },
    {
      "epoch": 98.8065952385793,
      "grad_norm": 0.20643794536590576,
      "learning_rate": 0.0004505936890045543,
      "loss": 0.3892,
      "step": 1215000
    },
    {
      "epoch": 98.84725638888324,
      "grad_norm": 0.20225512981414795,
      "learning_rate": 0.00045057335718932983,
      "loss": 0.3889,
      "step": 1215500
    },
    {
      "epoch": 98.88791753918719,
      "grad_norm": 0.19981354475021362,
      "learning_rate": 0.0004505530253741054,
      "loss": 0.3894,
      "step": 1216000
    },
    {
      "epoch": 98.92857868949113,
      "grad_norm": 0.20927992463111877,
      "learning_rate": 0.0004505326935588809,
      "loss": 0.3893,
      "step": 1216500
    },
    {
      "epoch": 98.96923983979507,
      "grad_norm": 0.20975229144096375,
      "learning_rate": 0.0004505123617436565,
      "loss": 0.389,
      "step": 1217000
    },
    {
      "epoch": 99.00990099009901,
      "grad_norm": 0.20739194750785828,
      "learning_rate": 0.000450492029928432,
      "loss": 0.3887,
      "step": 1217500
    },
    {
      "epoch": 99.05056214040295,
      "grad_norm": 0.2056507021188736,
      "learning_rate": 0.0004504716981132075,
      "loss": 0.3851,
      "step": 1218000
    },
    {
      "epoch": 99.09122329070689,
      "grad_norm": 0.21798357367515564,
      "learning_rate": 0.0004504513662979831,
      "loss": 0.386,
      "step": 1218500
    },
    {
      "epoch": 99.13188444101084,
      "grad_norm": 0.19884735345840454,
      "learning_rate": 0.0004504310344827586,
      "loss": 0.3865,
      "step": 1219000
    },
    {
      "epoch": 99.17254559131477,
      "grad_norm": 0.22679224610328674,
      "learning_rate": 0.0004504107026675341,
      "loss": 0.3866,
      "step": 1219500
    },
    {
      "epoch": 99.21320674161872,
      "grad_norm": 0.24554532766342163,
      "learning_rate": 0.0004503903708523097,
      "loss": 0.3867,
      "step": 1220000
    },
    {
      "epoch": 99.25386789192267,
      "grad_norm": 0.23995648324489594,
      "learning_rate": 0.0004503700390370852,
      "loss": 0.3871,
      "step": 1220500
    },
    {
      "epoch": 99.2945290422266,
      "grad_norm": 0.22642086446285248,
      "learning_rate": 0.0004503497072218607,
      "loss": 0.3873,
      "step": 1221000
    },
    {
      "epoch": 99.33519019253055,
      "grad_norm": 0.20666684210300446,
      "learning_rate": 0.0004503293754066363,
      "loss": 0.3877,
      "step": 1221500
    },
    {
      "epoch": 99.3758513428345,
      "grad_norm": 0.22683240473270416,
      "learning_rate": 0.0004503090435914118,
      "loss": 0.3871,
      "step": 1222000
    },
    {
      "epoch": 99.41651249313843,
      "grad_norm": 0.20244717597961426,
      "learning_rate": 0.0004502887117761874,
      "loss": 0.3872,
      "step": 1222500
    },
    {
      "epoch": 99.45717364344237,
      "grad_norm": 0.22697947919368744,
      "learning_rate": 0.0004502683799609629,
      "loss": 0.3882,
      "step": 1223000
    },
    {
      "epoch": 99.49783479374632,
      "grad_norm": 0.2226523458957672,
      "learning_rate": 0.0004502480481457384,
      "loss": 0.3879,
      "step": 1223500
    },
    {
      "epoch": 99.53849594405025,
      "grad_norm": 0.19273900985717773,
      "learning_rate": 0.00045022771633051404,
      "loss": 0.3878,
      "step": 1224000
    },
    {
      "epoch": 99.5791570943542,
      "grad_norm": 0.2078300565481186,
      "learning_rate": 0.00045020738451528956,
      "loss": 0.388,
      "step": 1224500
    },
    {
      "epoch": 99.61981824465815,
      "grad_norm": 0.21764418482780457,
      "learning_rate": 0.0004501870527000651,
      "loss": 0.3885,
      "step": 1225000
    },
    {
      "epoch": 99.66047939496208,
      "grad_norm": 0.19249074161052704,
      "learning_rate": 0.00045016672088484064,
      "loss": 0.3878,
      "step": 1225500
    },
    {
      "epoch": 99.70114054526603,
      "grad_norm": 0.19709783792495728,
      "learning_rate": 0.00045014638906961616,
      "loss": 0.3889,
      "step": 1226000
    },
    {
      "epoch": 99.74180169556996,
      "grad_norm": 0.23504416644573212,
      "learning_rate": 0.00045012605725439173,
      "loss": 0.3885,
      "step": 1226500
    },
    {
      "epoch": 99.78246284587391,
      "grad_norm": 0.2335658073425293,
      "learning_rate": 0.00045010572543916725,
      "loss": 0.3894,
      "step": 1227000
    },
    {
      "epoch": 99.82312399617786,
      "grad_norm": 0.1749373972415924,
      "learning_rate": 0.00045008539362394276,
      "loss": 0.3885,
      "step": 1227500
    },
    {
      "epoch": 99.86378514648179,
      "grad_norm": 0.22605063021183014,
      "learning_rate": 0.00045006506180871834,
      "loss": 0.3888,
      "step": 1228000
    },
    {
      "epoch": 99.90444629678574,
      "grad_norm": 0.1871388554573059,
      "learning_rate": 0.00045004472999349385,
      "loss": 0.3889,
      "step": 1228500
    },
    {
      "epoch": 99.94510744708968,
      "grad_norm": 0.20071691274642944,
      "learning_rate": 0.00045002439817826937,
      "loss": 0.3887,
      "step": 1229000
    },
    {
      "epoch": 99.98576859739362,
      "grad_norm": 0.20299628376960754,
      "learning_rate": 0.00045000406636304494,
      "loss": 0.3891,
      "step": 1229500
    },
    {
      "epoch": 100.02642974769756,
      "grad_norm": 0.22434689104557037,
      "learning_rate": 0.00044998373454782046,
      "loss": 0.3861,
      "step": 1230000
    },
    {
      "epoch": 100.06709089800151,
      "grad_norm": 0.2044047862291336,
      "learning_rate": 0.00044996340273259597,
      "loss": 0.3859,
      "step": 1230500
    },
    {
      "epoch": 100.10775204830544,
      "grad_norm": 0.22421279549598694,
      "learning_rate": 0.00044994307091737154,
      "loss": 0.386,
      "step": 1231000
    },
    {
      "epoch": 100.14841319860939,
      "grad_norm": 0.22775940597057343,
      "learning_rate": 0.00044992273910214706,
      "loss": 0.3864,
      "step": 1231500
    },
    {
      "epoch": 100.18907434891334,
      "grad_norm": 0.2250286340713501,
      "learning_rate": 0.00044990240728692263,
      "loss": 0.3858,
      "step": 1232000
    },
    {
      "epoch": 100.22973549921727,
      "grad_norm": 0.2294192761182785,
      "learning_rate": 0.00044988207547169815,
      "loss": 0.3864,
      "step": 1232500
    },
    {
      "epoch": 100.27039664952122,
      "grad_norm": 0.1979227215051651,
      "learning_rate": 0.00044986174365647366,
      "loss": 0.3875,
      "step": 1233000
    },
    {
      "epoch": 100.31105779982515,
      "grad_norm": 0.20843921601772308,
      "learning_rate": 0.00044984141184124923,
      "loss": 0.3873,
      "step": 1233500
    },
    {
      "epoch": 100.3517189501291,
      "grad_norm": 0.21087777614593506,
      "learning_rate": 0.00044982108002602475,
      "loss": 0.3869,
      "step": 1234000
    },
    {
      "epoch": 100.39238010043304,
      "grad_norm": 0.20642660558223724,
      "learning_rate": 0.00044980074821080027,
      "loss": 0.3875,
      "step": 1234500
    },
    {
      "epoch": 100.43304125073698,
      "grad_norm": 0.19875508546829224,
      "learning_rate": 0.00044978041639557584,
      "loss": 0.3879,
      "step": 1235000
    },
    {
      "epoch": 100.47370240104092,
      "grad_norm": 0.23019340634346008,
      "learning_rate": 0.00044976008458035135,
      "loss": 0.3875,
      "step": 1235500
    },
    {
      "epoch": 100.51436355134487,
      "grad_norm": 0.21975049376487732,
      "learning_rate": 0.00044973975276512687,
      "loss": 0.388,
      "step": 1236000
    },
    {
      "epoch": 100.5550247016488,
      "grad_norm": 0.21763068437576294,
      "learning_rate": 0.00044971942094990244,
      "loss": 0.3878,
      "step": 1236500
    },
    {
      "epoch": 100.59568585195275,
      "grad_norm": 0.2142203003168106,
      "learning_rate": 0.00044969908913467796,
      "loss": 0.388,
      "step": 1237000
    },
    {
      "epoch": 100.6363470022567,
      "grad_norm": 0.2037387639284134,
      "learning_rate": 0.0004496787573194535,
      "loss": 0.3874,
      "step": 1237500
    },
    {
      "epoch": 100.67700815256063,
      "grad_norm": 0.246235191822052,
      "learning_rate": 0.00044965842550422904,
      "loss": 0.3885,
      "step": 1238000
    },
    {
      "epoch": 100.71766930286458,
      "grad_norm": 0.19812509417533875,
      "learning_rate": 0.00044963809368900456,
      "loss": 0.3881,
      "step": 1238500
    },
    {
      "epoch": 100.75833045316853,
      "grad_norm": 0.2094288319349289,
      "learning_rate": 0.00044961776187378013,
      "loss": 0.3883,
      "step": 1239000
    },
    {
      "epoch": 100.79899160347246,
      "grad_norm": 0.2082221657037735,
      "learning_rate": 0.00044959743005855565,
      "loss": 0.3888,
      "step": 1239500
    },
    {
      "epoch": 100.8396527537764,
      "grad_norm": 0.23340551555156708,
      "learning_rate": 0.00044957709824333116,
      "loss": 0.3884,
      "step": 1240000
    },
    {
      "epoch": 100.88031390408035,
      "grad_norm": 0.20842692255973816,
      "learning_rate": 0.00044955676642810673,
      "loss": 0.3891,
      "step": 1240500
    },
    {
      "epoch": 100.92097505438429,
      "grad_norm": 0.22207078337669373,
      "learning_rate": 0.00044953643461288225,
      "loss": 0.3894,
      "step": 1241000
    },
    {
      "epoch": 100.96163620468823,
      "grad_norm": 0.21922795474529266,
      "learning_rate": 0.0004495161027976578,
      "loss": 0.3889,
      "step": 1241500
    },
    {
      "epoch": 101.00229735499217,
      "grad_norm": 0.2085517793893814,
      "learning_rate": 0.00044949577098243334,
      "loss": 0.3888,
      "step": 1242000
    },
    {
      "epoch": 101.04295850529611,
      "grad_norm": 0.2117597609758377,
      "learning_rate": 0.00044947543916720885,
      "loss": 0.3849,
      "step": 1242500
    },
    {
      "epoch": 101.08361965560006,
      "grad_norm": 0.2236788272857666,
      "learning_rate": 0.0004494551073519844,
      "loss": 0.3856,
      "step": 1243000
    },
    {
      "epoch": 101.124280805904,
      "grad_norm": 0.20152530074119568,
      "learning_rate": 0.00044943477553675994,
      "loss": 0.3857,
      "step": 1243500
    },
    {
      "epoch": 101.16494195620794,
      "grad_norm": 0.22745753824710846,
      "learning_rate": 0.00044941444372153546,
      "loss": 0.3861,
      "step": 1244000
    },
    {
      "epoch": 101.20560310651189,
      "grad_norm": 0.20258009433746338,
      "learning_rate": 0.00044939411190631103,
      "loss": 0.3864,
      "step": 1244500
    },
    {
      "epoch": 101.24626425681582,
      "grad_norm": 0.22467990219593048,
      "learning_rate": 0.00044937378009108654,
      "loss": 0.3864,
      "step": 1245000
    },
    {
      "epoch": 101.28692540711977,
      "grad_norm": 0.2080828696489334,
      "learning_rate": 0.00044935344827586206,
      "loss": 0.3867,
      "step": 1245500
    },
    {
      "epoch": 101.32758655742371,
      "grad_norm": 0.20662011206150055,
      "learning_rate": 0.00044933311646063763,
      "loss": 0.3875,
      "step": 1246000
    },
    {
      "epoch": 101.36824770772765,
      "grad_norm": 0.22902284562587738,
      "learning_rate": 0.00044931278464541315,
      "loss": 0.387,
      "step": 1246500
    },
    {
      "epoch": 101.4089088580316,
      "grad_norm": 0.22851873934268951,
      "learning_rate": 0.0004492924528301887,
      "loss": 0.3876,
      "step": 1247000
    },
    {
      "epoch": 101.44957000833554,
      "grad_norm": 0.1925787776708603,
      "learning_rate": 0.00044927212101496423,
      "loss": 0.3873,
      "step": 1247500
    },
    {
      "epoch": 101.49023115863947,
      "grad_norm": 0.1894083172082901,
      "learning_rate": 0.00044925178919973975,
      "loss": 0.3877,
      "step": 1248000
    },
    {
      "epoch": 101.53089230894342,
      "grad_norm": 0.2528088092803955,
      "learning_rate": 0.0004492314573845153,
      "loss": 0.3883,
      "step": 1248500
    },
    {
      "epoch": 101.57155345924737,
      "grad_norm": 0.23419702053070068,
      "learning_rate": 0.00044921112556929084,
      "loss": 0.3876,
      "step": 1249000
    },
    {
      "epoch": 101.6122146095513,
      "grad_norm": 0.21790142357349396,
      "learning_rate": 0.00044919079375406635,
      "loss": 0.3878,
      "step": 1249500
    },
    {
      "epoch": 101.65287575985525,
      "grad_norm": 0.21653634309768677,
      "learning_rate": 0.0004491704619388419,
      "loss": 0.3876,
      "step": 1250000
    },
    {
      "epoch": 101.69353691015918,
      "grad_norm": 0.23246963322162628,
      "learning_rate": 0.00044915013012361744,
      "loss": 0.388,
      "step": 1250500
    },
    {
      "epoch": 101.73419806046313,
      "grad_norm": 0.21975159645080566,
      "learning_rate": 0.00044912979830839296,
      "loss": 0.3884,
      "step": 1251000
    },
    {
      "epoch": 101.77485921076708,
      "grad_norm": 0.20111016929149628,
      "learning_rate": 0.00044910946649316853,
      "loss": 0.3888,
      "step": 1251500
    },
    {
      "epoch": 101.81552036107101,
      "grad_norm": 0.22796224057674408,
      "learning_rate": 0.00044908913467794405,
      "loss": 0.3881,
      "step": 1252000
    },
    {
      "epoch": 101.85618151137496,
      "grad_norm": 0.18301673233509064,
      "learning_rate": 0.0004490688028627196,
      "loss": 0.3886,
      "step": 1252500
    },
    {
      "epoch": 101.8968426616789,
      "grad_norm": 0.19764049351215363,
      "learning_rate": 0.00044904847104749513,
      "loss": 0.3888,
      "step": 1253000
    },
    {
      "epoch": 101.93750381198284,
      "grad_norm": 0.217835932970047,
      "learning_rate": 0.00044902813923227065,
      "loss": 0.3886,
      "step": 1253500
    },
    {
      "epoch": 101.97816496228678,
      "grad_norm": 0.20104219019412994,
      "learning_rate": 0.0004490078074170462,
      "loss": 0.3892,
      "step": 1254000
    },
    {
      "epoch": 102.01882611259073,
      "grad_norm": 0.22055763006210327,
      "learning_rate": 0.00044898747560182174,
      "loss": 0.3869,
      "step": 1254500
    },
    {
      "epoch": 102.05948726289466,
      "grad_norm": 0.219086155295372,
      "learning_rate": 0.00044896714378659725,
      "loss": 0.3852,
      "step": 1255000
    },
    {
      "epoch": 102.10014841319861,
      "grad_norm": 0.20389512181282043,
      "learning_rate": 0.0004489468119713728,
      "loss": 0.3856,
      "step": 1255500
    },
    {
      "epoch": 102.14080956350256,
      "grad_norm": 0.20880785584449768,
      "learning_rate": 0.00044892648015614834,
      "loss": 0.3855,
      "step": 1256000
    },
    {
      "epoch": 102.18147071380649,
      "grad_norm": 0.2022770792245865,
      "learning_rate": 0.0004489061483409239,
      "loss": 0.3862,
      "step": 1256500
    },
    {
      "epoch": 102.22213186411044,
      "grad_norm": 0.21247738599777222,
      "learning_rate": 0.0004488858165256994,
      "loss": 0.3863,
      "step": 1257000
    },
    {
      "epoch": 102.26279301441438,
      "grad_norm": 0.2083471417427063,
      "learning_rate": 0.00044886548471047494,
      "loss": 0.3868,
      "step": 1257500
    },
    {
      "epoch": 102.30345416471832,
      "grad_norm": 0.21976284682750702,
      "learning_rate": 0.0004488451528952505,
      "loss": 0.3864,
      "step": 1258000
    },
    {
      "epoch": 102.34411531502226,
      "grad_norm": 0.21447432041168213,
      "learning_rate": 0.00044882482108002603,
      "loss": 0.3863,
      "step": 1258500
    },
    {
      "epoch": 102.3847764653262,
      "grad_norm": 0.2016969472169876,
      "learning_rate": 0.00044880448926480155,
      "loss": 0.3875,
      "step": 1259000
    },
    {
      "epoch": 102.42543761563014,
      "grad_norm": 0.23412947356700897,
      "learning_rate": 0.0004487841574495771,
      "loss": 0.3872,
      "step": 1259500
    },
    {
      "epoch": 102.46609876593409,
      "grad_norm": 0.20431576669216156,
      "learning_rate": 0.00044876382563435263,
      "loss": 0.3871,
      "step": 1260000
    },
    {
      "epoch": 102.50675991623802,
      "grad_norm": 0.234221950173378,
      "learning_rate": 0.00044874349381912815,
      "loss": 0.3874,
      "step": 1260500
    },
    {
      "epoch": 102.54742106654197,
      "grad_norm": 0.21260064840316772,
      "learning_rate": 0.0004487231620039037,
      "loss": 0.388,
      "step": 1261000
    },
    {
      "epoch": 102.58808221684592,
      "grad_norm": 0.21565744280815125,
      "learning_rate": 0.00044870283018867924,
      "loss": 0.3879,
      "step": 1261500
    },
    {
      "epoch": 102.62874336714985,
      "grad_norm": 0.23275958001613617,
      "learning_rate": 0.0004486824983734548,
      "loss": 0.3877,
      "step": 1262000
    },
    {
      "epoch": 102.6694045174538,
      "grad_norm": 0.21536950767040253,
      "learning_rate": 0.0004486621665582303,
      "loss": 0.3887,
      "step": 1262500
    },
    {
      "epoch": 102.71006566775775,
      "grad_norm": 0.22729305922985077,
      "learning_rate": 0.00044864183474300584,
      "loss": 0.3883,
      "step": 1263000
    },
    {
      "epoch": 102.75072681806168,
      "grad_norm": 0.23975111544132233,
      "learning_rate": 0.0004486215029277814,
      "loss": 0.3884,
      "step": 1263500
    },
    {
      "epoch": 102.79138796836563,
      "grad_norm": 0.21309266984462738,
      "learning_rate": 0.00044860117111255693,
      "loss": 0.3888,
      "step": 1264000
    },
    {
      "epoch": 102.83204911866957,
      "grad_norm": 0.20468604564666748,
      "learning_rate": 0.00044858083929733244,
      "loss": 0.3882,
      "step": 1264500
    },
    {
      "epoch": 102.8727102689735,
      "grad_norm": 0.2137664258480072,
      "learning_rate": 0.000448560507482108,
      "loss": 0.3883,
      "step": 1265000
    },
    {
      "epoch": 102.91337141927745,
      "grad_norm": 0.2290658950805664,
      "learning_rate": 0.00044854017566688353,
      "loss": 0.3888,
      "step": 1265500
    },
    {
      "epoch": 102.9540325695814,
      "grad_norm": 0.22061142325401306,
      "learning_rate": 0.00044851984385165905,
      "loss": 0.3885,
      "step": 1266000
    },
    {
      "epoch": 102.99469371988533,
      "grad_norm": 0.22889046370983124,
      "learning_rate": 0.0004484995120364346,
      "loss": 0.3891,
      "step": 1266500
    },
    {
      "epoch": 103.03535487018928,
      "grad_norm": 0.21451760828495026,
      "learning_rate": 0.00044847918022121013,
      "loss": 0.3852,
      "step": 1267000
    },
    {
      "epoch": 103.07601602049321,
      "grad_norm": 0.21896667778491974,
      "learning_rate": 0.0004484588484059857,
      "loss": 0.3851,
      "step": 1267500
    },
    {
      "epoch": 103.11667717079716,
      "grad_norm": 0.21162821352481842,
      "learning_rate": 0.0004484385165907612,
      "loss": 0.3861,
      "step": 1268000
    },
    {
      "epoch": 103.15733832110111,
      "grad_norm": 0.21114148199558258,
      "learning_rate": 0.00044841818477553674,
      "loss": 0.3859,
      "step": 1268500
    },
    {
      "epoch": 103.19799947140504,
      "grad_norm": 0.21291618049144745,
      "learning_rate": 0.0004483978529603123,
      "loss": 0.3861,
      "step": 1269000
    },
    {
      "epoch": 103.23866062170899,
      "grad_norm": 0.2102540284395218,
      "learning_rate": 0.0004483775211450878,
      "loss": 0.3865,
      "step": 1269500
    },
    {
      "epoch": 103.27932177201293,
      "grad_norm": 0.2097703069448471,
      "learning_rate": 0.00044835718932986334,
      "loss": 0.3869,
      "step": 1270000
    },
    {
      "epoch": 103.31998292231687,
      "grad_norm": 0.20601220428943634,
      "learning_rate": 0.0004483368575146389,
      "loss": 0.3863,
      "step": 1270500
    },
    {
      "epoch": 103.36064407262081,
      "grad_norm": 0.22116662561893463,
      "learning_rate": 0.00044831652569941443,
      "loss": 0.3869,
      "step": 1271000
    },
    {
      "epoch": 103.40130522292476,
      "grad_norm": 0.2133554071187973,
      "learning_rate": 0.00044829619388418994,
      "loss": 0.3869,
      "step": 1271500
    },
    {
      "epoch": 103.4419663732287,
      "grad_norm": 0.21101689338684082,
      "learning_rate": 0.0004482758620689655,
      "loss": 0.3872,
      "step": 1272000
    },
    {
      "epoch": 103.48262752353264,
      "grad_norm": 0.23101098835468292,
      "learning_rate": 0.00044825553025374103,
      "loss": 0.3876,
      "step": 1272500
    },
    {
      "epoch": 103.52328867383659,
      "grad_norm": 0.19334758818149567,
      "learning_rate": 0.0004482351984385166,
      "loss": 0.3877,
      "step": 1273000
    },
    {
      "epoch": 103.56394982414052,
      "grad_norm": 0.26033928990364075,
      "learning_rate": 0.0004482148666232921,
      "loss": 0.3876,
      "step": 1273500
    },
    {
      "epoch": 103.60461097444447,
      "grad_norm": 0.20452778041362762,
      "learning_rate": 0.00044819453480806764,
      "loss": 0.3876,
      "step": 1274000
    },
    {
      "epoch": 103.6452721247484,
      "grad_norm": 0.20564498007297516,
      "learning_rate": 0.0004481742029928432,
      "loss": 0.3875,
      "step": 1274500
    },
    {
      "epoch": 103.68593327505235,
      "grad_norm": 0.20433123409748077,
      "learning_rate": 0.0004481538711776187,
      "loss": 0.3883,
      "step": 1275000
    },
    {
      "epoch": 103.7265944253563,
      "grad_norm": 0.22035057842731476,
      "learning_rate": 0.00044813353936239424,
      "loss": 0.3883,
      "step": 1275500
    },
    {
      "epoch": 103.76725557566023,
      "grad_norm": 0.22481019794940948,
      "learning_rate": 0.0004481132075471698,
      "loss": 0.3884,
      "step": 1276000
    },
    {
      "epoch": 103.80791672596418,
      "grad_norm": 0.23942507803440094,
      "learning_rate": 0.0004480928757319453,
      "loss": 0.3881,
      "step": 1276500
    },
    {
      "epoch": 103.84857787626812,
      "grad_norm": 0.21850070357322693,
      "learning_rate": 0.0004480725439167209,
      "loss": 0.3882,
      "step": 1277000
    },
    {
      "epoch": 103.88923902657206,
      "grad_norm": 0.2027171552181244,
      "learning_rate": 0.0004480522121014964,
      "loss": 0.3885,
      "step": 1277500
    },
    {
      "epoch": 103.929900176876,
      "grad_norm": 0.23158785700798035,
      "learning_rate": 0.00044803188028627193,
      "loss": 0.3879,
      "step": 1278000
    },
    {
      "epoch": 103.97056132717995,
      "grad_norm": 0.2104395478963852,
      "learning_rate": 0.0004480115484710475,
      "loss": 0.3888,
      "step": 1278500
    },
    {
      "epoch": 104.01122247748388,
      "grad_norm": 0.22118960320949554,
      "learning_rate": 0.000447991216655823,
      "loss": 0.3877,
      "step": 1279000
    },
    {
      "epoch": 104.05188362778783,
      "grad_norm": 0.2210027277469635,
      "learning_rate": 0.00044797088484059853,
      "loss": 0.3844,
      "step": 1279500
    },
    {
      "epoch": 104.09254477809178,
      "grad_norm": 0.20274217426776886,
      "learning_rate": 0.0004479505530253741,
      "loss": 0.3851,
      "step": 1280000
    },
    {
      "epoch": 104.13320592839571,
      "grad_norm": 0.20300504565238953,
      "learning_rate": 0.0004479302212101496,
      "loss": 0.3859,
      "step": 1280500
    },
    {
      "epoch": 104.17386707869966,
      "grad_norm": 0.22460447251796722,
      "learning_rate": 0.00044790988939492514,
      "loss": 0.386,
      "step": 1281000
    },
    {
      "epoch": 104.2145282290036,
      "grad_norm": 0.19732585549354553,
      "learning_rate": 0.00044788955757970076,
      "loss": 0.386,
      "step": 1281500
    },
    {
      "epoch": 104.25518937930754,
      "grad_norm": 0.210521399974823,
      "learning_rate": 0.0004478692257644763,
      "loss": 0.3863,
      "step": 1282000
    },
    {
      "epoch": 104.29585052961149,
      "grad_norm": 0.2285839170217514,
      "learning_rate": 0.00044784889394925185,
      "loss": 0.3865,
      "step": 1282500
    },
    {
      "epoch": 104.33651167991542,
      "grad_norm": 0.23142006993293762,
      "learning_rate": 0.00044782856213402736,
      "loss": 0.3866,
      "step": 1283000
    },
    {
      "epoch": 104.37717283021937,
      "grad_norm": 0.20558524131774902,
      "learning_rate": 0.0004478082303188029,
      "loss": 0.3868,
      "step": 1283500
    },
    {
      "epoch": 104.41783398052331,
      "grad_norm": 0.22102497518062592,
      "learning_rate": 0.00044778789850357845,
      "loss": 0.3875,
      "step": 1284000
    },
    {
      "epoch": 104.45849513082725,
      "grad_norm": 0.21808750927448273,
      "learning_rate": 0.00044776756668835397,
      "loss": 0.3873,
      "step": 1284500
    },
    {
      "epoch": 104.49915628113119,
      "grad_norm": 0.20020519196987152,
      "learning_rate": 0.0004477472348731295,
      "loss": 0.3876,
      "step": 1285000
    },
    {
      "epoch": 104.53981743143514,
      "grad_norm": 0.2625903785228729,
      "learning_rate": 0.00044772690305790506,
      "loss": 0.3877,
      "step": 1285500
    },
    {
      "epoch": 104.58047858173907,
      "grad_norm": 0.23344388604164124,
      "learning_rate": 0.00044770657124268057,
      "loss": 0.3875,
      "step": 1286000
    },
    {
      "epoch": 104.62113973204302,
      "grad_norm": 0.19146126508712769,
      "learning_rate": 0.0004476862394274561,
      "loss": 0.3882,
      "step": 1286500
    },
    {
      "epoch": 104.66180088234697,
      "grad_norm": 0.2185308188199997,
      "learning_rate": 0.00044766590761223166,
      "loss": 0.3879,
      "step": 1287000
    },
    {
      "epoch": 104.7024620326509,
      "grad_norm": 0.20422689616680145,
      "learning_rate": 0.0004476455757970072,
      "loss": 0.3877,
      "step": 1287500
    },
    {
      "epoch": 104.74312318295485,
      "grad_norm": 0.22418363392353058,
      "learning_rate": 0.00044762524398178275,
      "loss": 0.3878,
      "step": 1288000
    },
    {
      "epoch": 104.7837843332588,
      "grad_norm": 0.22151586413383484,
      "learning_rate": 0.00044760491216655826,
      "loss": 0.3883,
      "step": 1288500
    },
    {
      "epoch": 104.82444548356273,
      "grad_norm": 0.1922367960214615,
      "learning_rate": 0.0004475845803513338,
      "loss": 0.3879,
      "step": 1289000
    },
    {
      "epoch": 104.86510663386667,
      "grad_norm": 0.20876047015190125,
      "learning_rate": 0.00044756424853610935,
      "loss": 0.3886,
      "step": 1289500
    },
    {
      "epoch": 104.90576778417062,
      "grad_norm": 0.21440769731998444,
      "learning_rate": 0.00044754391672088487,
      "loss": 0.3885,
      "step": 1290000
    },
    {
      "epoch": 104.94642893447455,
      "grad_norm": 0.20574867725372314,
      "learning_rate": 0.0004475235849056604,
      "loss": 0.3883,
      "step": 1290500
    },
    {
      "epoch": 104.9870900847785,
      "grad_norm": 0.21660415828227997,
      "learning_rate": 0.00044750325309043595,
      "loss": 0.3886,
      "step": 1291000
    },
    {
      "epoch": 105.02775123508243,
      "grad_norm": 0.21843795478343964,
      "learning_rate": 0.00044748292127521147,
      "loss": 0.3859,
      "step": 1291500
    },
    {
      "epoch": 105.06841238538638,
      "grad_norm": 0.2098463922739029,
      "learning_rate": 0.00044746258945998704,
      "loss": 0.385,
      "step": 1292000
    },
    {
      "epoch": 105.10907353569033,
      "grad_norm": 0.22575446963310242,
      "learning_rate": 0.00044744225764476256,
      "loss": 0.3851,
      "step": 1292500
    },
    {
      "epoch": 105.14973468599426,
      "grad_norm": 0.21644271910190582,
      "learning_rate": 0.00044742192582953807,
      "loss": 0.386,
      "step": 1293000
    },
    {
      "epoch": 105.19039583629821,
      "grad_norm": 0.23563261330127716,
      "learning_rate": 0.00044740159401431364,
      "loss": 0.3861,
      "step": 1293500
    },
    {
      "epoch": 105.23105698660216,
      "grad_norm": 0.2018425166606903,
      "learning_rate": 0.00044738126219908916,
      "loss": 0.3863,
      "step": 1294000
    },
    {
      "epoch": 105.27171813690609,
      "grad_norm": 0.2196255624294281,
      "learning_rate": 0.0004473609303838647,
      "loss": 0.3861,
      "step": 1294500
    },
    {
      "epoch": 105.31237928721004,
      "grad_norm": 0.19440345466136932,
      "learning_rate": 0.00044734059856864025,
      "loss": 0.3863,
      "step": 1295000
    },
    {
      "epoch": 105.35304043751398,
      "grad_norm": 0.20993386209011078,
      "learning_rate": 0.00044732026675341576,
      "loss": 0.3866,
      "step": 1295500
    },
    {
      "epoch": 105.39370158781792,
      "grad_norm": 0.25151917338371277,
      "learning_rate": 0.0004472999349381913,
      "loss": 0.3867,
      "step": 1296000
    },
    {
      "epoch": 105.43436273812186,
      "grad_norm": 0.20725035667419434,
      "learning_rate": 0.00044727960312296685,
      "loss": 0.3867,
      "step": 1296500
    },
    {
      "epoch": 105.47502388842581,
      "grad_norm": 0.21189244091510773,
      "learning_rate": 0.00044725927130774237,
      "loss": 0.3876,
      "step": 1297000
    },
    {
      "epoch": 105.51568503872974,
      "grad_norm": 0.20095254480838776,
      "learning_rate": 0.00044723893949251794,
      "loss": 0.3873,
      "step": 1297500
    },
    {
      "epoch": 105.55634618903369,
      "grad_norm": 0.20618408918380737,
      "learning_rate": 0.00044721860767729345,
      "loss": 0.3872,
      "step": 1298000
    },
    {
      "epoch": 105.59700733933764,
      "grad_norm": 0.22049900889396667,
      "learning_rate": 0.00044719827586206897,
      "loss": 0.387,
      "step": 1298500
    },
    {
      "epoch": 105.63766848964157,
      "grad_norm": 0.21189911663532257,
      "learning_rate": 0.00044717794404684454,
      "loss": 0.3875,
      "step": 1299000
    },
    {
      "epoch": 105.67832963994552,
      "grad_norm": 0.22458788752555847,
      "learning_rate": 0.00044715761223162006,
      "loss": 0.3879,
      "step": 1299500
    },
    {
      "epoch": 105.71899079024945,
      "grad_norm": 0.20477668941020966,
      "learning_rate": 0.0004471372804163956,
      "loss": 0.388,
      "step": 1300000
    },
    {
      "epoch": 105.7596519405534,
      "grad_norm": 0.19014574587345123,
      "learning_rate": 0.00044711694860117114,
      "loss": 0.3877,
      "step": 1300500
    },
    {
      "epoch": 105.80031309085734,
      "grad_norm": 0.22366641461849213,
      "learning_rate": 0.00044709661678594666,
      "loss": 0.3883,
      "step": 1301000
    },
    {
      "epoch": 105.84097424116128,
      "grad_norm": 0.20676729083061218,
      "learning_rate": 0.0004470762849707222,
      "loss": 0.3881,
      "step": 1301500
    },
    {
      "epoch": 105.88163539146522,
      "grad_norm": 0.2054746150970459,
      "learning_rate": 0.00044705595315549775,
      "loss": 0.3884,
      "step": 1302000
    },
    {
      "epoch": 105.92229654176917,
      "grad_norm": 0.22302058339118958,
      "learning_rate": 0.00044703562134027326,
      "loss": 0.3881,
      "step": 1302500
    },
    {
      "epoch": 105.9629576920731,
      "grad_norm": 0.21055175364017487,
      "learning_rate": 0.00044701528952504883,
      "loss": 0.3883,
      "step": 1303000
    },
    {
      "epoch": 106.00361884237705,
      "grad_norm": 0.23194192349910736,
      "learning_rate": 0.00044699495770982435,
      "loss": 0.3882,
      "step": 1303500
    },
    {
      "epoch": 106.044279992681,
      "grad_norm": 0.21744458377361298,
      "learning_rate": 0.00044697462589459987,
      "loss": 0.3844,
      "step": 1304000
    },
    {
      "epoch": 106.08494114298493,
      "grad_norm": 0.19844186305999756,
      "learning_rate": 0.00044695429407937544,
      "loss": 0.385,
      "step": 1304500
    },
    {
      "epoch": 106.12560229328888,
      "grad_norm": 0.20805412530899048,
      "learning_rate": 0.00044693396226415095,
      "loss": 0.3859,
      "step": 1305000
    },
    {
      "epoch": 106.16626344359283,
      "grad_norm": 0.21819841861724854,
      "learning_rate": 0.00044691363044892647,
      "loss": 0.386,
      "step": 1305500
    },
    {
      "epoch": 106.20692459389676,
      "grad_norm": 0.1948084682226181,
      "learning_rate": 0.00044689329863370204,
      "loss": 0.386,
      "step": 1306000
    },
    {
      "epoch": 106.2475857442007,
      "grad_norm": 0.20857472717761993,
      "learning_rate": 0.00044687296681847756,
      "loss": 0.3861,
      "step": 1306500
    },
    {
      "epoch": 106.28824689450465,
      "grad_norm": 0.22034971415996552,
      "learning_rate": 0.00044685263500325313,
      "loss": 0.3862,
      "step": 1307000
    },
    {
      "epoch": 106.32890804480859,
      "grad_norm": 0.2179964929819107,
      "learning_rate": 0.00044683230318802865,
      "loss": 0.3861,
      "step": 1307500
    },
    {
      "epoch": 106.36956919511253,
      "grad_norm": 0.20126500725746155,
      "learning_rate": 0.00044681197137280416,
      "loss": 0.3866,
      "step": 1308000
    },
    {
      "epoch": 106.41023034541647,
      "grad_norm": 0.20779827237129211,
      "learning_rate": 0.00044679163955757973,
      "loss": 0.3865,
      "step": 1308500
    },
    {
      "epoch": 106.45089149572041,
      "grad_norm": 0.21981926262378693,
      "learning_rate": 0.00044677130774235525,
      "loss": 0.3868,
      "step": 1309000
    },
    {
      "epoch": 106.49155264602436,
      "grad_norm": 0.231073260307312,
      "learning_rate": 0.00044675097592713077,
      "loss": 0.3875,
      "step": 1309500
    },
    {
      "epoch": 106.53221379632829,
      "grad_norm": 0.1956528127193451,
      "learning_rate": 0.00044673064411190634,
      "loss": 0.3869,
      "step": 1310000
    },
    {
      "epoch": 106.57287494663224,
      "grad_norm": 0.1995610147714615,
      "learning_rate": 0.00044671031229668185,
      "loss": 0.3873,
      "step": 1310500
    },
    {
      "epoch": 106.61353609693619,
      "grad_norm": 0.2250250279903412,
      "learning_rate": 0.00044668998048145737,
      "loss": 0.3875,
      "step": 1311000
    },
    {
      "epoch": 106.65419724724012,
      "grad_norm": 0.22709909081459045,
      "learning_rate": 0.00044666964866623294,
      "loss": 0.3875,
      "step": 1311500
    },
    {
      "epoch": 106.69485839754407,
      "grad_norm": 0.19472736120224,
      "learning_rate": 0.00044664931685100846,
      "loss": 0.3874,
      "step": 1312000
    },
    {
      "epoch": 106.73551954784801,
      "grad_norm": 0.20835494995117188,
      "learning_rate": 0.000446628985035784,
      "loss": 0.3878,
      "step": 1312500
    },
    {
      "epoch": 106.77618069815195,
      "grad_norm": 0.23099642992019653,
      "learning_rate": 0.00044660865322055954,
      "loss": 0.3879,
      "step": 1313000
    },
    {
      "epoch": 106.8168418484559,
      "grad_norm": 0.22180725634098053,
      "learning_rate": 0.00044658832140533506,
      "loss": 0.3875,
      "step": 1313500
    },
    {
      "epoch": 106.85750299875984,
      "grad_norm": 0.20594951510429382,
      "learning_rate": 0.00044656798959011063,
      "loss": 0.3878,
      "step": 1314000
    },
    {
      "epoch": 106.89816414906377,
      "grad_norm": 0.22355419397354126,
      "learning_rate": 0.00044654765777488615,
      "loss": 0.3887,
      "step": 1314500
    },
    {
      "epoch": 106.93882529936772,
      "grad_norm": 0.21292153000831604,
      "learning_rate": 0.00044652732595966166,
      "loss": 0.3881,
      "step": 1315000
    },
    {
      "epoch": 106.97948644967165,
      "grad_norm": 0.20614197850227356,
      "learning_rate": 0.00044650699414443723,
      "loss": 0.3883,
      "step": 1315500
    },
    {
      "epoch": 107.0201475999756,
      "grad_norm": 0.22794786095619202,
      "learning_rate": 0.00044648666232921275,
      "loss": 0.3863,
      "step": 1316000
    },
    {
      "epoch": 107.06080875027955,
      "grad_norm": 0.23694559931755066,
      "learning_rate": 0.00044646633051398827,
      "loss": 0.3849,
      "step": 1316500
    },
    {
      "epoch": 107.10146990058348,
      "grad_norm": 0.1962040513753891,
      "learning_rate": 0.00044644599869876384,
      "loss": 0.3851,
      "step": 1317000
    },
    {
      "epoch": 107.14213105088743,
      "grad_norm": 0.23212875425815582,
      "learning_rate": 0.00044642566688353935,
      "loss": 0.3858,
      "step": 1317500
    },
    {
      "epoch": 107.18279220119138,
      "grad_norm": 0.20986752212047577,
      "learning_rate": 0.0004464053350683149,
      "loss": 0.3852,
      "step": 1318000
    },
    {
      "epoch": 107.22345335149531,
      "grad_norm": 0.21438443660736084,
      "learning_rate": 0.00044638500325309044,
      "loss": 0.3859,
      "step": 1318500
    },
    {
      "epoch": 107.26411450179926,
      "grad_norm": 0.19618551433086395,
      "learning_rate": 0.00044636467143786596,
      "loss": 0.3859,
      "step": 1319000
    },
    {
      "epoch": 107.3047756521032,
      "grad_norm": 0.2316109538078308,
      "learning_rate": 0.00044634433962264153,
      "loss": 0.3862,
      "step": 1319500
    },
    {
      "epoch": 107.34543680240714,
      "grad_norm": 0.21305979788303375,
      "learning_rate": 0.00044632400780741704,
      "loss": 0.386,
      "step": 1320000
    },
    {
      "epoch": 107.38609795271108,
      "grad_norm": 0.20333142578601837,
      "learning_rate": 0.00044630367599219256,
      "loss": 0.3867,
      "step": 1320500
    },
    {
      "epoch": 107.42675910301503,
      "grad_norm": 0.21445593237876892,
      "learning_rate": 0.00044628334417696813,
      "loss": 0.387,
      "step": 1321000
    },
    {
      "epoch": 107.46742025331896,
      "grad_norm": 0.2371228188276291,
      "learning_rate": 0.00044626301236174365,
      "loss": 0.3868,
      "step": 1321500
    },
    {
      "epoch": 107.50808140362291,
      "grad_norm": 0.20753894746303558,
      "learning_rate": 0.00044624268054651916,
      "loss": 0.3873,
      "step": 1322000
    },
    {
      "epoch": 107.54874255392686,
      "grad_norm": 0.21862481534481049,
      "learning_rate": 0.00044622234873129473,
      "loss": 0.3868,
      "step": 1322500
    },
    {
      "epoch": 107.58940370423079,
      "grad_norm": 0.24413280189037323,
      "learning_rate": 0.00044620201691607025,
      "loss": 0.3872,
      "step": 1323000
    },
    {
      "epoch": 107.63006485453474,
      "grad_norm": 0.20246566832065582,
      "learning_rate": 0.0004461816851008458,
      "loss": 0.3871,
      "step": 1323500
    },
    {
      "epoch": 107.67072600483867,
      "grad_norm": 0.22520209848880768,
      "learning_rate": 0.00044616135328562134,
      "loss": 0.3879,
      "step": 1324000
    },
    {
      "epoch": 107.71138715514262,
      "grad_norm": 0.2061004787683487,
      "learning_rate": 0.00044614102147039685,
      "loss": 0.3873,
      "step": 1324500
    },
    {
      "epoch": 107.75204830544656,
      "grad_norm": 0.20960120856761932,
      "learning_rate": 0.0004461206896551724,
      "loss": 0.3875,
      "step": 1325000
    },
    {
      "epoch": 107.7927094557505,
      "grad_norm": 0.23830120265483856,
      "learning_rate": 0.00044610035783994794,
      "loss": 0.3875,
      "step": 1325500
    },
    {
      "epoch": 107.83337060605444,
      "grad_norm": 0.2045740932226181,
      "learning_rate": 0.00044608002602472346,
      "loss": 0.3875,
      "step": 1326000
    },
    {
      "epoch": 107.87403175635839,
      "grad_norm": 0.21314193308353424,
      "learning_rate": 0.00044605969420949903,
      "loss": 0.3876,
      "step": 1326500
    },
    {
      "epoch": 107.91469290666232,
      "grad_norm": 0.24548637866973877,
      "learning_rate": 0.00044603936239427454,
      "loss": 0.3881,
      "step": 1327000
    },
    {
      "epoch": 107.95535405696627,
      "grad_norm": 0.22643733024597168,
      "learning_rate": 0.0004460190305790501,
      "loss": 0.3883,
      "step": 1327500
    },
    {
      "epoch": 107.99601520727022,
      "grad_norm": 0.219278484582901,
      "learning_rate": 0.00044599869876382563,
      "loss": 0.3886,
      "step": 1328000
    },
    {
      "epoch": 108.03667635757415,
      "grad_norm": 0.21768558025360107,
      "learning_rate": 0.00044597836694860115,
      "loss": 0.3845,
      "step": 1328500
    },
    {
      "epoch": 108.0773375078781,
      "grad_norm": 0.24747897684574127,
      "learning_rate": 0.0004459580351333767,
      "loss": 0.3846,
      "step": 1329000
    },
    {
      "epoch": 108.11799865818205,
      "grad_norm": 0.1881948560476303,
      "learning_rate": 0.00044593770331815224,
      "loss": 0.3851,
      "step": 1329500
    },
    {
      "epoch": 108.15865980848598,
      "grad_norm": 0.22788721323013306,
      "learning_rate": 0.00044591737150292775,
      "loss": 0.3861,
      "step": 1330000
    },
    {
      "epoch": 108.19932095878993,
      "grad_norm": 0.22346310317516327,
      "learning_rate": 0.0004458970396877033,
      "loss": 0.3857,
      "step": 1330500
    },
    {
      "epoch": 108.23998210909387,
      "grad_norm": 0.24832089245319366,
      "learning_rate": 0.00044587670787247884,
      "loss": 0.386,
      "step": 1331000
    },
    {
      "epoch": 108.2806432593978,
      "grad_norm": 0.2201814204454422,
      "learning_rate": 0.00044585637605725436,
      "loss": 0.3855,
      "step": 1331500
    },
    {
      "epoch": 108.32130440970175,
      "grad_norm": 0.2333749681711197,
      "learning_rate": 0.0004458360442420299,
      "loss": 0.3865,
      "step": 1332000
    },
    {
      "epoch": 108.36196556000569,
      "grad_norm": 0.21379902958869934,
      "learning_rate": 0.00044581571242680544,
      "loss": 0.3868,
      "step": 1332500
    },
    {
      "epoch": 108.40262671030963,
      "grad_norm": 0.21278710663318634,
      "learning_rate": 0.000445795380611581,
      "loss": 0.3863,
      "step": 1333000
    },
    {
      "epoch": 108.44328786061358,
      "grad_norm": 0.2053336650133133,
      "learning_rate": 0.00044577504879635653,
      "loss": 0.3867,
      "step": 1333500
    },
    {
      "epoch": 108.48394901091751,
      "grad_norm": 0.2296031266450882,
      "learning_rate": 0.00044575471698113205,
      "loss": 0.3864,
      "step": 1334000
    },
    {
      "epoch": 108.52461016122146,
      "grad_norm": 0.23718571662902832,
      "learning_rate": 0.0004457343851659076,
      "loss": 0.3869,
      "step": 1334500
    },
    {
      "epoch": 108.56527131152541,
      "grad_norm": 0.26446211338043213,
      "learning_rate": 0.00044571405335068313,
      "loss": 0.3875,
      "step": 1335000
    },
    {
      "epoch": 108.60593246182934,
      "grad_norm": 0.22717003524303436,
      "learning_rate": 0.00044569372153545865,
      "loss": 0.3871,
      "step": 1335500
    },
    {
      "epoch": 108.64659361213329,
      "grad_norm": 0.21660593152046204,
      "learning_rate": 0.0004456733897202342,
      "loss": 0.3872,
      "step": 1336000
    },
    {
      "epoch": 108.68725476243723,
      "grad_norm": 0.22790424525737762,
      "learning_rate": 0.00044565305790500974,
      "loss": 0.3871,
      "step": 1336500
    },
    {
      "epoch": 108.72791591274117,
      "grad_norm": 0.21078595519065857,
      "learning_rate": 0.00044563272608978525,
      "loss": 0.3871,
      "step": 1337000
    },
    {
      "epoch": 108.76857706304511,
      "grad_norm": 0.23014836013317108,
      "learning_rate": 0.0004456123942745608,
      "loss": 0.3876,
      "step": 1337500
    },
    {
      "epoch": 108.80923821334906,
      "grad_norm": 0.20168271660804749,
      "learning_rate": 0.00044559206245933634,
      "loss": 0.3879,
      "step": 1338000
    },
    {
      "epoch": 108.849899363653,
      "grad_norm": 0.22628194093704224,
      "learning_rate": 0.00044557173064411196,
      "loss": 0.3877,
      "step": 1338500
    },
    {
      "epoch": 108.89056051395694,
      "grad_norm": 0.20839622616767883,
      "learning_rate": 0.0004455513988288875,
      "loss": 0.3879,
      "step": 1339000
    },
    {
      "epoch": 108.93122166426089,
      "grad_norm": 0.22583270072937012,
      "learning_rate": 0.000445531067013663,
      "loss": 0.3876,
      "step": 1339500
    },
    {
      "epoch": 108.97188281456482,
      "grad_norm": 0.2155665010213852,
      "learning_rate": 0.00044551073519843857,
      "loss": 0.3878,
      "step": 1340000
    },
    {
      "epoch": 109.01254396486877,
      "grad_norm": 0.23058834671974182,
      "learning_rate": 0.0004454904033832141,
      "loss": 0.3873,
      "step": 1340500
    },
    {
      "epoch": 109.0532051151727,
      "grad_norm": 0.22222313284873962,
      "learning_rate": 0.0004454700715679896,
      "loss": 0.3843,
      "step": 1341000
    },
    {
      "epoch": 109.09386626547665,
      "grad_norm": 0.23347048461437225,
      "learning_rate": 0.00044544973975276517,
      "loss": 0.3849,
      "step": 1341500
    },
    {
      "epoch": 109.1345274157806,
      "grad_norm": 0.18139193952083588,
      "learning_rate": 0.0004454294079375407,
      "loss": 0.3849,
      "step": 1342000
    },
    {
      "epoch": 109.17518856608453,
      "grad_norm": 0.2132055163383484,
      "learning_rate": 0.00044540907612231626,
      "loss": 0.3854,
      "step": 1342500
    },
    {
      "epoch": 109.21584971638848,
      "grad_norm": 0.2268202006816864,
      "learning_rate": 0.0004453887443070918,
      "loss": 0.3859,
      "step": 1343000
    },
    {
      "epoch": 109.25651086669242,
      "grad_norm": 0.20071515440940857,
      "learning_rate": 0.0004453684124918673,
      "loss": 0.3862,
      "step": 1343500
    },
    {
      "epoch": 109.29717201699636,
      "grad_norm": 0.22571903467178345,
      "learning_rate": 0.00044534808067664286,
      "loss": 0.3861,
      "step": 1344000
    },
    {
      "epoch": 109.3378331673003,
      "grad_norm": 0.2145540416240692,
      "learning_rate": 0.0004453277488614184,
      "loss": 0.3859,
      "step": 1344500
    },
    {
      "epoch": 109.37849431760425,
      "grad_norm": 0.22005116939544678,
      "learning_rate": 0.0004453074170461939,
      "loss": 0.3867,
      "step": 1345000
    },
    {
      "epoch": 109.41915546790818,
      "grad_norm": 0.21612703800201416,
      "learning_rate": 0.00044528708523096947,
      "loss": 0.3869,
      "step": 1345500
    },
    {
      "epoch": 109.45981661821213,
      "grad_norm": 0.2487146407365799,
      "learning_rate": 0.000445266753415745,
      "loss": 0.3865,
      "step": 1346000
    },
    {
      "epoch": 109.50047776851608,
      "grad_norm": 0.23401397466659546,
      "learning_rate": 0.0004452464216005205,
      "loss": 0.3866,
      "step": 1346500
    },
    {
      "epoch": 109.54113891882001,
      "grad_norm": 0.23697218298912048,
      "learning_rate": 0.00044522608978529607,
      "loss": 0.3874,
      "step": 1347000
    },
    {
      "epoch": 109.58180006912396,
      "grad_norm": 0.25781866908073425,
      "learning_rate": 0.0004452057579700716,
      "loss": 0.387,
      "step": 1347500
    },
    {
      "epoch": 109.62246121942789,
      "grad_norm": 0.2118082493543625,
      "learning_rate": 0.00044518542615484716,
      "loss": 0.3868,
      "step": 1348000
    },
    {
      "epoch": 109.66312236973184,
      "grad_norm": 0.22916962206363678,
      "learning_rate": 0.00044516509433962267,
      "loss": 0.3873,
      "step": 1348500
    },
    {
      "epoch": 109.70378352003578,
      "grad_norm": 0.22102105617523193,
      "learning_rate": 0.0004451447625243982,
      "loss": 0.3873,
      "step": 1349000
    },
    {
      "epoch": 109.74444467033972,
      "grad_norm": 0.2151237428188324,
      "learning_rate": 0.00044512443070917376,
      "loss": 0.3873,
      "step": 1349500
    },
    {
      "epoch": 109.78510582064366,
      "grad_norm": 0.2194591611623764,
      "learning_rate": 0.0004451040988939493,
      "loss": 0.3877,
      "step": 1350000
    },
    {
      "epoch": 109.82576697094761,
      "grad_norm": 0.24879853427410126,
      "learning_rate": 0.0004450837670787248,
      "loss": 0.3877,
      "step": 1350500
    },
    {
      "epoch": 109.86642812125154,
      "grad_norm": 0.1965503990650177,
      "learning_rate": 0.00044506343526350036,
      "loss": 0.3876,
      "step": 1351000
    },
    {
      "epoch": 109.90708927155549,
      "grad_norm": 0.23814286291599274,
      "learning_rate": 0.0004450431034482759,
      "loss": 0.3876,
      "step": 1351500
    },
    {
      "epoch": 109.94775042185944,
      "grad_norm": 0.21897511184215546,
      "learning_rate": 0.0004450227716330514,
      "loss": 0.3879,
      "step": 1352000
    },
    {
      "epoch": 109.98841157216337,
      "grad_norm": 0.22636841237545013,
      "learning_rate": 0.00044500243981782697,
      "loss": 0.3879,
      "step": 1352500
    },
    {
      "epoch": 110.02907272246732,
      "grad_norm": 0.25683969259262085,
      "learning_rate": 0.0004449821080026025,
      "loss": 0.3851,
      "step": 1353000
    },
    {
      "epoch": 110.06973387277127,
      "grad_norm": 0.20121917128562927,
      "learning_rate": 0.00044496177618737805,
      "loss": 0.3842,
      "step": 1353500
    },
    {
      "epoch": 110.1103950230752,
      "grad_norm": 0.24604783952236176,
      "learning_rate": 0.00044494144437215357,
      "loss": 0.3846,
      "step": 1354000
    },
    {
      "epoch": 110.15105617337915,
      "grad_norm": 0.23290809988975525,
      "learning_rate": 0.0004449211125569291,
      "loss": 0.3851,
      "step": 1354500
    },
    {
      "epoch": 110.1917173236831,
      "grad_norm": 0.21706829965114594,
      "learning_rate": 0.00044490078074170466,
      "loss": 0.3855,
      "step": 1355000
    },
    {
      "epoch": 110.23237847398703,
      "grad_norm": 0.22132417559623718,
      "learning_rate": 0.0004448804489264802,
      "loss": 0.3853,
      "step": 1355500
    },
    {
      "epoch": 110.27303962429097,
      "grad_norm": 0.21077710390090942,
      "learning_rate": 0.0004448601171112557,
      "loss": 0.3864,
      "step": 1356000
    },
    {
      "epoch": 110.3137007745949,
      "grad_norm": 0.23012778162956238,
      "learning_rate": 0.00044483978529603126,
      "loss": 0.3863,
      "step": 1356500
    },
    {
      "epoch": 110.35436192489885,
      "grad_norm": 0.2539319396018982,
      "learning_rate": 0.0004448194534808068,
      "loss": 0.3864,
      "step": 1357000
    },
    {
      "epoch": 110.3950230752028,
      "grad_norm": 0.24155957996845245,
      "learning_rate": 0.0004447991216655823,
      "loss": 0.3866,
      "step": 1357500
    },
    {
      "epoch": 110.43568422550673,
      "grad_norm": 0.21142877638339996,
      "learning_rate": 0.00044477878985035786,
      "loss": 0.3865,
      "step": 1358000
    },
    {
      "epoch": 110.47634537581068,
      "grad_norm": 0.21784523129463196,
      "learning_rate": 0.0004447584580351334,
      "loss": 0.3868,
      "step": 1358500
    },
    {
      "epoch": 110.51700652611463,
      "grad_norm": 0.2277183085680008,
      "learning_rate": 0.00044473812621990895,
      "loss": 0.3865,
      "step": 1359000
    },
    {
      "epoch": 110.55766767641856,
      "grad_norm": 0.2566206753253937,
      "learning_rate": 0.00044471779440468447,
      "loss": 0.3873,
      "step": 1359500
    },
    {
      "epoch": 110.59832882672251,
      "grad_norm": 0.22330662608146667,
      "learning_rate": 0.00044469746258946,
      "loss": 0.3872,
      "step": 1360000
    },
    {
      "epoch": 110.63898997702645,
      "grad_norm": 0.19720913469791412,
      "learning_rate": 0.00044467713077423555,
      "loss": 0.3872,
      "step": 1360500
    },
    {
      "epoch": 110.67965112733039,
      "grad_norm": 0.22340817749500275,
      "learning_rate": 0.00044465679895901107,
      "loss": 0.387,
      "step": 1361000
    },
    {
      "epoch": 110.72031227763433,
      "grad_norm": 0.2148931473493576,
      "learning_rate": 0.0004446364671437866,
      "loss": 0.387,
      "step": 1361500
    },
    {
      "epoch": 110.76097342793828,
      "grad_norm": 0.22756367921829224,
      "learning_rate": 0.00044461613532856216,
      "loss": 0.3867,
      "step": 1362000
    },
    {
      "epoch": 110.80163457824221,
      "grad_norm": 0.22178110480308533,
      "learning_rate": 0.0004445958035133377,
      "loss": 0.3872,
      "step": 1362500
    },
    {
      "epoch": 110.84229572854616,
      "grad_norm": 0.21941637992858887,
      "learning_rate": 0.00044457547169811325,
      "loss": 0.3876,
      "step": 1363000
    },
    {
      "epoch": 110.88295687885011,
      "grad_norm": 0.18830238282680511,
      "learning_rate": 0.00044455513988288876,
      "loss": 0.3879,
      "step": 1363500
    },
    {
      "epoch": 110.92361802915404,
      "grad_norm": 0.21079427003860474,
      "learning_rate": 0.0004445348080676643,
      "loss": 0.3876,
      "step": 1364000
    },
    {
      "epoch": 110.96427917945799,
      "grad_norm": 0.2080744057893753,
      "learning_rate": 0.00044451447625243985,
      "loss": 0.3877,
      "step": 1364500
    },
    {
      "epoch": 111.00494032976192,
      "grad_norm": 0.2315666377544403,
      "learning_rate": 0.00044449414443721537,
      "loss": 0.3875,
      "step": 1365000
    },
    {
      "epoch": 111.04560148006587,
      "grad_norm": 0.22783036530017853,
      "learning_rate": 0.0004444738126219909,
      "loss": 0.384,
      "step": 1365500
    },
    {
      "epoch": 111.08626263036982,
      "grad_norm": 0.18319590389728546,
      "learning_rate": 0.00044445348080676645,
      "loss": 0.3846,
      "step": 1366000
    },
    {
      "epoch": 111.12692378067375,
      "grad_norm": 0.22019487619400024,
      "learning_rate": 0.00044443314899154197,
      "loss": 0.385,
      "step": 1366500
    },
    {
      "epoch": 111.1675849309777,
      "grad_norm": 0.21702367067337036,
      "learning_rate": 0.0004444128171763175,
      "loss": 0.3852,
      "step": 1367000
    },
    {
      "epoch": 111.20824608128164,
      "grad_norm": 0.22005577385425568,
      "learning_rate": 0.00044439248536109306,
      "loss": 0.3857,
      "step": 1367500
    },
    {
      "epoch": 111.24890723158558,
      "grad_norm": 0.24362818896770477,
      "learning_rate": 0.00044437215354586857,
      "loss": 0.3854,
      "step": 1368000
    },
    {
      "epoch": 111.28956838188952,
      "grad_norm": 0.24952349066734314,
      "learning_rate": 0.00044435182173064414,
      "loss": 0.3853,
      "step": 1368500
    },
    {
      "epoch": 111.33022953219347,
      "grad_norm": 0.2761046290397644,
      "learning_rate": 0.00044433148991541966,
      "loss": 0.3859,
      "step": 1369000
    },
    {
      "epoch": 111.3708906824974,
      "grad_norm": 0.21833398938179016,
      "learning_rate": 0.0004443111581001952,
      "loss": 0.3862,
      "step": 1369500
    },
    {
      "epoch": 111.41155183280135,
      "grad_norm": 0.22584716975688934,
      "learning_rate": 0.00044429082628497075,
      "loss": 0.3864,
      "step": 1370000
    },
    {
      "epoch": 111.4522129831053,
      "grad_norm": 0.2056831568479538,
      "learning_rate": 0.00044427049446974626,
      "loss": 0.3864,
      "step": 1370500
    },
    {
      "epoch": 111.49287413340923,
      "grad_norm": 0.19534584879875183,
      "learning_rate": 0.0004442501626545218,
      "loss": 0.3865,
      "step": 1371000
    },
    {
      "epoch": 111.53353528371318,
      "grad_norm": 0.23419404029846191,
      "learning_rate": 0.00044422983083929735,
      "loss": 0.3867,
      "step": 1371500
    },
    {
      "epoch": 111.57419643401713,
      "grad_norm": 0.24720992147922516,
      "learning_rate": 0.00044420949902407287,
      "loss": 0.3864,
      "step": 1372000
    },
    {
      "epoch": 111.61485758432106,
      "grad_norm": 0.2472035139799118,
      "learning_rate": 0.0004441891672088484,
      "loss": 0.3872,
      "step": 1372500
    },
    {
      "epoch": 111.655518734625,
      "grad_norm": 0.244231179356575,
      "learning_rate": 0.00044416883539362395,
      "loss": 0.3875,
      "step": 1373000
    },
    {
      "epoch": 111.69617988492894,
      "grad_norm": 0.2388879805803299,
      "learning_rate": 0.00044414850357839947,
      "loss": 0.387,
      "step": 1373500
    },
    {
      "epoch": 111.73684103523289,
      "grad_norm": 0.24671898782253265,
      "learning_rate": 0.00044412817176317504,
      "loss": 0.3872,
      "step": 1374000
    },
    {
      "epoch": 111.77750218553683,
      "grad_norm": 0.24006809294223785,
      "learning_rate": 0.00044410783994795056,
      "loss": 0.3872,
      "step": 1374500
    },
    {
      "epoch": 111.81816333584077,
      "grad_norm": 0.20518858730793,
      "learning_rate": 0.0004440875081327261,
      "loss": 0.387,
      "step": 1375000
    },
    {
      "epoch": 111.85882448614471,
      "grad_norm": 0.25784432888031006,
      "learning_rate": 0.00044406717631750164,
      "loss": 0.3869,
      "step": 1375500
    },
    {
      "epoch": 111.89948563644866,
      "grad_norm": 0.21998274326324463,
      "learning_rate": 0.00044404684450227716,
      "loss": 0.3877,
      "step": 1376000
    },
    {
      "epoch": 111.94014678675259,
      "grad_norm": 0.19001157581806183,
      "learning_rate": 0.0004440265126870527,
      "loss": 0.3876,
      "step": 1376500
    },
    {
      "epoch": 111.98080793705654,
      "grad_norm": 0.23039233684539795,
      "learning_rate": 0.00044400618087182825,
      "loss": 0.3882,
      "step": 1377000
    },
    {
      "epoch": 112.02146908736049,
      "grad_norm": 0.24114078283309937,
      "learning_rate": 0.00044398584905660376,
      "loss": 0.3856,
      "step": 1377500
    },
    {
      "epoch": 112.06213023766442,
      "grad_norm": 0.2660077214241028,
      "learning_rate": 0.00044396551724137933,
      "loss": 0.3841,
      "step": 1378000
    },
    {
      "epoch": 112.10279138796837,
      "grad_norm": 0.2326534241437912,
      "learning_rate": 0.00044394518542615485,
      "loss": 0.3842,
      "step": 1378500
    },
    {
      "epoch": 112.14345253827231,
      "grad_norm": 0.21597933769226074,
      "learning_rate": 0.00044392485361093037,
      "loss": 0.3845,
      "step": 1379000
    },
    {
      "epoch": 112.18411368857625,
      "grad_norm": 0.20262207090854645,
      "learning_rate": 0.00044390452179570594,
      "loss": 0.3856,
      "step": 1379500
    },
    {
      "epoch": 112.2247748388802,
      "grad_norm": 0.2149229198694229,
      "learning_rate": 0.00044388418998048145,
      "loss": 0.3856,
      "step": 1380000
    },
    {
      "epoch": 112.26543598918414,
      "grad_norm": 0.2094842493534088,
      "learning_rate": 0.00044386385816525697,
      "loss": 0.3853,
      "step": 1380500
    },
    {
      "epoch": 112.30609713948807,
      "grad_norm": 0.19916406273841858,
      "learning_rate": 0.00044384352635003254,
      "loss": 0.3854,
      "step": 1381000
    },
    {
      "epoch": 112.34675828979202,
      "grad_norm": 0.23101606965065002,
      "learning_rate": 0.00044382319453480806,
      "loss": 0.3861,
      "step": 1381500
    },
    {
      "epoch": 112.38741944009595,
      "grad_norm": 0.20973306894302368,
      "learning_rate": 0.0004438028627195836,
      "loss": 0.3863,
      "step": 1382000
    },
    {
      "epoch": 112.4280805903999,
      "grad_norm": 0.22493967413902283,
      "learning_rate": 0.00044378253090435915,
      "loss": 0.3861,
      "step": 1382500
    },
    {
      "epoch": 112.46874174070385,
      "grad_norm": 0.21270844340324402,
      "learning_rate": 0.00044376219908913466,
      "loss": 0.3867,
      "step": 1383000
    },
    {
      "epoch": 112.50940289100778,
      "grad_norm": 0.22316095232963562,
      "learning_rate": 0.00044374186727391023,
      "loss": 0.3871,
      "step": 1383500
    },
    {
      "epoch": 112.55006404131173,
      "grad_norm": 0.2049732506275177,
      "learning_rate": 0.00044372153545868575,
      "loss": 0.3867,
      "step": 1384000
    },
    {
      "epoch": 112.59072519161568,
      "grad_norm": 0.19587835669517517,
      "learning_rate": 0.00044370120364346126,
      "loss": 0.3862,
      "step": 1384500
    },
    {
      "epoch": 112.63138634191961,
      "grad_norm": 0.23377074301242828,
      "learning_rate": 0.00044368087182823684,
      "loss": 0.3871,
      "step": 1385000
    },
    {
      "epoch": 112.67204749222356,
      "grad_norm": 0.21732288599014282,
      "learning_rate": 0.00044366054001301235,
      "loss": 0.3873,
      "step": 1385500
    },
    {
      "epoch": 112.7127086425275,
      "grad_norm": 0.21327683329582214,
      "learning_rate": 0.00044364020819778787,
      "loss": 0.3868,
      "step": 1386000
    },
    {
      "epoch": 112.75336979283144,
      "grad_norm": 0.2556439936161041,
      "learning_rate": 0.00044361987638256344,
      "loss": 0.3873,
      "step": 1386500
    },
    {
      "epoch": 112.79403094313538,
      "grad_norm": 0.1947292536497116,
      "learning_rate": 0.00044359954456733896,
      "loss": 0.3874,
      "step": 1387000
    },
    {
      "epoch": 112.83469209343933,
      "grad_norm": 0.22967898845672607,
      "learning_rate": 0.00044357921275211447,
      "loss": 0.3873,
      "step": 1387500
    },
    {
      "epoch": 112.87535324374326,
      "grad_norm": 0.23838289082050323,
      "learning_rate": 0.00044355888093689004,
      "loss": 0.387,
      "step": 1388000
    },
    {
      "epoch": 112.91601439404721,
      "grad_norm": 0.19867022335529327,
      "learning_rate": 0.00044353854912166556,
      "loss": 0.3879,
      "step": 1388500
    },
    {
      "epoch": 112.95667554435116,
      "grad_norm": 0.2287507802248001,
      "learning_rate": 0.00044351821730644113,
      "loss": 0.3875,
      "step": 1389000
    },
    {
      "epoch": 112.99733669465509,
      "grad_norm": 0.21902436017990112,
      "learning_rate": 0.00044349788549121665,
      "loss": 0.3878,
      "step": 1389500
    },
    {
      "epoch": 113.03799784495904,
      "grad_norm": 0.21818481385707855,
      "learning_rate": 0.00044347755367599216,
      "loss": 0.3847,
      "step": 1390000
    },
    {
      "epoch": 113.07865899526297,
      "grad_norm": 0.21715913712978363,
      "learning_rate": 0.00044345722186076773,
      "loss": 0.3837,
      "step": 1390500
    },
    {
      "epoch": 113.11932014556692,
      "grad_norm": 0.2212735116481781,
      "learning_rate": 0.00044343689004554325,
      "loss": 0.3847,
      "step": 1391000
    },
    {
      "epoch": 113.15998129587086,
      "grad_norm": 0.23819312453269958,
      "learning_rate": 0.00044341655823031877,
      "loss": 0.385,
      "step": 1391500
    },
    {
      "epoch": 113.2006424461748,
      "grad_norm": 0.2553780972957611,
      "learning_rate": 0.00044339622641509434,
      "loss": 0.385,
      "step": 1392000
    },
    {
      "epoch": 113.24130359647874,
      "grad_norm": 0.2365700751543045,
      "learning_rate": 0.00044337589459986985,
      "loss": 0.3851,
      "step": 1392500
    },
    {
      "epoch": 113.28196474678269,
      "grad_norm": 0.22739289700984955,
      "learning_rate": 0.0004433555627846454,
      "loss": 0.3855,
      "step": 1393000
    },
    {
      "epoch": 113.32262589708662,
      "grad_norm": 0.23705126345157623,
      "learning_rate": 0.00044333523096942094,
      "loss": 0.386,
      "step": 1393500
    },
    {
      "epoch": 113.36328704739057,
      "grad_norm": 0.24772676825523376,
      "learning_rate": 0.00044331489915419646,
      "loss": 0.3856,
      "step": 1394000
    },
    {
      "epoch": 113.40394819769452,
      "grad_norm": 0.22147487103939056,
      "learning_rate": 0.0004432945673389721,
      "loss": 0.3856,
      "step": 1394500
    },
    {
      "epoch": 113.44460934799845,
      "grad_norm": 0.22586210072040558,
      "learning_rate": 0.0004432742355237476,
      "loss": 0.3863,
      "step": 1395000
    },
    {
      "epoch": 113.4852704983024,
      "grad_norm": 0.24376310408115387,
      "learning_rate": 0.0004432539037085231,
      "loss": 0.3863,
      "step": 1395500
    },
    {
      "epoch": 113.52593164860635,
      "grad_norm": 0.21525950729846954,
      "learning_rate": 0.0004432335718932987,
      "loss": 0.3867,
      "step": 1396000
    },
    {
      "epoch": 113.56659279891028,
      "grad_norm": 0.2573041319847107,
      "learning_rate": 0.0004432132400780742,
      "loss": 0.3867,
      "step": 1396500
    },
    {
      "epoch": 113.60725394921423,
      "grad_norm": 0.2353949099779129,
      "learning_rate": 0.0004431929082628497,
      "loss": 0.3867,
      "step": 1397000
    },
    {
      "epoch": 113.64791509951816,
      "grad_norm": 0.22926081717014313,
      "learning_rate": 0.0004431725764476253,
      "loss": 0.3865,
      "step": 1397500
    },
    {
      "epoch": 113.6885762498221,
      "grad_norm": 0.21576480567455292,
      "learning_rate": 0.0004431522446324008,
      "loss": 0.3873,
      "step": 1398000
    },
    {
      "epoch": 113.72923740012605,
      "grad_norm": 0.21355943381786346,
      "learning_rate": 0.0004431319128171764,
      "loss": 0.3871,
      "step": 1398500
    },
    {
      "epoch": 113.76989855042999,
      "grad_norm": 0.20200957357883453,
      "learning_rate": 0.0004431115810019519,
      "loss": 0.3867,
      "step": 1399000
    },
    {
      "epoch": 113.81055970073393,
      "grad_norm": 0.1923532634973526,
      "learning_rate": 0.0004430912491867274,
      "loss": 0.3869,
      "step": 1399500
    },
    {
      "epoch": 113.85122085103788,
      "grad_norm": 0.22106920182704926,
      "learning_rate": 0.000443070917371503,
      "loss": 0.387,
      "step": 1400000
    },
    {
      "epoch": 113.89188200134181,
      "grad_norm": 0.23412713408470154,
      "learning_rate": 0.0004430505855562785,
      "loss": 0.3872,
      "step": 1400500
    },
    {
      "epoch": 113.93254315164576,
      "grad_norm": 0.22062523663043976,
      "learning_rate": 0.000443030253741054,
      "loss": 0.3877,
      "step": 1401000
    },
    {
      "epoch": 113.9732043019497,
      "grad_norm": 0.246613547205925,
      "learning_rate": 0.0004430099219258296,
      "loss": 0.3877,
      "step": 1401500
    },
    {
      "epoch": 114.01386545225364,
      "grad_norm": 0.2267560511827469,
      "learning_rate": 0.0004429895901106051,
      "loss": 0.386,
      "step": 1402000
    },
    {
      "epoch": 114.05452660255759,
      "grad_norm": 0.25910693407058716,
      "learning_rate": 0.0004429692582953806,
      "loss": 0.3838,
      "step": 1402500
    },
    {
      "epoch": 114.09518775286153,
      "grad_norm": 0.22433824837207794,
      "learning_rate": 0.0004429489264801562,
      "loss": 0.3847,
      "step": 1403000
    },
    {
      "epoch": 114.13584890316547,
      "grad_norm": 0.23186834156513214,
      "learning_rate": 0.0004429285946649317,
      "loss": 0.3847,
      "step": 1403500
    },
    {
      "epoch": 114.17651005346941,
      "grad_norm": 0.22105765342712402,
      "learning_rate": 0.0004429082628497073,
      "loss": 0.3846,
      "step": 1404000
    },
    {
      "epoch": 114.21717120377336,
      "grad_norm": 0.21194584667682648,
      "learning_rate": 0.0004428879310344828,
      "loss": 0.3854,
      "step": 1404500
    },
    {
      "epoch": 114.2578323540773,
      "grad_norm": 0.2386498749256134,
      "learning_rate": 0.0004428675992192583,
      "loss": 0.3851,
      "step": 1405000
    },
    {
      "epoch": 114.29849350438124,
      "grad_norm": 0.23179273307323456,
      "learning_rate": 0.0004428472674040339,
      "loss": 0.3854,
      "step": 1405500
    },
    {
      "epoch": 114.33915465468517,
      "grad_norm": 0.2135528326034546,
      "learning_rate": 0.0004428269355888094,
      "loss": 0.3859,
      "step": 1406000
    },
    {
      "epoch": 114.37981580498912,
      "grad_norm": 0.23254993557929993,
      "learning_rate": 0.0004428066037735849,
      "loss": 0.3853,
      "step": 1406500
    },
    {
      "epoch": 114.42047695529307,
      "grad_norm": 0.2035682052373886,
      "learning_rate": 0.0004427862719583605,
      "loss": 0.3863,
      "step": 1407000
    },
    {
      "epoch": 114.461138105597,
      "grad_norm": 0.24255599081516266,
      "learning_rate": 0.000442765940143136,
      "loss": 0.3865,
      "step": 1407500
    },
    {
      "epoch": 114.50179925590095,
      "grad_norm": 0.24643447995185852,
      "learning_rate": 0.0004427456083279115,
      "loss": 0.3865,
      "step": 1408000
    },
    {
      "epoch": 114.5424604062049,
      "grad_norm": 0.21014989912509918,
      "learning_rate": 0.0004427252765126871,
      "loss": 0.3866,
      "step": 1408500
    },
    {
      "epoch": 114.58312155650883,
      "grad_norm": 0.2347736358642578,
      "learning_rate": 0.0004427049446974626,
      "loss": 0.3864,
      "step": 1409000
    },
    {
      "epoch": 114.62378270681278,
      "grad_norm": 0.20129449665546417,
      "learning_rate": 0.00044268461288223817,
      "loss": 0.3863,
      "step": 1409500
    },
    {
      "epoch": 114.66444385711672,
      "grad_norm": 0.2220822125673294,
      "learning_rate": 0.0004426642810670137,
      "loss": 0.3866,
      "step": 1410000
    },
    {
      "epoch": 114.70510500742066,
      "grad_norm": 0.22828467190265656,
      "learning_rate": 0.0004426439492517892,
      "loss": 0.387,
      "step": 1410500
    },
    {
      "epoch": 114.7457661577246,
      "grad_norm": 0.23468342423439026,
      "learning_rate": 0.0004426236174365648,
      "loss": 0.3869,
      "step": 1411000
    },
    {
      "epoch": 114.78642730802855,
      "grad_norm": 0.19485464692115784,
      "learning_rate": 0.0004426032856213403,
      "loss": 0.387,
      "step": 1411500
    },
    {
      "epoch": 114.82708845833248,
      "grad_norm": 0.226848766207695,
      "learning_rate": 0.0004425829538061158,
      "loss": 0.387,
      "step": 1412000
    },
    {
      "epoch": 114.86774960863643,
      "grad_norm": 0.24496695399284363,
      "learning_rate": 0.0004425626219908914,
      "loss": 0.3873,
      "step": 1412500
    },
    {
      "epoch": 114.90841075894038,
      "grad_norm": 0.1942402422428131,
      "learning_rate": 0.0004425422901756669,
      "loss": 0.3874,
      "step": 1413000
    },
    {
      "epoch": 114.94907190924431,
      "grad_norm": 0.1796654611825943,
      "learning_rate": 0.00044252195836044246,
      "loss": 0.3871,
      "step": 1413500
    },
    {
      "epoch": 114.98973305954826,
      "grad_norm": 0.20444169640541077,
      "learning_rate": 0.000442501626545218,
      "loss": 0.3877,
      "step": 1414000
    },
    {
      "epoch": 115.03039420985219,
      "grad_norm": 0.19938936829566956,
      "learning_rate": 0.0004424812947299935,
      "loss": 0.3846,
      "step": 1414500
    },
    {
      "epoch": 115.07105536015614,
      "grad_norm": 0.2137089967727661,
      "learning_rate": 0.00044246096291476907,
      "loss": 0.3838,
      "step": 1415000
    },
    {
      "epoch": 115.11171651046008,
      "grad_norm": 0.21858073770999908,
      "learning_rate": 0.0004424406310995446,
      "loss": 0.3842,
      "step": 1415500
    },
    {
      "epoch": 115.15237766076402,
      "grad_norm": 0.2199511080980301,
      "learning_rate": 0.0004424202992843201,
      "loss": 0.3849,
      "step": 1416000
    },
    {
      "epoch": 115.19303881106796,
      "grad_norm": 0.22769272327423096,
      "learning_rate": 0.00044239996746909567,
      "loss": 0.3853,
      "step": 1416500
    },
    {
      "epoch": 115.23369996137191,
      "grad_norm": 0.26064422726631165,
      "learning_rate": 0.0004423796356538712,
      "loss": 0.3848,
      "step": 1417000
    },
    {
      "epoch": 115.27436111167584,
      "grad_norm": 0.21346402168273926,
      "learning_rate": 0.0004423593038386467,
      "loss": 0.3851,
      "step": 1417500
    },
    {
      "epoch": 115.31502226197979,
      "grad_norm": 0.20320433378219604,
      "learning_rate": 0.0004423389720234223,
      "loss": 0.3856,
      "step": 1418000
    },
    {
      "epoch": 115.35568341228374,
      "grad_norm": 0.2127152979373932,
      "learning_rate": 0.0004423186402081978,
      "loss": 0.3856,
      "step": 1418500
    },
    {
      "epoch": 115.39634456258767,
      "grad_norm": 0.24229247868061066,
      "learning_rate": 0.00044229830839297336,
      "loss": 0.3861,
      "step": 1419000
    },
    {
      "epoch": 115.43700571289162,
      "grad_norm": 0.230076402425766,
      "learning_rate": 0.0004422779765777489,
      "loss": 0.3859,
      "step": 1419500
    },
    {
      "epoch": 115.47766686319557,
      "grad_norm": 0.22602005302906036,
      "learning_rate": 0.0004422576447625244,
      "loss": 0.3865,
      "step": 1420000
    },
    {
      "epoch": 115.5183280134995,
      "grad_norm": 0.23475022614002228,
      "learning_rate": 0.00044223731294729997,
      "loss": 0.3864,
      "step": 1420500
    },
    {
      "epoch": 115.55898916380345,
      "grad_norm": 0.2187144160270691,
      "learning_rate": 0.0004422169811320755,
      "loss": 0.3867,
      "step": 1421000
    },
    {
      "epoch": 115.5996503141074,
      "grad_norm": 0.2310207039117813,
      "learning_rate": 0.000442196649316851,
      "loss": 0.3867,
      "step": 1421500
    },
    {
      "epoch": 115.64031146441133,
      "grad_norm": 0.20394296944141388,
      "learning_rate": 0.00044217631750162657,
      "loss": 0.3865,
      "step": 1422000
    },
    {
      "epoch": 115.68097261471527,
      "grad_norm": 0.2201021909713745,
      "learning_rate": 0.0004421559856864021,
      "loss": 0.3864,
      "step": 1422500
    },
    {
      "epoch": 115.7216337650192,
      "grad_norm": 0.2397475242614746,
      "learning_rate": 0.0004421356538711776,
      "loss": 0.3861,
      "step": 1423000
    },
    {
      "epoch": 115.76229491532315,
      "grad_norm": 0.2348865270614624,
      "learning_rate": 0.00044211532205595317,
      "loss": 0.3868,
      "step": 1423500
    },
    {
      "epoch": 115.8029560656271,
      "grad_norm": 0.233064666390419,
      "learning_rate": 0.0004420949902407287,
      "loss": 0.3867,
      "step": 1424000
    },
    {
      "epoch": 115.84361721593103,
      "grad_norm": 0.22592365741729736,
      "learning_rate": 0.00044207465842550426,
      "loss": 0.3872,
      "step": 1424500
    },
    {
      "epoch": 115.88427836623498,
      "grad_norm": 0.2285083532333374,
      "learning_rate": 0.0004420543266102798,
      "loss": 0.3869,
      "step": 1425000
    },
    {
      "epoch": 115.92493951653893,
      "grad_norm": 0.21695250272750854,
      "learning_rate": 0.0004420339947950553,
      "loss": 0.3871,
      "step": 1425500
    },
    {
      "epoch": 115.96560066684286,
      "grad_norm": 0.21700850129127502,
      "learning_rate": 0.00044201366297983086,
      "loss": 0.3878,
      "step": 1426000
    },
    {
      "epoch": 116.00626181714681,
      "grad_norm": 0.2346101850271225,
      "learning_rate": 0.0004419933311646064,
      "loss": 0.3871,
      "step": 1426500
    },
    {
      "epoch": 116.04692296745075,
      "grad_norm": 0.22551329433918,
      "learning_rate": 0.0004419729993493819,
      "loss": 0.3838,
      "step": 1427000
    },
    {
      "epoch": 116.08758411775469,
      "grad_norm": 0.21773824095726013,
      "learning_rate": 0.00044195266753415747,
      "loss": 0.3836,
      "step": 1427500
    },
    {
      "epoch": 116.12824526805863,
      "grad_norm": 0.23479820787906647,
      "learning_rate": 0.000441932335718933,
      "loss": 0.3846,
      "step": 1428000
    },
    {
      "epoch": 116.16890641836258,
      "grad_norm": 0.20407438278198242,
      "learning_rate": 0.00044191200390370855,
      "loss": 0.3842,
      "step": 1428500
    },
    {
      "epoch": 116.20956756866651,
      "grad_norm": 0.26320216059684753,
      "learning_rate": 0.00044189167208848407,
      "loss": 0.3851,
      "step": 1429000
    },
    {
      "epoch": 116.25022871897046,
      "grad_norm": 0.21498821675777435,
      "learning_rate": 0.0004418713402732596,
      "loss": 0.3853,
      "step": 1429500
    },
    {
      "epoch": 116.2908898692744,
      "grad_norm": 0.22420193254947662,
      "learning_rate": 0.00044185100845803516,
      "loss": 0.3855,
      "step": 1430000
    },
    {
      "epoch": 116.33155101957834,
      "grad_norm": 0.20582535862922668,
      "learning_rate": 0.0004418306766428107,
      "loss": 0.3855,
      "step": 1430500
    },
    {
      "epoch": 116.37221216988229,
      "grad_norm": 0.2195393145084381,
      "learning_rate": 0.0004418103448275862,
      "loss": 0.3856,
      "step": 1431000
    },
    {
      "epoch": 116.41287332018622,
      "grad_norm": 0.21078543365001678,
      "learning_rate": 0.00044179001301236176,
      "loss": 0.3859,
      "step": 1431500
    },
    {
      "epoch": 116.45353447049017,
      "grad_norm": 0.2260867804288864,
      "learning_rate": 0.0004417696811971373,
      "loss": 0.3862,
      "step": 1432000
    },
    {
      "epoch": 116.49419562079412,
      "grad_norm": 0.22754733264446259,
      "learning_rate": 0.0004417493493819128,
      "loss": 0.3859,
      "step": 1432500
    },
    {
      "epoch": 116.53485677109805,
      "grad_norm": 0.24120664596557617,
      "learning_rate": 0.00044172901756668836,
      "loss": 0.3867,
      "step": 1433000
    },
    {
      "epoch": 116.575517921402,
      "grad_norm": 0.23073634505271912,
      "learning_rate": 0.0004417086857514639,
      "loss": 0.3864,
      "step": 1433500
    },
    {
      "epoch": 116.61617907170594,
      "grad_norm": 0.262744277715683,
      "learning_rate": 0.00044168835393623945,
      "loss": 0.3861,
      "step": 1434000
    },
    {
      "epoch": 116.65684022200988,
      "grad_norm": 0.2036510407924652,
      "learning_rate": 0.00044166802212101497,
      "loss": 0.3864,
      "step": 1434500
    },
    {
      "epoch": 116.69750137231382,
      "grad_norm": 0.20807510614395142,
      "learning_rate": 0.0004416476903057905,
      "loss": 0.3861,
      "step": 1435000
    },
    {
      "epoch": 116.73816252261777,
      "grad_norm": 0.21198607981204987,
      "learning_rate": 0.00044162735849056605,
      "loss": 0.387,
      "step": 1435500
    },
    {
      "epoch": 116.7788236729217,
      "grad_norm": 0.23852115869522095,
      "learning_rate": 0.00044160702667534157,
      "loss": 0.3866,
      "step": 1436000
    },
    {
      "epoch": 116.81948482322565,
      "grad_norm": 0.19933216273784637,
      "learning_rate": 0.0004415866948601171,
      "loss": 0.3866,
      "step": 1436500
    },
    {
      "epoch": 116.8601459735296,
      "grad_norm": 0.20847441256046295,
      "learning_rate": 0.00044156636304489266,
      "loss": 0.3869,
      "step": 1437000
    },
    {
      "epoch": 116.90080712383353,
      "grad_norm": 0.2433832287788391,
      "learning_rate": 0.0004415460312296682,
      "loss": 0.3868,
      "step": 1437500
    },
    {
      "epoch": 116.94146827413748,
      "grad_norm": 0.20576518774032593,
      "learning_rate": 0.0004415256994144437,
      "loss": 0.3872,
      "step": 1438000
    },
    {
      "epoch": 116.98212942444141,
      "grad_norm": 0.2122567743062973,
      "learning_rate": 0.00044150536759921926,
      "loss": 0.3875,
      "step": 1438500
    },
    {
      "epoch": 117.02279057474536,
      "grad_norm": 0.2205926775932312,
      "learning_rate": 0.0004414850357839948,
      "loss": 0.385,
      "step": 1439000
    },
    {
      "epoch": 117.0634517250493,
      "grad_norm": 0.2243151068687439,
      "learning_rate": 0.00044146470396877035,
      "loss": 0.3838,
      "step": 1439500
    },
    {
      "epoch": 117.10411287535324,
      "grad_norm": 0.22698387503623962,
      "learning_rate": 0.00044144437215354587,
      "loss": 0.3844,
      "step": 1440000
    },
    {
      "epoch": 117.14477402565718,
      "grad_norm": 0.2109622359275818,
      "learning_rate": 0.0004414240403383214,
      "loss": 0.3845,
      "step": 1440500
    },
    {
      "epoch": 117.18543517596113,
      "grad_norm": 0.2510048449039459,
      "learning_rate": 0.00044140370852309695,
      "loss": 0.3848,
      "step": 1441000
    },
    {
      "epoch": 117.22609632626506,
      "grad_norm": 0.280943363904953,
      "learning_rate": 0.00044138337670787247,
      "loss": 0.3851,
      "step": 1441500
    },
    {
      "epoch": 117.26675747656901,
      "grad_norm": 0.22641880810260773,
      "learning_rate": 0.000441363044892648,
      "loss": 0.3852,
      "step": 1442000
    },
    {
      "epoch": 117.30741862687296,
      "grad_norm": 0.24385489523410797,
      "learning_rate": 0.00044134271307742356,
      "loss": 0.3851,
      "step": 1442500
    },
    {
      "epoch": 117.34807977717689,
      "grad_norm": 0.2123357057571411,
      "learning_rate": 0.00044132238126219907,
      "loss": 0.3851,
      "step": 1443000
    },
    {
      "epoch": 117.38874092748084,
      "grad_norm": 0.22612427175045013,
      "learning_rate": 0.00044130204944697464,
      "loss": 0.3853,
      "step": 1443500
    },
    {
      "epoch": 117.42940207778479,
      "grad_norm": 0.22537630796432495,
      "learning_rate": 0.00044128171763175016,
      "loss": 0.3854,
      "step": 1444000
    },
    {
      "epoch": 117.47006322808872,
      "grad_norm": 0.24980616569519043,
      "learning_rate": 0.0004412613858165257,
      "loss": 0.3858,
      "step": 1444500
    },
    {
      "epoch": 117.51072437839267,
      "grad_norm": 0.24838317930698395,
      "learning_rate": 0.00044124105400130125,
      "loss": 0.386,
      "step": 1445000
    },
    {
      "epoch": 117.55138552869661,
      "grad_norm": 0.21214459836483002,
      "learning_rate": 0.00044122072218607676,
      "loss": 0.3866,
      "step": 1445500
    },
    {
      "epoch": 117.59204667900055,
      "grad_norm": 0.2046087682247162,
      "learning_rate": 0.0004412003903708523,
      "loss": 0.3864,
      "step": 1446000
    },
    {
      "epoch": 117.6327078293045,
      "grad_norm": 0.22208353877067566,
      "learning_rate": 0.00044118005855562785,
      "loss": 0.3864,
      "step": 1446500
    },
    {
      "epoch": 117.67336897960843,
      "grad_norm": 0.2532939314842224,
      "learning_rate": 0.00044115972674040337,
      "loss": 0.3862,
      "step": 1447000
    },
    {
      "epoch": 117.71403012991237,
      "grad_norm": 0.21949592232704163,
      "learning_rate": 0.0004411393949251789,
      "loss": 0.3865,
      "step": 1447500
    },
    {
      "epoch": 117.75469128021632,
      "grad_norm": 0.19916966557502747,
      "learning_rate": 0.00044111906310995445,
      "loss": 0.387,
      "step": 1448000
    },
    {
      "epoch": 117.79535243052025,
      "grad_norm": 0.24581746757030487,
      "learning_rate": 0.00044109873129472997,
      "loss": 0.3864,
      "step": 1448500
    },
    {
      "epoch": 117.8360135808242,
      "grad_norm": 0.22474418580532074,
      "learning_rate": 0.00044107839947950554,
      "loss": 0.387,
      "step": 1449000
    },
    {
      "epoch": 117.87667473112815,
      "grad_norm": 0.23394273221492767,
      "learning_rate": 0.00044105806766428106,
      "loss": 0.3873,
      "step": 1449500
    },
    {
      "epoch": 117.91733588143208,
      "grad_norm": 0.24048173427581787,
      "learning_rate": 0.0004410377358490566,
      "loss": 0.3871,
      "step": 1450000
    },
    {
      "epoch": 117.95799703173603,
      "grad_norm": 0.20418742299079895,
      "learning_rate": 0.00044101740403383214,
      "loss": 0.3868,
      "step": 1450500
    },
    {
      "epoch": 117.99865818203997,
      "grad_norm": 0.230767622590065,
      "learning_rate": 0.00044099707221860766,
      "loss": 0.3872,
      "step": 1451000
    },
    {
      "epoch": 118.03931933234391,
      "grad_norm": 0.2332453727722168,
      "learning_rate": 0.0004409767404033832,
      "loss": 0.3832,
      "step": 1451500
    },
    {
      "epoch": 118.07998048264785,
      "grad_norm": 0.2302192598581314,
      "learning_rate": 0.0004409564085881588,
      "loss": 0.3833,
      "step": 1452000
    },
    {
      "epoch": 118.1206416329518,
      "grad_norm": 0.2396446317434311,
      "learning_rate": 0.0004409360767729343,
      "loss": 0.3842,
      "step": 1452500
    },
    {
      "epoch": 118.16130278325573,
      "grad_norm": 0.21202030777931213,
      "learning_rate": 0.00044091574495770983,
      "loss": 0.3842,
      "step": 1453000
    },
    {
      "epoch": 118.20196393355968,
      "grad_norm": 0.25289663672447205,
      "learning_rate": 0.0004408954131424854,
      "loss": 0.3843,
      "step": 1453500
    },
    {
      "epoch": 118.24262508386363,
      "grad_norm": 0.2066299021244049,
      "learning_rate": 0.0004408750813272609,
      "loss": 0.3852,
      "step": 1454000
    },
    {
      "epoch": 118.28328623416756,
      "grad_norm": 0.2526792585849762,
      "learning_rate": 0.0004408547495120365,
      "loss": 0.385,
      "step": 1454500
    },
    {
      "epoch": 118.32394738447151,
      "grad_norm": 0.2427423596382141,
      "learning_rate": 0.000440834417696812,
      "loss": 0.3851,
      "step": 1455000
    },
    {
      "epoch": 118.36460853477544,
      "grad_norm": 0.2731187045574188,
      "learning_rate": 0.0004408140858815875,
      "loss": 0.3851,
      "step": 1455500
    },
    {
      "epoch": 118.40526968507939,
      "grad_norm": 0.21648851037025452,
      "learning_rate": 0.0004407937540663631,
      "loss": 0.3856,
      "step": 1456000
    },
    {
      "epoch": 118.44593083538334,
      "grad_norm": 0.20492903888225555,
      "learning_rate": 0.0004407734222511386,
      "loss": 0.3854,
      "step": 1456500
    },
    {
      "epoch": 118.48659198568727,
      "grad_norm": 0.2878863513469696,
      "learning_rate": 0.00044075309043591413,
      "loss": 0.3859,
      "step": 1457000
    },
    {
      "epoch": 118.52725313599122,
      "grad_norm": 0.21353097259998322,
      "learning_rate": 0.0004407327586206897,
      "loss": 0.386,
      "step": 1457500
    },
    {
      "epoch": 118.56791428629516,
      "grad_norm": 0.23449553549289703,
      "learning_rate": 0.0004407124268054652,
      "loss": 0.386,
      "step": 1458000
    },
    {
      "epoch": 118.6085754365991,
      "grad_norm": 0.22870278358459473,
      "learning_rate": 0.00044069209499024073,
      "loss": 0.386,
      "step": 1458500
    },
    {
      "epoch": 118.64923658690304,
      "grad_norm": 0.2304779291152954,
      "learning_rate": 0.0004406717631750163,
      "loss": 0.3866,
      "step": 1459000
    },
    {
      "epoch": 118.68989773720699,
      "grad_norm": 0.21472302079200745,
      "learning_rate": 0.0004406514313597918,
      "loss": 0.3867,
      "step": 1459500
    },
    {
      "epoch": 118.73055888751092,
      "grad_norm": 0.25023630261421204,
      "learning_rate": 0.0004406310995445674,
      "loss": 0.3863,
      "step": 1460000
    },
    {
      "epoch": 118.77122003781487,
      "grad_norm": 0.22228120267391205,
      "learning_rate": 0.0004406107677293429,
      "loss": 0.3866,
      "step": 1460500
    },
    {
      "epoch": 118.81188118811882,
      "grad_norm": 0.2513159215450287,
      "learning_rate": 0.0004405904359141184,
      "loss": 0.3871,
      "step": 1461000
    },
    {
      "epoch": 118.85254233842275,
      "grad_norm": 0.21304237842559814,
      "learning_rate": 0.000440570104098894,
      "loss": 0.3868,
      "step": 1461500
    },
    {
      "epoch": 118.8932034887267,
      "grad_norm": 0.1881372630596161,
      "learning_rate": 0.0004405497722836695,
      "loss": 0.3868,
      "step": 1462000
    },
    {
      "epoch": 118.93386463903065,
      "grad_norm": 0.22465640306472778,
      "learning_rate": 0.000440529440468445,
      "loss": 0.387,
      "step": 1462500
    },
    {
      "epoch": 118.97452578933458,
      "grad_norm": 0.22612649202346802,
      "learning_rate": 0.0004405091086532206,
      "loss": 0.3875,
      "step": 1463000
    },
    {
      "epoch": 119.01518693963853,
      "grad_norm": 0.2030918300151825,
      "learning_rate": 0.0004404887768379961,
      "loss": 0.3853,
      "step": 1463500
    },
    {
      "epoch": 119.05584808994246,
      "grad_norm": 0.21581269800662994,
      "learning_rate": 0.0004404684450227717,
      "loss": 0.3835,
      "step": 1464000
    },
    {
      "epoch": 119.0965092402464,
      "grad_norm": 0.19990912079811096,
      "learning_rate": 0.0004404481132075472,
      "loss": 0.3839,
      "step": 1464500
    },
    {
      "epoch": 119.13717039055035,
      "grad_norm": 0.22818543016910553,
      "learning_rate": 0.0004404277813923227,
      "loss": 0.3845,
      "step": 1465000
    },
    {
      "epoch": 119.17783154085429,
      "grad_norm": 0.22192975878715515,
      "learning_rate": 0.0004404074495770983,
      "loss": 0.3841,
      "step": 1465500
    },
    {
      "epoch": 119.21849269115823,
      "grad_norm": 0.22377286851406097,
      "learning_rate": 0.0004403871177618738,
      "loss": 0.3843,
      "step": 1466000
    },
    {
      "epoch": 119.25915384146218,
      "grad_norm": 0.23623143136501312,
      "learning_rate": 0.0004403667859466493,
      "loss": 0.385,
      "step": 1466500
    },
    {
      "epoch": 119.29981499176611,
      "grad_norm": 0.2140379399061203,
      "learning_rate": 0.0004403464541314249,
      "loss": 0.3849,
      "step": 1467000
    },
    {
      "epoch": 119.34047614207006,
      "grad_norm": 0.30155321955680847,
      "learning_rate": 0.0004403261223162004,
      "loss": 0.3847,
      "step": 1467500
    },
    {
      "epoch": 119.381137292374,
      "grad_norm": 0.28111153841018677,
      "learning_rate": 0.0004403057905009759,
      "loss": 0.3852,
      "step": 1468000
    },
    {
      "epoch": 119.42179844267794,
      "grad_norm": 0.23637498915195465,
      "learning_rate": 0.0004402854586857515,
      "loss": 0.3853,
      "step": 1468500
    },
    {
      "epoch": 119.46245959298189,
      "grad_norm": 0.24207095801830292,
      "learning_rate": 0.000440265126870527,
      "loss": 0.3859,
      "step": 1469000
    },
    {
      "epoch": 119.50312074328583,
      "grad_norm": 0.21236659586429596,
      "learning_rate": 0.0004402447950553026,
      "loss": 0.3858,
      "step": 1469500
    },
    {
      "epoch": 119.54378189358977,
      "grad_norm": 0.22732356190681458,
      "learning_rate": 0.0004402244632400781,
      "loss": 0.3857,
      "step": 1470000
    },
    {
      "epoch": 119.58444304389371,
      "grad_norm": 0.22317492961883545,
      "learning_rate": 0.0004402041314248536,
      "loss": 0.3861,
      "step": 1470500
    },
    {
      "epoch": 119.62510419419766,
      "grad_norm": 0.20721065998077393,
      "learning_rate": 0.0004401837996096292,
      "loss": 0.3863,
      "step": 1471000
    },
    {
      "epoch": 119.6657653445016,
      "grad_norm": 0.2210649996995926,
      "learning_rate": 0.0004401634677944047,
      "loss": 0.3864,
      "step": 1471500
    },
    {
      "epoch": 119.70642649480554,
      "grad_norm": 0.27194058895111084,
      "learning_rate": 0.0004401431359791802,
      "loss": 0.3861,
      "step": 1472000
    },
    {
      "epoch": 119.74708764510947,
      "grad_norm": 0.24814490973949432,
      "learning_rate": 0.0004401228041639558,
      "loss": 0.3864,
      "step": 1472500
    },
    {
      "epoch": 119.78774879541342,
      "grad_norm": 0.20996473729610443,
      "learning_rate": 0.0004401024723487313,
      "loss": 0.3867,
      "step": 1473000
    },
    {
      "epoch": 119.82840994571737,
      "grad_norm": 0.25588688254356384,
      "learning_rate": 0.0004400821405335068,
      "loss": 0.3866,
      "step": 1473500
    },
    {
      "epoch": 119.8690710960213,
      "grad_norm": 0.23978863656520844,
      "learning_rate": 0.0004400618087182824,
      "loss": 0.3866,
      "step": 1474000
    },
    {
      "epoch": 119.90973224632525,
      "grad_norm": 0.22216716408729553,
      "learning_rate": 0.0004400414769030579,
      "loss": 0.3863,
      "step": 1474500
    },
    {
      "epoch": 119.9503933966292,
      "grad_norm": 0.22915595769882202,
      "learning_rate": 0.0004400211450878335,
      "loss": 0.387,
      "step": 1475000
    },
    {
      "epoch": 119.99105454693313,
      "grad_norm": 0.2241504043340683,
      "learning_rate": 0.000440000813272609,
      "loss": 0.387,
      "step": 1475500
    },
    {
      "epoch": 120.03171569723708,
      "grad_norm": 0.23978225886821747,
      "learning_rate": 0.0004399804814573845,
      "loss": 0.3842,
      "step": 1476000
    },
    {
      "epoch": 120.07237684754102,
      "grad_norm": 0.2370343655347824,
      "learning_rate": 0.0004399601496421601,
      "loss": 0.3832,
      "step": 1476500
    },
    {
      "epoch": 120.11303799784496,
      "grad_norm": 0.21154308319091797,
      "learning_rate": 0.0004399398178269356,
      "loss": 0.384,
      "step": 1477000
    },
    {
      "epoch": 120.1536991481489,
      "grad_norm": 0.21440492570400238,
      "learning_rate": 0.0004399194860117111,
      "loss": 0.3843,
      "step": 1477500
    },
    {
      "epoch": 120.19436029845285,
      "grad_norm": 0.20472438633441925,
      "learning_rate": 0.0004398991541964867,
      "loss": 0.3841,
      "step": 1478000
    },
    {
      "epoch": 120.23502144875678,
      "grad_norm": 0.21777783334255219,
      "learning_rate": 0.0004398788223812622,
      "loss": 0.3844,
      "step": 1478500
    },
    {
      "epoch": 120.27568259906073,
      "grad_norm": 0.22571198642253876,
      "learning_rate": 0.00043985849056603777,
      "loss": 0.385,
      "step": 1479000
    },
    {
      "epoch": 120.31634374936466,
      "grad_norm": 0.22129486501216888,
      "learning_rate": 0.0004398381587508133,
      "loss": 0.3845,
      "step": 1479500
    },
    {
      "epoch": 120.35700489966861,
      "grad_norm": 0.2328275889158249,
      "learning_rate": 0.0004398178269355888,
      "loss": 0.3854,
      "step": 1480000
    },
    {
      "epoch": 120.39766604997256,
      "grad_norm": 0.22683382034301758,
      "learning_rate": 0.0004397974951203644,
      "loss": 0.3849,
      "step": 1480500
    },
    {
      "epoch": 120.43832720027649,
      "grad_norm": 0.21765613555908203,
      "learning_rate": 0.0004397771633051399,
      "loss": 0.3858,
      "step": 1481000
    },
    {
      "epoch": 120.47898835058044,
      "grad_norm": 0.21186943352222443,
      "learning_rate": 0.0004397568314899154,
      "loss": 0.3853,
      "step": 1481500
    },
    {
      "epoch": 120.51964950088438,
      "grad_norm": 0.2545211911201477,
      "learning_rate": 0.000439736499674691,
      "loss": 0.3862,
      "step": 1482000
    },
    {
      "epoch": 120.56031065118832,
      "grad_norm": 0.22342990338802338,
      "learning_rate": 0.0004397161678594665,
      "loss": 0.386,
      "step": 1482500
    },
    {
      "epoch": 120.60097180149226,
      "grad_norm": 0.20633037388324738,
      "learning_rate": 0.000439695836044242,
      "loss": 0.3858,
      "step": 1483000
    },
    {
      "epoch": 120.64163295179621,
      "grad_norm": 0.26221221685409546,
      "learning_rate": 0.0004396755042290176,
      "loss": 0.3857,
      "step": 1483500
    },
    {
      "epoch": 120.68229410210014,
      "grad_norm": 0.22070635855197906,
      "learning_rate": 0.0004396551724137931,
      "loss": 0.3861,
      "step": 1484000
    },
    {
      "epoch": 120.72295525240409,
      "grad_norm": 0.24320240318775177,
      "learning_rate": 0.00043963484059856867,
      "loss": 0.3865,
      "step": 1484500
    },
    {
      "epoch": 120.76361640270804,
      "grad_norm": 0.1822638362646103,
      "learning_rate": 0.0004396145087833442,
      "loss": 0.3864,
      "step": 1485000
    },
    {
      "epoch": 120.80427755301197,
      "grad_norm": 0.2032185047864914,
      "learning_rate": 0.0004395941769681197,
      "loss": 0.3861,
      "step": 1485500
    },
    {
      "epoch": 120.84493870331592,
      "grad_norm": 0.21673166751861572,
      "learning_rate": 0.0004395738451528953,
      "loss": 0.3868,
      "step": 1486000
    },
    {
      "epoch": 120.88559985361987,
      "grad_norm": 0.24826978147029877,
      "learning_rate": 0.0004395535133376708,
      "loss": 0.3865,
      "step": 1486500
    },
    {
      "epoch": 120.9262610039238,
      "grad_norm": 0.23248036205768585,
      "learning_rate": 0.0004395331815224463,
      "loss": 0.3869,
      "step": 1487000
    },
    {
      "epoch": 120.96692215422775,
      "grad_norm": 0.2316686362028122,
      "learning_rate": 0.0004395128497072219,
      "loss": 0.3871,
      "step": 1487500
    },
    {
      "epoch": 121.00758330453168,
      "grad_norm": 0.23985281586647034,
      "learning_rate": 0.0004394925178919974,
      "loss": 0.3863,
      "step": 1488000
    },
    {
      "epoch": 121.04824445483563,
      "grad_norm": 0.23178711533546448,
      "learning_rate": 0.0004394721860767729,
      "loss": 0.3833,
      "step": 1488500
    },
    {
      "epoch": 121.08890560513957,
      "grad_norm": 0.24321183562278748,
      "learning_rate": 0.0004394518542615485,
      "loss": 0.3834,
      "step": 1489000
    },
    {
      "epoch": 121.1295667554435,
      "grad_norm": 0.23188498616218567,
      "learning_rate": 0.000439431522446324,
      "loss": 0.3841,
      "step": 1489500
    },
    {
      "epoch": 121.17022790574745,
      "grad_norm": 0.2132882922887802,
      "learning_rate": 0.00043941119063109957,
      "loss": 0.3841,
      "step": 1490000
    },
    {
      "epoch": 121.2108890560514,
      "grad_norm": 0.22757327556610107,
      "learning_rate": 0.0004393908588158751,
      "loss": 0.3839,
      "step": 1490500
    },
    {
      "epoch": 121.25155020635533,
      "grad_norm": 0.22448725998401642,
      "learning_rate": 0.0004393705270006506,
      "loss": 0.3846,
      "step": 1491000
    },
    {
      "epoch": 121.29221135665928,
      "grad_norm": 0.21243827044963837,
      "learning_rate": 0.00043935019518542617,
      "loss": 0.3846,
      "step": 1491500
    },
    {
      "epoch": 121.33287250696323,
      "grad_norm": 0.20865295827388763,
      "learning_rate": 0.0004393298633702017,
      "loss": 0.3849,
      "step": 1492000
    },
    {
      "epoch": 121.37353365726716,
      "grad_norm": 0.24879001080989838,
      "learning_rate": 0.0004393095315549772,
      "loss": 0.3857,
      "step": 1492500
    },
    {
      "epoch": 121.4141948075711,
      "grad_norm": 0.2624812424182892,
      "learning_rate": 0.0004392891997397528,
      "loss": 0.385,
      "step": 1493000
    },
    {
      "epoch": 121.45485595787505,
      "grad_norm": 0.22815577685832977,
      "learning_rate": 0.0004392688679245283,
      "loss": 0.3856,
      "step": 1493500
    },
    {
      "epoch": 121.49551710817899,
      "grad_norm": 0.19534188508987427,
      "learning_rate": 0.0004392485361093038,
      "loss": 0.3859,
      "step": 1494000
    },
    {
      "epoch": 121.53617825848293,
      "grad_norm": 0.22415375709533691,
      "learning_rate": 0.0004392282042940794,
      "loss": 0.3856,
      "step": 1494500
    },
    {
      "epoch": 121.57683940878688,
      "grad_norm": 0.23894476890563965,
      "learning_rate": 0.0004392078724788549,
      "loss": 0.3855,
      "step": 1495000
    },
    {
      "epoch": 121.61750055909081,
      "grad_norm": 0.23448501527309418,
      "learning_rate": 0.00043918754066363047,
      "loss": 0.3861,
      "step": 1495500
    },
    {
      "epoch": 121.65816170939476,
      "grad_norm": 0.23781874775886536,
      "learning_rate": 0.000439167208848406,
      "loss": 0.386,
      "step": 1496000
    },
    {
      "epoch": 121.6988228596987,
      "grad_norm": 0.21993866562843323,
      "learning_rate": 0.0004391468770331815,
      "loss": 0.3854,
      "step": 1496500
    },
    {
      "epoch": 121.73948401000264,
      "grad_norm": 0.24183648824691772,
      "learning_rate": 0.00043912654521795707,
      "loss": 0.3862,
      "step": 1497000
    },
    {
      "epoch": 121.78014516030659,
      "grad_norm": 0.22907620668411255,
      "learning_rate": 0.0004391062134027326,
      "loss": 0.3869,
      "step": 1497500
    },
    {
      "epoch": 121.82080631061052,
      "grad_norm": 0.24257433414459229,
      "learning_rate": 0.0004390858815875081,
      "loss": 0.3867,
      "step": 1498000
    },
    {
      "epoch": 121.86146746091447,
      "grad_norm": 0.2293250858783722,
      "learning_rate": 0.00043906554977228367,
      "loss": 0.3873,
      "step": 1498500
    },
    {
      "epoch": 121.90212861121842,
      "grad_norm": 0.23068909347057343,
      "learning_rate": 0.0004390452179570592,
      "loss": 0.3871,
      "step": 1499000
    },
    {
      "epoch": 121.94278976152235,
      "grad_norm": 0.23902882635593414,
      "learning_rate": 0.00043902488614183476,
      "loss": 0.3867,
      "step": 1499500
    },
    {
      "epoch": 121.9834509118263,
      "grad_norm": 0.20558984577655792,
      "learning_rate": 0.0004390045543266103,
      "loss": 0.3866,
      "step": 1500000
    },
    {
      "epoch": 122.02411206213024,
      "grad_norm": 0.21248963475227356,
      "learning_rate": 0.0004389842225113858,
      "loss": 0.3844,
      "step": 1500500
    },
    {
      "epoch": 122.06477321243418,
      "grad_norm": 0.23800206184387207,
      "learning_rate": 0.00043896389069616136,
      "loss": 0.3836,
      "step": 1501000
    },
    {
      "epoch": 122.10543436273812,
      "grad_norm": 0.247499018907547,
      "learning_rate": 0.0004389435588809369,
      "loss": 0.3839,
      "step": 1501500
    },
    {
      "epoch": 122.14609551304207,
      "grad_norm": 0.21301347017288208,
      "learning_rate": 0.0004389232270657124,
      "loss": 0.3837,
      "step": 1502000
    },
    {
      "epoch": 122.186756663346,
      "grad_norm": 0.2358551323413849,
      "learning_rate": 0.00043890289525048797,
      "loss": 0.3842,
      "step": 1502500
    },
    {
      "epoch": 122.22741781364995,
      "grad_norm": 0.2178199142217636,
      "learning_rate": 0.0004388825634352635,
      "loss": 0.3847,
      "step": 1503000
    },
    {
      "epoch": 122.2680789639539,
      "grad_norm": 0.21381478011608124,
      "learning_rate": 0.000438862231620039,
      "loss": 0.3845,
      "step": 1503500
    },
    {
      "epoch": 122.30874011425783,
      "grad_norm": 0.25858619809150696,
      "learning_rate": 0.00043884189980481457,
      "loss": 0.3845,
      "step": 1504000
    },
    {
      "epoch": 122.34940126456178,
      "grad_norm": 0.21618841588497162,
      "learning_rate": 0.0004388215679895901,
      "loss": 0.3851,
      "step": 1504500
    },
    {
      "epoch": 122.39006241486571,
      "grad_norm": 0.22362510859966278,
      "learning_rate": 0.00043880123617436566,
      "loss": 0.3848,
      "step": 1505000
    },
    {
      "epoch": 122.43072356516966,
      "grad_norm": 0.24223095178604126,
      "learning_rate": 0.0004387809043591412,
      "loss": 0.385,
      "step": 1505500
    },
    {
      "epoch": 122.4713847154736,
      "grad_norm": 0.22422046959400177,
      "learning_rate": 0.0004387605725439167,
      "loss": 0.3858,
      "step": 1506000
    },
    {
      "epoch": 122.51204586577754,
      "grad_norm": 0.2516544461250305,
      "learning_rate": 0.00043874024072869226,
      "loss": 0.386,
      "step": 1506500
    },
    {
      "epoch": 122.55270701608148,
      "grad_norm": 0.22225360572338104,
      "learning_rate": 0.0004387199089134678,
      "loss": 0.3855,
      "step": 1507000
    },
    {
      "epoch": 122.59336816638543,
      "grad_norm": 0.2304384857416153,
      "learning_rate": 0.0004386995770982433,
      "loss": 0.3861,
      "step": 1507500
    },
    {
      "epoch": 122.63402931668936,
      "grad_norm": 0.22775159776210785,
      "learning_rate": 0.00043867924528301886,
      "loss": 0.3857,
      "step": 1508000
    },
    {
      "epoch": 122.67469046699331,
      "grad_norm": 0.22481143474578857,
      "learning_rate": 0.0004386589134677944,
      "loss": 0.3861,
      "step": 1508500
    },
    {
      "epoch": 122.71535161729726,
      "grad_norm": 0.20437224209308624,
      "learning_rate": 0.0004386385816525699,
      "loss": 0.3864,
      "step": 1509000
    },
    {
      "epoch": 122.75601276760119,
      "grad_norm": 0.2575422525405884,
      "learning_rate": 0.0004386182498373455,
      "loss": 0.3863,
      "step": 1509500
    },
    {
      "epoch": 122.79667391790514,
      "grad_norm": 0.23913362622261047,
      "learning_rate": 0.00043859791802212104,
      "loss": 0.3861,
      "step": 1510000
    },
    {
      "epoch": 122.83733506820909,
      "grad_norm": 0.21540915966033936,
      "learning_rate": 0.0004385775862068966,
      "loss": 0.3863,
      "step": 1510500
    },
    {
      "epoch": 122.87799621851302,
      "grad_norm": 0.217588409781456,
      "learning_rate": 0.0004385572543916721,
      "loss": 0.3867,
      "step": 1511000
    },
    {
      "epoch": 122.91865736881697,
      "grad_norm": 0.2339443415403366,
      "learning_rate": 0.00043853692257644764,
      "loss": 0.3866,
      "step": 1511500
    },
    {
      "epoch": 122.9593185191209,
      "grad_norm": 0.22634585201740265,
      "learning_rate": 0.0004385165907612232,
      "loss": 0.3865,
      "step": 1512000
    },
    {
      "epoch": 122.99997966942485,
      "grad_norm": 0.21270032227039337,
      "learning_rate": 0.00043849625894599873,
      "loss": 0.3863,
      "step": 1512500
    },
    {
      "epoch": 123.0406408197288,
      "grad_norm": 0.24081185460090637,
      "learning_rate": 0.00043847592713077424,
      "loss": 0.3831,
      "step": 1513000
    },
    {
      "epoch": 123.08130197003273,
      "grad_norm": 0.2231844812631607,
      "learning_rate": 0.0004384555953155498,
      "loss": 0.3833,
      "step": 1513500
    },
    {
      "epoch": 123.12196312033667,
      "grad_norm": 0.25371015071868896,
      "learning_rate": 0.00043843526350032533,
      "loss": 0.3833,
      "step": 1514000
    },
    {
      "epoch": 123.16262427064062,
      "grad_norm": 0.23516136407852173,
      "learning_rate": 0.0004384149316851009,
      "loss": 0.3844,
      "step": 1514500
    },
    {
      "epoch": 123.20328542094455,
      "grad_norm": 0.22248475253582,
      "learning_rate": 0.0004383945998698764,
      "loss": 0.3838,
      "step": 1515000
    },
    {
      "epoch": 123.2439465712485,
      "grad_norm": 0.2318653166294098,
      "learning_rate": 0.00043837426805465194,
      "loss": 0.3845,
      "step": 1515500
    },
    {
      "epoch": 123.28460772155245,
      "grad_norm": 0.2272542268037796,
      "learning_rate": 0.0004383539362394275,
      "loss": 0.3844,
      "step": 1516000
    },
    {
      "epoch": 123.32526887185638,
      "grad_norm": 0.2281697392463684,
      "learning_rate": 0.000438333604424203,
      "loss": 0.3849,
      "step": 1516500
    },
    {
      "epoch": 123.36593002216033,
      "grad_norm": 0.26699888706207275,
      "learning_rate": 0.00043831327260897854,
      "loss": 0.3849,
      "step": 1517000
    },
    {
      "epoch": 123.40659117246427,
      "grad_norm": 0.2510722875595093,
      "learning_rate": 0.0004382929407937541,
      "loss": 0.3848,
      "step": 1517500
    },
    {
      "epoch": 123.44725232276821,
      "grad_norm": 0.19971498847007751,
      "learning_rate": 0.0004382726089785296,
      "loss": 0.3853,
      "step": 1518000
    },
    {
      "epoch": 123.48791347307215,
      "grad_norm": 0.23265235126018524,
      "learning_rate": 0.00043825227716330514,
      "loss": 0.3857,
      "step": 1518500
    },
    {
      "epoch": 123.5285746233761,
      "grad_norm": 0.23608312010765076,
      "learning_rate": 0.0004382319453480807,
      "loss": 0.3855,
      "step": 1519000
    },
    {
      "epoch": 123.56923577368003,
      "grad_norm": 0.21238915622234344,
      "learning_rate": 0.00043821161353285623,
      "loss": 0.386,
      "step": 1519500
    },
    {
      "epoch": 123.60989692398398,
      "grad_norm": 0.21086840331554413,
      "learning_rate": 0.0004381912817176318,
      "loss": 0.3855,
      "step": 1520000
    },
    {
      "epoch": 123.65055807428791,
      "grad_norm": 0.21039249002933502,
      "learning_rate": 0.0004381709499024073,
      "loss": 0.3859,
      "step": 1520500
    },
    {
      "epoch": 123.69121922459186,
      "grad_norm": 0.23049473762512207,
      "learning_rate": 0.00043815061808718283,
      "loss": 0.386,
      "step": 1521000
    },
    {
      "epoch": 123.73188037489581,
      "grad_norm": 0.2154548615217209,
      "learning_rate": 0.0004381302862719584,
      "loss": 0.3857,
      "step": 1521500
    },
    {
      "epoch": 123.77254152519974,
      "grad_norm": 0.2317715585231781,
      "learning_rate": 0.0004381099544567339,
      "loss": 0.3862,
      "step": 1522000
    },
    {
      "epoch": 123.81320267550369,
      "grad_norm": 0.21590429544448853,
      "learning_rate": 0.00043808962264150944,
      "loss": 0.3857,
      "step": 1522500
    },
    {
      "epoch": 123.85386382580764,
      "grad_norm": 0.23531126976013184,
      "learning_rate": 0.000438069290826285,
      "loss": 0.3865,
      "step": 1523000
    },
    {
      "epoch": 123.89452497611157,
      "grad_norm": 0.20968791842460632,
      "learning_rate": 0.0004380489590110605,
      "loss": 0.3865,
      "step": 1523500
    },
    {
      "epoch": 123.93518612641552,
      "grad_norm": 0.21858489513397217,
      "learning_rate": 0.00043802862719583604,
      "loss": 0.3863,
      "step": 1524000
    },
    {
      "epoch": 123.97584727671946,
      "grad_norm": 0.2228679358959198,
      "learning_rate": 0.0004380082953806116,
      "loss": 0.3866,
      "step": 1524500
    },
    {
      "epoch": 124.0165084270234,
      "grad_norm": 0.22741425037384033,
      "learning_rate": 0.0004379879635653871,
      "loss": 0.385,
      "step": 1525000
    },
    {
      "epoch": 124.05716957732734,
      "grad_norm": 0.23020455241203308,
      "learning_rate": 0.0004379676317501627,
      "loss": 0.3828,
      "step": 1525500
    },
    {
      "epoch": 124.09783072763129,
      "grad_norm": 0.2513996958732605,
      "learning_rate": 0.0004379472999349382,
      "loss": 0.3829,
      "step": 1526000
    },
    {
      "epoch": 124.13849187793522,
      "grad_norm": 0.25355252623558044,
      "learning_rate": 0.00043792696811971373,
      "loss": 0.3835,
      "step": 1526500
    },
    {
      "epoch": 124.17915302823917,
      "grad_norm": 0.25855720043182373,
      "learning_rate": 0.0004379066363044893,
      "loss": 0.3839,
      "step": 1527000
    },
    {
      "epoch": 124.21981417854312,
      "grad_norm": 0.2245916724205017,
      "learning_rate": 0.0004378863044892648,
      "loss": 0.3839,
      "step": 1527500
    },
    {
      "epoch": 124.26047532884705,
      "grad_norm": 0.2553561329841614,
      "learning_rate": 0.00043786597267404033,
      "loss": 0.3846,
      "step": 1528000
    },
    {
      "epoch": 124.301136479151,
      "grad_norm": 0.1950780153274536,
      "learning_rate": 0.0004378456408588159,
      "loss": 0.3847,
      "step": 1528500
    },
    {
      "epoch": 124.34179762945493,
      "grad_norm": 0.24514348804950714,
      "learning_rate": 0.0004378253090435914,
      "loss": 0.3847,
      "step": 1529000
    },
    {
      "epoch": 124.38245877975888,
      "grad_norm": 0.2244201898574829,
      "learning_rate": 0.000437804977228367,
      "loss": 0.3851,
      "step": 1529500
    },
    {
      "epoch": 124.42311993006282,
      "grad_norm": 0.22535237669944763,
      "learning_rate": 0.0004377846454131425,
      "loss": 0.3848,
      "step": 1530000
    },
    {
      "epoch": 124.46378108036676,
      "grad_norm": 0.2169201672077179,
      "learning_rate": 0.000437764313597918,
      "loss": 0.3858,
      "step": 1530500
    },
    {
      "epoch": 124.5044422306707,
      "grad_norm": 0.22757716476917267,
      "learning_rate": 0.0004377439817826936,
      "loss": 0.3855,
      "step": 1531000
    },
    {
      "epoch": 124.54510338097465,
      "grad_norm": 0.2538015842437744,
      "learning_rate": 0.0004377236499674691,
      "loss": 0.3854,
      "step": 1531500
    },
    {
      "epoch": 124.58576453127858,
      "grad_norm": 0.2520930767059326,
      "learning_rate": 0.00043770331815224463,
      "loss": 0.3856,
      "step": 1532000
    },
    {
      "epoch": 124.62642568158253,
      "grad_norm": 0.2268054485321045,
      "learning_rate": 0.0004376829863370202,
      "loss": 0.3855,
      "step": 1532500
    },
    {
      "epoch": 124.66708683188648,
      "grad_norm": 0.2377472072839737,
      "learning_rate": 0.0004376626545217957,
      "loss": 0.386,
      "step": 1533000
    },
    {
      "epoch": 124.70774798219041,
      "grad_norm": 0.21142542362213135,
      "learning_rate": 0.00043764232270657123,
      "loss": 0.3856,
      "step": 1533500
    },
    {
      "epoch": 124.74840913249436,
      "grad_norm": 0.22256788611412048,
      "learning_rate": 0.0004376219908913468,
      "loss": 0.3865,
      "step": 1534000
    },
    {
      "epoch": 124.7890702827983,
      "grad_norm": 0.2092246413230896,
      "learning_rate": 0.0004376016590761223,
      "loss": 0.3858,
      "step": 1534500
    },
    {
      "epoch": 124.82973143310224,
      "grad_norm": 0.2374972552061081,
      "learning_rate": 0.0004375813272608979,
      "loss": 0.3864,
      "step": 1535000
    },
    {
      "epoch": 124.87039258340619,
      "grad_norm": 0.2526875436306,
      "learning_rate": 0.0004375609954456734,
      "loss": 0.3865,
      "step": 1535500
    },
    {
      "epoch": 124.91105373371013,
      "grad_norm": 0.23852641880512238,
      "learning_rate": 0.0004375406636304489,
      "loss": 0.3864,
      "step": 1536000
    },
    {
      "epoch": 124.95171488401407,
      "grad_norm": 0.22085224092006683,
      "learning_rate": 0.0004375203318152245,
      "loss": 0.3861,
      "step": 1536500
    },
    {
      "epoch": 124.99237603431801,
      "grad_norm": 0.2180221676826477,
      "learning_rate": 0.0004375,
      "loss": 0.3868,
      "step": 1537000
    },
    {
      "epoch": 125.03303718462195,
      "grad_norm": 0.23733963072299957,
      "learning_rate": 0.0004374796681847755,
      "loss": 0.3834,
      "step": 1537500
    },
    {
      "epoch": 125.0736983349259,
      "grad_norm": 0.22921471297740936,
      "learning_rate": 0.0004374593363695511,
      "loss": 0.3833,
      "step": 1538000
    },
    {
      "epoch": 125.11435948522984,
      "grad_norm": 0.21960239112377167,
      "learning_rate": 0.0004374390045543266,
      "loss": 0.3831,
      "step": 1538500
    },
    {
      "epoch": 125.15502063553377,
      "grad_norm": 0.24594731628894806,
      "learning_rate": 0.00043741867273910213,
      "loss": 0.3835,
      "step": 1539000
    },
    {
      "epoch": 125.19568178583772,
      "grad_norm": 0.23109179735183716,
      "learning_rate": 0.0004373983409238777,
      "loss": 0.3839,
      "step": 1539500
    },
    {
      "epoch": 125.23634293614167,
      "grad_norm": 0.2615276575088501,
      "learning_rate": 0.0004373780091086532,
      "loss": 0.384,
      "step": 1540000
    },
    {
      "epoch": 125.2770040864456,
      "grad_norm": 0.2227010428905487,
      "learning_rate": 0.0004373576772934288,
      "loss": 0.3842,
      "step": 1540500
    },
    {
      "epoch": 125.31766523674955,
      "grad_norm": 0.24716335535049438,
      "learning_rate": 0.0004373373454782043,
      "loss": 0.3846,
      "step": 1541000
    },
    {
      "epoch": 125.3583263870535,
      "grad_norm": 0.21824905276298523,
      "learning_rate": 0.0004373170136629798,
      "loss": 0.3848,
      "step": 1541500
    },
    {
      "epoch": 125.39898753735743,
      "grad_norm": 0.22584855556488037,
      "learning_rate": 0.0004372966818477554,
      "loss": 0.3849,
      "step": 1542000
    },
    {
      "epoch": 125.43964868766137,
      "grad_norm": 0.22294512391090393,
      "learning_rate": 0.0004372763500325309,
      "loss": 0.3853,
      "step": 1542500
    },
    {
      "epoch": 125.48030983796532,
      "grad_norm": 0.22611971199512482,
      "learning_rate": 0.0004372560182173064,
      "loss": 0.3848,
      "step": 1543000
    },
    {
      "epoch": 125.52097098826926,
      "grad_norm": 0.2596164643764496,
      "learning_rate": 0.000437235686402082,
      "loss": 0.3853,
      "step": 1543500
    },
    {
      "epoch": 125.5616321385732,
      "grad_norm": 0.22636909782886505,
      "learning_rate": 0.0004372153545868575,
      "loss": 0.385,
      "step": 1544000
    },
    {
      "epoch": 125.60229328887715,
      "grad_norm": 0.23977701365947723,
      "learning_rate": 0.000437195022771633,
      "loss": 0.3856,
      "step": 1544500
    },
    {
      "epoch": 125.64295443918108,
      "grad_norm": 0.2095993310213089,
      "learning_rate": 0.0004371746909564086,
      "loss": 0.3857,
      "step": 1545000
    },
    {
      "epoch": 125.68361558948503,
      "grad_norm": 0.2222803682088852,
      "learning_rate": 0.0004371543591411841,
      "loss": 0.3859,
      "step": 1545500
    },
    {
      "epoch": 125.72427673978896,
      "grad_norm": 0.2805376648902893,
      "learning_rate": 0.0004371340273259597,
      "loss": 0.3855,
      "step": 1546000
    },
    {
      "epoch": 125.76493789009291,
      "grad_norm": 0.25738513469696045,
      "learning_rate": 0.0004371136955107352,
      "loss": 0.3861,
      "step": 1546500
    },
    {
      "epoch": 125.80559904039686,
      "grad_norm": 0.2745586037635803,
      "learning_rate": 0.0004370933636955107,
      "loss": 0.3859,
      "step": 1547000
    },
    {
      "epoch": 125.84626019070079,
      "grad_norm": 0.25937554240226746,
      "learning_rate": 0.0004370730318802863,
      "loss": 0.3859,
      "step": 1547500
    },
    {
      "epoch": 125.88692134100474,
      "grad_norm": 0.23254846036434174,
      "learning_rate": 0.0004370527000650618,
      "loss": 0.3865,
      "step": 1548000
    },
    {
      "epoch": 125.92758249130868,
      "grad_norm": 0.2038465291261673,
      "learning_rate": 0.0004370323682498373,
      "loss": 0.3867,
      "step": 1548500
    },
    {
      "epoch": 125.96824364161262,
      "grad_norm": 0.23871617019176483,
      "learning_rate": 0.0004370120364346129,
      "loss": 0.3863,
      "step": 1549000
    },
    {
      "epoch": 126.00890479191656,
      "grad_norm": 0.22500823438167572,
      "learning_rate": 0.0004369917046193884,
      "loss": 0.3855,
      "step": 1549500
    },
    {
      "epoch": 126.04956594222051,
      "grad_norm": 0.24628789722919464,
      "learning_rate": 0.000436971372804164,
      "loss": 0.3828,
      "step": 1550000
    },
    {
      "epoch": 126.09022709252444,
      "grad_norm": 0.21816839277744293,
      "learning_rate": 0.0004369510409889395,
      "loss": 0.3831,
      "step": 1550500
    },
    {
      "epoch": 126.13088824282839,
      "grad_norm": 0.25060996413230896,
      "learning_rate": 0.000436930709173715,
      "loss": 0.3836,
      "step": 1551000
    },
    {
      "epoch": 126.17154939313234,
      "grad_norm": 0.23947305977344513,
      "learning_rate": 0.0004369103773584906,
      "loss": 0.3836,
      "step": 1551500
    },
    {
      "epoch": 126.21221054343627,
      "grad_norm": 0.2247600555419922,
      "learning_rate": 0.0004368900455432661,
      "loss": 0.3841,
      "step": 1552000
    },
    {
      "epoch": 126.25287169374022,
      "grad_norm": 0.24836142361164093,
      "learning_rate": 0.0004368697137280416,
      "loss": 0.384,
      "step": 1552500
    },
    {
      "epoch": 126.29353284404417,
      "grad_norm": 0.281691312789917,
      "learning_rate": 0.0004368493819128172,
      "loss": 0.3845,
      "step": 1553000
    },
    {
      "epoch": 126.3341939943481,
      "grad_norm": 0.2442067414522171,
      "learning_rate": 0.0004368290500975927,
      "loss": 0.3847,
      "step": 1553500
    },
    {
      "epoch": 126.37485514465205,
      "grad_norm": 0.22538381814956665,
      "learning_rate": 0.0004368087182823682,
      "loss": 0.3848,
      "step": 1554000
    },
    {
      "epoch": 126.41551629495598,
      "grad_norm": 0.23818717896938324,
      "learning_rate": 0.0004367883864671438,
      "loss": 0.385,
      "step": 1554500
    },
    {
      "epoch": 126.45617744525993,
      "grad_norm": 0.25701290369033813,
      "learning_rate": 0.0004367680546519193,
      "loss": 0.3848,
      "step": 1555000
    },
    {
      "epoch": 126.49683859556387,
      "grad_norm": 0.2516903877258301,
      "learning_rate": 0.0004367477228366949,
      "loss": 0.3851,
      "step": 1555500
    },
    {
      "epoch": 126.5374997458678,
      "grad_norm": 0.2503891885280609,
      "learning_rate": 0.0004367273910214704,
      "loss": 0.385,
      "step": 1556000
    },
    {
      "epoch": 126.57816089617175,
      "grad_norm": 0.21182134747505188,
      "learning_rate": 0.0004367070592062459,
      "loss": 0.3857,
      "step": 1556500
    },
    {
      "epoch": 126.6188220464757,
      "grad_norm": 0.20448920130729675,
      "learning_rate": 0.0004366867273910215,
      "loss": 0.3853,
      "step": 1557000
    },
    {
      "epoch": 126.65948319677963,
      "grad_norm": 0.23950393497943878,
      "learning_rate": 0.000436666395575797,
      "loss": 0.3855,
      "step": 1557500
    },
    {
      "epoch": 126.70014434708358,
      "grad_norm": 0.231531023979187,
      "learning_rate": 0.0004366460637605725,
      "loss": 0.3853,
      "step": 1558000
    },
    {
      "epoch": 126.74080549738753,
      "grad_norm": 0.24418774247169495,
      "learning_rate": 0.0004366257319453481,
      "loss": 0.3858,
      "step": 1558500
    },
    {
      "epoch": 126.78146664769146,
      "grad_norm": 0.23160183429718018,
      "learning_rate": 0.0004366054001301236,
      "loss": 0.3861,
      "step": 1559000
    },
    {
      "epoch": 126.8221277979954,
      "grad_norm": 0.2139647901058197,
      "learning_rate": 0.0004365850683148991,
      "loss": 0.3864,
      "step": 1559500
    },
    {
      "epoch": 126.86278894829935,
      "grad_norm": 0.23145630955696106,
      "learning_rate": 0.0004365647364996747,
      "loss": 0.3864,
      "step": 1560000
    },
    {
      "epoch": 126.90345009860329,
      "grad_norm": 0.2501121759414673,
      "learning_rate": 0.0004365444046844502,
      "loss": 0.3861,
      "step": 1560500
    },
    {
      "epoch": 126.94411124890723,
      "grad_norm": 0.20778325200080872,
      "learning_rate": 0.0004365240728692258,
      "loss": 0.3863,
      "step": 1561000
    },
    {
      "epoch": 126.98477239921117,
      "grad_norm": 0.23721404373645782,
      "learning_rate": 0.0004365037410540013,
      "loss": 0.386,
      "step": 1561500
    },
    {
      "epoch": 127.02543354951511,
      "grad_norm": 0.2334403097629547,
      "learning_rate": 0.0004364834092387768,
      "loss": 0.3838,
      "step": 1562000
    },
    {
      "epoch": 127.06609469981906,
      "grad_norm": 0.2364717274904251,
      "learning_rate": 0.0004364630774235524,
      "loss": 0.3829,
      "step": 1562500
    },
    {
      "epoch": 127.106755850123,
      "grad_norm": 0.24568819999694824,
      "learning_rate": 0.0004364427456083279,
      "loss": 0.3833,
      "step": 1563000
    },
    {
      "epoch": 127.14741700042694,
      "grad_norm": 0.20638461410999298,
      "learning_rate": 0.0004364224137931034,
      "loss": 0.3835,
      "step": 1563500
    },
    {
      "epoch": 127.18807815073089,
      "grad_norm": 0.21846753358840942,
      "learning_rate": 0.000436402081977879,
      "loss": 0.384,
      "step": 1564000
    },
    {
      "epoch": 127.22873930103482,
      "grad_norm": 0.25573551654815674,
      "learning_rate": 0.0004363817501626545,
      "loss": 0.3842,
      "step": 1564500
    },
    {
      "epoch": 127.26940045133877,
      "grad_norm": 0.2256564050912857,
      "learning_rate": 0.0004363614183474301,
      "loss": 0.384,
      "step": 1565000
    },
    {
      "epoch": 127.31006160164272,
      "grad_norm": 0.23653842508792877,
      "learning_rate": 0.00043634108653220564,
      "loss": 0.3842,
      "step": 1565500
    },
    {
      "epoch": 127.35072275194665,
      "grad_norm": 0.2032802850008011,
      "learning_rate": 0.00043632075471698115,
      "loss": 0.3843,
      "step": 1566000
    },
    {
      "epoch": 127.3913839022506,
      "grad_norm": 0.2397645115852356,
      "learning_rate": 0.0004363004229017567,
      "loss": 0.3845,
      "step": 1566500
    },
    {
      "epoch": 127.43204505255454,
      "grad_norm": 0.22395403683185577,
      "learning_rate": 0.00043628009108653224,
      "loss": 0.3844,
      "step": 1567000
    },
    {
      "epoch": 127.47270620285848,
      "grad_norm": 0.21230000257492065,
      "learning_rate": 0.00043625975927130776,
      "loss": 0.3849,
      "step": 1567500
    },
    {
      "epoch": 127.51336735316242,
      "grad_norm": 0.21694537997245789,
      "learning_rate": 0.00043623942745608333,
      "loss": 0.3852,
      "step": 1568000
    },
    {
      "epoch": 127.55402850346637,
      "grad_norm": 0.26461243629455566,
      "learning_rate": 0.00043621909564085885,
      "loss": 0.3854,
      "step": 1568500
    },
    {
      "epoch": 127.5946896537703,
      "grad_norm": 0.23034630715847015,
      "learning_rate": 0.00043619876382563436,
      "loss": 0.3854,
      "step": 1569000
    },
    {
      "epoch": 127.63535080407425,
      "grad_norm": 0.24058809876441956,
      "learning_rate": 0.00043617843201040993,
      "loss": 0.3856,
      "step": 1569500
    },
    {
      "epoch": 127.67601195437818,
      "grad_norm": 0.230850487947464,
      "learning_rate": 0.00043615810019518545,
      "loss": 0.3852,
      "step": 1570000
    },
    {
      "epoch": 127.71667310468213,
      "grad_norm": 0.22126540541648865,
      "learning_rate": 0.000436137768379961,
      "loss": 0.3859,
      "step": 1570500
    },
    {
      "epoch": 127.75733425498608,
      "grad_norm": 0.2112029641866684,
      "learning_rate": 0.00043611743656473654,
      "loss": 0.3858,
      "step": 1571000
    },
    {
      "epoch": 127.79799540529001,
      "grad_norm": 0.22229625284671783,
      "learning_rate": 0.00043609710474951205,
      "loss": 0.3858,
      "step": 1571500
    },
    {
      "epoch": 127.83865655559396,
      "grad_norm": 0.21930430829524994,
      "learning_rate": 0.0004360767729342876,
      "loss": 0.386,
      "step": 1572000
    },
    {
      "epoch": 127.8793177058979,
      "grad_norm": 0.2154647707939148,
      "learning_rate": 0.00043605644111906314,
      "loss": 0.3858,
      "step": 1572500
    },
    {
      "epoch": 127.91997885620184,
      "grad_norm": 0.23922781646251678,
      "learning_rate": 0.00043603610930383866,
      "loss": 0.3859,
      "step": 1573000
    },
    {
      "epoch": 127.96064000650578,
      "grad_norm": 0.2248331904411316,
      "learning_rate": 0.0004360157774886142,
      "loss": 0.3862,
      "step": 1573500
    },
    {
      "epoch": 128.00130115680972,
      "grad_norm": 0.22757725417613983,
      "learning_rate": 0.00043599544567338974,
      "loss": 0.386,
      "step": 1574000
    },
    {
      "epoch": 128.04196230711366,
      "grad_norm": 0.2378474473953247,
      "learning_rate": 0.00043597511385816526,
      "loss": 0.3823,
      "step": 1574500
    },
    {
      "epoch": 128.0826234574176,
      "grad_norm": 0.22981832921504974,
      "learning_rate": 0.00043595478204294083,
      "loss": 0.3825,
      "step": 1575000
    },
    {
      "epoch": 128.12328460772156,
      "grad_norm": 0.20549188554286957,
      "learning_rate": 0.00043593445022771635,
      "loss": 0.3832,
      "step": 1575500
    },
    {
      "epoch": 128.1639457580255,
      "grad_norm": 0.23024745285511017,
      "learning_rate": 0.0004359141184124919,
      "loss": 0.3834,
      "step": 1576000
    },
    {
      "epoch": 128.20460690832942,
      "grad_norm": 0.24613794684410095,
      "learning_rate": 0.00043589378659726743,
      "loss": 0.3837,
      "step": 1576500
    },
    {
      "epoch": 128.24526805863337,
      "grad_norm": 0.24210990965366364,
      "learning_rate": 0.00043587345478204295,
      "loss": 0.3839,
      "step": 1577000
    },
    {
      "epoch": 128.28592920893732,
      "grad_norm": 0.24515801668167114,
      "learning_rate": 0.0004358531229668185,
      "loss": 0.3841,
      "step": 1577500
    },
    {
      "epoch": 128.32659035924127,
      "grad_norm": 0.23986196517944336,
      "learning_rate": 0.00043583279115159404,
      "loss": 0.3845,
      "step": 1578000
    },
    {
      "epoch": 128.3672515095452,
      "grad_norm": 0.2400435358285904,
      "learning_rate": 0.00043581245933636955,
      "loss": 0.3842,
      "step": 1578500
    },
    {
      "epoch": 128.40791265984916,
      "grad_norm": 0.21518874168395996,
      "learning_rate": 0.0004357921275211451,
      "loss": 0.3844,
      "step": 1579000
    },
    {
      "epoch": 128.44857381015308,
      "grad_norm": 0.24836747348308563,
      "learning_rate": 0.00043577179570592064,
      "loss": 0.3848,
      "step": 1579500
    },
    {
      "epoch": 128.48923496045703,
      "grad_norm": 0.2329374998807907,
      "learning_rate": 0.0004357514638906962,
      "loss": 0.3852,
      "step": 1580000
    },
    {
      "epoch": 128.52989611076097,
      "grad_norm": 0.22124698758125305,
      "learning_rate": 0.00043573113207547173,
      "loss": 0.3852,
      "step": 1580500
    },
    {
      "epoch": 128.57055726106492,
      "grad_norm": 0.2430727779865265,
      "learning_rate": 0.00043571080026024724,
      "loss": 0.3853,
      "step": 1581000
    },
    {
      "epoch": 128.61121841136887,
      "grad_norm": 0.2611318528652191,
      "learning_rate": 0.0004356904684450228,
      "loss": 0.3854,
      "step": 1581500
    },
    {
      "epoch": 128.65187956167279,
      "grad_norm": 0.2587836980819702,
      "learning_rate": 0.00043567013662979833,
      "loss": 0.3849,
      "step": 1582000
    },
    {
      "epoch": 128.69254071197673,
      "grad_norm": 0.2481706440448761,
      "learning_rate": 0.00043564980481457385,
      "loss": 0.3856,
      "step": 1582500
    },
    {
      "epoch": 128.73320186228068,
      "grad_norm": 0.23036780953407288,
      "learning_rate": 0.0004356294729993494,
      "loss": 0.3857,
      "step": 1583000
    },
    {
      "epoch": 128.77386301258463,
      "grad_norm": 0.21702592074871063,
      "learning_rate": 0.00043560914118412493,
      "loss": 0.3858,
      "step": 1583500
    },
    {
      "epoch": 128.81452416288857,
      "grad_norm": 0.21626029908657074,
      "learning_rate": 0.00043558880936890045,
      "loss": 0.3857,
      "step": 1584000
    },
    {
      "epoch": 128.85518531319252,
      "grad_norm": 0.2427080273628235,
      "learning_rate": 0.000435568477553676,
      "loss": 0.3859,
      "step": 1584500
    },
    {
      "epoch": 128.89584646349644,
      "grad_norm": 0.28043821454048157,
      "learning_rate": 0.00043554814573845154,
      "loss": 0.3861,
      "step": 1585000
    },
    {
      "epoch": 128.9365076138004,
      "grad_norm": 0.2314719706773758,
      "learning_rate": 0.0004355278139232271,
      "loss": 0.3861,
      "step": 1585500
    },
    {
      "epoch": 128.97716876410433,
      "grad_norm": 0.22188280522823334,
      "learning_rate": 0.0004355074821080026,
      "loss": 0.3859,
      "step": 1586000
    },
    {
      "epoch": 129.01782991440828,
      "grad_norm": 0.22096262872219086,
      "learning_rate": 0.00043548715029277814,
      "loss": 0.3846,
      "step": 1586500
    },
    {
      "epoch": 129.05849106471223,
      "grad_norm": 0.22167342901229858,
      "learning_rate": 0.0004354668184775537,
      "loss": 0.3828,
      "step": 1587000
    },
    {
      "epoch": 129.09915221501618,
      "grad_norm": 0.2889489233493805,
      "learning_rate": 0.00043544648666232923,
      "loss": 0.3831,
      "step": 1587500
    },
    {
      "epoch": 129.1398133653201,
      "grad_norm": 0.23973682522773743,
      "learning_rate": 0.00043542615484710474,
      "loss": 0.3837,
      "step": 1588000
    },
    {
      "epoch": 129.18047451562404,
      "grad_norm": 0.27922865748405457,
      "learning_rate": 0.0004354058230318803,
      "loss": 0.3831,
      "step": 1588500
    },
    {
      "epoch": 129.221135665928,
      "grad_norm": 0.24645088613033295,
      "learning_rate": 0.00043538549121665583,
      "loss": 0.3834,
      "step": 1589000
    },
    {
      "epoch": 129.26179681623194,
      "grad_norm": 0.22482778131961823,
      "learning_rate": 0.00043536515940143135,
      "loss": 0.3844,
      "step": 1589500
    },
    {
      "epoch": 129.30245796653588,
      "grad_norm": 0.2390177696943283,
      "learning_rate": 0.0004353448275862069,
      "loss": 0.3846,
      "step": 1590000
    },
    {
      "epoch": 129.3431191168398,
      "grad_norm": 0.22155430912971497,
      "learning_rate": 0.00043532449577098244,
      "loss": 0.3838,
      "step": 1590500
    },
    {
      "epoch": 129.38378026714375,
      "grad_norm": 0.20992349088191986,
      "learning_rate": 0.000435304163955758,
      "loss": 0.3845,
      "step": 1591000
    },
    {
      "epoch": 129.4244414174477,
      "grad_norm": 0.23829446732997894,
      "learning_rate": 0.0004352838321405335,
      "loss": 0.3847,
      "step": 1591500
    },
    {
      "epoch": 129.46510256775164,
      "grad_norm": 0.22057601809501648,
      "learning_rate": 0.00043526350032530904,
      "loss": 0.385,
      "step": 1592000
    },
    {
      "epoch": 129.5057637180556,
      "grad_norm": 0.24546495079994202,
      "learning_rate": 0.0004352431685100846,
      "loss": 0.3845,
      "step": 1592500
    },
    {
      "epoch": 129.54642486835954,
      "grad_norm": 0.23057179152965546,
      "learning_rate": 0.0004352228366948601,
      "loss": 0.385,
      "step": 1593000
    },
    {
      "epoch": 129.58708601866346,
      "grad_norm": 0.26075074076652527,
      "learning_rate": 0.00043520250487963564,
      "loss": 0.3853,
      "step": 1593500
    },
    {
      "epoch": 129.6277471689674,
      "grad_norm": 0.2325958013534546,
      "learning_rate": 0.0004351821730644112,
      "loss": 0.3852,
      "step": 1594000
    },
    {
      "epoch": 129.66840831927135,
      "grad_norm": 0.23233754932880402,
      "learning_rate": 0.00043516184124918673,
      "loss": 0.3854,
      "step": 1594500
    },
    {
      "epoch": 129.7090694695753,
      "grad_norm": 0.2577170431613922,
      "learning_rate": 0.00043514150943396225,
      "loss": 0.3853,
      "step": 1595000
    },
    {
      "epoch": 129.74973061987924,
      "grad_norm": 0.2165185809135437,
      "learning_rate": 0.0004351211776187378,
      "loss": 0.3854,
      "step": 1595500
    },
    {
      "epoch": 129.7903917701832,
      "grad_norm": 0.21145905554294586,
      "learning_rate": 0.00043510084580351333,
      "loss": 0.3853,
      "step": 1596000
    },
    {
      "epoch": 129.8310529204871,
      "grad_norm": 0.20210236310958862,
      "learning_rate": 0.0004350805139882889,
      "loss": 0.3857,
      "step": 1596500
    },
    {
      "epoch": 129.87171407079106,
      "grad_norm": 0.24315877258777618,
      "learning_rate": 0.0004350601821730644,
      "loss": 0.3859,
      "step": 1597000
    },
    {
      "epoch": 129.912375221095,
      "grad_norm": 0.2225293219089508,
      "learning_rate": 0.00043503985035783994,
      "loss": 0.3855,
      "step": 1597500
    },
    {
      "epoch": 129.95303637139895,
      "grad_norm": 0.22150056064128876,
      "learning_rate": 0.0004350195185426155,
      "loss": 0.3863,
      "step": 1598000
    },
    {
      "epoch": 129.9936975217029,
      "grad_norm": 0.24368509650230408,
      "learning_rate": 0.000434999186727391,
      "loss": 0.3866,
      "step": 1598500
    },
    {
      "epoch": 130.03435867200682,
      "grad_norm": 0.22364605963230133,
      "learning_rate": 0.00043497885491216654,
      "loss": 0.3832,
      "step": 1599000
    },
    {
      "epoch": 130.07501982231076,
      "grad_norm": 0.2207714468240738,
      "learning_rate": 0.0004349585230969421,
      "loss": 0.3825,
      "step": 1599500
    },
    {
      "epoch": 130.1156809726147,
      "grad_norm": 0.23717792332172394,
      "learning_rate": 0.0004349381912817176,
      "loss": 0.3828,
      "step": 1600000
    },
    {
      "epoch": 130.15634212291866,
      "grad_norm": 0.23398639261722565,
      "learning_rate": 0.0004349178594664932,
      "loss": 0.3833,
      "step": 1600500
    },
    {
      "epoch": 130.1970032732226,
      "grad_norm": 0.2582217752933502,
      "learning_rate": 0.0004348975276512687,
      "loss": 0.3832,
      "step": 1601000
    },
    {
      "epoch": 130.23766442352655,
      "grad_norm": 0.2225673347711563,
      "learning_rate": 0.00043487719583604423,
      "loss": 0.3835,
      "step": 1601500
    },
    {
      "epoch": 130.27832557383047,
      "grad_norm": 0.22201131284236908,
      "learning_rate": 0.0004348568640208198,
      "loss": 0.3837,
      "step": 1602000
    },
    {
      "epoch": 130.31898672413442,
      "grad_norm": 0.22391057014465332,
      "learning_rate": 0.0004348365322055953,
      "loss": 0.3844,
      "step": 1602500
    },
    {
      "epoch": 130.35964787443837,
      "grad_norm": 0.23172765970230103,
      "learning_rate": 0.00043481620039037083,
      "loss": 0.3841,
      "step": 1603000
    },
    {
      "epoch": 130.4003090247423,
      "grad_norm": 0.22776523232460022,
      "learning_rate": 0.0004347958685751464,
      "loss": 0.3845,
      "step": 1603500
    },
    {
      "epoch": 130.44097017504626,
      "grad_norm": 0.25915056467056274,
      "learning_rate": 0.0004347755367599219,
      "loss": 0.3849,
      "step": 1604000
    },
    {
      "epoch": 130.4816313253502,
      "grad_norm": 0.2583223879337311,
      "learning_rate": 0.00043475520494469744,
      "loss": 0.3847,
      "step": 1604500
    },
    {
      "epoch": 130.52229247565413,
      "grad_norm": 0.23703375458717346,
      "learning_rate": 0.000434734873129473,
      "loss": 0.385,
      "step": 1605000
    },
    {
      "epoch": 130.56295362595807,
      "grad_norm": 0.21434101462364197,
      "learning_rate": 0.0004347145413142485,
      "loss": 0.3848,
      "step": 1605500
    },
    {
      "epoch": 130.60361477626202,
      "grad_norm": 0.22753497958183289,
      "learning_rate": 0.0004346942094990241,
      "loss": 0.3852,
      "step": 1606000
    },
    {
      "epoch": 130.64427592656597,
      "grad_norm": 0.22470049560070038,
      "learning_rate": 0.0004346738776837996,
      "loss": 0.3854,
      "step": 1606500
    },
    {
      "epoch": 130.68493707686991,
      "grad_norm": 0.2386549413204193,
      "learning_rate": 0.00043465354586857513,
      "loss": 0.3858,
      "step": 1607000
    },
    {
      "epoch": 130.72559822717383,
      "grad_norm": 0.25261327624320984,
      "learning_rate": 0.0004346332140533507,
      "loss": 0.3854,
      "step": 1607500
    },
    {
      "epoch": 130.76625937747778,
      "grad_norm": 0.2549276053905487,
      "learning_rate": 0.0004346128822381262,
      "loss": 0.386,
      "step": 1608000
    },
    {
      "epoch": 130.80692052778173,
      "grad_norm": 0.2469736486673355,
      "learning_rate": 0.00043459255042290173,
      "loss": 0.3858,
      "step": 1608500
    },
    {
      "epoch": 130.84758167808567,
      "grad_norm": 0.25022345781326294,
      "learning_rate": 0.0004345722186076773,
      "loss": 0.3852,
      "step": 1609000
    },
    {
      "epoch": 130.88824282838962,
      "grad_norm": 0.21486049890518188,
      "learning_rate": 0.0004345518867924528,
      "loss": 0.3856,
      "step": 1609500
    },
    {
      "epoch": 130.92890397869357,
      "grad_norm": 0.22203293442726135,
      "learning_rate": 0.00043453155497722833,
      "loss": 0.3856,
      "step": 1610000
    },
    {
      "epoch": 130.9695651289975,
      "grad_norm": 0.22726649045944214,
      "learning_rate": 0.0004345112231620039,
      "loss": 0.3863,
      "step": 1610500
    },
    {
      "epoch": 131.01022627930143,
      "grad_norm": 0.21791554987430573,
      "learning_rate": 0.0004344908913467794,
      "loss": 0.3849,
      "step": 1611000
    },
    {
      "epoch": 131.05088742960538,
      "grad_norm": 0.22720938920974731,
      "learning_rate": 0.000434470559531555,
      "loss": 0.3815,
      "step": 1611500
    },
    {
      "epoch": 131.09154857990933,
      "grad_norm": 0.2452157437801361,
      "learning_rate": 0.0004344502277163305,
      "loss": 0.3824,
      "step": 1612000
    },
    {
      "epoch": 131.13220973021328,
      "grad_norm": 0.25585034489631653,
      "learning_rate": 0.000434429895901106,
      "loss": 0.3828,
      "step": 1612500
    },
    {
      "epoch": 131.17287088051722,
      "grad_norm": 0.21343335509300232,
      "learning_rate": 0.0004344095640858816,
      "loss": 0.3833,
      "step": 1613000
    },
    {
      "epoch": 131.21353203082114,
      "grad_norm": 0.23400476574897766,
      "learning_rate": 0.0004343892322706571,
      "loss": 0.3838,
      "step": 1613500
    },
    {
      "epoch": 131.2541931811251,
      "grad_norm": 0.24441014230251312,
      "learning_rate": 0.00043436890045543263,
      "loss": 0.3833,
      "step": 1614000
    },
    {
      "epoch": 131.29485433142904,
      "grad_norm": 0.24287885427474976,
      "learning_rate": 0.0004343485686402082,
      "loss": 0.3838,
      "step": 1614500
    },
    {
      "epoch": 131.33551548173298,
      "grad_norm": 0.22364309430122375,
      "learning_rate": 0.0004343282368249837,
      "loss": 0.3844,
      "step": 1615000
    },
    {
      "epoch": 131.37617663203693,
      "grad_norm": 0.22489282488822937,
      "learning_rate": 0.0004343079050097593,
      "loss": 0.3844,
      "step": 1615500
    },
    {
      "epoch": 131.41683778234085,
      "grad_norm": 0.22161759436130524,
      "learning_rate": 0.0004342875731945348,
      "loss": 0.3847,
      "step": 1616000
    },
    {
      "epoch": 131.4574989326448,
      "grad_norm": 0.20698219537734985,
      "learning_rate": 0.0004342672413793103,
      "loss": 0.3843,
      "step": 1616500
    },
    {
      "epoch": 131.49816008294874,
      "grad_norm": 0.23214782774448395,
      "learning_rate": 0.0004342469095640859,
      "loss": 0.3848,
      "step": 1617000
    },
    {
      "epoch": 131.5388212332527,
      "grad_norm": 0.23628365993499756,
      "learning_rate": 0.0004342265777488614,
      "loss": 0.3843,
      "step": 1617500
    },
    {
      "epoch": 131.57948238355664,
      "grad_norm": 0.24704359471797943,
      "learning_rate": 0.0004342062459336369,
      "loss": 0.3851,
      "step": 1618000
    },
    {
      "epoch": 131.62014353386058,
      "grad_norm": 0.22848200798034668,
      "learning_rate": 0.0004341859141184125,
      "loss": 0.3855,
      "step": 1618500
    },
    {
      "epoch": 131.6608046841645,
      "grad_norm": 0.25305652618408203,
      "learning_rate": 0.000434165582303188,
      "loss": 0.3851,
      "step": 1619000
    },
    {
      "epoch": 131.70146583446845,
      "grad_norm": 0.2050378918647766,
      "learning_rate": 0.0004341452504879635,
      "loss": 0.3854,
      "step": 1619500
    },
    {
      "epoch": 131.7421269847724,
      "grad_norm": 0.2215348482131958,
      "learning_rate": 0.0004341249186727391,
      "loss": 0.3855,
      "step": 1620000
    },
    {
      "epoch": 131.78278813507634,
      "grad_norm": 0.2061583250761032,
      "learning_rate": 0.0004341045868575146,
      "loss": 0.3853,
      "step": 1620500
    },
    {
      "epoch": 131.8234492853803,
      "grad_norm": 0.22458289563655853,
      "learning_rate": 0.0004340842550422902,
      "loss": 0.3855,
      "step": 1621000
    },
    {
      "epoch": 131.86411043568424,
      "grad_norm": 0.23105567693710327,
      "learning_rate": 0.0004340639232270657,
      "loss": 0.3855,
      "step": 1621500
    },
    {
      "epoch": 131.90477158598816,
      "grad_norm": 0.2226831614971161,
      "learning_rate": 0.0004340435914118412,
      "loss": 0.3858,
      "step": 1622000
    },
    {
      "epoch": 131.9454327362921,
      "grad_norm": 0.23335236310958862,
      "learning_rate": 0.00043402325959661684,
      "loss": 0.3859,
      "step": 1622500
    },
    {
      "epoch": 131.98609388659605,
      "grad_norm": 0.2075672745704651,
      "learning_rate": 0.00043400292778139236,
      "loss": 0.386,
      "step": 1623000
    },
    {
      "epoch": 132.0267550369,
      "grad_norm": 0.22190064191818237,
      "learning_rate": 0.0004339825959661679,
      "loss": 0.3838,
      "step": 1623500
    },
    {
      "epoch": 132.06741618720395,
      "grad_norm": 0.23346278071403503,
      "learning_rate": 0.00043396226415094345,
      "loss": 0.3828,
      "step": 1624000
    },
    {
      "epoch": 132.10807733750786,
      "grad_norm": 0.2632865011692047,
      "learning_rate": 0.00043394193233571896,
      "loss": 0.3826,
      "step": 1624500
    },
    {
      "epoch": 132.1487384878118,
      "grad_norm": 0.24166817963123322,
      "learning_rate": 0.0004339216005204945,
      "loss": 0.3832,
      "step": 1625000
    },
    {
      "epoch": 132.18939963811576,
      "grad_norm": 0.24723374843597412,
      "learning_rate": 0.00043390126870527005,
      "loss": 0.3835,
      "step": 1625500
    },
    {
      "epoch": 132.2300607884197,
      "grad_norm": 0.22572702169418335,
      "learning_rate": 0.00043388093689004557,
      "loss": 0.3839,
      "step": 1626000
    },
    {
      "epoch": 132.27072193872365,
      "grad_norm": 0.21490387618541718,
      "learning_rate": 0.00043386060507482114,
      "loss": 0.384,
      "step": 1626500
    },
    {
      "epoch": 132.3113830890276,
      "grad_norm": 0.24522951245307922,
      "learning_rate": 0.00043384027325959665,
      "loss": 0.3839,
      "step": 1627000
    },
    {
      "epoch": 132.35204423933152,
      "grad_norm": 0.21469959616661072,
      "learning_rate": 0.00043381994144437217,
      "loss": 0.3842,
      "step": 1627500
    },
    {
      "epoch": 132.39270538963547,
      "grad_norm": 0.21690770983695984,
      "learning_rate": 0.00043379960962914774,
      "loss": 0.3836,
      "step": 1628000
    },
    {
      "epoch": 132.4333665399394,
      "grad_norm": 0.2342522144317627,
      "learning_rate": 0.00043377927781392326,
      "loss": 0.3846,
      "step": 1628500
    },
    {
      "epoch": 132.47402769024336,
      "grad_norm": 0.22936499118804932,
      "learning_rate": 0.00043375894599869877,
      "loss": 0.3845,
      "step": 1629000
    },
    {
      "epoch": 132.5146888405473,
      "grad_norm": 0.23731732368469238,
      "learning_rate": 0.00043373861418347434,
      "loss": 0.3847,
      "step": 1629500
    },
    {
      "epoch": 132.55534999085125,
      "grad_norm": 0.24075499176979065,
      "learning_rate": 0.00043371828236824986,
      "loss": 0.385,
      "step": 1630000
    },
    {
      "epoch": 132.59601114115517,
      "grad_norm": 0.21529145538806915,
      "learning_rate": 0.0004336979505530254,
      "loss": 0.3846,
      "step": 1630500
    },
    {
      "epoch": 132.63667229145912,
      "grad_norm": 0.20870934426784515,
      "learning_rate": 0.00043367761873780095,
      "loss": 0.3845,
      "step": 1631000
    },
    {
      "epoch": 132.67733344176307,
      "grad_norm": 0.21427378058433533,
      "learning_rate": 0.00043365728692257646,
      "loss": 0.385,
      "step": 1631500
    },
    {
      "epoch": 132.71799459206701,
      "grad_norm": 0.2131144404411316,
      "learning_rate": 0.00043363695510735203,
      "loss": 0.3851,
      "step": 1632000
    },
    {
      "epoch": 132.75865574237096,
      "grad_norm": 0.2611711919307709,
      "learning_rate": 0.00043361662329212755,
      "loss": 0.3854,
      "step": 1632500
    },
    {
      "epoch": 132.79931689267488,
      "grad_norm": 0.2381969839334488,
      "learning_rate": 0.00043359629147690307,
      "loss": 0.3852,
      "step": 1633000
    },
    {
      "epoch": 132.83997804297883,
      "grad_norm": 0.2605622112751007,
      "learning_rate": 0.00043357595966167864,
      "loss": 0.3853,
      "step": 1633500
    },
    {
      "epoch": 132.88063919328278,
      "grad_norm": 0.2140212208032608,
      "learning_rate": 0.00043355562784645415,
      "loss": 0.3856,
      "step": 1634000
    },
    {
      "epoch": 132.92130034358672,
      "grad_norm": 0.21710503101348877,
      "learning_rate": 0.00043353529603122967,
      "loss": 0.3864,
      "step": 1634500
    },
    {
      "epoch": 132.96196149389067,
      "grad_norm": 0.23885862529277802,
      "learning_rate": 0.00043351496421600524,
      "loss": 0.3856,
      "step": 1635000
    },
    {
      "epoch": 133.00262264419462,
      "grad_norm": 0.22885745763778687,
      "learning_rate": 0.00043349463240078076,
      "loss": 0.3856,
      "step": 1635500
    },
    {
      "epoch": 133.04328379449854,
      "grad_norm": 0.23801523447036743,
      "learning_rate": 0.00043347430058555633,
      "loss": 0.3816,
      "step": 1636000
    },
    {
      "epoch": 133.08394494480248,
      "grad_norm": 0.23401926457881927,
      "learning_rate": 0.00043345396877033184,
      "loss": 0.3828,
      "step": 1636500
    },
    {
      "epoch": 133.12460609510643,
      "grad_norm": 0.24181252717971802,
      "learning_rate": 0.00043343363695510736,
      "loss": 0.383,
      "step": 1637000
    },
    {
      "epoch": 133.16526724541038,
      "grad_norm": 0.2548735439777374,
      "learning_rate": 0.00043341330513988293,
      "loss": 0.3834,
      "step": 1637500
    },
    {
      "epoch": 133.20592839571432,
      "grad_norm": 0.24264542758464813,
      "learning_rate": 0.00043339297332465845,
      "loss": 0.3833,
      "step": 1638000
    },
    {
      "epoch": 133.24658954601827,
      "grad_norm": 0.2450064718723297,
      "learning_rate": 0.00043337264150943396,
      "loss": 0.3832,
      "step": 1638500
    },
    {
      "epoch": 133.2872506963222,
      "grad_norm": 0.23782537877559662,
      "learning_rate": 0.00043335230969420953,
      "loss": 0.3838,
      "step": 1639000
    },
    {
      "epoch": 133.32791184662614,
      "grad_norm": 0.21286922693252563,
      "learning_rate": 0.00043333197787898505,
      "loss": 0.3837,
      "step": 1639500
    },
    {
      "epoch": 133.36857299693008,
      "grad_norm": 0.20984581112861633,
      "learning_rate": 0.00043331164606376057,
      "loss": 0.384,
      "step": 1640000
    },
    {
      "epoch": 133.40923414723403,
      "grad_norm": 0.21878382563591003,
      "learning_rate": 0.00043329131424853614,
      "loss": 0.3839,
      "step": 1640500
    },
    {
      "epoch": 133.44989529753798,
      "grad_norm": 0.2255871742963791,
      "learning_rate": 0.00043327098243331165,
      "loss": 0.384,
      "step": 1641000
    },
    {
      "epoch": 133.4905564478419,
      "grad_norm": 0.20353205502033234,
      "learning_rate": 0.0004332506506180872,
      "loss": 0.3844,
      "step": 1641500
    },
    {
      "epoch": 133.53121759814584,
      "grad_norm": 0.2507624328136444,
      "learning_rate": 0.00043323031880286274,
      "loss": 0.3843,
      "step": 1642000
    },
    {
      "epoch": 133.5718787484498,
      "grad_norm": 0.23988738656044006,
      "learning_rate": 0.00043320998698763826,
      "loss": 0.3844,
      "step": 1642500
    },
    {
      "epoch": 133.61253989875374,
      "grad_norm": 0.2377125471830368,
      "learning_rate": 0.00043318965517241383,
      "loss": 0.3852,
      "step": 1643000
    },
    {
      "epoch": 133.65320104905769,
      "grad_norm": 0.2757398188114166,
      "learning_rate": 0.00043316932335718934,
      "loss": 0.385,
      "step": 1643500
    },
    {
      "epoch": 133.69386219936163,
      "grad_norm": 0.31127771735191345,
      "learning_rate": 0.00043314899154196486,
      "loss": 0.3853,
      "step": 1644000
    },
    {
      "epoch": 133.73452334966555,
      "grad_norm": 0.2260431945323944,
      "learning_rate": 0.00043312865972674043,
      "loss": 0.3851,
      "step": 1644500
    },
    {
      "epoch": 133.7751844999695,
      "grad_norm": 0.21267874538898468,
      "learning_rate": 0.00043310832791151595,
      "loss": 0.3849,
      "step": 1645000
    },
    {
      "epoch": 133.81584565027345,
      "grad_norm": 0.25083112716674805,
      "learning_rate": 0.00043308799609629146,
      "loss": 0.3853,
      "step": 1645500
    },
    {
      "epoch": 133.8565068005774,
      "grad_norm": 0.2445797324180603,
      "learning_rate": 0.00043306766428106704,
      "loss": 0.3854,
      "step": 1646000
    },
    {
      "epoch": 133.89716795088134,
      "grad_norm": 0.24011088907718658,
      "learning_rate": 0.00043304733246584255,
      "loss": 0.3859,
      "step": 1646500
    },
    {
      "epoch": 133.9378291011853,
      "grad_norm": 0.2561926245689392,
      "learning_rate": 0.0004330270006506181,
      "loss": 0.3855,
      "step": 1647000
    },
    {
      "epoch": 133.9784902514892,
      "grad_norm": 0.23300595581531525,
      "learning_rate": 0.00043300666883539364,
      "loss": 0.3858,
      "step": 1647500
    },
    {
      "epoch": 134.01915140179315,
      "grad_norm": 0.21138720214366913,
      "learning_rate": 0.00043298633702016916,
      "loss": 0.3842,
      "step": 1648000
    },
    {
      "epoch": 134.0598125520971,
      "grad_norm": 0.2851182222366333,
      "learning_rate": 0.0004329660052049447,
      "loss": 0.3818,
      "step": 1648500
    },
    {
      "epoch": 134.10047370240105,
      "grad_norm": 0.22620005905628204,
      "learning_rate": 0.00043294567338972024,
      "loss": 0.3827,
      "step": 1649000
    },
    {
      "epoch": 134.141134852705,
      "grad_norm": 0.23465664684772491,
      "learning_rate": 0.00043292534157449576,
      "loss": 0.3831,
      "step": 1649500
    },
    {
      "epoch": 134.1817960030089,
      "grad_norm": 0.24078485369682312,
      "learning_rate": 0.00043290500975927133,
      "loss": 0.3833,
      "step": 1650000
    },
    {
      "epoch": 134.22245715331286,
      "grad_norm": 0.22533762454986572,
      "learning_rate": 0.00043288467794404685,
      "loss": 0.3835,
      "step": 1650500
    },
    {
      "epoch": 134.2631183036168,
      "grad_norm": 0.22896693646907806,
      "learning_rate": 0.0004328643461288224,
      "loss": 0.3833,
      "step": 1651000
    },
    {
      "epoch": 134.30377945392075,
      "grad_norm": 0.23219655454158783,
      "learning_rate": 0.00043284401431359793,
      "loss": 0.3836,
      "step": 1651500
    },
    {
      "epoch": 134.3444406042247,
      "grad_norm": 0.19408658146858215,
      "learning_rate": 0.00043282368249837345,
      "loss": 0.3838,
      "step": 1652000
    },
    {
      "epoch": 134.38510175452865,
      "grad_norm": 0.227615624666214,
      "learning_rate": 0.000432803350683149,
      "loss": 0.3844,
      "step": 1652500
    },
    {
      "epoch": 134.42576290483257,
      "grad_norm": 0.22590769827365875,
      "learning_rate": 0.00043278301886792454,
      "loss": 0.3845,
      "step": 1653000
    },
    {
      "epoch": 134.4664240551365,
      "grad_norm": 0.23912782967090607,
      "learning_rate": 0.00043276268705270005,
      "loss": 0.3845,
      "step": 1653500
    },
    {
      "epoch": 134.50708520544046,
      "grad_norm": 0.2573073208332062,
      "learning_rate": 0.0004327423552374756,
      "loss": 0.3846,
      "step": 1654000
    },
    {
      "epoch": 134.5477463557444,
      "grad_norm": 0.2471216768026352,
      "learning_rate": 0.00043272202342225114,
      "loss": 0.385,
      "step": 1654500
    },
    {
      "epoch": 134.58840750604836,
      "grad_norm": 0.2370661497116089,
      "learning_rate": 0.00043270169160702666,
      "loss": 0.3848,
      "step": 1655000
    },
    {
      "epoch": 134.62906865635227,
      "grad_norm": 0.2144974023103714,
      "learning_rate": 0.0004326813597918022,
      "loss": 0.3848,
      "step": 1655500
    },
    {
      "epoch": 134.66972980665622,
      "grad_norm": 0.23396696150302887,
      "learning_rate": 0.00043266102797657774,
      "loss": 0.385,
      "step": 1656000
    },
    {
      "epoch": 134.71039095696017,
      "grad_norm": 0.2253277599811554,
      "learning_rate": 0.0004326406961613533,
      "loss": 0.385,
      "step": 1656500
    },
    {
      "epoch": 134.75105210726412,
      "grad_norm": 0.2389412224292755,
      "learning_rate": 0.00043262036434612883,
      "loss": 0.385,
      "step": 1657000
    },
    {
      "epoch": 134.79171325756806,
      "grad_norm": 0.2494407296180725,
      "learning_rate": 0.00043260003253090435,
      "loss": 0.3853,
      "step": 1657500
    },
    {
      "epoch": 134.832374407872,
      "grad_norm": 0.2329982966184616,
      "learning_rate": 0.0004325797007156799,
      "loss": 0.3856,
      "step": 1658000
    },
    {
      "epoch": 134.87303555817593,
      "grad_norm": 0.22363601624965668,
      "learning_rate": 0.00043255936890045543,
      "loss": 0.3855,
      "step": 1658500
    },
    {
      "epoch": 134.91369670847988,
      "grad_norm": 0.26499325037002563,
      "learning_rate": 0.00043253903708523095,
      "loss": 0.3854,
      "step": 1659000
    },
    {
      "epoch": 134.95435785878382,
      "grad_norm": 0.21719470620155334,
      "learning_rate": 0.0004325187052700065,
      "loss": 0.3854,
      "step": 1659500
    },
    {
      "epoch": 134.99501900908777,
      "grad_norm": 0.24329449236392975,
      "learning_rate": 0.00043249837345478204,
      "loss": 0.3858,
      "step": 1660000
    },
    {
      "epoch": 135.03568015939172,
      "grad_norm": 0.24135075509548187,
      "learning_rate": 0.00043247804163955755,
      "loss": 0.3824,
      "step": 1660500
    },
    {
      "epoch": 135.07634130969566,
      "grad_norm": 0.238755002617836,
      "learning_rate": 0.0004324577098243331,
      "loss": 0.3822,
      "step": 1661000
    },
    {
      "epoch": 135.11700245999958,
      "grad_norm": 0.2574158310890198,
      "learning_rate": 0.00043243737800910864,
      "loss": 0.3825,
      "step": 1661500
    },
    {
      "epoch": 135.15766361030353,
      "grad_norm": 0.23183590173721313,
      "learning_rate": 0.0004324170461938842,
      "loss": 0.3831,
      "step": 1662000
    },
    {
      "epoch": 135.19832476060748,
      "grad_norm": 0.2669202983379364,
      "learning_rate": 0.00043239671437865973,
      "loss": 0.3829,
      "step": 1662500
    },
    {
      "epoch": 135.23898591091142,
      "grad_norm": 0.22559083998203278,
      "learning_rate": 0.00043237638256343524,
      "loss": 0.3833,
      "step": 1663000
    },
    {
      "epoch": 135.27964706121537,
      "grad_norm": 0.2696855068206787,
      "learning_rate": 0.0004323560507482108,
      "loss": 0.3832,
      "step": 1663500
    },
    {
      "epoch": 135.3203082115193,
      "grad_norm": 0.2378455251455307,
      "learning_rate": 0.00043233571893298633,
      "loss": 0.384,
      "step": 1664000
    },
    {
      "epoch": 135.36096936182324,
      "grad_norm": 0.2594633400440216,
      "learning_rate": 0.00043231538711776185,
      "loss": 0.3843,
      "step": 1664500
    },
    {
      "epoch": 135.40163051212718,
      "grad_norm": 0.23080337047576904,
      "learning_rate": 0.0004322950553025374,
      "loss": 0.3845,
      "step": 1665000
    },
    {
      "epoch": 135.44229166243113,
      "grad_norm": 0.23676574230194092,
      "learning_rate": 0.00043227472348731293,
      "loss": 0.3845,
      "step": 1665500
    },
    {
      "epoch": 135.48295281273508,
      "grad_norm": 0.24154368042945862,
      "learning_rate": 0.0004322543916720885,
      "loss": 0.3845,
      "step": 1666000
    },
    {
      "epoch": 135.52361396303903,
      "grad_norm": 0.27131450176239014,
      "learning_rate": 0.000432234059856864,
      "loss": 0.3845,
      "step": 1666500
    },
    {
      "epoch": 135.56427511334294,
      "grad_norm": 0.2348906695842743,
      "learning_rate": 0.00043221372804163954,
      "loss": 0.3847,
      "step": 1667000
    },
    {
      "epoch": 135.6049362636469,
      "grad_norm": 0.2480766922235489,
      "learning_rate": 0.0004321933962264151,
      "loss": 0.3847,
      "step": 1667500
    },
    {
      "epoch": 135.64559741395084,
      "grad_norm": 0.24186639487743378,
      "learning_rate": 0.0004321730644111906,
      "loss": 0.385,
      "step": 1668000
    },
    {
      "epoch": 135.68625856425479,
      "grad_norm": 0.23639662563800812,
      "learning_rate": 0.00043215273259596614,
      "loss": 0.3852,
      "step": 1668500
    },
    {
      "epoch": 135.72691971455873,
      "grad_norm": 0.2388053983449936,
      "learning_rate": 0.0004321324007807417,
      "loss": 0.3847,
      "step": 1669000
    },
    {
      "epoch": 135.76758086486268,
      "grad_norm": 0.24075078964233398,
      "learning_rate": 0.00043211206896551723,
      "loss": 0.3849,
      "step": 1669500
    },
    {
      "epoch": 135.8082420151666,
      "grad_norm": 0.21913142502307892,
      "learning_rate": 0.00043209173715029275,
      "loss": 0.3849,
      "step": 1670000
    },
    {
      "epoch": 135.84890316547055,
      "grad_norm": 0.2460787147283554,
      "learning_rate": 0.0004320714053350683,
      "loss": 0.3852,
      "step": 1670500
    },
    {
      "epoch": 135.8895643157745,
      "grad_norm": 0.21364519000053406,
      "learning_rate": 0.00043205107351984383,
      "loss": 0.3852,
      "step": 1671000
    },
    {
      "epoch": 135.93022546607844,
      "grad_norm": 0.22834408283233643,
      "learning_rate": 0.0004320307417046194,
      "loss": 0.3854,
      "step": 1671500
    },
    {
      "epoch": 135.9708866163824,
      "grad_norm": 0.20136655867099762,
      "learning_rate": 0.0004320104098893949,
      "loss": 0.3855,
      "step": 1672000
    },
    {
      "epoch": 136.0115477666863,
      "grad_norm": 0.2173222005367279,
      "learning_rate": 0.00043199007807417044,
      "loss": 0.3845,
      "step": 1672500
    },
    {
      "epoch": 136.05220891699025,
      "grad_norm": 0.2540047764778137,
      "learning_rate": 0.000431969746258946,
      "loss": 0.3819,
      "step": 1673000
    },
    {
      "epoch": 136.0928700672942,
      "grad_norm": 0.2607608735561371,
      "learning_rate": 0.0004319494144437215,
      "loss": 0.3822,
      "step": 1673500
    },
    {
      "epoch": 136.13353121759815,
      "grad_norm": 0.2337549924850464,
      "learning_rate": 0.00043192908262849704,
      "loss": 0.3827,
      "step": 1674000
    },
    {
      "epoch": 136.1741923679021,
      "grad_norm": 0.23573240637779236,
      "learning_rate": 0.0004319087508132726,
      "loss": 0.3827,
      "step": 1674500
    },
    {
      "epoch": 136.21485351820604,
      "grad_norm": 0.23164168000221252,
      "learning_rate": 0.0004318884189980481,
      "loss": 0.383,
      "step": 1675000
    },
    {
      "epoch": 136.25551466850996,
      "grad_norm": 0.2479521930217743,
      "learning_rate": 0.00043186808718282364,
      "loss": 0.3833,
      "step": 1675500
    },
    {
      "epoch": 136.2961758188139,
      "grad_norm": 0.21706002950668335,
      "learning_rate": 0.0004318477553675992,
      "loss": 0.3836,
      "step": 1676000
    },
    {
      "epoch": 136.33683696911785,
      "grad_norm": 0.21750777959823608,
      "learning_rate": 0.00043182742355237473,
      "loss": 0.3838,
      "step": 1676500
    },
    {
      "epoch": 136.3774981194218,
      "grad_norm": 0.24806742370128632,
      "learning_rate": 0.0004318070917371503,
      "loss": 0.3839,
      "step": 1677000
    },
    {
      "epoch": 136.41815926972575,
      "grad_norm": 0.23788559436798096,
      "learning_rate": 0.0004317867599219258,
      "loss": 0.3842,
      "step": 1677500
    },
    {
      "epoch": 136.4588204200297,
      "grad_norm": 0.2629674971103668,
      "learning_rate": 0.00043176642810670133,
      "loss": 0.3843,
      "step": 1678000
    },
    {
      "epoch": 136.49948157033361,
      "grad_norm": 0.23343589901924133,
      "learning_rate": 0.0004317460962914769,
      "loss": 0.384,
      "step": 1678500
    },
    {
      "epoch": 136.54014272063756,
      "grad_norm": 0.21555235981941223,
      "learning_rate": 0.0004317257644762524,
      "loss": 0.3848,
      "step": 1679000
    },
    {
      "epoch": 136.5808038709415,
      "grad_norm": 0.22999988496303558,
      "learning_rate": 0.00043170543266102794,
      "loss": 0.3845,
      "step": 1679500
    },
    {
      "epoch": 136.62146502124546,
      "grad_norm": 0.23855836689472198,
      "learning_rate": 0.00043168510084580356,
      "loss": 0.3844,
      "step": 1680000
    },
    {
      "epoch": 136.6621261715494,
      "grad_norm": 0.21606630086898804,
      "learning_rate": 0.0004316647690305791,
      "loss": 0.385,
      "step": 1680500
    },
    {
      "epoch": 136.70278732185332,
      "grad_norm": 0.24642811715602875,
      "learning_rate": 0.0004316444372153546,
      "loss": 0.3851,
      "step": 1681000
    },
    {
      "epoch": 136.74344847215727,
      "grad_norm": 0.23350012302398682,
      "learning_rate": 0.00043162410540013017,
      "loss": 0.3854,
      "step": 1681500
    },
    {
      "epoch": 136.78410962246122,
      "grad_norm": 0.255834698677063,
      "learning_rate": 0.0004316037735849057,
      "loss": 0.3848,
      "step": 1682000
    },
    {
      "epoch": 136.82477077276516,
      "grad_norm": 0.25054648518562317,
      "learning_rate": 0.00043158344176968125,
      "loss": 0.385,
      "step": 1682500
    },
    {
      "epoch": 136.8654319230691,
      "grad_norm": 0.2710411250591278,
      "learning_rate": 0.00043156310995445677,
      "loss": 0.3852,
      "step": 1683000
    },
    {
      "epoch": 136.90609307337306,
      "grad_norm": 0.21874013543128967,
      "learning_rate": 0.0004315427781392323,
      "loss": 0.3853,
      "step": 1683500
    },
    {
      "epoch": 136.94675422367698,
      "grad_norm": 0.23646557331085205,
      "learning_rate": 0.00043152244632400786,
      "loss": 0.3854,
      "step": 1684000
    },
    {
      "epoch": 136.98741537398092,
      "grad_norm": 0.2335420548915863,
      "learning_rate": 0.00043150211450878337,
      "loss": 0.3856,
      "step": 1684500
    },
    {
      "epoch": 137.02807652428487,
      "grad_norm": 0.21852195262908936,
      "learning_rate": 0.0004314817826935589,
      "loss": 0.3824,
      "step": 1685000
    },
    {
      "epoch": 137.06873767458882,
      "grad_norm": 0.24282623827457428,
      "learning_rate": 0.00043146145087833446,
      "loss": 0.3819,
      "step": 1685500
    },
    {
      "epoch": 137.10939882489276,
      "grad_norm": 0.24189908802509308,
      "learning_rate": 0.00043144111906311,
      "loss": 0.3828,
      "step": 1686000
    },
    {
      "epoch": 137.1500599751967,
      "grad_norm": 0.24463148415088654,
      "learning_rate": 0.00043142078724788555,
      "loss": 0.383,
      "step": 1686500
    },
    {
      "epoch": 137.19072112550063,
      "grad_norm": 0.226436048746109,
      "learning_rate": 0.00043140045543266106,
      "loss": 0.3832,
      "step": 1687000
    },
    {
      "epoch": 137.23138227580458,
      "grad_norm": 0.27224019169807434,
      "learning_rate": 0.0004313801236174366,
      "loss": 0.3829,
      "step": 1687500
    },
    {
      "epoch": 137.27204342610852,
      "grad_norm": 0.2226657122373581,
      "learning_rate": 0.00043135979180221215,
      "loss": 0.3832,
      "step": 1688000
    },
    {
      "epoch": 137.31270457641247,
      "grad_norm": 0.2238047569990158,
      "learning_rate": 0.00043133945998698767,
      "loss": 0.3836,
      "step": 1688500
    },
    {
      "epoch": 137.35336572671642,
      "grad_norm": 0.23256686329841614,
      "learning_rate": 0.0004313191281717632,
      "loss": 0.3833,
      "step": 1689000
    },
    {
      "epoch": 137.39402687702034,
      "grad_norm": 0.2589110732078552,
      "learning_rate": 0.00043129879635653875,
      "loss": 0.3838,
      "step": 1689500
    },
    {
      "epoch": 137.43468802732428,
      "grad_norm": 0.24520349502563477,
      "learning_rate": 0.00043127846454131427,
      "loss": 0.3841,
      "step": 1690000
    },
    {
      "epoch": 137.47534917762823,
      "grad_norm": 0.2139516919851303,
      "learning_rate": 0.0004312581327260898,
      "loss": 0.3842,
      "step": 1690500
    },
    {
      "epoch": 137.51601032793218,
      "grad_norm": 0.22683559358119965,
      "learning_rate": 0.00043123780091086536,
      "loss": 0.3845,
      "step": 1691000
    },
    {
      "epoch": 137.55667147823613,
      "grad_norm": 0.25734180212020874,
      "learning_rate": 0.0004312174690956409,
      "loss": 0.3843,
      "step": 1691500
    },
    {
      "epoch": 137.59733262854007,
      "grad_norm": 0.2154717743396759,
      "learning_rate": 0.00043119713728041644,
      "loss": 0.3848,
      "step": 1692000
    },
    {
      "epoch": 137.637993778844,
      "grad_norm": 0.21974745392799377,
      "learning_rate": 0.00043117680546519196,
      "loss": 0.3845,
      "step": 1692500
    },
    {
      "epoch": 137.67865492914794,
      "grad_norm": 0.2264372706413269,
      "learning_rate": 0.0004311564736499675,
      "loss": 0.3847,
      "step": 1693000
    },
    {
      "epoch": 137.7193160794519,
      "grad_norm": 0.26548874378204346,
      "learning_rate": 0.00043113614183474305,
      "loss": 0.3849,
      "step": 1693500
    },
    {
      "epoch": 137.75997722975583,
      "grad_norm": 0.21777582168579102,
      "learning_rate": 0.00043111581001951856,
      "loss": 0.3845,
      "step": 1694000
    },
    {
      "epoch": 137.80063838005978,
      "grad_norm": 0.2343124896287918,
      "learning_rate": 0.0004310954782042941,
      "loss": 0.3851,
      "step": 1694500
    },
    {
      "epoch": 137.84129953036373,
      "grad_norm": 0.24720938503742218,
      "learning_rate": 0.00043107514638906965,
      "loss": 0.3852,
      "step": 1695000
    },
    {
      "epoch": 137.88196068066765,
      "grad_norm": 0.23984567821025848,
      "learning_rate": 0.00043105481457384517,
      "loss": 0.3851,
      "step": 1695500
    },
    {
      "epoch": 137.9226218309716,
      "grad_norm": 0.2444416731595993,
      "learning_rate": 0.0004310344827586207,
      "loss": 0.3852,
      "step": 1696000
    },
    {
      "epoch": 137.96328298127554,
      "grad_norm": 0.22765223681926727,
      "learning_rate": 0.00043101415094339625,
      "loss": 0.3855,
      "step": 1696500
    },
    {
      "epoch": 138.0039441315795,
      "grad_norm": 0.23383145034313202,
      "learning_rate": 0.00043099381912817177,
      "loss": 0.3854,
      "step": 1697000
    },
    {
      "epoch": 138.04460528188343,
      "grad_norm": 0.20526409149169922,
      "learning_rate": 0.00043097348731294734,
      "loss": 0.3814,
      "step": 1697500
    },
    {
      "epoch": 138.08526643218735,
      "grad_norm": 0.2547139823436737,
      "learning_rate": 0.00043095315549772286,
      "loss": 0.382,
      "step": 1698000
    },
    {
      "epoch": 138.1259275824913,
      "grad_norm": 0.2516810894012451,
      "learning_rate": 0.0004309328236824984,
      "loss": 0.3823,
      "step": 1698500
    },
    {
      "epoch": 138.16658873279525,
      "grad_norm": 0.23489350080490112,
      "learning_rate": 0.00043091249186727394,
      "loss": 0.3827,
      "step": 1699000
    },
    {
      "epoch": 138.2072498830992,
      "grad_norm": 0.2166580855846405,
      "learning_rate": 0.00043089216005204946,
      "loss": 0.3828,
      "step": 1699500
    },
    {
      "epoch": 138.24791103340314,
      "grad_norm": 0.26777878403663635,
      "learning_rate": 0.000430871828236825,
      "loss": 0.3833,
      "step": 1700000
    },
    {
      "epoch": 138.2885721837071,
      "grad_norm": 0.2661481499671936,
      "learning_rate": 0.00043085149642160055,
      "loss": 0.3833,
      "step": 1700500
    },
    {
      "epoch": 138.329233334011,
      "grad_norm": 0.22801855206489563,
      "learning_rate": 0.00043083116460637606,
      "loss": 0.3834,
      "step": 1701000
    },
    {
      "epoch": 138.36989448431495,
      "grad_norm": 0.24221310019493103,
      "learning_rate": 0.00043081083279115164,
      "loss": 0.3835,
      "step": 1701500
    },
    {
      "epoch": 138.4105556346189,
      "grad_norm": 0.2539416253566742,
      "learning_rate": 0.00043079050097592715,
      "loss": 0.3839,
      "step": 1702000
    },
    {
      "epoch": 138.45121678492285,
      "grad_norm": 0.2290881872177124,
      "learning_rate": 0.00043077016916070267,
      "loss": 0.3839,
      "step": 1702500
    },
    {
      "epoch": 138.4918779352268,
      "grad_norm": 0.23851333558559418,
      "learning_rate": 0.00043074983734547824,
      "loss": 0.3841,
      "step": 1703000
    },
    {
      "epoch": 138.53253908553074,
      "grad_norm": 0.27127858996391296,
      "learning_rate": 0.00043072950553025376,
      "loss": 0.3841,
      "step": 1703500
    },
    {
      "epoch": 138.57320023583466,
      "grad_norm": 0.226775124669075,
      "learning_rate": 0.00043070917371502927,
      "loss": 0.3839,
      "step": 1704000
    },
    {
      "epoch": 138.6138613861386,
      "grad_norm": 0.22345836460590363,
      "learning_rate": 0.00043068884189980484,
      "loss": 0.3851,
      "step": 1704500
    },
    {
      "epoch": 138.65452253644256,
      "grad_norm": 0.23402869701385498,
      "learning_rate": 0.00043066851008458036,
      "loss": 0.3848,
      "step": 1705000
    },
    {
      "epoch": 138.6951836867465,
      "grad_norm": 0.23873254656791687,
      "learning_rate": 0.0004306481782693559,
      "loss": 0.3848,
      "step": 1705500
    },
    {
      "epoch": 138.73584483705045,
      "grad_norm": 0.23894473910331726,
      "learning_rate": 0.00043062784645413145,
      "loss": 0.3847,
      "step": 1706000
    },
    {
      "epoch": 138.77650598735437,
      "grad_norm": 0.21529489755630493,
      "learning_rate": 0.00043060751463890696,
      "loss": 0.3849,
      "step": 1706500
    },
    {
      "epoch": 138.81716713765832,
      "grad_norm": 0.2626270353794098,
      "learning_rate": 0.00043058718282368253,
      "loss": 0.3849,
      "step": 1707000
    },
    {
      "epoch": 138.85782828796226,
      "grad_norm": 0.200204998254776,
      "learning_rate": 0.00043056685100845805,
      "loss": 0.3851,
      "step": 1707500
    },
    {
      "epoch": 138.8984894382662,
      "grad_norm": 0.23882631957530975,
      "learning_rate": 0.00043054651919323357,
      "loss": 0.3854,
      "step": 1708000
    },
    {
      "epoch": 138.93915058857016,
      "grad_norm": 0.24734282493591309,
      "learning_rate": 0.00043052618737800914,
      "loss": 0.3853,
      "step": 1708500
    },
    {
      "epoch": 138.9798117388741,
      "grad_norm": 0.23021554946899414,
      "learning_rate": 0.00043050585556278465,
      "loss": 0.3852,
      "step": 1709000
    },
    {
      "epoch": 139.02047288917802,
      "grad_norm": 0.23092220723628998,
      "learning_rate": 0.00043048552374756017,
      "loss": 0.3835,
      "step": 1709500
    },
    {
      "epoch": 139.06113403948197,
      "grad_norm": 0.2259545922279358,
      "learning_rate": 0.00043046519193233574,
      "loss": 0.3818,
      "step": 1710000
    },
    {
      "epoch": 139.10179518978592,
      "grad_norm": 0.22131933271884918,
      "learning_rate": 0.00043044486011711126,
      "loss": 0.3822,
      "step": 1710500
    },
    {
      "epoch": 139.14245634008986,
      "grad_norm": 0.22528432309627533,
      "learning_rate": 0.00043042452830188677,
      "loss": 0.3823,
      "step": 1711000
    },
    {
      "epoch": 139.1831174903938,
      "grad_norm": 0.24026867747306824,
      "learning_rate": 0.00043040419648666234,
      "loss": 0.3829,
      "step": 1711500
    },
    {
      "epoch": 139.22377864069776,
      "grad_norm": 0.21845263242721558,
      "learning_rate": 0.00043038386467143786,
      "loss": 0.3829,
      "step": 1712000
    }
  ],
  "logging_steps": 500,
  "max_steps": 12296000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 500,
  "total_flos": 9.337334431386586e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
