{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500.0,
  "global_step": 200000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0025,
      "grad_norm": 38.44445037841797,
      "learning_rate": 5e-08,
      "loss": 6.841,
      "step": 500
    },
    {
      "epoch": 0.005,
      "grad_norm": 6.4877753257751465,
      "learning_rate": 1e-07,
      "loss": 4.3835,
      "step": 1000
    },
    {
      "epoch": 0.0075,
      "grad_norm": 6.467931747436523,
      "learning_rate": 1.5e-07,
      "loss": 3.3806,
      "step": 1500
    },
    {
      "epoch": 0.01,
      "grad_norm": 5.236299514770508,
      "learning_rate": 2e-07,
      "loss": 2.8247,
      "step": 2000
    },
    {
      "epoch": 0.0125,
      "grad_norm": 5.700149059295654,
      "learning_rate": 2.5e-07,
      "loss": 2.5317,
      "step": 2500
    },
    {
      "epoch": 0.015,
      "grad_norm": 5.550288677215576,
      "learning_rate": 3e-07,
      "loss": 2.3653,
      "step": 3000
    },
    {
      "epoch": 0.0175,
      "grad_norm": 9.093704223632812,
      "learning_rate": 3.5e-07,
      "loss": 2.2426,
      "step": 3500
    },
    {
      "epoch": 0.02,
      "grad_norm": 5.227163314819336,
      "learning_rate": 4e-07,
      "loss": 2.134,
      "step": 4000
    },
    {
      "epoch": 0.0225,
      "grad_norm": 5.844882011413574,
      "learning_rate": 4.5e-07,
      "loss": 2.0322,
      "step": 4500
    },
    {
      "epoch": 0.025,
      "grad_norm": 4.96788215637207,
      "learning_rate": 5e-07,
      "loss": 1.9336,
      "step": 5000
    },
    {
      "epoch": 0.0275,
      "grad_norm": 6.551884651184082,
      "learning_rate": 5.5e-07,
      "loss": 1.8413,
      "step": 5500
    },
    {
      "epoch": 0.03,
      "grad_norm": 7.4743523597717285,
      "learning_rate": 6e-07,
      "loss": 1.7539,
      "step": 6000
    },
    {
      "epoch": 0.0325,
      "grad_norm": 6.0163893699646,
      "learning_rate": 6.5e-07,
      "loss": 1.665,
      "step": 6500
    },
    {
      "epoch": 0.035,
      "grad_norm": 6.539722919464111,
      "learning_rate": 7e-07,
      "loss": 1.5892,
      "step": 7000
    },
    {
      "epoch": 0.0375,
      "grad_norm": 6.3940629959106445,
      "learning_rate": 7.5e-07,
      "loss": 1.5152,
      "step": 7500
    },
    {
      "epoch": 0.04,
      "grad_norm": 4.574140548706055,
      "learning_rate": 8e-07,
      "loss": 1.4486,
      "step": 8000
    },
    {
      "epoch": 0.0425,
      "grad_norm": 5.6923956871032715,
      "learning_rate": 8.499999999999999e-07,
      "loss": 1.3887,
      "step": 8500
    },
    {
      "epoch": 0.045,
      "grad_norm": 6.922948837280273,
      "learning_rate": 9e-07,
      "loss": 1.3337,
      "step": 9000
    },
    {
      "epoch": 0.0475,
      "grad_norm": 6.032220363616943,
      "learning_rate": 9.499999999999999e-07,
      "loss": 1.2869,
      "step": 9500
    },
    {
      "epoch": 0.05,
      "grad_norm": 6.1115217208862305,
      "learning_rate": 1e-06,
      "loss": 1.2411,
      "step": 10000
    },
    {
      "epoch": 0.0525,
      "grad_norm": 6.635465145111084,
      "learning_rate": 9.973684210526315e-07,
      "loss": 1.2015,
      "step": 10500
    },
    {
      "epoch": 0.055,
      "grad_norm": 8.374279022216797,
      "learning_rate": 9.947368421052631e-07,
      "loss": 1.1715,
      "step": 11000
    },
    {
      "epoch": 0.0575,
      "grad_norm": 8.689223289489746,
      "learning_rate": 9.921052631578947e-07,
      "loss": 1.1428,
      "step": 11500
    },
    {
      "epoch": 0.06,
      "grad_norm": 6.84194278717041,
      "learning_rate": 9.894736842105263e-07,
      "loss": 1.1186,
      "step": 12000
    },
    {
      "epoch": 0.0625,
      "grad_norm": 4.753087997436523,
      "learning_rate": 9.868421052631579e-07,
      "loss": 1.0998,
      "step": 12500
    },
    {
      "epoch": 0.065,
      "grad_norm": 5.613441467285156,
      "learning_rate": 9.842105263157894e-07,
      "loss": 1.0798,
      "step": 13000
    },
    {
      "epoch": 0.0675,
      "grad_norm": 4.8269147872924805,
      "learning_rate": 9.81578947368421e-07,
      "loss": 1.0605,
      "step": 13500
    },
    {
      "epoch": 0.07,
      "grad_norm": 5.221214771270752,
      "learning_rate": 9.789473684210526e-07,
      "loss": 1.0483,
      "step": 14000
    },
    {
      "epoch": 0.0725,
      "grad_norm": 6.966050624847412,
      "learning_rate": 9.763157894736842e-07,
      "loss": 1.034,
      "step": 14500
    },
    {
      "epoch": 0.075,
      "grad_norm": 6.127124786376953,
      "learning_rate": 9.736842105263158e-07,
      "loss": 1.0197,
      "step": 15000
    },
    {
      "epoch": 0.0775,
      "grad_norm": 5.194082260131836,
      "learning_rate": 9.710526315789474e-07,
      "loss": 1.0093,
      "step": 15500
    },
    {
      "epoch": 0.08,
      "grad_norm": 5.753611087799072,
      "learning_rate": 9.68421052631579e-07,
      "loss": 0.998,
      "step": 16000
    },
    {
      "epoch": 0.0825,
      "grad_norm": 5.953752517700195,
      "learning_rate": 9.657894736842105e-07,
      "loss": 0.9913,
      "step": 16500
    },
    {
      "epoch": 0.085,
      "grad_norm": 6.101182460784912,
      "learning_rate": 9.63157894736842e-07,
      "loss": 0.9805,
      "step": 17000
    },
    {
      "epoch": 0.0875,
      "grad_norm": 5.241979122161865,
      "learning_rate": 9.605263157894737e-07,
      "loss": 0.9707,
      "step": 17500
    },
    {
      "epoch": 0.09,
      "grad_norm": 5.7999587059021,
      "learning_rate": 9.578947368421053e-07,
      "loss": 0.9643,
      "step": 18000
    },
    {
      "epoch": 0.0925,
      "grad_norm": 4.869426727294922,
      "learning_rate": 9.552631578947368e-07,
      "loss": 0.9554,
      "step": 18500
    },
    {
      "epoch": 0.095,
      "grad_norm": 5.887923717498779,
      "learning_rate": 9.526315789473683e-07,
      "loss": 0.9494,
      "step": 19000
    },
    {
      "epoch": 0.0975,
      "grad_norm": 5.747674942016602,
      "learning_rate": 9.499999999999999e-07,
      "loss": 0.9401,
      "step": 19500
    },
    {
      "epoch": 0.1,
      "grad_norm": 5.371726989746094,
      "learning_rate": 9.473684210526315e-07,
      "loss": 0.9363,
      "step": 20000
    },
    {
      "epoch": 0.1025,
      "grad_norm": 6.869240760803223,
      "learning_rate": 9.447368421052632e-07,
      "loss": 0.9288,
      "step": 20500
    },
    {
      "epoch": 0.105,
      "grad_norm": 5.385630130767822,
      "learning_rate": 9.421052631578948e-07,
      "loss": 0.9237,
      "step": 21000
    },
    {
      "epoch": 0.1075,
      "grad_norm": 4.643253803253174,
      "learning_rate": 9.394736842105263e-07,
      "loss": 0.916,
      "step": 21500
    },
    {
      "epoch": 0.11,
      "grad_norm": 4.629791736602783,
      "learning_rate": 9.368421052631579e-07,
      "loss": 0.9122,
      "step": 22000
    },
    {
      "epoch": 0.1125,
      "grad_norm": 4.400000095367432,
      "learning_rate": 9.342105263157895e-07,
      "loss": 0.9069,
      "step": 22500
    },
    {
      "epoch": 0.115,
      "grad_norm": 5.112158298492432,
      "learning_rate": 9.31578947368421e-07,
      "loss": 0.9009,
      "step": 23000
    },
    {
      "epoch": 0.1175,
      "grad_norm": 5.210470199584961,
      "learning_rate": 9.289473684210526e-07,
      "loss": 0.8991,
      "step": 23500
    },
    {
      "epoch": 0.12,
      "grad_norm": 5.949810028076172,
      "learning_rate": 9.263157894736841e-07,
      "loss": 0.8939,
      "step": 24000
    },
    {
      "epoch": 0.1225,
      "grad_norm": 5.447815895080566,
      "learning_rate": 9.236842105263157e-07,
      "loss": 0.8883,
      "step": 24500
    },
    {
      "epoch": 0.125,
      "grad_norm": 4.893320083618164,
      "learning_rate": 9.210526315789473e-07,
      "loss": 0.8858,
      "step": 25000
    },
    {
      "epoch": 0.1275,
      "grad_norm": 5.248805046081543,
      "learning_rate": 9.184210526315789e-07,
      "loss": 0.8805,
      "step": 25500
    },
    {
      "epoch": 0.13,
      "grad_norm": 5.646461486816406,
      "learning_rate": 9.157894736842105e-07,
      "loss": 0.877,
      "step": 26000
    },
    {
      "epoch": 0.1325,
      "grad_norm": 5.312832832336426,
      "learning_rate": 9.13157894736842e-07,
      "loss": 0.8731,
      "step": 26500
    },
    {
      "epoch": 0.135,
      "grad_norm": 5.116029739379883,
      "learning_rate": 9.105263157894737e-07,
      "loss": 0.8714,
      "step": 27000
    },
    {
      "epoch": 0.1375,
      "grad_norm": 5.302462577819824,
      "learning_rate": 9.078947368421053e-07,
      "loss": 0.8671,
      "step": 27500
    },
    {
      "epoch": 0.14,
      "grad_norm": 4.796843528747559,
      "learning_rate": 9.052631578947368e-07,
      "loss": 0.864,
      "step": 28000
    },
    {
      "epoch": 0.1425,
      "grad_norm": 4.780074596405029,
      "learning_rate": 9.026315789473684e-07,
      "loss": 0.8606,
      "step": 28500
    },
    {
      "epoch": 0.145,
      "grad_norm": 5.055174350738525,
      "learning_rate": 9e-07,
      "loss": 0.8572,
      "step": 29000
    },
    {
      "epoch": 0.1475,
      "grad_norm": 5.118841648101807,
      "learning_rate": 8.973684210526315e-07,
      "loss": 0.8548,
      "step": 29500
    },
    {
      "epoch": 0.15,
      "grad_norm": 4.790858268737793,
      "learning_rate": 8.947368421052631e-07,
      "loss": 0.8518,
      "step": 30000
    },
    {
      "epoch": 0.1525,
      "grad_norm": 5.690489768981934,
      "learning_rate": 8.921052631578947e-07,
      "loss": 0.8497,
      "step": 30500
    },
    {
      "epoch": 0.155,
      "grad_norm": 6.1177849769592285,
      "learning_rate": 8.894736842105263e-07,
      "loss": 0.8474,
      "step": 31000
    },
    {
      "epoch": 0.1575,
      "grad_norm": 4.699473857879639,
      "learning_rate": 8.868421052631579e-07,
      "loss": 0.8449,
      "step": 31500
    },
    {
      "epoch": 0.16,
      "grad_norm": 4.330212593078613,
      "learning_rate": 8.842105263157895e-07,
      "loss": 0.8404,
      "step": 32000
    },
    {
      "epoch": 0.1625,
      "grad_norm": 4.593630313873291,
      "learning_rate": 8.815789473684209e-07,
      "loss": 0.8382,
      "step": 32500
    },
    {
      "epoch": 0.165,
      "grad_norm": 4.971773624420166,
      "learning_rate": 8.789473684210525e-07,
      "loss": 0.8365,
      "step": 33000
    },
    {
      "epoch": 0.1675,
      "grad_norm": 5.192533016204834,
      "learning_rate": 8.763157894736841e-07,
      "loss": 0.8327,
      "step": 33500
    },
    {
      "epoch": 0.17,
      "grad_norm": 5.548885822296143,
      "learning_rate": 8.736842105263158e-07,
      "loss": 0.8302,
      "step": 34000
    },
    {
      "epoch": 0.1725,
      "grad_norm": 5.8036298751831055,
      "learning_rate": 8.710526315789474e-07,
      "loss": 0.8298,
      "step": 34500
    },
    {
      "epoch": 0.175,
      "grad_norm": 3.8379650115966797,
      "learning_rate": 8.684210526315789e-07,
      "loss": 0.8253,
      "step": 35000
    },
    {
      "epoch": 0.1775,
      "grad_norm": 5.089008808135986,
      "learning_rate": 8.657894736842105e-07,
      "loss": 0.825,
      "step": 35500
    },
    {
      "epoch": 0.18,
      "grad_norm": 6.015099048614502,
      "learning_rate": 8.631578947368421e-07,
      "loss": 0.8225,
      "step": 36000
    },
    {
      "epoch": 0.1825,
      "grad_norm": 5.496542453765869,
      "learning_rate": 8.605263157894737e-07,
      "loss": 0.8209,
      "step": 36500
    },
    {
      "epoch": 0.185,
      "grad_norm": 4.574227809906006,
      "learning_rate": 8.578947368421053e-07,
      "loss": 0.8172,
      "step": 37000
    },
    {
      "epoch": 0.1875,
      "grad_norm": 4.346624851226807,
      "learning_rate": 8.552631578947367e-07,
      "loss": 0.8139,
      "step": 37500
    },
    {
      "epoch": 0.19,
      "grad_norm": 4.290589809417725,
      "learning_rate": 8.526315789473683e-07,
      "loss": 0.8151,
      "step": 38000
    },
    {
      "epoch": 0.1925,
      "grad_norm": 5.564870834350586,
      "learning_rate": 8.499999999999999e-07,
      "loss": 0.8128,
      "step": 38500
    },
    {
      "epoch": 0.195,
      "grad_norm": 4.423109531402588,
      "learning_rate": 8.473684210526315e-07,
      "loss": 0.8114,
      "step": 39000
    },
    {
      "epoch": 0.1975,
      "grad_norm": 4.806586742401123,
      "learning_rate": 8.447368421052631e-07,
      "loss": 0.809,
      "step": 39500
    },
    {
      "epoch": 0.2,
      "grad_norm": 5.040576934814453,
      "learning_rate": 8.421052631578947e-07,
      "loss": 0.8071,
      "step": 40000
    },
    {
      "epoch": 0.2025,
      "grad_norm": 4.702362537384033,
      "learning_rate": 8.394736842105262e-07,
      "loss": 0.8053,
      "step": 40500
    },
    {
      "epoch": 0.205,
      "grad_norm": 4.7340521812438965,
      "learning_rate": 8.368421052631579e-07,
      "loss": 0.8055,
      "step": 41000
    },
    {
      "epoch": 0.2075,
      "grad_norm": 4.512388706207275,
      "learning_rate": 8.342105263157895e-07,
      "loss": 0.8016,
      "step": 41500
    },
    {
      "epoch": 0.21,
      "grad_norm": 5.167755603790283,
      "learning_rate": 8.315789473684211e-07,
      "loss": 0.8014,
      "step": 42000
    },
    {
      "epoch": 0.2125,
      "grad_norm": 4.4968767166137695,
      "learning_rate": 8.289473684210527e-07,
      "loss": 0.7993,
      "step": 42500
    },
    {
      "epoch": 0.215,
      "grad_norm": 4.227855205535889,
      "learning_rate": 8.263157894736841e-07,
      "loss": 0.7996,
      "step": 43000
    },
    {
      "epoch": 0.2175,
      "grad_norm": 4.1859283447265625,
      "learning_rate": 8.236842105263157e-07,
      "loss": 0.7961,
      "step": 43500
    },
    {
      "epoch": 0.22,
      "grad_norm": 4.702017307281494,
      "learning_rate": 8.210526315789473e-07,
      "loss": 0.7964,
      "step": 44000
    },
    {
      "epoch": 0.2225,
      "grad_norm": 5.9881062507629395,
      "learning_rate": 8.184210526315789e-07,
      "loss": 0.7921,
      "step": 44500
    },
    {
      "epoch": 0.225,
      "grad_norm": 6.244349002838135,
      "learning_rate": 8.157894736842105e-07,
      "loss": 0.7911,
      "step": 45000
    },
    {
      "epoch": 0.2275,
      "grad_norm": 4.385039806365967,
      "learning_rate": 8.131578947368421e-07,
      "loss": 0.7922,
      "step": 45500
    },
    {
      "epoch": 0.23,
      "grad_norm": 4.2475433349609375,
      "learning_rate": 8.105263157894736e-07,
      "loss": 0.7901,
      "step": 46000
    },
    {
      "epoch": 0.2325,
      "grad_norm": 4.7563018798828125,
      "learning_rate": 8.078947368421052e-07,
      "loss": 0.7901,
      "step": 46500
    },
    {
      "epoch": 0.235,
      "grad_norm": 4.29437255859375,
      "learning_rate": 8.052631578947368e-07,
      "loss": 0.7878,
      "step": 47000
    },
    {
      "epoch": 0.2375,
      "grad_norm": 4.72237491607666,
      "learning_rate": 8.026315789473685e-07,
      "loss": 0.7857,
      "step": 47500
    },
    {
      "epoch": 0.24,
      "grad_norm": 4.573785305023193,
      "learning_rate": 8e-07,
      "loss": 0.7862,
      "step": 48000
    },
    {
      "epoch": 0.2425,
      "grad_norm": 4.04630184173584,
      "learning_rate": 7.973684210526315e-07,
      "loss": 0.7838,
      "step": 48500
    },
    {
      "epoch": 0.245,
      "grad_norm": 4.3445868492126465,
      "learning_rate": 7.947368421052631e-07,
      "loss": 0.7836,
      "step": 49000
    },
    {
      "epoch": 0.2475,
      "grad_norm": 5.318534851074219,
      "learning_rate": 7.921052631578947e-07,
      "loss": 0.7808,
      "step": 49500
    },
    {
      "epoch": 0.25,
      "grad_norm": 4.581177234649658,
      "learning_rate": 7.894736842105263e-07,
      "loss": 0.7791,
      "step": 50000
    },
    {
      "epoch": 0.2525,
      "grad_norm": 4.2996649742126465,
      "learning_rate": 7.868421052631579e-07,
      "loss": 0.7763,
      "step": 50500
    },
    {
      "epoch": 0.255,
      "grad_norm": 4.210526466369629,
      "learning_rate": 7.842105263157895e-07,
      "loss": 0.7785,
      "step": 51000
    },
    {
      "epoch": 0.2575,
      "grad_norm": 4.455772399902344,
      "learning_rate": 7.81578947368421e-07,
      "loss": 0.7774,
      "step": 51500
    },
    {
      "epoch": 0.26,
      "grad_norm": 4.352372169494629,
      "learning_rate": 7.789473684210526e-07,
      "loss": 0.776,
      "step": 52000
    },
    {
      "epoch": 0.2625,
      "grad_norm": 5.4085307121276855,
      "learning_rate": 7.763157894736841e-07,
      "loss": 0.772,
      "step": 52500
    },
    {
      "epoch": 0.265,
      "grad_norm": 4.156289100646973,
      "learning_rate": 7.736842105263157e-07,
      "loss": 0.7727,
      "step": 53000
    },
    {
      "epoch": 0.2675,
      "grad_norm": 4.013081073760986,
      "learning_rate": 7.710526315789473e-07,
      "loss": 0.7719,
      "step": 53500
    },
    {
      "epoch": 0.27,
      "grad_norm": 4.4248046875,
      "learning_rate": 7.684210526315788e-07,
      "loss": 0.7726,
      "step": 54000
    },
    {
      "epoch": 0.2725,
      "grad_norm": 4.54379415512085,
      "learning_rate": 7.657894736842105e-07,
      "loss": 0.768,
      "step": 54500
    },
    {
      "epoch": 0.275,
      "grad_norm": 4.371565818786621,
      "learning_rate": 7.631578947368421e-07,
      "loss": 0.7695,
      "step": 55000
    },
    {
      "epoch": 0.2775,
      "grad_norm": 4.091033458709717,
      "learning_rate": 7.605263157894737e-07,
      "loss": 0.7698,
      "step": 55500
    },
    {
      "epoch": 0.28,
      "grad_norm": 5.025193691253662,
      "learning_rate": 7.578947368421053e-07,
      "loss": 0.7668,
      "step": 56000
    },
    {
      "epoch": 0.2825,
      "grad_norm": 4.627081871032715,
      "learning_rate": 7.552631578947369e-07,
      "loss": 0.7651,
      "step": 56500
    },
    {
      "epoch": 0.285,
      "grad_norm": 4.351393222808838,
      "learning_rate": 7.526315789473684e-07,
      "loss": 0.7667,
      "step": 57000
    },
    {
      "epoch": 0.2875,
      "grad_norm": 4.578866004943848,
      "learning_rate": 7.5e-07,
      "loss": 0.765,
      "step": 57500
    },
    {
      "epoch": 0.29,
      "grad_norm": 4.5532636642456055,
      "learning_rate": 7.473684210526315e-07,
      "loss": 0.7651,
      "step": 58000
    },
    {
      "epoch": 0.2925,
      "grad_norm": 5.108657360076904,
      "learning_rate": 7.447368421052631e-07,
      "loss": 0.7623,
      "step": 58500
    },
    {
      "epoch": 0.295,
      "grad_norm": 4.04977560043335,
      "learning_rate": 7.421052631578947e-07,
      "loss": 0.7612,
      "step": 59000
    },
    {
      "epoch": 0.2975,
      "grad_norm": 5.253478050231934,
      "learning_rate": 7.394736842105262e-07,
      "loss": 0.7634,
      "step": 59500
    },
    {
      "epoch": 0.3,
      "grad_norm": 4.7739481925964355,
      "learning_rate": 7.368421052631578e-07,
      "loss": 0.7588,
      "step": 60000
    },
    {
      "epoch": 0.3025,
      "grad_norm": 4.418645858764648,
      "learning_rate": 7.342105263157894e-07,
      "loss": 0.7594,
      "step": 60500
    },
    {
      "epoch": 0.305,
      "grad_norm": 4.026749610900879,
      "learning_rate": 7.315789473684211e-07,
      "loss": 0.7595,
      "step": 61000
    },
    {
      "epoch": 0.3075,
      "grad_norm": 4.470380783081055,
      "learning_rate": 7.289473684210527e-07,
      "loss": 0.7581,
      "step": 61500
    },
    {
      "epoch": 0.31,
      "grad_norm": 3.8722379207611084,
      "learning_rate": 7.263157894736843e-07,
      "loss": 0.7571,
      "step": 62000
    },
    {
      "epoch": 0.3125,
      "grad_norm": 4.148573875427246,
      "learning_rate": 7.236842105263158e-07,
      "loss": 0.7562,
      "step": 62500
    },
    {
      "epoch": 0.315,
      "grad_norm": 4.0593743324279785,
      "learning_rate": 7.210526315789473e-07,
      "loss": 0.7563,
      "step": 63000
    },
    {
      "epoch": 0.3175,
      "grad_norm": 4.733522891998291,
      "learning_rate": 7.184210526315789e-07,
      "loss": 0.7545,
      "step": 63500
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.810825824737549,
      "learning_rate": 7.157894736842105e-07,
      "loss": 0.7544,
      "step": 64000
    },
    {
      "epoch": 0.3225,
      "grad_norm": 5.366748332977295,
      "learning_rate": 7.131578947368421e-07,
      "loss": 0.7515,
      "step": 64500
    },
    {
      "epoch": 0.325,
      "grad_norm": 4.724297523498535,
      "learning_rate": 7.105263157894736e-07,
      "loss": 0.752,
      "step": 65000
    },
    {
      "epoch": 0.3275,
      "grad_norm": 4.160364627838135,
      "learning_rate": 7.078947368421052e-07,
      "loss": 0.7508,
      "step": 65500
    },
    {
      "epoch": 0.33,
      "grad_norm": 4.2631354331970215,
      "learning_rate": 7.052631578947368e-07,
      "loss": 0.7531,
      "step": 66000
    },
    {
      "epoch": 0.3325,
      "grad_norm": 3.6289212703704834,
      "learning_rate": 7.026315789473684e-07,
      "loss": 0.7503,
      "step": 66500
    },
    {
      "epoch": 0.335,
      "grad_norm": 3.9535720348358154,
      "learning_rate": 7e-07,
      "loss": 0.7486,
      "step": 67000
    },
    {
      "epoch": 0.3375,
      "grad_norm": 4.365769386291504,
      "learning_rate": 6.973684210526314e-07,
      "loss": 0.7496,
      "step": 67500
    },
    {
      "epoch": 0.34,
      "grad_norm": 4.126394748687744,
      "learning_rate": 6.947368421052631e-07,
      "loss": 0.7477,
      "step": 68000
    },
    {
      "epoch": 0.3425,
      "grad_norm": 4.488913536071777,
      "learning_rate": 6.921052631578947e-07,
      "loss": 0.7479,
      "step": 68500
    },
    {
      "epoch": 0.345,
      "grad_norm": 5.090478897094727,
      "learning_rate": 6.894736842105263e-07,
      "loss": 0.7482,
      "step": 69000
    },
    {
      "epoch": 0.3475,
      "grad_norm": 4.560856819152832,
      "learning_rate": 6.868421052631579e-07,
      "loss": 0.747,
      "step": 69500
    },
    {
      "epoch": 0.35,
      "grad_norm": 3.872039794921875,
      "learning_rate": 6.842105263157895e-07,
      "loss": 0.7462,
      "step": 70000
    },
    {
      "epoch": 0.3525,
      "grad_norm": 3.841975450515747,
      "learning_rate": 6.81578947368421e-07,
      "loss": 0.7449,
      "step": 70500
    },
    {
      "epoch": 0.355,
      "grad_norm": 4.177123069763184,
      "learning_rate": 6.789473684210526e-07,
      "loss": 0.7456,
      "step": 71000
    },
    {
      "epoch": 0.3575,
      "grad_norm": 4.1787190437316895,
      "learning_rate": 6.763157894736842e-07,
      "loss": 0.7445,
      "step": 71500
    },
    {
      "epoch": 0.36,
      "grad_norm": 4.104720592498779,
      "learning_rate": 6.736842105263158e-07,
      "loss": 0.7427,
      "step": 72000
    },
    {
      "epoch": 0.3625,
      "grad_norm": 3.946625232696533,
      "learning_rate": 6.710526315789473e-07,
      "loss": 0.7414,
      "step": 72500
    },
    {
      "epoch": 0.365,
      "grad_norm": 5.591818809509277,
      "learning_rate": 6.684210526315788e-07,
      "loss": 0.7435,
      "step": 73000
    },
    {
      "epoch": 0.3675,
      "grad_norm": 4.131257057189941,
      "learning_rate": 6.657894736842104e-07,
      "loss": 0.7414,
      "step": 73500
    },
    {
      "epoch": 0.37,
      "grad_norm": 4.601494312286377,
      "learning_rate": 6.63157894736842e-07,
      "loss": 0.7417,
      "step": 74000
    },
    {
      "epoch": 0.3725,
      "grad_norm": 4.174797058105469,
      "learning_rate": 6.605263157894737e-07,
      "loss": 0.7432,
      "step": 74500
    },
    {
      "epoch": 0.375,
      "grad_norm": 4.580254554748535,
      "learning_rate": 6.578947368421053e-07,
      "loss": 0.7388,
      "step": 75000
    },
    {
      "epoch": 0.3775,
      "grad_norm": 4.022863388061523,
      "learning_rate": 6.552631578947369e-07,
      "loss": 0.739,
      "step": 75500
    },
    {
      "epoch": 0.38,
      "grad_norm": 3.9865171909332275,
      "learning_rate": 6.526315789473684e-07,
      "loss": 0.7394,
      "step": 76000
    },
    {
      "epoch": 0.3825,
      "grad_norm": 3.7014341354370117,
      "learning_rate": 6.5e-07,
      "loss": 0.7395,
      "step": 76500
    },
    {
      "epoch": 0.385,
      "grad_norm": 4.104966640472412,
      "learning_rate": 6.473684210526316e-07,
      "loss": 0.7394,
      "step": 77000
    },
    {
      "epoch": 0.3875,
      "grad_norm": 3.774867057800293,
      "learning_rate": 6.447368421052632e-07,
      "loss": 0.7371,
      "step": 77500
    },
    {
      "epoch": 0.39,
      "grad_norm": 3.8254332542419434,
      "learning_rate": 6.421052631578947e-07,
      "loss": 0.7369,
      "step": 78000
    },
    {
      "epoch": 0.3925,
      "grad_norm": 4.71447229385376,
      "learning_rate": 6.394736842105262e-07,
      "loss": 0.7335,
      "step": 78500
    },
    {
      "epoch": 0.395,
      "grad_norm": 4.619505405426025,
      "learning_rate": 6.368421052631578e-07,
      "loss": 0.7355,
      "step": 79000
    },
    {
      "epoch": 0.3975,
      "grad_norm": 4.108183860778809,
      "learning_rate": 6.342105263157894e-07,
      "loss": 0.7348,
      "step": 79500
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.674427032470703,
      "learning_rate": 6.31578947368421e-07,
      "loss": 0.7337,
      "step": 80000
    },
    {
      "epoch": 0.4025,
      "grad_norm": 3.8835723400115967,
      "learning_rate": 6.289473684210526e-07,
      "loss": 0.7346,
      "step": 80500
    },
    {
      "epoch": 0.405,
      "grad_norm": 4.244559288024902,
      "learning_rate": 6.263157894736842e-07,
      "loss": 0.7317,
      "step": 81000
    },
    {
      "epoch": 0.4075,
      "grad_norm": 4.558086395263672,
      "learning_rate": 6.236842105263158e-07,
      "loss": 0.7328,
      "step": 81500
    },
    {
      "epoch": 0.41,
      "grad_norm": 4.135880470275879,
      "learning_rate": 6.210526315789474e-07,
      "loss": 0.733,
      "step": 82000
    },
    {
      "epoch": 0.4125,
      "grad_norm": 4.454000473022461,
      "learning_rate": 6.18421052631579e-07,
      "loss": 0.7322,
      "step": 82500
    },
    {
      "epoch": 0.415,
      "grad_norm": 4.128785610198975,
      "learning_rate": 6.157894736842105e-07,
      "loss": 0.7329,
      "step": 83000
    },
    {
      "epoch": 0.4175,
      "grad_norm": 3.759394884109497,
      "learning_rate": 6.131578947368421e-07,
      "loss": 0.7328,
      "step": 83500
    },
    {
      "epoch": 0.42,
      "grad_norm": 3.6742794513702393,
      "learning_rate": 6.105263157894736e-07,
      "loss": 0.7302,
      "step": 84000
    },
    {
      "epoch": 0.4225,
      "grad_norm": 4.580813407897949,
      "learning_rate": 6.078947368421052e-07,
      "loss": 0.732,
      "step": 84500
    },
    {
      "epoch": 0.425,
      "grad_norm": 3.8209619522094727,
      "learning_rate": 6.052631578947368e-07,
      "loss": 0.729,
      "step": 85000
    },
    {
      "epoch": 0.4275,
      "grad_norm": 5.0377726554870605,
      "learning_rate": 6.026315789473684e-07,
      "loss": 0.7309,
      "step": 85500
    },
    {
      "epoch": 0.43,
      "grad_norm": 3.568615198135376,
      "learning_rate": 6e-07,
      "loss": 0.7295,
      "step": 86000
    },
    {
      "epoch": 0.4325,
      "grad_norm": 3.553636312484741,
      "learning_rate": 5.973684210526316e-07,
      "loss": 0.7291,
      "step": 86500
    },
    {
      "epoch": 0.435,
      "grad_norm": 4.122202396392822,
      "learning_rate": 5.947368421052631e-07,
      "loss": 0.7294,
      "step": 87000
    },
    {
      "epoch": 0.4375,
      "grad_norm": 4.019780158996582,
      "learning_rate": 5.921052631578946e-07,
      "loss": 0.7278,
      "step": 87500
    },
    {
      "epoch": 0.44,
      "grad_norm": 4.212389945983887,
      "learning_rate": 5.894736842105262e-07,
      "loss": 0.7269,
      "step": 88000
    },
    {
      "epoch": 0.4425,
      "grad_norm": 3.5766284465789795,
      "learning_rate": 5.868421052631579e-07,
      "loss": 0.7273,
      "step": 88500
    },
    {
      "epoch": 0.445,
      "grad_norm": 4.076996326446533,
      "learning_rate": 5.842105263157895e-07,
      "loss": 0.7279,
      "step": 89000
    },
    {
      "epoch": 0.4475,
      "grad_norm": 3.731936454772949,
      "learning_rate": 5.81578947368421e-07,
      "loss": 0.7257,
      "step": 89500
    },
    {
      "epoch": 0.45,
      "grad_norm": 4.012051582336426,
      "learning_rate": 5.789473684210526e-07,
      "loss": 0.725,
      "step": 90000
    },
    {
      "epoch": 0.4525,
      "grad_norm": 4.128406524658203,
      "learning_rate": 5.763157894736842e-07,
      "loss": 0.7244,
      "step": 90500
    },
    {
      "epoch": 0.455,
      "grad_norm": 4.451712131500244,
      "learning_rate": 5.736842105263158e-07,
      "loss": 0.7258,
      "step": 91000
    },
    {
      "epoch": 0.4575,
      "grad_norm": 4.13367223739624,
      "learning_rate": 5.710526315789474e-07,
      "loss": 0.7254,
      "step": 91500
    },
    {
      "epoch": 0.46,
      "grad_norm": 4.014613151550293,
      "learning_rate": 5.68421052631579e-07,
      "loss": 0.7261,
      "step": 92000
    },
    {
      "epoch": 0.4625,
      "grad_norm": 4.213647365570068,
      "learning_rate": 5.657894736842104e-07,
      "loss": 0.7236,
      "step": 92500
    },
    {
      "epoch": 0.465,
      "grad_norm": 3.8916127681732178,
      "learning_rate": 5.63157894736842e-07,
      "loss": 0.7236,
      "step": 93000
    },
    {
      "epoch": 0.4675,
      "grad_norm": 4.3866801261901855,
      "learning_rate": 5.605263157894736e-07,
      "loss": 0.7245,
      "step": 93500
    },
    {
      "epoch": 0.47,
      "grad_norm": 3.9699559211730957,
      "learning_rate": 5.578947368421052e-07,
      "loss": 0.7216,
      "step": 94000
    },
    {
      "epoch": 0.4725,
      "grad_norm": 3.6203091144561768,
      "learning_rate": 5.552631578947368e-07,
      "loss": 0.7223,
      "step": 94500
    },
    {
      "epoch": 0.475,
      "grad_norm": 4.0245184898376465,
      "learning_rate": 5.526315789473684e-07,
      "loss": 0.7219,
      "step": 95000
    },
    {
      "epoch": 0.4775,
      "grad_norm": 4.105804443359375,
      "learning_rate": 5.5e-07,
      "loss": 0.7216,
      "step": 95500
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.6675305366516113,
      "learning_rate": 5.473684210526316e-07,
      "loss": 0.7207,
      "step": 96000
    },
    {
      "epoch": 0.4825,
      "grad_norm": 3.8412532806396484,
      "learning_rate": 5.447368421052632e-07,
      "loss": 0.7198,
      "step": 96500
    },
    {
      "epoch": 0.485,
      "grad_norm": 3.926048755645752,
      "learning_rate": 5.421052631578948e-07,
      "loss": 0.7215,
      "step": 97000
    },
    {
      "epoch": 0.4875,
      "grad_norm": 4.191093921661377,
      "learning_rate": 5.394736842105264e-07,
      "loss": 0.7201,
      "step": 97500
    },
    {
      "epoch": 0.49,
      "grad_norm": 4.0914387702941895,
      "learning_rate": 5.368421052631578e-07,
      "loss": 0.7194,
      "step": 98000
    },
    {
      "epoch": 0.4925,
      "grad_norm": 3.8451454639434814,
      "learning_rate": 5.342105263157894e-07,
      "loss": 0.7178,
      "step": 98500
    },
    {
      "epoch": 0.495,
      "grad_norm": 3.8510806560516357,
      "learning_rate": 5.31578947368421e-07,
      "loss": 0.7201,
      "step": 99000
    },
    {
      "epoch": 0.4975,
      "grad_norm": 3.889963150024414,
      "learning_rate": 5.289473684210526e-07,
      "loss": 0.7187,
      "step": 99500
    },
    {
      "epoch": 0.5,
      "grad_norm": 4.079945087432861,
      "learning_rate": 5.263157894736842e-07,
      "loss": 0.7189,
      "step": 100000
    },
    {
      "epoch": 0.5025,
      "grad_norm": 3.5443718433380127,
      "learning_rate": 5.236842105263157e-07,
      "loss": 0.7174,
      "step": 100500
    },
    {
      "epoch": 0.505,
      "grad_norm": 3.8786211013793945,
      "learning_rate": 5.210526315789473e-07,
      "loss": 0.7154,
      "step": 101000
    },
    {
      "epoch": 0.5075,
      "grad_norm": 4.628360271453857,
      "learning_rate": 5.184210526315789e-07,
      "loss": 0.7173,
      "step": 101500
    },
    {
      "epoch": 0.51,
      "grad_norm": 4.198030471801758,
      "learning_rate": 5.157894736842106e-07,
      "loss": 0.7169,
      "step": 102000
    },
    {
      "epoch": 0.5125,
      "grad_norm": 4.259772777557373,
      "learning_rate": 5.131578947368422e-07,
      "loss": 0.7178,
      "step": 102500
    },
    {
      "epoch": 0.515,
      "grad_norm": 4.407989025115967,
      "learning_rate": 5.105263157894736e-07,
      "loss": 0.7172,
      "step": 103000
    },
    {
      "epoch": 0.5175,
      "grad_norm": 3.929081439971924,
      "learning_rate": 5.078947368421052e-07,
      "loss": 0.7166,
      "step": 103500
    },
    {
      "epoch": 0.52,
      "grad_norm": 3.4622902870178223,
      "learning_rate": 5.052631578947368e-07,
      "loss": 0.7151,
      "step": 104000
    },
    {
      "epoch": 0.5225,
      "grad_norm": 4.218955039978027,
      "learning_rate": 5.026315789473684e-07,
      "loss": 0.7154,
      "step": 104500
    },
    {
      "epoch": 0.525,
      "grad_norm": 4.144702911376953,
      "learning_rate": 5e-07,
      "loss": 0.7142,
      "step": 105000
    },
    {
      "epoch": 0.5275,
      "grad_norm": 3.5863304138183594,
      "learning_rate": 4.973684210526316e-07,
      "loss": 0.7155,
      "step": 105500
    },
    {
      "epoch": 0.53,
      "grad_norm": 3.7855911254882812,
      "learning_rate": 4.947368421052631e-07,
      "loss": 0.7151,
      "step": 106000
    },
    {
      "epoch": 0.5325,
      "grad_norm": 4.38154411315918,
      "learning_rate": 4.921052631578947e-07,
      "loss": 0.7147,
      "step": 106500
    },
    {
      "epoch": 0.535,
      "grad_norm": 4.45122766494751,
      "learning_rate": 4.894736842105263e-07,
      "loss": 0.7138,
      "step": 107000
    },
    {
      "epoch": 0.5375,
      "grad_norm": 4.112565994262695,
      "learning_rate": 4.868421052631579e-07,
      "loss": 0.714,
      "step": 107500
    },
    {
      "epoch": 0.54,
      "grad_norm": 4.1855082511901855,
      "learning_rate": 4.842105263157895e-07,
      "loss": 0.7133,
      "step": 108000
    },
    {
      "epoch": 0.5425,
      "grad_norm": 3.592898368835449,
      "learning_rate": 4.81578947368421e-07,
      "loss": 0.7134,
      "step": 108500
    },
    {
      "epoch": 0.545,
      "grad_norm": 4.136054039001465,
      "learning_rate": 4.789473684210526e-07,
      "loss": 0.7135,
      "step": 109000
    },
    {
      "epoch": 0.5475,
      "grad_norm": 4.375097274780273,
      "learning_rate": 4.7631578947368416e-07,
      "loss": 0.7116,
      "step": 109500
    },
    {
      "epoch": 0.55,
      "grad_norm": 3.763664722442627,
      "learning_rate": 4.7368421052631574e-07,
      "loss": 0.7126,
      "step": 110000
    },
    {
      "epoch": 0.5525,
      "grad_norm": 3.6669065952301025,
      "learning_rate": 4.710526315789474e-07,
      "loss": 0.7127,
      "step": 110500
    },
    {
      "epoch": 0.555,
      "grad_norm": 4.705322742462158,
      "learning_rate": 4.6842105263157896e-07,
      "loss": 0.7105,
      "step": 111000
    },
    {
      "epoch": 0.5575,
      "grad_norm": 3.7394790649414062,
      "learning_rate": 4.657894736842105e-07,
      "loss": 0.712,
      "step": 111500
    },
    {
      "epoch": 0.56,
      "grad_norm": 4.512149810791016,
      "learning_rate": 4.6315789473684207e-07,
      "loss": 0.7106,
      "step": 112000
    },
    {
      "epoch": 0.5625,
      "grad_norm": 4.066745758056641,
      "learning_rate": 4.6052631578947365e-07,
      "loss": 0.7113,
      "step": 112500
    },
    {
      "epoch": 0.565,
      "grad_norm": 3.925905227661133,
      "learning_rate": 4.5789473684210523e-07,
      "loss": 0.7114,
      "step": 113000
    },
    {
      "epoch": 0.5675,
      "grad_norm": 4.321648120880127,
      "learning_rate": 4.5526315789473687e-07,
      "loss": 0.7094,
      "step": 113500
    },
    {
      "epoch": 0.57,
      "grad_norm": 3.8003995418548584,
      "learning_rate": 4.526315789473684e-07,
      "loss": 0.7101,
      "step": 114000
    },
    {
      "epoch": 0.5725,
      "grad_norm": 3.7758116722106934,
      "learning_rate": 4.5e-07,
      "loss": 0.7097,
      "step": 114500
    },
    {
      "epoch": 0.575,
      "grad_norm": 4.226670265197754,
      "learning_rate": 4.4736842105263156e-07,
      "loss": 0.7107,
      "step": 115000
    },
    {
      "epoch": 0.5775,
      "grad_norm": 3.6979732513427734,
      "learning_rate": 4.4473684210526314e-07,
      "loss": 0.7099,
      "step": 115500
    },
    {
      "epoch": 0.58,
      "grad_norm": 3.7048542499542236,
      "learning_rate": 4.421052631578947e-07,
      "loss": 0.7085,
      "step": 116000
    },
    {
      "epoch": 0.5825,
      "grad_norm": 4.918829441070557,
      "learning_rate": 4.3947368421052625e-07,
      "loss": 0.7099,
      "step": 116500
    },
    {
      "epoch": 0.585,
      "grad_norm": 3.8599579334259033,
      "learning_rate": 4.368421052631579e-07,
      "loss": 0.7078,
      "step": 117000
    },
    {
      "epoch": 0.5875,
      "grad_norm": 3.9646506309509277,
      "learning_rate": 4.3421052631578947e-07,
      "loss": 0.7086,
      "step": 117500
    },
    {
      "epoch": 0.59,
      "grad_norm": 4.001802444458008,
      "learning_rate": 4.3157894736842105e-07,
      "loss": 0.7085,
      "step": 118000
    },
    {
      "epoch": 0.5925,
      "grad_norm": 4.447700500488281,
      "learning_rate": 4.2894736842105263e-07,
      "loss": 0.708,
      "step": 118500
    },
    {
      "epoch": 0.595,
      "grad_norm": 3.7522354125976562,
      "learning_rate": 4.2631578947368416e-07,
      "loss": 0.707,
      "step": 119000
    },
    {
      "epoch": 0.5975,
      "grad_norm": 4.16211462020874,
      "learning_rate": 4.2368421052631575e-07,
      "loss": 0.7063,
      "step": 119500
    },
    {
      "epoch": 0.6,
      "grad_norm": 3.931779384613037,
      "learning_rate": 4.2105263157894733e-07,
      "loss": 0.7064,
      "step": 120000
    },
    {
      "epoch": 0.6025,
      "grad_norm": 4.341772079467773,
      "learning_rate": 4.1842105263157896e-07,
      "loss": 0.7077,
      "step": 120500
    },
    {
      "epoch": 0.605,
      "grad_norm": 3.8963160514831543,
      "learning_rate": 4.1578947368421054e-07,
      "loss": 0.7059,
      "step": 121000
    },
    {
      "epoch": 0.6075,
      "grad_norm": 4.025752544403076,
      "learning_rate": 4.1315789473684207e-07,
      "loss": 0.7071,
      "step": 121500
    },
    {
      "epoch": 0.61,
      "grad_norm": 3.817295551300049,
      "learning_rate": 4.1052631578947365e-07,
      "loss": 0.707,
      "step": 122000
    },
    {
      "epoch": 0.6125,
      "grad_norm": 4.444002151489258,
      "learning_rate": 4.0789473684210524e-07,
      "loss": 0.7054,
      "step": 122500
    },
    {
      "epoch": 0.615,
      "grad_norm": 3.673328399658203,
      "learning_rate": 4.052631578947368e-07,
      "loss": 0.706,
      "step": 123000
    },
    {
      "epoch": 0.6175,
      "grad_norm": 4.673981189727783,
      "learning_rate": 4.026315789473684e-07,
      "loss": 0.7057,
      "step": 123500
    },
    {
      "epoch": 0.62,
      "grad_norm": 3.7949602603912354,
      "learning_rate": 4e-07,
      "loss": 0.7048,
      "step": 124000
    },
    {
      "epoch": 0.6225,
      "grad_norm": 4.127279281616211,
      "learning_rate": 3.9736842105263156e-07,
      "loss": 0.7056,
      "step": 124500
    },
    {
      "epoch": 0.625,
      "grad_norm": 3.610515832901001,
      "learning_rate": 3.9473684210526315e-07,
      "loss": 0.7058,
      "step": 125000
    },
    {
      "epoch": 0.6275,
      "grad_norm": 3.9963324069976807,
      "learning_rate": 3.9210526315789473e-07,
      "loss": 0.7052,
      "step": 125500
    },
    {
      "epoch": 0.63,
      "grad_norm": 5.3302083015441895,
      "learning_rate": 3.894736842105263e-07,
      "loss": 0.7053,
      "step": 126000
    },
    {
      "epoch": 0.6325,
      "grad_norm": 3.6135482788085938,
      "learning_rate": 3.8684210526315784e-07,
      "loss": 0.7037,
      "step": 126500
    },
    {
      "epoch": 0.635,
      "grad_norm": 4.532868385314941,
      "learning_rate": 3.842105263157894e-07,
      "loss": 0.7044,
      "step": 127000
    },
    {
      "epoch": 0.6375,
      "grad_norm": 3.832761287689209,
      "learning_rate": 3.8157894736842105e-07,
      "loss": 0.7059,
      "step": 127500
    },
    {
      "epoch": 0.64,
      "grad_norm": 4.095343589782715,
      "learning_rate": 3.7894736842105264e-07,
      "loss": 0.7024,
      "step": 128000
    },
    {
      "epoch": 0.6425,
      "grad_norm": 3.810284376144409,
      "learning_rate": 3.763157894736842e-07,
      "loss": 0.7032,
      "step": 128500
    },
    {
      "epoch": 0.645,
      "grad_norm": 3.955117702484131,
      "learning_rate": 3.7368421052631575e-07,
      "loss": 0.7036,
      "step": 129000
    },
    {
      "epoch": 0.6475,
      "grad_norm": 3.767981767654419,
      "learning_rate": 3.7105263157894733e-07,
      "loss": 0.7031,
      "step": 129500
    },
    {
      "epoch": 0.65,
      "grad_norm": 3.6758077144622803,
      "learning_rate": 3.684210526315789e-07,
      "loss": 0.7037,
      "step": 130000
    },
    {
      "epoch": 0.6525,
      "grad_norm": 4.230991840362549,
      "learning_rate": 3.6578947368421055e-07,
      "loss": 0.7023,
      "step": 130500
    },
    {
      "epoch": 0.655,
      "grad_norm": 4.006688117980957,
      "learning_rate": 3.6315789473684213e-07,
      "loss": 0.7018,
      "step": 131000
    },
    {
      "epoch": 0.6575,
      "grad_norm": 3.752882242202759,
      "learning_rate": 3.6052631578947366e-07,
      "loss": 0.7025,
      "step": 131500
    },
    {
      "epoch": 0.66,
      "grad_norm": 3.8209547996520996,
      "learning_rate": 3.5789473684210524e-07,
      "loss": 0.7024,
      "step": 132000
    },
    {
      "epoch": 0.6625,
      "grad_norm": 4.010956287384033,
      "learning_rate": 3.552631578947368e-07,
      "loss": 0.7007,
      "step": 132500
    },
    {
      "epoch": 0.665,
      "grad_norm": 4.006511688232422,
      "learning_rate": 3.526315789473684e-07,
      "loss": 0.7023,
      "step": 133000
    },
    {
      "epoch": 0.6675,
      "grad_norm": 3.783923387527466,
      "learning_rate": 3.5e-07,
      "loss": 0.7,
      "step": 133500
    },
    {
      "epoch": 0.67,
      "grad_norm": 3.9791009426116943,
      "learning_rate": 3.4736842105263157e-07,
      "loss": 0.7005,
      "step": 134000
    },
    {
      "epoch": 0.6725,
      "grad_norm": 4.302498817443848,
      "learning_rate": 3.4473684210526315e-07,
      "loss": 0.7004,
      "step": 134500
    },
    {
      "epoch": 0.675,
      "grad_norm": 3.875100612640381,
      "learning_rate": 3.4210526315789473e-07,
      "loss": 0.7006,
      "step": 135000
    },
    {
      "epoch": 0.6775,
      "grad_norm": 3.78542423248291,
      "learning_rate": 3.394736842105263e-07,
      "loss": 0.7015,
      "step": 135500
    },
    {
      "epoch": 0.68,
      "grad_norm": 4.130906581878662,
      "learning_rate": 3.368421052631579e-07,
      "loss": 0.6992,
      "step": 136000
    },
    {
      "epoch": 0.6825,
      "grad_norm": 3.9031929969787598,
      "learning_rate": 3.342105263157894e-07,
      "loss": 0.7006,
      "step": 136500
    },
    {
      "epoch": 0.685,
      "grad_norm": 4.0105414390563965,
      "learning_rate": 3.31578947368421e-07,
      "loss": 0.699,
      "step": 137000
    },
    {
      "epoch": 0.6875,
      "grad_norm": 4.137153148651123,
      "learning_rate": 3.2894736842105264e-07,
      "loss": 0.7002,
      "step": 137500
    },
    {
      "epoch": 0.69,
      "grad_norm": 4.031179904937744,
      "learning_rate": 3.263157894736842e-07,
      "loss": 0.699,
      "step": 138000
    },
    {
      "epoch": 0.6925,
      "grad_norm": 4.005141258239746,
      "learning_rate": 3.236842105263158e-07,
      "loss": 0.6985,
      "step": 138500
    },
    {
      "epoch": 0.695,
      "grad_norm": 3.8894729614257812,
      "learning_rate": 3.2105263157894733e-07,
      "loss": 0.7003,
      "step": 139000
    },
    {
      "epoch": 0.6975,
      "grad_norm": 4.2765960693359375,
      "learning_rate": 3.184210526315789e-07,
      "loss": 0.7002,
      "step": 139500
    },
    {
      "epoch": 0.7,
      "grad_norm": 4.02558708190918,
      "learning_rate": 3.157894736842105e-07,
      "loss": 0.699,
      "step": 140000
    },
    {
      "epoch": 0.7025,
      "grad_norm": 4.400519847869873,
      "learning_rate": 3.131578947368421e-07,
      "loss": 0.6995,
      "step": 140500
    },
    {
      "epoch": 0.705,
      "grad_norm": 4.4780473709106445,
      "learning_rate": 3.105263157894737e-07,
      "loss": 0.6986,
      "step": 141000
    },
    {
      "epoch": 0.7075,
      "grad_norm": 4.057679653167725,
      "learning_rate": 3.0789473684210524e-07,
      "loss": 0.698,
      "step": 141500
    },
    {
      "epoch": 0.71,
      "grad_norm": 3.9127089977264404,
      "learning_rate": 3.052631578947368e-07,
      "loss": 0.6992,
      "step": 142000
    },
    {
      "epoch": 0.7125,
      "grad_norm": 3.768744707107544,
      "learning_rate": 3.026315789473684e-07,
      "loss": 0.6973,
      "step": 142500
    },
    {
      "epoch": 0.715,
      "grad_norm": 3.57293963432312,
      "learning_rate": 3e-07,
      "loss": 0.6998,
      "step": 143000
    },
    {
      "epoch": 0.7175,
      "grad_norm": 3.782562017440796,
      "learning_rate": 2.9736842105263157e-07,
      "loss": 0.6986,
      "step": 143500
    },
    {
      "epoch": 0.72,
      "grad_norm": 3.820235013961792,
      "learning_rate": 2.947368421052631e-07,
      "loss": 0.6976,
      "step": 144000
    },
    {
      "epoch": 0.7225,
      "grad_norm": 4.131710052490234,
      "learning_rate": 2.9210526315789473e-07,
      "loss": 0.6975,
      "step": 144500
    },
    {
      "epoch": 0.725,
      "grad_norm": 4.230330944061279,
      "learning_rate": 2.894736842105263e-07,
      "loss": 0.6965,
      "step": 145000
    },
    {
      "epoch": 0.7275,
      "grad_norm": 3.91056489944458,
      "learning_rate": 2.868421052631579e-07,
      "loss": 0.6966,
      "step": 145500
    },
    {
      "epoch": 0.73,
      "grad_norm": 4.280819892883301,
      "learning_rate": 2.842105263157895e-07,
      "loss": 0.6988,
      "step": 146000
    },
    {
      "epoch": 0.7325,
      "grad_norm": 5.5676445960998535,
      "learning_rate": 2.81578947368421e-07,
      "loss": 0.6966,
      "step": 146500
    },
    {
      "epoch": 0.735,
      "grad_norm": 4.255350112915039,
      "learning_rate": 2.789473684210526e-07,
      "loss": 0.6964,
      "step": 147000
    },
    {
      "epoch": 0.7375,
      "grad_norm": 3.6316545009613037,
      "learning_rate": 2.763157894736842e-07,
      "loss": 0.6978,
      "step": 147500
    },
    {
      "epoch": 0.74,
      "grad_norm": 4.02787971496582,
      "learning_rate": 2.736842105263158e-07,
      "loss": 0.6958,
      "step": 148000
    },
    {
      "epoch": 0.7425,
      "grad_norm": 3.9794931411743164,
      "learning_rate": 2.710526315789474e-07,
      "loss": 0.6973,
      "step": 148500
    },
    {
      "epoch": 0.745,
      "grad_norm": 4.5525946617126465,
      "learning_rate": 2.684210526315789e-07,
      "loss": 0.6957,
      "step": 149000
    },
    {
      "epoch": 0.7475,
      "grad_norm": 4.188408851623535,
      "learning_rate": 2.657894736842105e-07,
      "loss": 0.6971,
      "step": 149500
    },
    {
      "epoch": 0.75,
      "grad_norm": 4.42637825012207,
      "learning_rate": 2.631578947368421e-07,
      "loss": 0.6959,
      "step": 150000
    },
    {
      "epoch": 0.7525,
      "grad_norm": 4.204093933105469,
      "learning_rate": 2.6052631578947366e-07,
      "loss": 0.6949,
      "step": 150500
    },
    {
      "epoch": 0.755,
      "grad_norm": 4.0667595863342285,
      "learning_rate": 2.578947368421053e-07,
      "loss": 0.6962,
      "step": 151000
    },
    {
      "epoch": 0.7575,
      "grad_norm": 3.581162214279175,
      "learning_rate": 2.552631578947368e-07,
      "loss": 0.695,
      "step": 151500
    },
    {
      "epoch": 0.76,
      "grad_norm": 4.084874153137207,
      "learning_rate": 2.526315789473684e-07,
      "loss": 0.6974,
      "step": 152000
    },
    {
      "epoch": 0.7625,
      "grad_norm": 3.8248918056488037,
      "learning_rate": 2.5e-07,
      "loss": 0.6961,
      "step": 152500
    },
    {
      "epoch": 0.765,
      "grad_norm": 4.24776029586792,
      "learning_rate": 2.4736842105263157e-07,
      "loss": 0.6942,
      "step": 153000
    },
    {
      "epoch": 0.7675,
      "grad_norm": 3.7034835815429688,
      "learning_rate": 2.4473684210526315e-07,
      "loss": 0.695,
      "step": 153500
    },
    {
      "epoch": 0.77,
      "grad_norm": 3.7641491889953613,
      "learning_rate": 2.4210526315789473e-07,
      "loss": 0.6947,
      "step": 154000
    },
    {
      "epoch": 0.7725,
      "grad_norm": 3.9966015815734863,
      "learning_rate": 2.394736842105263e-07,
      "loss": 0.6955,
      "step": 154500
    },
    {
      "epoch": 0.775,
      "grad_norm": 4.063261032104492,
      "learning_rate": 2.3684210526315787e-07,
      "loss": 0.6929,
      "step": 155000
    },
    {
      "epoch": 0.7775,
      "grad_norm": 4.2010931968688965,
      "learning_rate": 2.3421052631578948e-07,
      "loss": 0.695,
      "step": 155500
    },
    {
      "epoch": 0.78,
      "grad_norm": 4.593263149261475,
      "learning_rate": 2.3157894736842104e-07,
      "loss": 0.6947,
      "step": 156000
    },
    {
      "epoch": 0.7825,
      "grad_norm": 4.019844055175781,
      "learning_rate": 2.2894736842105262e-07,
      "loss": 0.6944,
      "step": 156500
    },
    {
      "epoch": 0.785,
      "grad_norm": 4.396482467651367,
      "learning_rate": 2.263157894736842e-07,
      "loss": 0.6956,
      "step": 157000
    },
    {
      "epoch": 0.7875,
      "grad_norm": 4.671215057373047,
      "learning_rate": 2.2368421052631578e-07,
      "loss": 0.6941,
      "step": 157500
    },
    {
      "epoch": 0.79,
      "grad_norm": 4.260108470916748,
      "learning_rate": 2.2105263157894736e-07,
      "loss": 0.6947,
      "step": 158000
    },
    {
      "epoch": 0.7925,
      "grad_norm": 4.044659614562988,
      "learning_rate": 2.1842105263157894e-07,
      "loss": 0.6931,
      "step": 158500
    },
    {
      "epoch": 0.795,
      "grad_norm": 3.9769768714904785,
      "learning_rate": 2.1578947368421053e-07,
      "loss": 0.6951,
      "step": 159000
    },
    {
      "epoch": 0.7975,
      "grad_norm": 3.9319329261779785,
      "learning_rate": 2.1315789473684208e-07,
      "loss": 0.6951,
      "step": 159500
    },
    {
      "epoch": 0.8,
      "grad_norm": 3.8207597732543945,
      "learning_rate": 2.1052631578947366e-07,
      "loss": 0.6936,
      "step": 160000
    },
    {
      "epoch": 0.8025,
      "grad_norm": 4.243794918060303,
      "learning_rate": 2.0789473684210527e-07,
      "loss": 0.6923,
      "step": 160500
    },
    {
      "epoch": 0.805,
      "grad_norm": 3.9992623329162598,
      "learning_rate": 2.0526315789473683e-07,
      "loss": 0.6935,
      "step": 161000
    },
    {
      "epoch": 0.8075,
      "grad_norm": 3.8356659412384033,
      "learning_rate": 2.026315789473684e-07,
      "loss": 0.693,
      "step": 161500
    },
    {
      "epoch": 0.81,
      "grad_norm": 3.5464625358581543,
      "learning_rate": 2e-07,
      "loss": 0.6936,
      "step": 162000
    },
    {
      "epoch": 0.8125,
      "grad_norm": 3.8339409828186035,
      "learning_rate": 1.9736842105263157e-07,
      "loss": 0.6933,
      "step": 162500
    },
    {
      "epoch": 0.815,
      "grad_norm": 4.1824469566345215,
      "learning_rate": 1.9473684210526315e-07,
      "loss": 0.6931,
      "step": 163000
    },
    {
      "epoch": 0.8175,
      "grad_norm": 4.3102617263793945,
      "learning_rate": 1.921052631578947e-07,
      "loss": 0.6942,
      "step": 163500
    },
    {
      "epoch": 0.82,
      "grad_norm": 3.8112471103668213,
      "learning_rate": 1.8947368421052632e-07,
      "loss": 0.693,
      "step": 164000
    },
    {
      "epoch": 0.8225,
      "grad_norm": 3.730663776397705,
      "learning_rate": 1.8684210526315787e-07,
      "loss": 0.6938,
      "step": 164500
    },
    {
      "epoch": 0.825,
      "grad_norm": 4.266357421875,
      "learning_rate": 1.8421052631578946e-07,
      "loss": 0.6915,
      "step": 165000
    },
    {
      "epoch": 0.8275,
      "grad_norm": 3.828408718109131,
      "learning_rate": 1.8157894736842106e-07,
      "loss": 0.6936,
      "step": 165500
    },
    {
      "epoch": 0.83,
      "grad_norm": 4.23342227935791,
      "learning_rate": 1.7894736842105262e-07,
      "loss": 0.6927,
      "step": 166000
    },
    {
      "epoch": 0.8325,
      "grad_norm": 4.157886028289795,
      "learning_rate": 1.763157894736842e-07,
      "loss": 0.6924,
      "step": 166500
    },
    {
      "epoch": 0.835,
      "grad_norm": 3.839111804962158,
      "learning_rate": 1.7368421052631578e-07,
      "loss": 0.6924,
      "step": 167000
    },
    {
      "epoch": 0.8375,
      "grad_norm": 4.51657772064209,
      "learning_rate": 1.7105263157894736e-07,
      "loss": 0.6936,
      "step": 167500
    },
    {
      "epoch": 0.84,
      "grad_norm": 3.6640701293945312,
      "learning_rate": 1.6842105263157895e-07,
      "loss": 0.69,
      "step": 168000
    },
    {
      "epoch": 0.8425,
      "grad_norm": 4.697273254394531,
      "learning_rate": 1.657894736842105e-07,
      "loss": 0.6956,
      "step": 168500
    },
    {
      "epoch": 0.845,
      "grad_norm": 4.215676307678223,
      "learning_rate": 1.631578947368421e-07,
      "loss": 0.693,
      "step": 169000
    },
    {
      "epoch": 0.8475,
      "grad_norm": 3.636216640472412,
      "learning_rate": 1.6052631578947367e-07,
      "loss": 0.6939,
      "step": 169500
    },
    {
      "epoch": 0.85,
      "grad_norm": 3.95481014251709,
      "learning_rate": 1.5789473684210525e-07,
      "loss": 0.6909,
      "step": 170000
    },
    {
      "epoch": 0.8525,
      "grad_norm": 3.8270227909088135,
      "learning_rate": 1.5526315789473686e-07,
      "loss": 0.6926,
      "step": 170500
    },
    {
      "epoch": 0.855,
      "grad_norm": 4.556095600128174,
      "learning_rate": 1.526315789473684e-07,
      "loss": 0.6909,
      "step": 171000
    },
    {
      "epoch": 0.8575,
      "grad_norm": 3.827822208404541,
      "learning_rate": 1.5e-07,
      "loss": 0.6913,
      "step": 171500
    },
    {
      "epoch": 0.86,
      "grad_norm": 4.105409145355225,
      "learning_rate": 1.4736842105263155e-07,
      "loss": 0.6908,
      "step": 172000
    },
    {
      "epoch": 0.8625,
      "grad_norm": 4.046328544616699,
      "learning_rate": 1.4473684210526316e-07,
      "loss": 0.6911,
      "step": 172500
    },
    {
      "epoch": 0.865,
      "grad_norm": 3.520242214202881,
      "learning_rate": 1.4210526315789474e-07,
      "loss": 0.6918,
      "step": 173000
    },
    {
      "epoch": 0.8675,
      "grad_norm": 4.334352016448975,
      "learning_rate": 1.394736842105263e-07,
      "loss": 0.6916,
      "step": 173500
    },
    {
      "epoch": 0.87,
      "grad_norm": 4.1197333335876465,
      "learning_rate": 1.368421052631579e-07,
      "loss": 0.6921,
      "step": 174000
    },
    {
      "epoch": 0.8725,
      "grad_norm": 4.072686672210693,
      "learning_rate": 1.3421052631578946e-07,
      "loss": 0.693,
      "step": 174500
    },
    {
      "epoch": 0.875,
      "grad_norm": 3.7020890712738037,
      "learning_rate": 1.3157894736842104e-07,
      "loss": 0.6915,
      "step": 175000
    },
    {
      "epoch": 0.8775,
      "grad_norm": 3.8825650215148926,
      "learning_rate": 1.2894736842105265e-07,
      "loss": 0.6907,
      "step": 175500
    },
    {
      "epoch": 0.88,
      "grad_norm": 4.1125593185424805,
      "learning_rate": 1.263157894736842e-07,
      "loss": 0.6914,
      "step": 176000
    },
    {
      "epoch": 0.8825,
      "grad_norm": 4.147935390472412,
      "learning_rate": 1.2368421052631579e-07,
      "loss": 0.6915,
      "step": 176500
    },
    {
      "epoch": 0.885,
      "grad_norm": 3.6618380546569824,
      "learning_rate": 1.2105263157894737e-07,
      "loss": 0.691,
      "step": 177000
    },
    {
      "epoch": 0.8875,
      "grad_norm": 3.7744410037994385,
      "learning_rate": 1.1842105263157894e-07,
      "loss": 0.6922,
      "step": 177500
    },
    {
      "epoch": 0.89,
      "grad_norm": 4.539909362792969,
      "learning_rate": 1.1578947368421052e-07,
      "loss": 0.6909,
      "step": 178000
    },
    {
      "epoch": 0.8925,
      "grad_norm": 4.106850624084473,
      "learning_rate": 1.131578947368421e-07,
      "loss": 0.6902,
      "step": 178500
    },
    {
      "epoch": 0.895,
      "grad_norm": 3.9429259300231934,
      "learning_rate": 1.1052631578947368e-07,
      "loss": 0.6909,
      "step": 179000
    },
    {
      "epoch": 0.8975,
      "grad_norm": 4.5638298988342285,
      "learning_rate": 1.0789473684210526e-07,
      "loss": 0.6892,
      "step": 179500
    },
    {
      "epoch": 0.9,
      "grad_norm": 4.079879283905029,
      "learning_rate": 1.0526315789473683e-07,
      "loss": 0.6918,
      "step": 180000
    },
    {
      "epoch": 0.9025,
      "grad_norm": 4.097373008728027,
      "learning_rate": 1.0263157894736841e-07,
      "loss": 0.6904,
      "step": 180500
    },
    {
      "epoch": 0.905,
      "grad_norm": 3.7984631061553955,
      "learning_rate": 1e-07,
      "loss": 0.6911,
      "step": 181000
    },
    {
      "epoch": 0.9075,
      "grad_norm": 4.157617092132568,
      "learning_rate": 9.736842105263158e-08,
      "loss": 0.6905,
      "step": 181500
    },
    {
      "epoch": 0.91,
      "grad_norm": 3.6399903297424316,
      "learning_rate": 9.473684210526316e-08,
      "loss": 0.6892,
      "step": 182000
    },
    {
      "epoch": 0.9125,
      "grad_norm": 3.946420431137085,
      "learning_rate": 9.210526315789473e-08,
      "loss": 0.6904,
      "step": 182500
    },
    {
      "epoch": 0.915,
      "grad_norm": 3.871739149093628,
      "learning_rate": 8.947368421052631e-08,
      "loss": 0.6915,
      "step": 183000
    },
    {
      "epoch": 0.9175,
      "grad_norm": 3.8283700942993164,
      "learning_rate": 8.684210526315789e-08,
      "loss": 0.6915,
      "step": 183500
    },
    {
      "epoch": 0.92,
      "grad_norm": 3.9784653186798096,
      "learning_rate": 8.421052631578947e-08,
      "loss": 0.6907,
      "step": 184000
    },
    {
      "epoch": 0.9225,
      "grad_norm": 4.05589485168457,
      "learning_rate": 8.157894736842106e-08,
      "loss": 0.6907,
      "step": 184500
    },
    {
      "epoch": 0.925,
      "grad_norm": 4.114725589752197,
      "learning_rate": 7.894736842105262e-08,
      "loss": 0.6888,
      "step": 185000
    },
    {
      "epoch": 0.9275,
      "grad_norm": 4.018821716308594,
      "learning_rate": 7.63157894736842e-08,
      "loss": 0.6901,
      "step": 185500
    },
    {
      "epoch": 0.93,
      "grad_norm": 3.8741469383239746,
      "learning_rate": 7.368421052631577e-08,
      "loss": 0.6883,
      "step": 186000
    },
    {
      "epoch": 0.9325,
      "grad_norm": 3.526463508605957,
      "learning_rate": 7.105263157894737e-08,
      "loss": 0.6904,
      "step": 186500
    },
    {
      "epoch": 0.935,
      "grad_norm": 4.016304969787598,
      "learning_rate": 6.842105263157895e-08,
      "loss": 0.6909,
      "step": 187000
    },
    {
      "epoch": 0.9375,
      "grad_norm": 3.9731669425964355,
      "learning_rate": 6.578947368421052e-08,
      "loss": 0.6893,
      "step": 187500
    },
    {
      "epoch": 0.94,
      "grad_norm": 4.204085350036621,
      "learning_rate": 6.31578947368421e-08,
      "loss": 0.6894,
      "step": 188000
    },
    {
      "epoch": 0.9425,
      "grad_norm": 3.9048209190368652,
      "learning_rate": 6.052631578947368e-08,
      "loss": 0.6899,
      "step": 188500
    },
    {
      "epoch": 0.945,
      "grad_norm": 3.9418015480041504,
      "learning_rate": 5.789473684210526e-08,
      "loss": 0.6904,
      "step": 189000
    },
    {
      "epoch": 0.9475,
      "grad_norm": 3.8141353130340576,
      "learning_rate": 5.526315789473684e-08,
      "loss": 0.6903,
      "step": 189500
    },
    {
      "epoch": 0.95,
      "grad_norm": 3.9734013080596924,
      "learning_rate": 5.2631578947368416e-08,
      "loss": 0.6889,
      "step": 190000
    },
    {
      "epoch": 0.9525,
      "grad_norm": 3.6681694984436035,
      "learning_rate": 5e-08,
      "loss": 0.6898,
      "step": 190500
    },
    {
      "epoch": 0.955,
      "grad_norm": 4.077125549316406,
      "learning_rate": 4.736842105263158e-08,
      "loss": 0.6905,
      "step": 191000
    },
    {
      "epoch": 0.9575,
      "grad_norm": 3.712308645248413,
      "learning_rate": 4.4736842105263155e-08,
      "loss": 0.6897,
      "step": 191500
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.8258426189422607,
      "learning_rate": 4.2105263157894737e-08,
      "loss": 0.69,
      "step": 192000
    },
    {
      "epoch": 0.9625,
      "grad_norm": 4.0939412117004395,
      "learning_rate": 3.947368421052631e-08,
      "loss": 0.6891,
      "step": 192500
    },
    {
      "epoch": 0.965,
      "grad_norm": 3.5586180686950684,
      "learning_rate": 3.684210526315789e-08,
      "loss": 0.6881,
      "step": 193000
    },
    {
      "epoch": 0.9675,
      "grad_norm": 3.897303342819214,
      "learning_rate": 3.4210526315789476e-08,
      "loss": 0.692,
      "step": 193500
    },
    {
      "epoch": 0.97,
      "grad_norm": 3.940556764602661,
      "learning_rate": 3.157894736842105e-08,
      "loss": 0.6883,
      "step": 194000
    },
    {
      "epoch": 0.9725,
      "grad_norm": 4.23101282119751,
      "learning_rate": 2.894736842105263e-08,
      "loss": 0.6891,
      "step": 194500
    },
    {
      "epoch": 0.975,
      "grad_norm": 4.488062858581543,
      "learning_rate": 2.6315789473684208e-08,
      "loss": 0.6897,
      "step": 195000
    },
    {
      "epoch": 0.9775,
      "grad_norm": 4.175571918487549,
      "learning_rate": 2.368421052631579e-08,
      "loss": 0.6889,
      "step": 195500
    },
    {
      "epoch": 0.98,
      "grad_norm": 4.008491039276123,
      "learning_rate": 2.1052631578947368e-08,
      "loss": 0.6895,
      "step": 196000
    },
    {
      "epoch": 0.9825,
      "grad_norm": 4.122620105743408,
      "learning_rate": 1.8421052631578944e-08,
      "loss": 0.6893,
      "step": 196500
    },
    {
      "epoch": 0.985,
      "grad_norm": 3.9395508766174316,
      "learning_rate": 1.5789473684210525e-08,
      "loss": 0.6896,
      "step": 197000
    },
    {
      "epoch": 0.9875,
      "grad_norm": 4.183102130889893,
      "learning_rate": 1.3157894736842104e-08,
      "loss": 0.6895,
      "step": 197500
    },
    {
      "epoch": 0.99,
      "grad_norm": 4.421983242034912,
      "learning_rate": 1.0526315789473684e-08,
      "loss": 0.6892,
      "step": 198000
    },
    {
      "epoch": 0.9925,
      "grad_norm": 4.0066118240356445,
      "learning_rate": 7.894736842105263e-09,
      "loss": 0.6882,
      "step": 198500
    },
    {
      "epoch": 0.995,
      "grad_norm": 3.319255828857422,
      "learning_rate": 5.263157894736842e-09,
      "loss": 0.6883,
      "step": 199000
    },
    {
      "epoch": 0.9975,
      "grad_norm": 4.0392537117004395,
      "learning_rate": 2.631578947368421e-09,
      "loss": 0.6889,
      "step": 199500
    },
    {
      "epoch": 1.0,
      "grad_norm": 4.034811496734619,
      "learning_rate": 0.0,
      "loss": 0.6879,
      "step": 200000
    }
  ],
  "logging_steps": 500,
  "max_steps": 200000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.4166478707552877e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
